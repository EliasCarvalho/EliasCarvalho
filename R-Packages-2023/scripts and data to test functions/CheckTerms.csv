"textToCheck","ObjectiveOfBN"
"Background The grades of recommendation, assessment, development and evaluation (GRADE) approach is widely implemented in systematic reviews, health technology assessment and guideline development organisations throughout the world. We have previously reported on the development of the Semi-Automated Quality Assessment Tool (SAQAT), which enables a semi-automated validity assessment based on GRADE criteria. The main advantage to our approach is the potential to improve inter-rater agreement of GRADE assessments particularly when used by less experienced researchers, because such judgements can be complex and challenging to apply without training. This is the first study examining the inter-rater agreement of the SAQAT. Methods We conducted two studies to compare: a) the inter-rater agreement of two researchers using the SAQAT independently on 28 meta-analyses and b) the inter-rater agreement between a researcher using the SAQAT (who had no experience of using GRADE) and an experienced member of the GRADE working group conducting a standard GRADE assessment on 15 meta-analyses. Results There was substantial agreement between independent researchers using the Quality Assessment Tool for all domains (for example, overall GRADE rating: weighted kappa 0.79; 95% CI 0.65 to 0.93). Comparison between the SAQAT and a standard GRADE assessment suggested that inconsistency was parameterised too conservatively by the SAQAT. Therefore the tool was amended. Following amendment we found fair-to-moderate agreement between the standard GRADE assessment and the SAQAT (for example, overall GRADE rating: weighted kappa 0.35; 95% CI 0.09 to 0.87). Conclusions Despite a need for further research, the SAQAT may aid consistent application of GRADE, particularly by less experienced researchers.",""
"BACKGROUND: Personalized cancer treatments depend on the determination of a patient's genetic status according to known genetic profiles for which targeted treatments exist. Such genetic profiles must be scientifically validated before they is applied to general patient population. Reproducibility of findings that support such genetic profiles is a fundamental challenge in validation studies. The percentage of overlapping genes (POG) criterion and derivative methods produce unstable and misleading results. Furthermore, in a complex disease, comparisons between different tumor subtypes can produce high POG scores that do not capture the consistencies in the functions. RESULTS: We focused on the quality rather than the quantity of the overlapping genes. We defined the rank value of each gene according to importance or quality by PageRank on basis of a particular topological structure. Then, we used the p-value of the rank-sum of the overlapping genes (PRSOG) to evaluate the quality of reproducibility. Though the POG scores were low in different studies of the same disease, the PRSOG was statistically significant, which suggests that sets of differentially expressed genes might be highly reproducible. CONCLUSIONS: Evaluations of eight datasets from breast cancer, lung cancer and four other disorders indicate that quality-based PRSOG method performs better than a quantity-based method. Our analysis of the components of the sets of overlapping genes supports the utility of the PRSOG method.",""
"Dynamic Bayesian Networks (DBN) have been widely used to recover gene regulatory relationships from time-series data in computational systems biology. Its standard assumption is 'stationarity', and therefore, several research efforts have been recently proposed to relax this restriction. However, those methods suffer from three challenges: long running time, low accuracy and reliance on parameter settings. To address these problems, we propose a novel non-stationary DBN model by extending each hidden node of Hidden Markov Model into a DBN (called HMDBN), which properly handles the underlying time-evolving networks. Correspondingly, an improved structural EM algorithm is proposed to learn the HMDBN. It dramatically reduces searching space, thereby substantially improving computational efficiency. Additionally, we derived a novel generalized Bayesian Information Criterion under the non-stationary assumption (called BWBIC), which can help significantly improve the reconstruction accuracy and largely reduce over-fitting. Moreover, the re-estimation formulas for all parameters of our model are derived, enabling us to avoid reliance on parameter settings. Compared to the state-of-the-art methods, the experimental evaluation of our proposed method on both synthetic and real biological data demonstrates more stably high prediction accuracy and significantly improved computation efficiency, even with no prior knowledge and parameter settings.",""
"Current experimental research in several scientific areas must deal with the issue of high dimensionality and complexity. In particular, experimental design strategies are hindered by the limited number of points that can be tested due to technical and economic constraints. In this paper we propose a novel approach called QueBN-design (Querying Bayesian network design) derived by coupling conditional probabilistic inference in Bayesian network models and evolutionary principles. As proof-of-principle, we evaluate the performance of our approach in a simulation study achieving very good results also in comparison with other commonly used designs. Further, we address the problem of engineering synthetic proteins, and in particular the 1AGY serine esterase protein. Also in this case results indicate that QueBN-design can effectively guide the search in very large experimental spaces testing a very limited number of points, outperforming other evolutionary and classical benchmark designs. (C) 2015 Elsevier B.V. All rights reserved.","In this paper we propose a novel approach called QueBN-design (Querying Bayesian network design) derived by coupling conditional probabilistic inference in Bayesian network models and evolutionary principles."
"A framework for the reliability evaluation of grid-connected PV (photovoltaic) systems with intermittent faults is proposed using DBNs (dynamic Bayesian networks). A three-state Markov model is constructed to represent the state transition relationship of no faults, intermittent faults, and permanent faults for PV components. The model is subsequently fused into the DBNs. The reliability and availability of three simple PV systems with centralized, string, and multistring configurations, as well as a complex PV system, are analyzed through the proposed framework. The sequence of the degree of importance of PV components is investigated using mutual information. The effects of intermittent fault parameters, including the coefficients of intermittent fault, permanent fault, and intermittent repair, on the reliability and availability are explored. Results show that the reliability and availability of the PV system with centralized configuration rapidly decrease, compared with those of the PV systems with string and multistring configurations. The sequence of the degree of importance of PV components is DC/AC inverter, DC/DC converter, DC combiner, and PV module arranged from the largest to the smallest. The finding indicates that the DC/AC inverter should be given considerable attention to improve the reliability and availability and to prevent their possible failures. (C) 2015 Elsevier Ltd. All rights reserved.",""
"Bayesian network (BN), a simple graphical notation for conditional independence assertions, is promised to represent the probabilistic relationships between diseases and symptoms. Learning the structure of a Bayesian network classifier (BNC) encodes conditional independence assumption between attributes, which may deteriorate the classification performance. One major approach to mitigate the BNC's primary weakness (the attributes independence assumption) is the locally weighted approach. And this type of approach has been proved to achieve good performance for naive Bayes, a BNC with simple structure. However, we do not know whether or how effective it works for improving the performance of the complex BNC. In this paper, we first do a survey on the complex structure models for BNCs and their improvements, then carry out a systematically experimental analysis to investigate the effectiveness of locally weighted method for complex BNCs, e.g., tree-augmented naive Bayes (TAN), averaged one-dependence estimators AODE and hidden naive Bayes (HNB), measured by classification accuracy (ACC) and the area under the ROC curve ranking (AUC). Experiments and comparisons on 36 benchmark data sets collected from University of California, Irvine (UCI) in Weka system demonstrate that locally weighting technologies just slightly outperforms unweighted complex BNCs on ACC and AUC. In other words, although locally weighting could significantly improve the performance of NB (a BNC with simple structure), it could not work well on BNCs with complex structures. This is because the performance improvements of BNCs are attributed to their structures not the locally weighting.","Learning the structure of a Bayesian network classifier (BNC) encodes conditional independence assumption between attributes, which may deteriorate the classification performance."
"In violent crimes, adhesive tapes such as duct tape are often used by perpetrators e.g. to tie up a victim. In the forensic examination of such tapes many different types of traces can be found, such as finger marks and human biological traces. These traces are first interpreted at source level. However, even when it is certain that a trace was donated by the suspect this does not necessarily mean that he donated the trace while taping the victim, as he could have e.g. used the tape roll from which the pieces came previous to the crime. Therefore, the trace can also be interpreted at activity level. For this, factors such as transfer, persistence and recovery, as well as the position of the trace as it would have been on the original roll have to be taken into consideration. In this study, we have developed a Bayesian network which can aid the forensic practitioner in his interpretation. From a sensitivity analysis, we have concluded that it would be most desirable to set up further studies to determine the most likely positions of DNA on tape rolls if there has only been innocent contact.",""
"Identifying specific human body fluids and establishing their presence in traces can be crucial to help reconstructing alleged incidents in criminal cases. It is up to the forensic practitioner to test for the presence of body fluids, interpret the test results and draw scientifically supported conclusions that can be used in a court of law. This study presents a Bayesian network for the interpretation of test results for human saliva based on the presence of human salivary a-amylase. TheBayesian network can be used by forensic practitioners as an exploratory tool to form their expert opinion on the presence or absence of saliva in a trace.",""
"The Libra Toolkit is a collection of algorithms for learning and inference with discrete probabilistic models, including Bayesian networks, Markov networks, dependency networks, and sum-product networks. Compared to other toolkits, Libra places a greater emphasis on learning the structure of tractable models in which exact inference is efficient. It also includes a variety of algorithms for learning graphical models in which inference is potentially intractable, and for performing exact and approximate inference. Libra is released under a 2-clause BSD license to encourage broad use in academia and industry.","The Libra Toolkit is a collection of algorithms for learning and inference with discrete probabilistic models, including Bayesian networks, Markov networks, dependency networks, and sum-product networks."
"Bayesian network classifiers are a powerful machine learning tool. In order to evaluate the expressive power of these models, we compute families of polynomials that sign-represent decision functions induced by Bayesian network classifiers. We prove that those families are linear combinations of products of Lagrange basis polynomials. In absence of V-structures in the predictor sub-graph, we are also able to prove that this family of polynomials does indeed characterize the specific classifier considered. We then use this representation to bound the number of decision functions representable by Bayesian network classifiers with a given structure.","Bayesian network classifiers are a powerful machine learning tool."
"Background Psychosomatic health complaints are significant indicators of adolescent well-being. The aim of this study is to describe the full set of interactions between health complaints and the presence of subjective resources, represented by the quality of relationships with parents and peers and by a positive school perception, in a population of 15-year-old adolescents. Smoking and alcohol consumption were also included in the analyses. Methods Bayesian networks were built formales and females separately, in order to understand the interactions among all considered variables in a representative sample of 16 018 Italian adolescents participating in the Health Behaviour in School-Aged Children survey 2009-10. Results The resulting networks show that school is the crucial node linking adolescents' well-being with parents and peer relationships, as well as with smoking and alcohol consumption. Conclusions Adolescents' well-being, as well as the prevalence of typical risk behaviours, such as smoking and alcohol consumption, is mediated by the adolescents' academic stress. Therefore, public health interventions, to be effective, should consider addressing the school environment by making it a more inclusive environment promoting critical thinking and sense of belonging rather than just focusing on personal behaviours.",""
"Conceptual ecological models synthesize information about complex systems into simplified visual maps and can be used to prioritize system components for research or management attention. In this article, we introduce conceptual modeling methods that incorporate expert ratings about a suite of properties of system components, including assessment of the state of knowledge, the strength of ecological impact, and the state of management or research attention devoted to a given component. Quantitative ratings of the properties of system components are subsequently used to prioritize model components objectively for research or management attention. Two case studies, one on plankton-herring-baleen whale dynamics and one on Chinook salmon strategic research planning, are presented to illustrate techniques. For example, in the Chinook salmon case study, participants constructed a prioritization score that identified system components rated as high ecological impact, but low state of knowledge and low state of management or research attention. By addressing gaps in both knowledge and attention, participants implemented a strategy for research planning that complemented existing Chinook salmon research and management in the study region. The case studies demonstrated that conceptual ecological models could be completed successfully with an economy of time. Conceptual modeling has been implemented across a range of disciplines and provides a useful tool that natural resource management and research groups can use to organize collaborative efforts and communicate research-or management progress to stakeholders or funders.",""
"Risk assessment performs a critical decision support role in maintenance decision making. This is through assisting maintenance practitioners systematically identify, analyze, evaluate and mitigate equipment failures. Often, such failures are mitigated through formulating effective maintenance strategies. In asset maintenance, well-known risk assessment techniques include the Failure Mode and Effect Analysis (FMEA), Fault Tree Analysis (FTA), and Bayesian Networks (BN). In recent years, considerable research attention has been directed towards improving existing techniques, often at the expense of a structured framework for selecting suitable risk assessment techniques. Often, several criteria influence the selection process. Moreover, the criteria are closely linked to specific organizational competencies that vary from one firm to another. In this study, a selection methodology for risk assessment techniques in the maintenance decision making domain is proposed. In the methodology, generic selection criteria for the FMEA, FTA and BN are derived based on the risk assessment process outlined in the ISO 31000:2009 standard. The criteria are prioritized using the Analytic Network Process (ANP), taking into account the judgment and opinion of academic and industrial domain experts. The results illustrate the usefulness of the proposed methodology towards assisting maintenance practitioners discern important competencies relevant to the specific technique and as such select the technique best suited for the organization. (C) 2015 Elsevier B.V. All rights reserved.",""
"In the marine environment, humans exploit natural ecosystems for food and economic benefit. Challenging policy goals have been set to protect resources, species, communities and habitats, yet ecologists often have sparse data on interactions occurring in the system to assess policy outcomes. This paper presents a technique, loosely based on Bayesian Belief Networks, to create simple models which I) predict whether individual species within a community will decline or increase in population size, 2) encapsulate uncertainty in the predictions in an intuitive manner and 3) require limited knowledge of the ecosystem and functional parameters required to model it. We develop our model for a UK rocky shore community, to utilise existing knowledge of species interactions for model validation purposes. However, we also test the role of expert opinion, without full scientific knowledge of species interactions, by asking non-UK based marine scientists to derive parameters for the model (non-UK scientists are not familiar with the exact communities being described and will need to extrapolate from existing knowledge in a similar manner to model a poorly studied system). We find these differ little from the parameters derived by ourselves and make little difference to the final model predictions. We also test our model against simple experimental manipulations, and find that the most important changes in community structure as a result of manipulations correspond well to the model predictions with both our, and non-UK expert parameterisation. The simplicity of the model, nature of the outputs, and the user-friendly interface makes it potentially suitable for policy, conservation and management work on multispecies interactions in a wide range of marine ecosystems. (C) 2015 Elsevier Ltd. All rights reserved.",""
"Conformal predictors are usually defined and studied under the exchangeability assumption. However, their definition can be extended to a wide class of statistical models, called online compression models, while retaining their property of automatic validity. This paper is devoted to conformal prediction under hypergraphical models that are more specific than the exchangeability model. Namely, we define two natural classes of conformity measures for such hypergraphical models and study the corresponding conformal predictors empirically on benchmark LED data sets. Our experiments show that they are more efficient than conformal predictors that use only the exchangeability assumption.",""
"Bacillus thuringiensis (Bt) is a Gram-positive bacterium. The entomopathogenic activity of Bt is related to the existence of the crystal consisting of protoxins, also called delta-endotoxins. In order to optimize and explain the production of delta-endotoxins of Bacillus thuringiensis kurstaki, we studied seven medium components: soybean meal, starch, KH2PO4, K2HPO4, FeSO4, MnSO4, and MgSO4 and their relationships with the concentration of delta-endotoxins using an experimental design (Plackett-Burman design) and Bayesian networks modelling. The effects of the ingredients of the culture medium on delta-endotoxins production were estimated. The developed model showed that different medium components are important for the Bacillus thuringiensis fermentation. The most important factors influenced the production of delta-endotoxins are FeSO4, K2HPO4, starch and soybean meal. Indeed, it was found that soybean meal, K2HPO4, KH2PO4 and starch also showed positive effect on the delta-endotoxins production. However, FeSO4 and MnSO4 expressed opposite effect. The developed model, based on Bayesian techniques, can automatically learn emerging models in data to serve in the prediction of delta-endotoxins concentrations. The constructed model in the present study implies that experimental design (Plackett-Burman design) joined with Bayesian networks method could be used for identification of effect variables on delta-endotoxins variation.",""
"The purpose of medium secure services (MSS) is to provide accommodation, support, and treatment to individuals with enduring mental health problems who usually come into contact with the criminal justice system. These individuals are, therefore, believed to pose a risk of violence to themselves as well as to other individuals. Assessing and managing the risk of violence is considered to be a critical component for discharged decision making in MSS. Methods for violence risk assessment in this area of research are typically based on regression models or checklists with no statistical composition and which naturally demonstrate mediocre predictive performance and, more importantly, without providing genuine decision support While Bayesian networks have become popular tools for decision support in the medical field over the last couple of decades, they have not been extensively studied in forensic psychiatry. In this paper, we describe a decision support system using Bayesian networks, which is mainly parameterised based on questionnaire, interviewing and clinical assessment data, for violence risk assessment and risk management in patients discharged from MSS. The results demonstrate moderate to significant improvements in forecasting capability. More importantly, we demonstrate how decision support is improved over the well-established approaches in this area of research, primarily by incorporating causal interventions and taking advantage of the model's ability in answering complex probabilistic queries for unobserved variables. (C) 2015 Elsevier B.V. All rights reserved.","Methods for violence risk assessment in this area of research are typically based on regression models or checklists with no statistical composition and which naturally demonstrate mediocre predictive performance and, more importantly, without providing genuine decision support While Bayesian networks have become popular tools for decision support in the medical field over the last couple of decades, they have not been extensively studied in forensic psychiatry."
"Code completion is an integral part of modern Integrated Development Environments (IDEs). Developers often use it to explore Application Programming Interfaces (APIs). It is also useful to reduce the required amount of typing and to help avoid typos. Traditional code completion systems propose all type-correct methods to the developer. Such a list is often very long with many irrelevant items. More intelligent code completion systems have been proposed in prior work to reduce the list of proposed methods to relevant items. This work extends one of these existing approaches, the Best Matching Neighbor (BMN) algorithm. We introduce Bayesian networks as an alternative underlying model, use additional context information for more precise recommendations, and apply clustering techniques to improve model sizes. We compare our new approach, Pattern-based Bayesian Networks (PBN), to the existing BMN algorithm. We extend previously used evaluation methodologies and, in addition to prediction quality, we also evaluate model size and inference speed. Our results show that the additional context information we collect improves prediction quality, especially for queries that do not contain method calls. We also show that PBN can obtain comparable prediction quality to BMN, while model size and inference speed scale better with large input sizes.","We extend previously used evaluation methodologies and, in addition to prediction quality, we also evaluate model size and inference speed."
"We used a statistical learning framework to evaluate the ability of three machine-learning methods to predict nitrate concentration in shallow groundwater of the Central Valley, California: boosted regression trees (BRT), artificial neural networks (ANN), and Bayesian networks (BN). Machine learning methods can learn complex patterns in the data but because of overfitting may not generalize well to new data. The statistical learning framework involves cross-validation (CV) training and testing data and a separate hold-out data set for model evaluation, with the goal of optimizing predictive performance by controlling for model overfit. The order of prediction performance according to both CV testing R-2 and that for the hold-out data set was BRT > BN > ANN. For each method we identified two models based on CV testing results: that with maximum testing R-2 and a version with R-2 within one standard error of the maximum (the 1SE model). The former yielded CV training R-2 values of 0.94-1.0. Cross-validation testing le values indicate predictive performance, and these were 0.22-0.39 for the maximum R-2 models and 0.19-0.36 for the 1SE models. Evaluation with hold-out data suggested that the 1SE BRT and ANN models predicted better for an independent data set compared with the maximum R-2 versions, which is relevant to extrapolation by mapping. Scatterplots of predicted vs. observed hold-out data obtained for final models helped identify prediction bias, which was fairly pronounced for ANN and BN. Lastly, the models were compared with multiple linear regression (MLR) and a previous random forest regression (RFR) model. Whereas BRT results were comparable to RFR, MLR had low hold-out R-2 (0.07) and explained less than half the variation in the training data. Spatial patterns of predictions by the final, 1SE BRT model agreed reasonably well with previously observed patterns of nitrate occurrence in groundwater of the Central Valley. Published by Elsevier B.V.","We used a statistical learning framework to evaluate the ability of three machine-learning methods to predict nitrate concentration in shallow groundwater of the Central Valley, California: boosted regression trees (BRT), artificial neural networks (ANN), and Bayesian networks (BN)."
"The study aimed to identify methodological confounding factors affecting patient satisfaction survey results. The data gathered from CINAHL and PubMed databases consisted of 355 surveys published from 2006 to 2012. Linear regression and Bayesian models, with seven potential survey-related confounders together with patient age and gender as explanatory variables, were constructed. According to the linear model, up to 12% of the original variation in patient satisfaction was explained by confounding variables, not by the actual variation in satisfaction. The presence of an interviewer resulted in lower satisfaction levels, and the satisfaction results correlated negatively with the number of items in the questionnaire. According to the Bayesian model, if patients were over 60 years old and the questionnaire consisted mainly of positively phrased items, the probability of rating their experiences as very satisfied was 75%. The Bayesian and linear models endorsed each other and revealed specifically that the surveys reporting high patient satisfaction could be predicted on the basis of confounding variables. The following recommendations are given for constructing a patient satisfaction survey: use neutral rather than negatively or positively phrased items, and use enough items to increase the likelihood that the least satisfactory care components are also included in order to better enable comparisons across sporadic surveys.","Linear regression and Bayesian models, with seven potential survey-related confounders together with patient age and gender as explanatory variables, were constructed."
"Agent-based micro-simulation models require a complete list of agents with detailed demographic/socioeconomic information for the purpose of behavior modeling and simulation. This paper introduces a new alternative for population synthesis based on Bayesian networks. A Bayesian network is a graphical representation of a joint probability distribution, encoding probabilistic relationships among a set of variables in an efficient way. Similar to the previously developed probabilistic approach, in this paper, we consider the population synthesis problem to be the inference of a joint probability distribution. In this sense, the Bayesian network model becomes an efficient tool that allows us to compactly represent/reproduce the structure of the population system and preserve privacy and confidentiality in the meanwhile. We demonstrate and assess the performance of this approach in generating synthetic population for Singapore, by using the Household Interview Travel Survey (HITS) data as the known test population. Our results show that the introduced Bayesian network approach is powerful in characterizing the underlying joint distribution, and meanwhile the overfitting of data can be avoided as much as possible. (C) 2015 Elsevier Ltd. All rights reserved.","Similar to the previously developed probabilistic approach, in this paper, we consider the population synthesis problem to be the inference of a joint probability distribution."
"Compared with the observable behavior, it is difficult to predict the hidden behavior of a complex system. In the existing methods for predicting the hidden behavior, a lot of testing data (usually quantitative information) are needed to be sampled. However, some complex engineering systems have the following characteristics: 1) The systems cannot be tested periodically, and the observable information is incomplete; 2) the change process of hidden behavior may be affected by the test; and 3) only part of quantitative information and qualitative knowledge (i.e., semi-quantitative information) may be obtained. These characteristics all related to the test are named as testing influence for simplicity. Although a model and a corresponding optimal algorithm for training the model parameters have been proposed to predict the hidden behavior on the basis of semiquantitative information and belief rule base (BRB), the testing influence has not been considered. In order to solve the above problems, a new BRB-based model, which can use the semiquantitative information, is proposed under testing influence in this paper. In the newly proposed forecasting model, there are some parameters of which the initial values are usually assigned by experts and may not be accurate, which can lead to the inaccurate prediction results. As such, an improved optimal algorithm for training the parameters of the forecasting model is further developed on the basis of the expectation-maximization idea and the covariance matrix adaption evolution strategy (CMA-ES). By using the semiquantitative information, the proposed BRB-based model and the improved CMA-ES algorithm can operate together in an integrated manner so as to improve the forecasting precision. A case study is examined to demonstrate the ability and applicability of the newly proposed BRB-based forecasting model and the improved CMA-ES algorithm.",""
"OCF-networks provide the possibility to combine qualitative information expressed by rankings of (conditional) formulas with the strong structural information of a network, in this respect being a qualitative variant of the better known Bayesian networks. Like for Bayesian networks, a global ranking function can be calculated quickly and efficiently from the locally distributed information, whereas the latter significantly reduces the exponentially high complexity of the semantical ranking approach. This qualifies OCF-networks for applications. However, in practical applications the provided ranking information may not be in the format needed to be represented by an OCF-network, or some values may be simply missing. In this paper, we present techniques for filling in the missing values using methods of inductive reasoning and we elaborate on formal properties of OCF-networks. (C) 2015 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).",""
"We compare three approaches to learning numerical parameters of discrete Bayesian networks from continuous data streams: (1) the EM algorithm applied to all data, (2) the EM algorithm applied to data increments, and (3) the online EM algorithm. Our results show that learning from all data at each step, whenever feasible, leads to the highest parameter accuracy and model classification accuracy. When facing computational limitations, incremental learning approaches are a reasonable alternative. While the differences in speed between incremental algorithms are not large (online EM is slightly slower), for all but small data sets online EM tends to be more accurate than incremental EM. (C) 2015 Elsevier B.V. All rights reserved.","Our results show that learning from all data at each step, whenever feasible, leads to the highest parameter accuracy and model classification accuracy."
"The advent and availability of technology has brought us closer than ever through social networks. Consequently, there is a growing emphasis on mining social networks to extract information for knowledge and discovery. However, methods for social network analysis (SNA) have not kept pace with the data explosion. In this review, we describe directed and undirected probabilistic graphical models (PGMs), and highlight recent applications to social networks. PGMs represent a flexible class of models that can be adapted to address many of the current challenges in SNA. In this work, we motivate their use with simple and accessible examples to demonstrate the modeling and connect to theory. In addition, recent applications in modern SNA are highlighted, including the estimation and quantification of importance, propagation of influence, trust (and distrust), link and profile prediction, privacy protection, and news spread through microblogging. Applications are selected to demonstrate the flexibility and predictive capabilities of PGMs in SNA. Finally, we conclude with a discussion of challenges and opportunities for PGMs in social networks.",""
"Traffic flow prediction is a fundamental functionality of intelligent transportation systems. After presenting the state of the art, we focus on nearest neighbor regression methods, which are data-driven algorithms that are effective yet simple to implement. We try to strengthen their efficacy in two ways that are little explored in literature, i.e., by adopting a multivariate approach and by adding awareness of the time of the day. The combination of these two refinements, which represents a novelty, leads to the definition of a new class of methods that we call time-aware multivariate nearest neighbor regression (TaM-NNR) algorithms. To assess this class, we have used publicly available traffic data from a California highway. Computational results show the effectiveness of such algorithms in comparison with state-of-the-art parametric and non-parametric methods. In particular, they consistently perform better than their corresponding standard univariate versions. These facts highlight the importance of context elements in traffic prediction. The ideas presented here may be further investigated considering more context elements (e.g., weather conditions), more complex road topologies (e.g., urban networks), and different types of prediction methods.","After presenting the state of the art, we focus on nearest neighbor regression methods, which are data-driven algorithms that are effective yet simple to implement."
"In the calculation of dynamic fault trees, the existing state space-based methods, such as Markov chain method, are basically global-state models, which make the solution procedure very complex. Bayesian networks have become a popular tool to build probability models and conduct inference for reliability design and analysis in various industry fields. The state explosion problem can be alleviated by Bayesian networks. Furthermore, to obtain sufficient failure data sets in real engineering systems is extremely difficult and thus causes the parametric uncertainty in failure data. To address these issues, a novel dynamic fault tree analysis method based on the continuous-time Bayesian networks under fuzzy numbers is proposed in this article. The probability distributions under fuzzy numbers for the output variable of dynamic logic gates are determined. The calculation of fuzzy failure probability of a system is presented. Finally, an example is given to demonstrate the effectiveness of the proposed method.","Bayesian networks have become a popular tool to build probability models and conduct inference for reliability design and analysis in various industry fields."
"Bayesian network (BN) has been adopted as the underlying model for representing and inferring uncertain knowledge. As the basis of realistic applications centered on probabilistic inferences, learning a BN from data is a critical subject of machine learning, artificial intelligence, and big data paradigms. Currently, it is necessary to extend the classical methods for learning BNs with respect to data-intensive computing or in cloud environments. In this paper, we propose a parallel and incremental approach for data-intensive learning of BNs from massive, distributed, and dynamically changing data by extending the classical scoring and search algorithm and using MapReduce. First, we adopt the minimum description length as the scoring metric and give the two-pass MapReduce-based algorithms for computing the required marginal probabilities and scoring the candidate graphical model from sample data. Then, we give the corresponding strategy for extending the classical hill-climbing algorithm to obtain the optimal structure, as well as that for storing a BN by <key, value> pairs. Further, in view of the dynamic characteristics of the changing data, we give the concept of influence degree to measure the coincidence of the current BN with new data, and then propose the corresponding two-pass MapReduce-based algorithms for BNs incremental learning. Experimental results show the efficiency, scalability, and effectiveness of our methods.","As the basis of realistic applications centered on probabilistic inferences, learning a BN from data is a critical subject of machine learning, artificial intelligence, and big data paradigms."
"Multiple novel interpersonal communications services have emerged recently, but how their usage and perceived importance are related to the personal characteristics of the users is still relatively unexplored. Therefore, the aim of this study is to explore the effect of an individual's age on the perceived importance and usage intensity of communications services based on Bayesian Networks using a survey of 3008 Finns during 2011. In the case of Short Message Service (SMS), Instant Messaging (IM), Internet forums and communities (e.g., Facebook & Twitter), and e-mail the results indicate that the perceived importance of the communications services decreases as the age increases. With phone calls and letters, however, no clear dependencies with age were identified. In the causal analysis the importance of Internet forums and communities was the only variable which can be stated to be directly caused by an individual's age. This variable also acts as a mediator in the path from age towards perceived importance of other communication services and also towards their usage intensity. These results about the central role of Internet forums and communities can be exploited, for example, by device manufacturers when designing their products, and by service providers when designing their consumer services. The study also provides new information for mobile operators about the dependencies between mobile communications services and a documented example workflow for research community to construct a causal Bayesian Network from a combination of observational data and domain expertise.",""
"Fuzzy cognitive maps have been widely used as abstract models for complex networks. Traditional ways to construct fuzzy cognitive maps rely on domain knowledge. In this paper, we propose to use fuzzy cognitive map learning algorithms to discover domain knowledge in the form of causal networks from data. More specifically, we propose to infer gene regulatory networks from gene expression data. Furthermore, a new efficient fuzzy cognitive map learning algorithm based on a decomposed genetic algorithm is developed to learn large scale networks. In the proposed algorithm, the simulation error is used as the objective function, while the model error is expected to be minimized. Experiments are performed to explore the feasibility of this approach. The high accuracy of the generated models and the approximate correlation between simulation errors and model errors suggest that it is possible to discover causal networks using fuzzy cognitive map learning. We also compared the proposed algorithm with ant colony optimization, differential evolution, and particle swarm optimization in a decomposed framework. Comparison results reveal the advantage of the decomposed genetic algorithm on datasets with small data volumes, large network scales, or the presence of noise. (C) 2015 Elsevier B.V. All rights reserved.",""
"This paper proposes a systematized presentation and a terminology for observations in a Bayesian network. It focuses on the three main concepts of uncertain evidence, namely likelihood evidence and fixed and not-fixed probabilistic evidence, using a review of previous literature. A probabilistic finding on a variable is specified by a local probability distribution and replaces any former belief in that variable. It is said to be fixed or not fixed regarding whether it has to be kept unchanged or not after the arrival of observation on other variables. Fixed probabilistic evidence is defined by Valtorta et al. (J Approx Reason 29(1):71-106 2002) under the name soft evidence, whereas the concept of not-fixed probabilistic evidence has been discussed by Chan and Darwiche (Artif Intell 163(1):67-90 2005). Both concepts have to be clearly distinguished from likelihood evidence defined by Pearl (1988), also called virtual evidence, for which evidence is specified as a likelihood ratio, that often represents the unreliability of the evidence. Since these three concepts of uncertain evidence are not widely understood, and the terms used to describe these concepts are not well established, most Bayesian networks engines do not offer well defined propagation functions to handle them. Firstly, we present a review of uncertain evidence and the proposed terminology, definitions and concepts related to the use of uncertain evidence in Bayesian networks. Then we describe updating algorithms for the propagation of uncertain evidence. Finally, we propose several results where the use of fixed or not-fixed probabilistic evidence is required.",""
"Defining habitats vulnerable to invasion is important to support the management of invasive alien species (IAS). We developed and applied data-driven and knowledge-supported data-driven Bayesian Belief Networks (BBNs) to assess the habitat suitability for alien gammarids. Data-driven model development using a Naive Bayes classifier and equal width discretization resulted in a habitat suitability model with a moderate technical performance (CCl = 68% K = 0.33). Although the structure of the knowledge-supported model yielded important ecological insight between environmental and biotic variables and the occurrence of alien gammarids, the performance was lower (CCl = 60% K = 0.19) compared to the purely data-driven model. The lower predictive performance of the knowledge-supported model may be attributed to its higher model complexity. Our study shows that BBNs can support the management of IAS as they are visually appealing, transparent models that facilitate integration of monitoring data and expert knowledge. (C) 2015 Elsevier Ltd. All rights reserved.","Data-driven model development using a Naive Bayes classifier and equal width discretization resulted in a habitat suitability model with a moderate technical performance (CCl = 68% K = 0."
"PRISM is a probabilistic logic programming formalism which allows defining a probability distribution over possible worlds. This paper investigates learning a class of generative PRISM programs known as failure-free. The aim is to learn recursive PRISM programs which can be used to model stochastic processes. These programs generalise dynamic Bayesian networks by defining a halting distribution over the generative process. Dynamic Bayesian networks model infinite stochastic processes. Sampling from infinite process can only be done by specifying the length of sequences that the process generates. In this case, only observations of a fixed length of sequences can be obtained. On the other hand, the recursive PRISM programs considered in this paper are self-terminating upon some halting conditions. Thus, they generate observations of different lengths of sequences. The direction taken by this paper is to combine ideas from inductive logic programming and learning Bayesian networks to learn PRISM programs. It builds upon the inductive logic programming approach of learning from entailment. (C) 2015 Elsevier Inc. All rights reserved.",""
"Applications in various domains often lead to high dimensional dependence modelling. A Bayesian network (BN) is a probabilistic graphical model that provides an elegant way of expressing the joint distribution of a large number of interrelated variables. BNs have been successfully used to represent uncertain knowledge in a variety of fields. The majority of applications use discrete BNs, i.e. BNs whose nodes represent discrete variables. Integrating continuous variables in BNs is an area fraught with difficulty. Several methods that handle discrete-continuous BNs have been proposed in the literature. This paper concentrates only on one method called non-parametric BNs (NPBNs). NPBNs were introduced in 2004 and they have been or are currently being used in at least twelve professional applications. This paper provides a short introduction to NPBNs, a couple of theoretical advances, and an overview of applications. The aim of the paper is twofold: one is to present the latest improvements of the theory underlying NPBNs, and the other is to complement the existing overviews of BNs applications with the NPNBs applications. The latter opens the opportunity to discuss some difficulties that applications pose to the theoretical framework and in this way offers some NPBN modelling guidance to practitioners. (C) 2015 Elsevier Ltd. All rights reserved.",""
"Application of risk assessments developed for the design phase to support decision-making in operational settings has exposed weaknesses in how risk is analysed and expressed in an operational context. The purpose of this paper is to clarify what we actually need to express when we use risk information to support various decision scenarios. We distinguish decision scenarios into strategic decisions, operational decisions, instantaneous decisions and emergency decisions. This forms a basis for discussing the different role risk and risk assessment plays in these decisions. Five categories of risk information (average risk, site-specific average risk, activity risk (activity performance risk and activity consequence risk), period risk and time-dependent action risk) are proposed and applications for different types of decisions are discussed. An example illustrates the use of the proposed risk types. The classification has novel aspects in providing a structure that should help in understanding how we need different aspects of risk and different ways of expressing risk in different situations. In addition, it improves communication among decision-makers by clarifying what aspects we are addressing when we use the term \"risk\". (C) 2015 Elsevier Ltd. All rights reserved.","The classification has novel aspects in providing a structure that should help in understanding how we need different aspects of risk and different ways of expressing risk in different situations."
"Two corresponding issues concerning Digital Soil Mapping are the demand for up-to-date, fine resolution soil data and the need to determine soil-landscape relationships. In this study, we propose a Bayesian Network framework as a suitable modelling approach to fulfil these requirements. Bayesian Networks are graphical probabilistic models in which predictions are obtained using prior probabilities derived from either measured data or expert opinion. They represent cause and effect relationships through connections in a network system. The advantage of the Bayesian Networks approach is that the models are easy to interpret and the uncertainty inherent in the relationships between variables can be expressed in terms of probability. In this study we will define the fundamentals of a Bayesian Network and the probability theory that underpins predictions. Then, using case studies, we demonstrate how they can be applied to predict soil properties (bulk density) and soil taxonomic class (associations). (C) 2015 Elsevier B.V. All rights reserved.",""
"Forensic medical practitioners and scientists have for several years sought improved decision support for determining and managing care and release of prisoners with mental health problems. Some of these prisoners can pose a serious threat of violence to society after release. It is, therefore, critical that the risk of violent reoffending is accurately measured and, more importantly, well managed with causal interventions to reduce this risk after release. The well-established predictors in this area of research are typically based on regression models or even some rule-based methods with no statistical composition, and these have proven to be unsuitable for simulating causal interventions for risk management. In collaboration with the medical practitioners of the Violence Prevention Research Unit (VPRU), Queen Mary University of London, we have developed a Bayesian network (BN) model for this purpose, which we call DSVM-P (Decision Support for Violence Management Prisoners). The BN model captures the causal relationships between risk factors, interventions and violence and demonstrates significantly higher accuracy (cross-validated AUC score of 0.78) compared to well-established predictors (AUC scores ranging from 0.665 to 0.717) within this area of research, with respect to whether a prisoner is determined suitable for release. Even more important, however, the BN model also allows for specific risk factors to be targeted for causal intervention for risk management of future re-offending. Hence, unlike the previous predictors, this makes the model useful in terms of answering complex clinical questions that are based on unobserved evidence. Clinicians and probation officers who work in these areas would benefit from a system that takes account of these complex risk management considerations, since these decision support features are not available in the previous generation of models used by forensic psychiatrists. (C) 2015 Elsevier Ltd. All rights reserved.","The well-established predictors in this area of research are typically based on regression models or even some rule-based methods with no statistical composition, and these have proven to be unsuitable for simulating causal interventions for risk management."
"Reforestation is an expensive undertaking. It is a long-term, complex, and trans-disciplinary process and it involves uncertainties and changing conditions. There is also a complex array of drivers (including biophysical, technical, socio-economic, institutional, and management drivers) that affect reforestation success. Previous research has documented the independent effects of biophysical and technical, environmental and socio-economic drivers on reforestation success. However, research over the last decade has revealed that the outcome of multiple factor interactions is commonly non-additive (i.e. synergies and antagonisms). Therefore, in order to provide better decision support for reforestation planning and policy setting it is necessary to understand the interactive effects that drivers have on reforestation success. To understand these interactive effects, we developed a Bayesian network model based on data collected from 43 reforestation projects on Leyte Island, the Philippines. Non-additive interactions among reforestation success drivers (i.e. synergies and antagonisms) were found to account for up to 90% of interactions tested. This result suggests an urgent need to account for these non-additive interactions in reforestation policy and planning in order to avoid unanticipated outcomes, wasted effort and missed opportunities. (C) 2015 Elsevier B.V. All rights reserved.",""
"Background: Reconstructing gene regulatory networks (GRNs) from expression data is a challenging task that has become essential to the understanding of complex regulatory mechanisms in cells. The major issues are the usually very high ratio of number of genes to sample size, and the noise in the available data. Integrating biological prior knowledge to the learning process is a natural and promising way to partially compensate for the lack of reliable expression data and to increase the accuracy of network reconstruction algorithms. Results: In this manuscript, we present PriorPC, a new algorithm based on the PC algorithm. PC algorithm is one of the most popular methods for Bayesian network reconstruction. The result of PC is known to depend on the order in which conditional independence tests are processed, especially for large networks. PriorPC uses prior knowledge to exclude unlikely edges from network estimation and introduces a particular ordering for the conditional independence tests. We show on synthetic data that the structural accuracy of networks obtained with PriorPC is greatly improved compared to PC. Conclusion: PriorPC improves structural accuracy of inferred gene networks by using soft priors which assign to edges a probability of existence. It is robust to false prior which is not avoidable in the context of biological data. PriorPC is also fast and scales well for large networks which is important for its applicability to real data.",""
"Regulators of the histone H3-trimethyl lysine-4 (H3K4me3) mark are significantly associated with the genetic risk architecture of common neurodevelopmental disease, including schizophrenia and autism. Typical H3K4me3 is primarily localized in the form of sharp peaks, extending in neuronal chromatin on average only across 500-1500 base pairs mostly in close proximity to annotated transcription start sites. Here, through integrative computational analysis of epigenomic and transcriptomic data based on next-generation sequencing, we investigated H3K4me3 landscapes of sorted neuronal and non-neuronal nuclei in human postmortem, non-human primate and mouse prefrontal cortex (PFC), and blood. To explore whether H3K4me3 peak signals could also extend across much broader domains, we examined broadest domain cell-type-specific H3K4me3 peaks in an unbiased manner with an innovative approach on 41+12 ChIP-seq and RNA-seq data sets. In PFC neurons, broadest H3K4me3 distribution ranged from 3.9 to 12 kb, with extremely broad peaks (similar to 10 kb or broader) related to synaptic function and GABAergic signaling (DLX1, ELFN1, GAD1, IGSF9B and LINC00966). Broadest neuronal peaks showed distinct motif signatures and were centrally positioned in prefrontal gene-regulatory Bayesian networks and sensitive to defective neurodevelopment. Approximately 120 of the broadest H3K4me3 peaks in human PFC neurons, including many genes related to glutamatergic and dopaminergic signaling, were fully conserved in chimpanzee, macaque and mouse cortical neurons. Exploration of spread and breadth of lysine methylation markings could provide novel insights into epigenetic mechanism involved in neuropsychiatric disease and neuronal genome evolution.",""
"Aberrant activation of sonic Hegdehog (SHH) signaling has been found to disrupt cellular differentiation in many human cancers and to increase proliferation. The SHH pathway is known to cross-talk with EGFR dependent signaling. Recent studies experimentally addressed this interplay in Daoy cells, which are presumable a model system for medulloblastoma, a highly malignant brain tumor that predominately occurs in children. Currently ongoing are several clinical trials for different solid cancers, which are designed to validate the clinical benefits of targeting the SHH in combination with other pathways. This has motivated us to investigate interactions between EGFR and SHH dependent signaling in greater depth. To our knowledge, there is no mathematical model describing the interplay between EGFR and SHH dependent signaling in medulloblastoma so far. Here we come up with a fully probabilistic approach using Dynamic Bayesian Networks (DBNs). To build our model, we made use of literature based knowledge describing SHH and EGFR signaling and integrated gene expression (Illumina) and cellular location dependent time series protein expression data (Reverse Phase Protein Arrays). We validated our model by sub-sampling training data and making Bayesian predictions on the left out test data. Our predictions focusing on key transcription factors and p70S6K, showed a high level of concordance with experimental data. Furthermore, the stability of our model was tested by a parametric bootstrap approach. Stable network features were in agreement with published data. Altogether we believe that our model improved our understanding of the interplay between two highly oncogenic signaling pathways in Daoy cells. This may open new perspectives for the future therapy of Hedghog/EGF-dependent solid tumors.",""
"Risk management for wastewater treatment and reuse have led to growing interest in understanding and optimising pathogen reduction during biological treatment processes. However, modelling pathogen reduction is often limited by poor characterization of the relationships between variables and incomplete knowledge of removal mechanisms. The aim of this paper was to assess the applicability of Bayesian belief network models to represent associations between pathogen reduction, and operating conditions and monitoring parameters and predict AS performance. Naive Bayes and semi-naive Bayes networks were constructed from an activated sludge dataset including operating and monitoring parameters, and removal efficiencies for two pathogens (native Giardia lamblia and seeded Cryptosporidium parvum) and five native microbial indicators (F-RNA bacteriophage, Clostridium perfringens, Escherichia coli, coliforms and enterococci). First we defined the Bayesian network structures for the two pathogen log(10) reduction values (LRVs) class nodes discretized into two states (< and >= 1 LRV) using two different learning algorithms. Eight metrics, such as Prediction Accuracy (PA) and Area Under the receiver operating Curve (AUC), provided a comparison of model prediction performance, certainty and goodness of fit. This comparison was used to select the optimum models. The optimum Tree Augmented naive models predicted removal efficiency with high AUC when all system parameters were used simultaneously (AUCs for C. parvum and G. lamblia LRVs of 0.95 and 0.87 respectively). However, metrics for individual system parameters showed only the C. parvum model was reliable. By contrast individual parameters for G. lamblia LRV prediction typically obtained low AUC scores (AUC < 0.81). Useful predictors for C. parvum LRV included solids retention time, turbidity and total coliform LRV. The methodology developed appears applicable for predicting pathogen removal efficiency in water treatment systems generally. (C) 2015 Elsevier Ltd. All rights reserved.",""
"Background: Inference of gene networks from expression data is an important problem in computational biology. Many algorithms have been proposed for solving the problem efficiently. However, many of the available implementations are programming libraries that require users to write code, which limits their accessibility. Results: We have developed a tool called CyNetworkBMA for inferring gene networks from expression data that integrates with Cytoscape. Our application offers a graphical user interface for networkBMA, an efficient implementation of Bayesian Model Averaging methods for network construction. The client-server architecture of CyNetworkBMA makes it possible to distribute or centralize computation depending on user needs. Conclusions: CyNetworkBMA is an easy-to-use tool that makes network inference accessible to non-programmers through seamless integration with Cytoscape. CyNetworkBMA is available on the Cytoscape App Store at http://apps.cytoscape.org/apps/cynetworkbma.","Background: Inference of gene networks from expression data is an important problem in computational biology."
"Because accurate identification cannot be obtained when the Identification Friend or Foe (IFF) sensor is employed separately, a radar sensor network (RSN) is designed to improve the identification capability in this paper. The content of this paper is focused on the information fusion algorithm, which is one of the key technologies in the RSN. The fuzzy c-means and the Bayesian network are chosen as the fusion algorithm. This algorithm can implement the identification friend or foe automatically after being trained by the training samples and expert's experience, and reduce the effect of uncertainties in the process of identification. At the same time, the algorithm can update the identification result with the augmentation of observations. The RSN can be expanded, if more information can be obtained, to adapt to the complicated environment, on the basis of this algorithm. The simulation results prove the validity and efficiency of the algorithm. Copyright (c) 2012 John Wiley & Sons, Ltd.",""
"This paper intends to develop some novel hybrid intelligent systems by combining naive Bayes with decision trees (NBDT) and by combining non-nested generalized exemplar (NNge) and extended repeated incremental pruning (JRip) rule-based classifiers (NNJR) to construct a multiple classifier system to efficiently detect network intrusions. We also use ensemble design using AdaBoost to enhance the detection rate of the proposed hybrid system. Further, to have a better overall detection, we propose to combine farthest first traversal (FFT) clustering with classification techniques to obtain another two hybrid methods such as DTFF (DT+FFT) and FFNN (NNge+FFT). Finally, we use Bayesian belief network with Tabu search combined with NNge for better detection rate. Because most of the anomaly detection uses binary labels, that is, anomaly or normal, without discussing more details about the attack types, we perform two-class classification for our proposed methodologies in this paper. Substantial experiments are conducted using NSL-KDD dataset, which is a modified version of KDD99 intrusion dataset. Finally, empirical results with a detailed analysis for all the approaches show that hybrid classification with clustering DTFF provides the best anomaly detection rate among all others. Copyright (c) 2012 John Wiley & Sons, Ltd.","This paper intends to develop some novel hybrid intelligent systems by combining naive Bayes with decision trees (NBDT) and by combining non-nested generalized exemplar (NNge) and extended repeated incremental pruning (JRip) rule-based classifiers (NNJR) to construct a multiple classifier system to efficiently detect network intrusions."
"Background: Cytokine-hormone network deregulations underpin pathologies ranging from autoimmune disorders to cancer, but our understanding of these networks in physiological/pathophysiological states remains patchy. We employed Bayesian networks to analyze cytokine-hormone interactions in vivo using murine lactation as a dynamic, physiological model system. Results: Circulatory levels of estrogen, progesterone, prolactin and twenty-three cytokines were profiled in post partum mice with/without pups. The resultant networks were very robust and assembled about structural hubs, with evidence that interleukin (IL)-12 (p40), IL-13 and monocyte chemoattractant protein (MCP)-1 were the primary drivers of network behavior. Network structural conservation across physiological scenarios coupled with the successful empirical validation of our approach suggested that in silico network perturbations can predict in vivo qualitative responses. In silico perturbation of network components also captured biological features of cytokine interactions (antagonism, synergy, redundancy). Conclusion: These findings highlight the potential of network-based approaches in identifying novel cytokine pharmacological targets and in predicting the effects of their exogenous manipulation in inflammatory/immune disorders.",""
"Differentiating between Parkinson's disease (PD) and atypical parkinsonian syndromes (APS) is still a challenge, specially at early stages when the patients show similar symptoms. During last years, several computer systems have been proposed in order to improve the diagnosis of PD, but their accuracy is still limited. In this work we demonstrate a full automatic computer system to assist the diagnosis of PD using F-18-DMFP PET data. First, a few regions of interest are selected by means of a two-sample t-test. The accuracy of the selected regions to separate PD from APS patients is then computed using a support vector machine classifier. The accuracy values are finally used to train a Bayesian network that can be used to predict the class of new unseen data. This methodology was evaluated using a database with 87 neuroimages, achieving accuracy rates over 78%. A fair comparison with other similar approaches is also provided.","The accuracy of the selected regions to separate PD from APS patients is then computed using a support vector machine classifier."
"This is the second part of a study on diagnostic Bayesian networks (DBNs)-based method for diagnosing faults in air handling units (AHUs) in buildings. In this part, 4 DBNs are developed to diagnose faults in heating/cooling coils, sensors and faults in secondary supply chilled water/heating water systems. There are 18 typical faults concerned and 35 fault detectors introduced. The DBNs are developed mainly on the basis of first principles and fault patterns resulted from literature and three AHU fault detection and diagnosis (FDD) projects. Efficient fault detection rules/methods from a comprehensive literature survey are integrated into the DBNs. Also, some new fault detection rules are developed. The 4 DBNs were evaluated using experimental data from ASHRAE Project RP-1312. Results show that the proposed DBNs effectively diagnosed AHU faults. (C) 2015 Elsevier Ltd. All rights reserved.",""
"We introduce a novel numerical approach to parameter estimation in partial differential equations in a Bayesian inference context. The main idea is to translate the equation into a state-discrete dynamic Bayesian network with the discretization of cellular probabilistic automata. There exists a vast pool of inference algorithms in the probabilistic graphical models field, which can be applied to the network. In particular, we reformulate the parameter estimation as a filtering problem, discuss requirements for according tools in our specific setup, and choose the Boyen-Koller algorithm. To demonstrate our ideas, the scheme is applied to the problem of arsenate advection and adsorption in a water pipe: from measurements of the concentration of dissolved arsenate at the outflow boundary condition, we infer the strength of an arsenate source at the inflow boundary condition. Copyright (C) 2015 John Wiley & Sons, Ltd.","We introduce a novel numerical approach to parameter estimation in partial differential equations in a Bayesian inference context."
"Many technological systems that are composed of technical parts embedded in human, organizational, and environmental contexts can be categorized as complex systems. They have various interactions and a nonlinear relationship between their components. They are also open to their environment and make exchanges with it. Almost all traditional risk assessment techniques, such as Failure Modes and Effect Analysis (FMEA), Hazard and Operability Analysis (HAZOP), Fault Tree Analysis (FTA), and Probabilistic Risk Analysis (PRA) rely on a chain of linear cause and effect analysis. These techniques also have some limitations in terms of incorporating efficient links between risk models and human and organizational factors for studying modern complex technological systems. This paper generally reviews existing approaches of risk assessment for complex technological and specifically studies risk assessment of wind turbines. Then it proposes an integrated risk assessment framework for complex technological systems through a Bayesian network considering various system levels and their interaction using a cause and effect approach. Since wind turbines are instances of complex power generating systems consisting of several structural, electrical, and mechanical components interacting with human resource and organizational factors within natural, political, economic, and social environments, the proposed model is applied to assess risk and reliability in a wind turbine. Different scenarios of reliability analyses were investigated, which illustrated that Bayesian networks are effective for the reliability assessment of the chosen system and very useful for understanding the system behavior. (C) 2015 Elsevier Ltd. All rights reserved.",""
"We develop a penalized likelihood estimation framework to learn the structure of Gaussian Bayesian networks from observational data. In contrast to recent methods which accelerate the learning problem by restricting the search space, our main contribution is a fast algorithm for score-based structure learning which does not restrict the search space in any way and works on high-dimensional data sets with thousands of variables. Our use of concave regularization, as opposed to the more popular l(0) (e.g. BIC) penalty, is new. Moreover, we provide theoretical guarantees which generalize existing asymptotic results when the underlying distribution is Gaussian. Most notably, our framework does not require the existence of a so-called faithful DAG representation, and as a result, the theory must handle the inherent nonidentifiability of the estimation problem in a novel way. Finally, as a matter of independent interest, we provide a comprehensive comparison of our approach to several standard structure learning methods using open-source packages developed for the R language. Based on these experiments, we show that our algorithm obtains higher sensitivity with comparable false discovery rates for high-dimensional data and scales efficiently as the number of nodes increases. In particular, the total runtime for our method to generate a solution path of 20 estimates for DAGs with 8000 nodes is around one hour.",""
"Over the past couple of decades, there has been an exponential increase in the collection of large-scale GPS data from household/personal travel surveys all over the world. A range of algorithms, which vary from specific rules to advanced machine learning methods, have been applied to extract travel modes from raw GPS data collected by smartphone-based travel surveys. However, most of the methods applied neither describe the interaction between features influencing the travel mode decision nor effectively deal with the ambiguity inherently incorporated in these features. This paper identifies travel modes with a Bayesian network, whose structure is established based on a K2 algorithm and corresponding conditional probability tables are estimated with maximum likelihood methods. Five representative travel modes walk, bike, e-bike, bus and car are distinguished using the resulting Bayesian network. Additionally, the low speed rate and the average heading change are introduced to reduce uncertainties between bike and e-bike segments and between bus and car segments. The derived travel modes are then compared with those retrieved in the prompted recall survey by telephones. Consequently, more than 86% of segments have the travel mode correctly identified for each travel mode, with over 97% of walk segments being properly flagged. Results from the study demonstrate that GPS travel surveys provide an opportunity to supplement traditional travel surveys. (C) 2015 Elsevier Ltd. All rights reserved.",""
"This paper presents a new approach for the prediction of substructures in building facades based on sparse observations. We automatically generate a small number of most likely hypotheses and provide probabilities for each of them. Probability density functions of model parameters which in most cases are non Gaussian and multimodal are learned from training data and approximated by Gaussian mixtures. Relations between model parameters are represented by non-linear constraints. For stochastic reasoning we design and apply a special kind of Bayesian networks which involves both discrete as well as continuous variables, a scenario which often suggests the use of approximate inference which however is infeasible in the face of a huge number of competing model hypotheses. In order to be able to scan huge model spaces avoiding the pitfalls of approximate reasoning and to exploit the potential of both observations and models, we combined Bayesian networks with constraint logic programs. We designed a method which breaks down the problem into a feasible number of subproblems for which exact inference can be applied. We illustrate our approach with building facades and demonstrate that particularly for buildings with strong symmetries number and position of windows can be deduced on the basis of ground plans alone. (C) 2015 Elsevier Ltd. All rights reserved.","For stochastic reasoning we design and apply a special kind of Bayesian networks which involves both discrete as well as continuous variables, a scenario which often suggests the use of approximate inference which however is infeasible in the face of a huge number of competing model hypotheses."
"Saliva plus DNA from a suspect is commonly encountered in sexual assault cases on bodily swabs. However, without background knowledge, the weight of this evidence is unknown. It may indicate the presence of saliva resulting from cunnilingus, or it may represent indirect transfer. In this study, females who refrained from cunnilingus donated 43 items of underwear and 19 vaginal swabs. The samples were subjected to Phadebas (R), RSID (TM)-Saliva and mRNA profiling and were subsequently DNA-profiled to determine the prevalence of background saliva in the female population. The results report that 15.8% of females who refrained from cunnilingus were positive for saliva and a further 10.5% also had DNA from unknown source(s). These findings of the rate of indirect transfer were evaluated with the Bayesian approach, and it was found that the evidence of saliva plus a high foreign DNA source adds moderately strong support to the allegation of cunnilingus.",""
"Ecosystems consist of complex dynamic interactions among species and the environment, the understanding of which has implications for predicting the environmental response to changes in climate and biodiversity. However, with the recent adoption of more explorative tools, like Bayesian networks, in predictive ecology, few assumptions can be made about the data and complex, spatially varying interactions can be recovered from collected field data. In this study, we compare Bayesian network modelling approaches accounting for latent effects to reveal species dynamics for 7 geographically and temporally varied areas within the North Sea. We also apply structure learning techniques to identify functional relationships such as prey predator between trophic groups of species that vary across space and time. We examine if the use of a general hidden variable can reflect overall changes in the trophic dynamics of each spatial system and whether the inclusion of a specific hidden variable can model unmeasured group of species. The general hidden variable appears to capture changes in the variance of different groups of species biomass. Models that include both general and specific hidden variables resulted in identifying similarity with the underlying food web dynamics and modelling spatial unmeasured effect. We predict the biomass of the trophic groups and find that predictive accuracy varies with the models' features and across the different spatial areas thus proposing a model that allows for spatial autocorrelation and two hidden variables. Our proposed model was able to produce novel insights on this ecosystem's dynamics and ecological interactions mainly because we account for the heterogeneous nature of the driving factors within each area and their changes overtime. Our findings demonstrate that accounting for additional sources of variation, by combining structure learning from data and experts' knowledge in the model architecture, has the potential for gaining deeper insights into the structure and stability of ecosystems. Finally, we were able to discover meaningful functional networks that were spatially and temporally differentiated with the particular mechanisms varying from trophic associations through interactions with climate and commercial fisheries. (C) 2015 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licensesiby/4.0/).",""
"Scientists and managers are not the only holders of knowledge regarding environmental issues: other stakeholders such as farmers or fishers do have empirical and relevant knowledge. Thus, new approaches for knowledge representation in the case of multiple knowledge sources, but still enabling reasoning, are needed. Cognitive maps and Bayesian networks constitute some useful formalisms to address knowledge representations. Cognitive maps are powerful graphical models for knowledge gathering or displaying. If they offer an easy means to express individual judgments, drawing inferences in cognitive maps remains a difficult task. Bayesian networks are widely used for decision making processes that face uncertain information or diagnosis. But they are difficult to elicitate. To take advantage of each formalism and to overcome their drawbacks, Bayesian causal maps have been developed. In this approach, cognitive maps are used to build the network and obtain conditional probability tables. We propose here a complete framework applied on a real problem. From the different views of a group of shellfish dredgers about their activity, we derive a derision facilitating tool, enabling scenarios testing for fisheries management. (C) 2015 Elsevier B.V. All rights reserved.","If they offer an easy means to express individual judgments, drawing inferences in cognitive maps remains a difficult task."
"In order to understand the impact of climatic and environmental changes as well as socio-economic drivers on human migration, it remains a challenging task to find a method to analyse the knowledge from different scientific disciplines in an integrated way. The Sahel region with its high ecological dynamic has a long history of migratory movements. Within this work, we integrate and analyse socio- and natural-scientific data from two Sahelian study areas in Mali and Senegal using Bayesian belief networks. The core of the network's structure is formed by four main motives to migrate which are education, family, visit and curiosity and sustenance and employment. It is assumed that these motives determine the spatial and temporal patterns of migration. On the basis of submodels for each migration motive, we identify the decisive factors that constitute the socio-economic and ecological conditions with a combination of sensitivity analyses and train-and-test validation method. In combining these factors, the model avoids implying monocausal dependencies and allows an analytical view on the likely consequences of different settings of social-ecological conditions on migration. Furthermore, we use the model to estimate the consequences of alternative future developments in contrasting scenarios. The results show that changing environmental conditions lead to changing patterns of migration, regarding its duration and destination. These patterns can be very specific for different motives and their underlying factors. One principal result of the analysis is that uncertainty in the main income sources correlates with an increase of short-term migrations in order to increase the households' possibilities for income generation. Nevertheless, socio-economic conditions show a greater impact on the people's decision to migrate than environmental conditions. (C) 2015 Elsevier Ltd. All rights reserved.",""
"Probabilistic graphical models are powerful mathematical formalisms for machine learning and reasoning under uncertainty that are widely used for cognitive computing. However, they cannot be employed efficiently for large problems (with variables in the order of 100K or larger) on conventional systems, due to inefficiencies resulting from layers of abstraction and separation of logic and memory in CMOS implementations. In this paper, we present a magnetoelectric probabilistic technology framework for implementing probabilistic reasoning functions. The technology leverages straintronic magneto-tunneling junction (S-MTJ) devices in a novel mixed-signal circuit framework for direct computations on probabilities while enabling in-memory computations with persistence. Initial evaluations of the Bayesian likelihood estimation operation occurring during Bayesian Network inference indicate up to 127 x lower area, 214 x lower active power, and 70 x lower latency compared to an equivalent 45-nm CMOS Boolean implementation.","Initial evaluations of the Bayesian likelihood estimation operation occurring during Bayesian Network inference indicate up to 127 x lower area, 214 x lower active power, and 70 x lower latency compared to an equivalent 45-nm CMOS Boolean implementation."
"Recent research has shown the potential of using probabilistic graphical models to identify and visualize interactions in the Earth's climate system. Studying the resulting pathways is of great interest to scientists because it helps them learn subtle details about the underlying dynamical mechanisms governing our planet's climate.",""
"This work is about integrated analysis of data collected as official statistics with administrative data from operational systems in order to increase the quality of information. Information quality, or InfoQ, is the potential of a data set to achieve a specific goal by using a given empirical analysis method'. InfoQ is based on the identification of four interacting components: the analysis goal, the data, the data analysis and the utility, and it is assessed through eight dimensions: data resolution, data structure, data integration, temporal relevance, generalizability, chronology of data and goal, construct operationalization and communication. The paper illustrates, through case studies, a novel strategy to increase InfoQ based on the integration of official statistics with administrative data using copulas and Bayesian Networks. Official statistics are extraordinary sources of information. However, because of temporal relevance and chronology of data and goals, these fundamental sources of information are often not properly leveraged resulting in a poor level of InfoQ in the use of official statistics. This leads to low valued statistical analyses and to the lack of sufficiently informative results. By improving temporal relevance and chronology of data and goals, the use of Bayesian Networks allows us to calibrate official with administrative data, thus strengthening the quality of the information derived from official surveys, and, overall, enhancing InfoQ. We show, with examples, how to design and implement such a calibration strategy. Copyright (c) 2015 John Wiley & Sons, Ltd.",""
"We propose an improved discrete dynamical system model to reconstruct the gene regulatory network (GRN), then estimate the variable topology using discrete-time autosynchronization and predict the expression rate under the same condition. Although our method adopts a small number of sample time points to estimate the GRN, it could discern not only the role of the activator or repressor for each specific regulator, but also the regulatory ability of the regulator to the transcription rate of the target gene. Several examples are illustrated to verify that this method is feasible and effective for modeling the GRN and predicting the expression profile in the next cell cycle, the expression profile in the interval between two sample time points or the deficiency data. Additionally, this method provides a general tool for topology estimation of discrete-time dynamical networks. (c) 2014 Wiley Periodicals, Inc.",""
"Mitigating crash severity on regional transportation roadways is an important concern in road safety research. This paper presents a comprehensive geospatial approach based on the fuzzy classification and regression tree (FCART) to predict motor vehicle crashes and their severity on two-lane, two-way roads. The combined use of fuzzy and decision tree in FCART model solves the uncertainty associated with input data; the model can be easily understood and interpreted because of its graphical tree structure. The FCART model uses fuzzy logic to resolve the difficulty of analyzing input variables where no definitive boundary exists between the categories. Moreover, a bagging algorithm is applied in the FCART model to deal with high-variance crash data and improve the performance of the learning process. The bagged-FCART algorithm is tested against FCART, the classification and regression tree (CART), and the support vector machine (SVM) as inferential engines to predict crash severity and uncover spatial and nonspatial factors that systematically relate to crash severity. The results show that applying the bagging algorithm in the FCART model considerably improves the prediction accuracy and that the bagged-FCART model is superior to other tested models in predicting crash severity. A sensitivity analysis was also conducted to determine the importance of input factors. Parts of the results obtained from this analysis are consistent with the existing traffic safety literature and demonstrate that vehicle failure, drivers wearing seat belts, and weather condition factors are some of the most important factors contributing to crash severity. The proposed approach illustrates that in addition to these factors, geographical factors such as proximity to curves and adjacent facilities and land use have a significant effect on crash severity. These results support the prioritization of effective safety measures that are geographically targeted and behaviorally sound on two-lane, two-way roads. (c) 2014 American Society of Civil Engineers.","This paper presents a comprehensive geospatial approach based on the fuzzy classification and regression tree (FCART) to predict motor vehicle crashes and their severity on two-lane, two-way roads."
"An algorithmic framework that identifies irrelevant data (i.e., data that may be ignored without any loss of optimality) at agents of a sequential team is presented. This framework relies on capturing the properties of a sequential team that do not depend on the specifics of state spaces, the probability law, the system dynamics, or the cost functions. To capture these properties the notion of a team form is developed. A team form is then modeled as a directed acyclic graph and irrelevant data is identified using D-separation properties of specific subsets of nodes in the graph. This framework provides an algorithmic procedure for identifying and ignoring irrelevant data at agents, and thereby simplifying the form of control laws that need to be implemented. (C) 2015 Elsevier Ltd. All rights reserved.",""
"1. The ecological health of rivers worldwide continues to decline despite increasing effort and investment in river science and management. Bayesian belief networks (BBNs) are increasingly being used as a mechanism for decision-making in river management because they provide a simple visual framework to explore different management scenarios for the multiple stressors that impact rivers. However, most applications of BBN modelling to resource management use expert knowledge and/or limited real data, and fail to accurately assess the ability of the model to make predictions. 2. We developed a BBN to model ecological condition in a New Zealand river using field/GIS data (from multiple rivers), rather than expert opinion, and assessed its predictive ability on an independent data set. The developed BBN performed moderately better than a number of other modelling techniques (e.g. artificial neural networks, classification trees, random forest, logistic regression), although model construction was more time consuming. Thus, the predictive ability of BBNs is (in this case at least) on a par with other modelling methods but the approach is distinctly better for its ability to visually present the data linkages, issues and potential outcomes of management options in real time. 3. The BBN suggested management of habitat quality, such as riparian planting, along with the current management focus on limiting nutrient leaching from agricultural land may be most effective in improving ecological condition. 4. BBNs can be a powerful and accurate method of effectively portraying the multiple interacting drivers of environmental condition in an easily understood manner. However, most BBN applications fail to appropriately test the model fit prior to use. We believe this lack of testing may seriously undermine their long-term effectiveness in resource management and recommend that BBNs should be used in conjunction with some measure of uncertainty about model predictions. We have demonstrated this for a BBN of ecological condition in a New Zealand river, shown that model fit is better than that for other modelling techniques, and that improving habitat would be equally effective to reducing nutrients to improve ecological condition.","artificial neural networks, classification trees, random forest, logistic regression), although model construction was more time consuming."
"In this paper, we discuss the construction of a multivariate generalisation of the Dirichlet-multinomial distribution. An example from forensic genetics in the statistical analysis of DNA mixtures motivates the study of this multivariate extension. In forensic genetics, adjustment of the match probabilities due to remote ancestry in the population is often done using the so-called theta-correction. This correction increases the probability of observing multiple copies of rare alleles in a subpopulation and thereby reduces the weight of the evidence for rare genotypes. A recent publication by Cowell et al. (2015) showed elegantly how to use Bayesian networks for efficient computations of likelihood ratios in a forensic genetic context. However, their underlying population genetic model assumed independence of alleles, which is not realistic in real populations. We demonstrate how the so-called theta-correction can be incorporated in Bayesian networks to make efficient computations by modifying the Markov structure of Cowell et al. (2015). By numerical examples, we show how the theta-correction incorporated in the multivariate Dirichlet-multinomial distribution affects the weight of evidence. (C) 2015 Elsevier Inc. All rights reserved.",""
"Efficiently processing massive data is a big issue in high-speed network intrusion detection, as network traffic has become increasingly large and complex. In this work, instead of constructing a large number of features from massive network traffic, the authors aim to select the most important features and use them to detect intrusions in a fast and effective manner. The authors first employed several techniques, that is, information gain (IG), wrapper with Bayesian networks (BN) and Decision trees (C4.5), to select important subsets of features for network intrusion detection based on KDD'99 data. The authors then validate the feature selection schemes in a real network test bed to detect distributed denial-of-service attacks. The feature selection schemes are extensively evaluated based on the two data sets. The empirical results demonstrate that with only the most important 10 features selected from all the original 41 features, the attack detection accuracy almost remains the same or even becomes better based on both BN and C4.5 classifiers. Constructing fewer features can also improve the efficiency of network intrusion detection."," classifiers."
"Image segmentation plays an important role in many medical applications. The threshold based medical image segmentation approach is the most common and effective method for medical image segmentation. However, it is disadvantageous in high complexity, poor real time capability and premature convergence and etc. To address these issues, an improved evolution strategy is proposed for medical image segmentation. There are 2 populations are involved during the evolution, in which one focuses on local search for optimal solution. The other population is implemented based on chaotic theory. It focuses on global search in order to keep the variety of individuals and can jump out from the local maximum to overcome premature convergence issue. Besides, this paper also establishes the encoding scheme, fitness function, and evolution operators. As validated by the experimental results, the effectiveness and efficiency of the proposed have been confirmed.",""
"This paper aims to develop a data-driven approximate causal inference model using the newly-proposed evidential reasoning (ER) rule. The ER rule constitutes a generic conjunctive probabilistic reasoning process and generalises Dempster's rule and Bayesian inference. The belief rule based (BRB) methodology was developed to model complicated nonlinear causal relationships between antecedent attributes and consequents on the basis of the ER algorithm and traditional IF-THEN rule-based systems, and in essence it keeps methodological consistency with Bayesian Network (BN). In this paper, we firstly introduce the ER rule and then analyse its inference patterns with respect to the bounded sum of individual support and the orthogonal sum of collective support from multiple pieces of independent evidence. Furthermore, we propose an approximate causal inference model with the kernel mechanism of data-based approximate causal modelling and optimal learning. The exploratory approximate causal inference model inherits the main strengths of BN, BRB and relevant techniques, and can potentially extend the boundaries of applying approximate causal inference to complex decision and risk analysis, system identification, fault diagnosis, etc. A numerical study on the practical pipeline leak detection problem demonstrates the applicability and capability of the proposed data-driven approximate causal inference model. (C) 2015 Elsevier B.V. All rights reserved.","This paper aims to develop a data-driven approximate causal inference model using the newly-proposed evidential reasoning (ER) rule."
"There is a widespread need for the use of quantitative microbial risk assessment (QMRA) to determine reclaimed water quality for specific uses, however neither faecal indicator levels nor pathogen concentrations alone are adequate for assessing exposure health risk. The aim of this study was to build a conceptual model representing factors contributing to the microbiological health risks of reusing water treated in maturation ponds. This paper describes the development of an unparameterised model that provides a visual representation of theoretical constructs and variables of interest. Information was collected from the peer-reviewed literature and through consultation with experts from regulatory authorities and academic disciplines. In this paper we explore how, considering microbial risk as a modular system, following the QMRA framework enables incorporation of the many factors influencing human exposure and dose response, to better characterise likely human health impacts. By using and expanding upon the QMRA framework we deliver new insights into this important field of environmental exposures. We present a conceptual model of health risk of microbial exposure which can be used for maturation ponds and, more importantly, as a generic tool to assess health risk in diverse wastewater reuse scenarios. (C) 2015 Elsevier Ltd. All rights reserved.",""
"Invasive species pose a substantial risk to native biodiversity. As distributions of invasive species shift in response to changes in climate so will management priorities and investment. To develop cost-effective invasive species management strategies into the future it is necessary to understand how species distributions are likely to change over time and space. For most species however, few data are available on their current distributions, let alone projected future distributions. We demonstrate the benefits of Bayesian Networks (BNs) for projecting distributions of invasive species under various climate futures, when empirical data are lacking. Using the introduced pasture species, buffel grass (Cenchrus ciliaris) in Australia as an example, we employ a framework by which expert knowledge and available empirical data are used to build a BN. The framework models the susceptibility and suitability of the Australian continent to buffel grass colonization using three invasion requirements; the introduction of plant propagules to a site, the establishment of new plants at a site, and the persistence of established, reproducing populations. Our results highlight the potential for buffel grass management to become increasingly important in the southern part of the continent, whereas in the north conditions are projected to become less suitable. With respect to biodiversity impacts, our modelling suggests that the risk of buffel grass invasion within Australia's National Reserve System is likely to increase with climate change as a result of the high number of reserves located in the central and southern portion of the continent. In situations where data are limited, we find BNs to be a flexible and inexpensive tool for incorporating existing process-understanding alongside bioclimatic and edaphic variables for projecting future distributions of species invasions.",""
"A new method for model selection for Gaussian Bayesian networks and Markov networks, with extensions towards ancestral graphs, is constructed to have good mean squared error properties. The method is based on the focused information criterion, and offers the possibility of fitting individual-tailored models. The focus of the research, that is, the purpose of the model, directs the selection. It is shown that using the focused information criterion leads to a graph with small mean squared error. The low mean squared error ensures accurate estimation using a graphical model; here estimation rather than explanation is the main objective. Two situations that commonly occur in practice are treated: a data-driven estimation of a graphical model and the improvement of an already pre-specified feasible model. The search algorithms are illustrated by means of data examples and are compared with existing methods in a simulation study.",""
"In video-based human gesture recognition, it is very important to combine useful features and analyze the dynamic structure thereof as efficiently as possible. In this paper, we proposed a dynamic Bayesian network model that is a simplified model of dynamics at the level of hidden variables and employs observation windows of observation time slices for robust modeling and handling of noise and other variabilities. The proposed Simplified dynamic Bayesian network (DBN) was tested on a gesture database and an American sign language database. According to the experiments, the proposed DBN outperformed other methods: Conditional Random Fields (CRFs), conventional Bayesian Networks (BNs), DBNs, and Hidden Markov Models (HMMs). The proposed DBN achieved 98 % recognition accuracy in gesture recognition and 94.6 % in ASL recognition whereas the HMM and the CRF did 80 and 86 % in gesture recognition and 75.4 and 85.4 % in ASL (American Sign Language) recognition, respectively.",""
"In many safety-critical systems, it is necessary to maintain operators' situation awareness at a high level to ensure the safety of operations. Today, in many such systems, operators have to rely on the principles and design of human-system interfaces (HSIs) to observe and comprehend the overwhelming amount of process data. Thus, poor HSIs may cause serious consequences, such as Occupational accidents and diseases including stress, and they have therefore been considered an emerging risk. Despite the importance of this, very few methods have as yet been developed to assess the risk of HSIs. This paper presents a new risk assessment method that relies upon operators' mental models, human reliability analysis (HRA) event tree, and the situation awareness global assessment technique (SAGAT) to produce a risk profile for the intended HSI. In the proposed method, the operator's understanding (i.e. mental models) about possible abnormal situations in the intended plant is modeled on the basis of the capabilities of Bayesian networks. The situation models are combined with the HRA event tree, which paves the way for the incorporation of operator responses in the assessment method. Probe questions in line with the SAGAT through simulated scenarios in a virtual environment are then administrated to gather operator responses. Finally, the proposed method determines a risk level for the HSI by assigning the operator responses to the developed situational networks. The performance of the proposed method is investigated through a case study at a chemical plant. (C) 2015 Elsevier Ltd. All rights reserved.",""
"The evolution of domino scenarios triggered by fire critically depends on the presence and the performance of safety barriers that may have the potential to prevent escalation, delaying or avoiding the heat-up of secondary targets. The aim of the present study is the quantitative assessment of safety barrier performance in preventing the escalation of fired domino scenarios. A LOPA (layer of protection analysis) based methodology, aimed at the definition and quantification of safety barrier performance in the prevention of escalation was developed. Data on the more common types of safety barriers were obtained in order to characterize the effectiveness and probability of failure on demand of relevant safety barriers. The methodology was exemplified with a case study. The results obtained define a procedure for the estimation of safety barrier performance in the prevention of fire escalation in domino scenarios. (C) 2015 Elsevier Ltd. All rights reserved.",""
"Dealing with large quantities of flammable and explosive materials, usually at high-pressure high-temperature conditions, makes process plants very vulnerable to cascading effects compared with other infrastructures. The combination of the extremely low frequency of cascading effects and the high complexity and interdependencies of process plants makes risk assessment and vulnerability analysis of process plants very challenging in the context of such events. In the present study, cascading effects were represented as a directed graph; accordingly, the efficacy of a set of graph metrics and measurements was examined in both unit and plant-wide vulnerability analysis of process plants. We demonstrated that vertex-level closeness and betweenness can be used in the unit vulnerability analysis of process plants for the identification of critical units within a process plant. Furthermore, the graph-level closeness metric can be used in the plant-wide vulnerability analysis for the identification of the most vulnerable plant layout with respect to the escalation of cascading effects. Furthermore, the results from the application of the graph metrics have been verified using a Bayesian network methodology. (C) 2015 Elsevier Ltd. All rights reserved.",""
"The issue of pharmaceuticals in the environment has caused increasing concern in the recent years and various strategies have been proposed to tackle this problem. This work describes a Bayesian network (BN)-based socioecological impact assessment of a set of measures aimed at reducing the entry of pharmaceuticals in the aquatic environment. The measures investigated were selected across three sectors: public health market, environmental politics and drug design innovation. The BN model was developed for two drugs, Metformin and Metoprolol, and it models the distribution of the Predicted Environmental Concentration (PEC) values as a function of different measures. Results show that the sensitivity of the PEC for the two drugs to the measures investigated reflects the distinct drug characteristics, suggesting that in order to ensure the successful reduction of a broad range of substances, a spectrum of measures targeting the entire lifecycle of a pharmaceutical should be implemented. Furthermore, evaluation of two scenarios reflecting different emission management strategies highlights that the integrated implementation of a comprehensive set of measures across the three sectors results in a more extensive reduction of the contamination. Finally, the BN provides an initial forecasting tool to model the PEC of a drug as a function of a combination of measures in a context-specific manner and possible adaptations of the model are proposed. (c) 2015 Elsevier B.V. All rights reserved.",""
"The challenge of agility for adopting new business norms creates the need for measuring performance under changing conditions. This study aims to demonstrate the financial and non-financial consequences of implementing different combinations of lean techniques on the business performance. Bayesian Belief Network is used in studying the effects of factors under changing conditions. There are seven lean factors and four achievements studied to analyze the impact on three performance indicators. Bayesian Belief Network is constructed on the lean aspects that stimuli flexibility, reliability, quality and time of operations, which will have positive impacts on the financial, non-financial and sustainability performances of suppliers. A case study is carried out for suppliers in the automotive industry and scenarios with different combinations of lean factors are studied. This study gives a new vision in applying Bayesian network for business performance measures considering both the tangible and intangible results under changing business conditions. (C) 2015 Elsevier Ltd. All rights reserved.",""
"A unified Bayesian model that simultaneously performs behavioural modelling, information fusion and classification is presented. The model is expressed in the form of a dynamic Bayesian network (DBN). Behavioural modelling is performed by tracking the continuous dynamics of a entity and incorporating various contextual elements that influence behaviour. The entity is classified according to its behaviour. Classification is expressed as a conditional probability of the entity class given its tracked trajectory and the contextual elements. Inference in the DBN is performed using a derived Gaussian sum filter. The model is applied to classify vessels, according to their behaviour, in a maritime piracy situation. The novel aspects of this work include the unified approach to behaviour modelling and classification, the way in which contextual information is fused, the unique approach to classification according to behaviour and the associated derived Gaussian sum filter inference algorithm. (C) 2015 Elsevier Ltd. All rights reserved.","A unified Bayesian model that simultaneously performs behavioural modelling, information fusion and classification is presented."
"In this letter, we derive a new stepsize adaptation for the normalized least mean square algorithm (NLMS) by describing the task of linear acoustic echo cancellation from a Bayesian network perspective. Similar to the well-known Kalman filter equations, we model the acoustic wave propagation from the loudspeaker to the microphone by a latent state vector and define a linear observation equation (to model the relation between the state vector and the observation) as well as a linear process equation (to model the temporal progress of the state vector). Based on additional assumptions on the statistics of the random variables in observation and process equation, we apply the expectation-maximization (EM) algorithm to derive an NLMS-like filter adaptation. By exploiting the conditional independence rules for Bayesian networks, we reveal that the resulting EM-NLMS algorithm has a stepsize update equivalent to the optimal-stepsize calculation proposed by Yamamoto and Kitayama in 1982, which has been adopted in many textbooks. As main difference, the instantaneous stepsize value is estimated in the M step of the EM algorithm (instead of being approximated by artificially extending the acoustic echo path). The EM-NLMS algorithm is experimentally verified for synthesized scenarios with both, white noise and male speech as input signal.",""
"Conditional independence provides an essential framework to deal with knowledge and uncertainty in Artificial Intelligence, and is fundamental in probability and multivariate statistics. Its associated implication problem is paramount for building Bayesian networks. Unfortunately, the problem does not enjoy an axiomatization, by a finite set of Horn rules, and is already coNP-complete to decide for some fragments of conditional independencies. Saturated conditional independencies form an important fragment whose implication problem is decidable in almost linear time. We establish an axiomatization, by a finite set of Horn rules, for the fragment of generalized saturated conditional independencies. These state the conditional independence between finitely many sets of random variables. The special case of two sets captures Geiger and Pearl's finite axiomatization for saturated conditional independencies. Even for this special case, our completeness proof is new. The proof argument utilizes special probability models where two events have probability one half. Special probability models allow us to establish an equivalence between the implication of generalized saturated conditional independencies and that of formulae in a Boolean propositional fragment. This duality is then extended to a trinity including the implication problem of Delobel's full first-order hierarchical database decompositions. Already for the independence between two sets of random variables, the dualities cannot be extended to cover conditional independencies in general, or first-order hierarchical decompositions. (c) 2015 Elsevier B.V. All rights reserved.",""
"Increasing trend in global business integration and movement of material around the world has caused supply chain system susceptible to disruption involving higher risks. This paper presents a methodology for supplier selection in a global sourcing environment by considering multiple cost and risk factors. Failure modes and effects analysis technique from reliability engineering field and Bayesian belief networks are used to quantify the risk posed by each factor. The probability and the cost of each risk are then incorporated into a decision tree model to compute the total expected costs for each supply option. The supplier selection decision is made based on the total purchasing costs including both deterministic costs (such as product and transportation costs) and the risk-associated costs. The proposed approach is demonstrated using an example of a US-based Chemical distributor. This framework provides a visual tool for supply chain managers to see how cost and risks are distributed across the different alternatives. Lastly, managers can calculate expected value of perfect information to avoid a certain risk.",""
"Currently, Bayesian Networks (BNs) have become one of the most complete, self-sustained and coherent formalisms used for knowledge acquisition, representation and application through computer systems. However, learning of BNs structures from data has been shown to be an NP-hard problem. It has turned out to be one of the most exciting challenges in machine learning. In this context, the present work's major objective lies in setting up a further solution conceived to be a remedy for the intricate algorithmic complexity imposed during the learning of BN-structure with a massively-huge data backlog. (C) 2015 Elsevier B.V. All rights reserved.",""
"Landscape physiography affects temperature, soil moisture and solar radiation. In turn, these variables are thought to determine how species are distributed across landscapes. Systems involving direct and indirect associations between variables can be described using path models. However, studies applying these to species distribution modelling are rare. Bayesian Networks are path models designed to represent associations across observed variables. Here, we demonstrate the use of Bayesian Networks to disentangle the direct and indirect associations between landscape physiography, soil moisture, solar radiation, temperature and the distribution patterns of four plants at their northern range limit in Sweden. Fine scale variations in maximum temperatures were associated with variations in elevation, distance to coast and solar radiation. In contrast, fine scale variations in minimum temperature were associated with distance to coast, cold air drainage and soil moisture. These associations between landscape physiography and minimum and maximum temperature were predicted, furthermore, to be associated with growing season length, growing degree day and ultimately species distributions. All species were indirectly associated with aspect through their responses to either solar radiation or temperature. The models demonstrated strong indirect associations between landscape physiography and species distributions. The models suggested that local variation in light can be as important as temperature for species distributions. Disentangling the direct and indirect associations between landscape physiography, environmental variables and species distribution can provide new and important insights into how landscape components are linked to species distributions. (C) 2015 Elsevier B.V. All rights reserved.",""
"The conditional script questionnaire (CSQ) makes possible to study the conditions under which drivers find it legitimate to transgress the Highway Code. In this paper, we propose to analyse conditional respect towards the pedestrian with a new methodology based on Bayesian networks (BN). This methodology is designed to give a useful decision support tool for the analyst. Starting from data encoded in the CSQ, we use structure learning algorithms in order to build a BN. Then, we exploit it for two purposes: to extract new knowledge about the main topics expressed in the CSQ and to make inferences. This methodology helps to better understand the behaviour of drivers interacting with pedestrians and what might be the cause of their decisions of legitimate transgressions. The efficiency of the methodology proposed here is illustrated and a context-dependent mapping' of the legitimate transgressions established.","Then, we exploit it for two purposes: to extract new knowledge about the main topics expressed in the CSQ and to make inferences."
"In cognitive radio networks where secondary users (SUs) use the time-frequency gaps of primary users' (PUs) licensed spectrum opportunistically, the experienced throughput of SUs depend not only on the traffic load of the PUs but also on the PUs' service type. Each service has its own pattern of channel usage, and if the SUs know the dominant pattern of primary channel usage, then they can make a better decision on choosing which service is better to be used at a specific time to get the best advantage of the primary channel, in terms of higher achievable throughput. However, it is difficult to inform directly SUs of PUs' dominant used services in each area, for practical reasons. This paper proposes a learning mechanism embedded in SUs to sense the primary channel for a specific length of time. This algorithm recommends the SUs upon sensing a free primary channel, to choose the best service in order to get the best performance, in terms of maximum achieved throughput and the minimum experienced delay. The proposed learning mechanism is based on a Bayesian approach that can predict the performance of a requested service for a given SU. Simulation results show that this service selection method outperforms the blind opportunistic SU service selection, significantly.",""
"The multifaceted nature of cyber-physical systems needs holistic study methods to detect essential aspects and interrelations among physical and cyber components. Like the systems themselves, security threats feature both cyber and physical elements. Although to apply divide et impera approaches helps handling system complexity, to consider just one aspect at a time does not provide adequate risk awareness and hence does not allow to design the most appropriate countermeasures. To support this claim, in this paper we provide a joint application of two model-driven techniques for physical and cyber-security evaluation. We apply two UML profiles, namely SecAM (for cyber-security) and CIP_VAM (for physical security), in combination. In such a way, we demonstrate the synergy between both profiles and the need for their tighter integration in the context of a reference case study from the railway domain. (C) 2015 Elsevier Ltd. All rights reserved.",""
"Grape berry maturation depends on complex and coupled physiological and biochemical reactions which are climate dependant. Moreover one experiment represents one year and the climate variability could not be covered exclusively by the experiments. Consequently, harvest mostly relies on expert prediction. A big challenge for the wine industry is nevertheless to be able to anticipate the reactions for sustainability purposes. We propose to implement a robust mathematical model able (1) to capitalize the heterogeneous fragmented available knowledge including data and expertise by means of probabilistic graphical approaches; and (2) to predict sugar, acidity and anthocyanin concentrations over the maturity. (C) 2015 Elsevier B.V. All rights reserved.",""
"For the past 50 years, the endangered Persian fallow deer (Dama mesopotamica) have been translocated to various sites throughout Iran. To better understand the varying degrees of success at the translocation sites, population growth rates were measured for all the sites, and factors believed to affect the growth rate, such as initial population structures of the translocated herds and habitat characteristics, were identified and modeled. The population growth rate was used as a proxy for translocation success. Quantitative ecological data for Persian fallow deer is scarce, but expert knowledge was readily available to inform and enhance fallow deer management options. We integrated the available quantitative data and qualitative information in a Bayesian Belief Network (BBN) model to predict Persian fallow deer translocation success. The BBN model was tested using scenarios based on previous translocations to 13 sites in Iran. It correctly predicted the success of translocated populations in 11 out of the 13 sites. This model may be used as a decision support tool for future translocations, and can assist in designing reintroduction programs of the Persian fallow deer. Moreover, it should be adapted to incorporate new knowledge as evidence of translocation successes and failures emerge. Although the BBN model was developed specifically for the translocation of Persian fallow deer, this approach can clearly be applied to design and assess the success of translocation programs of other endangered species, and may be extended to design and assess alternative conservation management strategies.",""
"Here, we synthesize conceptual frameworks, applied modeling approaches, and as case studies to highlight complex social-ecological system (SES) dynamics that inform environmental policy, conservation and management. Although a set of \"good practices\" about what constitutes a good SES study are emerging, there is still a disconnection between generating SES scientific studies and providing decisionrelevant information to policy makers. Classical single variable/hypothesis studies rooted in one or two disciplines are still most common, leading to incremental growth in knowledge about the natural or social system, but rarely both. The recognition of human dimensions is a key aspect of successful planning and implementation in natural resource management, ecosystem-based management, fisheries management, and marine conservation. The lack of social data relating to human-nature interactions in this particular context is now seen as an omission, which can often erode the efficacy of any resource management or conservation action. There have been repeated calls for a transdisciplinary approach to complex SESs that incorporates resilience, complexity science characterized by intricate feedback interactions, emergent processes, non-linear dynamics and uncertainty. To achieve this vision, we need to embrace diverse research methodologies that incorporate ecology, sociology, anthropology, political science, economics and other disciplines that are anchored in empirical data. We conclude that to make SES research most useful in adding practical value to conservation planning, marine resource management planning processes and implementation, and the integration of resilience thinking into adaptation strategies, more research is needed on (1) understanding social-ecological landscapes and seascapes and patterns that would ensure planning process legitimacy, (2) costs of transformation (financial, social, environmental) to a stable resilient social-ecological system, (3) overcoming place-based data collection challenges as well as modeling challenges. (C) 2015 Elsevier Ltd.",""
"Microarray-based transcriptomic analysis has been demonstrated to hold the opportunity to study the effects of human exposure to, e.g., chemical carcinogens at the whole genome level, thus yielding broad-ranging molecular information on possible carcinogenic effects. Since genes do not operate individually but rather through concerted interactions, analyzing and visualizing networks of genes should provide important mechanistic information, especially upon connecting them to functional parameters, such as those derived from measurements of biomarkers for exposure and carcinogenic risk. Conventional methods such as hierarchical clustering and correlation analyses are frequently used to address these complex interactions but are limited as they do not provide directional causal dependence relationships. Therefore, our aim was to apply Bayesian network inference with the purpose of phenotypic anchoring of modified gene expressions. We investigated a use case on transcriptomic responses to cigarette smoking in humans, in association with plasma cotinine levels as biomarkers of exposure and aromatic DNA-adducts in blood cells as biomarkers of carcinogenic risk. Many of the genes that appear in the Bayesian networks surrounding plasma cotinine, and to a lesser extent around aromatic DNA-adducts, hold biologically relevant functions in inducing severe adverse effects of smoking. In conclusion, this study shows that Bayesian network inference enables unbiased phenotypic anchoring of transcriptomics responses. Furthermore, in all inferred Bayesian networks several dependencies are found which point to known but also to new relationships between the expression of specific genes, cigarette smoke exposure, DNA damaging-effects, and smoking-related diseases, in particular associated with apoptosis, DNA repair, and tumor suppression, as well as with autoimmunity.","Therefore, our aim was to apply Bayesian network inference with the purpose of phenotypic anchoring of modified gene expressions."
"The increasing prevalence of diabetes and its related complications is raising the need for effective methods to predict patient evolution and for stratifying cohorts in terms of risk of developing diabetes-related complications. In this paper, we present a novel approach to the simulation of a type 1 diabetes population, based on Dynamic Bayesian Networks, which combines literature knowledge with data mining of a rich longitudinal cohort of type 1 diabetes patients, the DCCT/EDIC study. In particular, in our approach we simulate the patient health state and complications through discretized variables. Two types of models are presented, one entirely learned from the data and the other partially driven by literature derived knowledge. The whole cohort is simulated for fifteen years, and the simulation error (i.e. for each variable, the percentage of patients predicted in the wrong state) is calculated every year on independent test data. For each variable, the population predicted in the wrong state is below 10% on both models over time. Furthermore, the distributions of real vs. simulated patients greatly overlap. Thus, the proposed models are viable tools to support decision making in type 1 diabetes. (C) 2015 Elsevier Inc. All rights reserved.",""
"We introduce a new model called the observed time conjunctive Bayesian network (OT-CBN) that describes the accumulation of genetic events (mutations) under partial temporal ordering constraints. Unlike other CBN models, the OT-CBN model uses sampling time points of genotypes in addition to genotypes themselves to estimate model parameters. We developed an expectation-maximization algorithm to obtain approximate maximum likelihood estimates by accounting for this additional information. In a simulation study, we show that the OT-CBN model outperforms the continuous time CBN (CT-CBN) (Beerenwinkel and Sullivant, 2009. Markov models for accumulating mutations. Biometrika 96(3), 645-661), which does not take into account individual sampling times for parameter estimation. We also show superiority of the OT-CBN model on several datasets of HIV drug resistance mutations extracted from the Swiss HIV Cohort Study database.",""
"Many coastal regions are encountering issues with the spread of nonindigenous species (NIS). In this study, we conducted a regional risk assessment using a Bayesian network relative risk model (BN-RRM) to analyze multiple vectors of NIS introductions to Padilla Bay, Washington, a National Estuarine Research Reserve. We had 3 objectives in this study. The 1st objective was to determine whether the BN-RRM could be used to calculate risk from NIS introductions for Padilla Bay. Our 2nd objective was to determine which regions and endpoints were at greatest risk from NIS introductions. Our 3rd objective was to incorporate a management option into the model and predict endpoint risk if it were to be implemented. Eradication can occur at different stages of NIS invasions, such as the elimination of these species before being introduced to the habitat or removal of the species after settlement. We incorporated the ballast water treatment management scenario into the model, observed the risk to the endpoints, and compared this risk with the initial risk estimates. The model results indicated that the southern portion of the bay was at greatest risk because of NIS. Changes in community composition, Dungeness crab, and eelgrass were the endpoints most at risk from NIS introductions. The currents node, which controls the exposure of NIS to the bay from the surrounding marine environment, was the parameter that had the greatest influence on risk. The ballast water management scenario displayed an approximate 1% reduction in risk in this Padilla Bay case study. The models we developed provide an adaptable template for decision makers interested in managing NIS in other coastal regions and large bodies of water. (C) 2015 SETAC",""
"A rapid increase in maritime traffic together with challenging navigation conditions and a vulnerable ecosystem has evoked calls for improving maritime safety in the Gulf of Finland, the Baltic Sea. It is suggested that these improvements will be the result of adopting a regionally effective proactive approach to safety policy formulation and management. A proactive approach is grounded on a formal process of identifying, assessing and evaluating accident risks, and adjusting policies or management practices before accidents happen. Currently, maritime safety is globally regulated by internationally agreed prescriptive rules, which are usually revised in reaction to accidents. The proactive Formal Safety Assessment (FSA) is applied to risks common to a ship type or to a particular hazard, when deemed necessary, whereas regional FSA applications are rare. An extensive literature review was conducted in order to examine the opportunities for developing a framework for the GoF for handling regional risks at regional level. Best practices were sought from nuclear safety management and fisheries management, and from a particular case related to maritime risk management. A regional approach that sees maritime safety as a holistic system, and manages it by combining a scientific risk assessment with stakeholder input to identify risks and risk control options, and to evaluate risks is proposed. A regional risk governance framework can improve safety by focusing on actual regional risks, designing tailor-made safety measures to control them, enhancing a positive safety culture in the shipping industry, and by increasing trust among all involved. (C) 2015 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license (http://cceativecommons.org/licemesiby-nc-nd/4.0/).",""
"Soils need to be resilient to deliver the functions required of them when subjected to perturbations within the context of short- and long-term environmental change. A better understanding of the basis of resilience will likely underpin improved management of poorer soils to enhance their resilience, as well as allowing effective management of all soils in the longer term. In this study, resilience was defined as resistance (degree of change) coupled with recovery (rate and extent of subsequent recovery) from a disturbance. Modelling of factors that drive resilience of a prescribed physical (void ratio) and biological (respiration) soil function was carried out on published data pertaining to 38 English and Scottish soils. This revealed that soil taxonomic class, parent material and soil texture appeared dominant in determining soil resilience in general, and aspects of the soil microbial community were also pertinent. It is notable that land use and organic matter content, which are commonly hypothesized to be influential in this regard, ranked amongst the lowest significant factors. However, these conclusions are based upon the very limited coherent data sets currently available. The key implication of an apparent context dependency of the resistance and resilience phenomena is that management of them is likely to be possible, but not via a single or direct approach. It may require specific approaches in particular circumstances, and it is possible that the system-level configuration of the soil is of greater consequence than individual factors. Hence, a system-level approach to management of soils is likely to be the most effective strategy, and this should be considered in developing policy scenarios in relation to soil management.",""
"Fog events occur at Melbourne Airport, Melbourne, Victoria, Australia, approximately 12 times each year. Unforecast events are costly to the aviation industry, cause disruption, and are a safety risk. Thus, there is a need to improve operational fog forecasting. However, fog events are difficult to forecast because of the complexity of the physical processes and the impact of local geography and weather elements. Bayesian networks (BNs) are a probabilistic reasoning tool widely used for prediction, diagnosis, and risk assessment in a range of application domains. Several BNs for probabilistic weather prediction have been previously reported, but to date none have included an explicit forecast decision component and none have been used for operational weather forecasting. A Bayesian decision network [Bayesian Objective Fog Forecast Information Network (BOFFIN)] has been developed for fog forecasting at Melbourne Airport based on 34 years' worth of data (1972-2005). Parameters were calibrated to ensure that the network had equivalent or better performance to prior operational forecast methods, which led to its adoption as an operational decision support tool. The current study was undertaken to evaluate the operational use of the network by forecasters over an 8-yr period (2006-13). This evaluation shows significantly improved forecasting accuracy by the forecasters using the network, as compared with previous years. BOFFIN-Melbourne has been accepted by forecasters because of its skill, visualization, and explanation facilities, and because it offers forecasters control over inputs where a predictor is considered unreliable.",""
"Rainfall is a fundamental process in the hydrologic cycle. This study investigated the cause-effect relationship in which precipitation at lower frequencies affects the amount of emitted radiation and at higher frequencies affects the amount of backscattered terrestrial radiation. Because the advantage of a probabilistic graphical model is its graphical representation, which allows easy causality interpretation using the arc directions, two Bayesian networks (BNs) were used, namely, a naive Bayes classifier and a tree-augmented naive Bayes model. To empirically evaluate and compare BN-based models, \"black box\"-based models, including nearest-neighbor searches and artificial neural network (ANN)-based multilayer perceptron and logistic regression, were used as benchmarks. For the two study regions-namely, the Tanshui River basin in northern Taiwan and Chianan Plain in southern Taiwan-rain occurrences during typhoon seasons were examined using passive microwave imagery recorded using the Special Sensor Microwave Imager/Sounder. The results show that although black box models exhibit excellent prediction ability, interpretation of their behavior is unsatisfactory. By contrast, probabilistic graphical models can explicitly reveal the causal relationship between brightness temperatures and nonrain/rain discrimination. For the Tanshui River basin, 19.35-, 22.23-, 37.0-, and 85.5-GHz vertically polarized brightness temperatures were found to diagnose rain occurrences. For the Chianan Plain, a more sensitive indicator of rain-scattering signals was obtained using 85-GHz measurements. The results demonstrate the potential use of BNs in identifying rain occurrences in regions with land features comprising various absorbing and scattering materials.","Because the advantage of a probabilistic graphical model is its graphical representation, which allows easy causality interpretation using the arc directions, two Bayesian networks (BNs) were used, namely, a naive Bayes classifier and a tree-augmented naive Bayes model."
"The main interest of this paper is to illustrate a new representation of the Principal Component Analysis (PCA) for fault detection under a Conditional Gaussian Network (CGN), a special case of Bayesian networks. PCA and its associated quadratic statistics such as T-2 and SPE are integrated under a sole CGN. The proposed framework projects a new observation into an orthogonal space and gives probabilities on the state of the system. It could do so even when some data in the sample test are missing. This paper also gives the probabilities thresholds to use in order to match quadratic statistics decisions. The proposed network is validated and compared to the standard PCA scheme for fault detection on the Tennessee Eastman Process and the Hot Forming Process. (C) 2015 Elsevier Ltd. All rights reserved.",""
"Although agricultural ecosystems can provide humans with a wide set of benefits agricultural production system management is mainly driven by food production. As a consequence, a need to ensure food security globally has been accompanied by a significant decline in the state of ecosystems. In order to reduce negative trade-offs and identify potential synergies it is necessary to improve our understanding of the relationships between various ecosystem services (ES) as well as the impacts of farm management on ES provision. We present a spatially explicit application that captures and quantifies ES trade-offs in the crop systems of Llanada Alavesa in the Basque Country. Our analysis presents a quantitative assessment of selected ES including crop yield, water supply and quality, climate regulation and air quality. The study is conducted using semantic meta-modeling, a technique that enables flexible integration of models to overcome the service-by-service modeling approach applied traditionally in ES assessment. (C) 2014 Elsevier Ltd. All rights reserved.",""
"Traditional machine learning algorithms depend heavily on the assumption that there is sufficient data to learn a reliable model. This is not always the case, and in situations where data is limited, transfer learning can be applied to compensate for the lack of information by learning from several sources. In this work, we present a novel methodology for inducing a Temporal Nodes Bayesian Network (TNBN) when training data is scarce by applying a transfer learning strategy. A TNBN is a probabilistic graphical model that offers a compact representation for dynamic domains by defining multiple time intervals in which events can occur. Learning a TNBN poses additional challenges to learning traditional Bayesian networks due to the incorporation of time intervals. Our proposal incorporates novel approaches to transfer knowledge from several TNBNs to learn the structure, parameters and intervals of a target TNBN. To evaluate our algorithm, we performed experiments with a synthetic network, where we created auxiliary models by altering the structure, parameters and temporal intervals of the original model. Results show that the proposed algorithm is capable of retrieving a reliable model even when few records are available for the target domain. We also performed experiments with a real-world data set belonging to the medical domain of HIV, where we were able to learn some documented mutational pathways and their temporal relations by applying transfer learning.",""
"Bayesian network (BN) is a strong framework for handling probabilistic events which have received limited attention in power system reliability assessment. By applying BN for bulk power system reliability assessment, additional capabilities are provided at modelling and analysis levels in comparison with conventional methods. This study proposes a methodology to apply BNs to composite power system (CPS) reliability modelling, reliability assessment and reliability-based analyses. A minimal cutset (MC)-based method is proposed to extract the BN structure. Moreover, BN parameters are defined based on logical relationships between components, MCs and system failure. In addition, some issues of BN application to large systems are investigated. A variety of reliability-based analyses, useful in different power system studies are introduced and discussed in details which are provided by applying BNs to CPS reliability. The computational efficiency of the presented method is demonstrated by comparing it with state enumeration and Monte Carlo simulation methods. The proposed methodology is implemented on Roy Billinton Test System to extract its BN model based on which analysis issues are considered. Finally, the proposed methodology is applied to the simple, but representative, system the IEEE-reliability test system to show its feasibility in larger systems.",""
"Classification and clustering of streaming data are relevant in finance, computer science, and engineering while they are becoming increasingly important in medicine and biology. Streaming data are analyzed with algorithms and models capable to represent dynamics, sequences and time. Dynamic Bayesian networks and hidden Markov models are commonly used to analyze streaming data. However, they are concerned with evenly spaced time series data and thus suffer from several limitations. Indeed, it is not clear how timestamps should be discretized even if some approaches to mitigate this problem have been recently made available. In this paper we describe the class of continuous time Bayesian networks classifiers and develop algorithms for their parametric and structural learning to solve classification and clustering of multivariate discrete state continuous time trajectories. Numerical experiments on synthetic and real world data are used to compare the performance of continuous time Bayesian network models to that achieved by dynamic Bayesian networks. In particular, post-stroke rehabilitation data is used for the classification task while urban traffic data from continuous time loop is used for the clusteirng task. The achieved results confirm the effectiveness of the proposed approaches.","Classification and clustering of streaming data are relevant in finance, computer science, and engineering while they are becoming increasingly important in medicine and biology."
"Early Warning Systems (EWS) are increasingly applied to mitigate the risks posed by natural hazards. To compare the effect of EWS with alternative risk reduction measures and to optimize their design and operation, their reliability and effectiveness must be quantified. In the present contribution, a framework approach to the evaluation of threshold-based EWS for natural hazards is presented. The system reliability is classically represented by the Probability of Detection (POD) and Probability of False Alarms (PFA). We demonstrate how the EWS effectiveness, which is a measure of risk reduction, can be formulated as a function of POD and PFA. To model the EWS and compute the reliability, we develop a framework based on Bayesian Networks, which is further extended to a decision graph, facilitating the optimization of the warning system. In a case study, the framework is applied to the assessment of an existing debris flow EWS. The application demonstrates the potential of the framework for identifying the important factors influencing the effectiveness of the EWS and determining optimal warning strategies and system configurations. (C) 2015 The Authors. Published by Elsevier Ltd.",""
"Background: One of the goals of the Systems Biology community is to have a detailed map of all biological interactions in an organism. One small yet important step in this direction is the creation of biological networks from post-genomic data. Bayesian networks are a very promising model for the inference of regulatory networks in Systems Biology. Usually, Bayesian networks are sampled with a Markov Chain Monte Carlo (MCMC) sampler in the structure space. Unfortunately, conventional MCMC sampling schemes are often slow in mixing and convergence. To improve MCMC convergence, an alternative method is proposed and tested with different sets of data. Moreover, the proposed method is compared with the traditional MCMC sampling scheme. Results: In the proposed method, a simpler and faster method for the inference of regulatory networks, Graphical Gaussian Models (GGMs), is integrated into the Bayesian network inference, trough a Hierarchical Bayesian model. In this manner, information about the structure obtained from the data with GGMs is taken into account in the MCMC scheme, thus improving mixing and convergence. The proposed method is tested with three types of data, two from simulated models and one from real data. The results are compared with the results of the traditional MCMC sampling scheme in terms of network recovery accuracy and convergence. The results show that when compared with a traditional MCMC scheme, the proposed method presents improved convergence leading to better network reconstruction with less MCMC iterations. Conclusions: The proposed method is a viable alternative to improve mixing and convergence of traditional MCMC schemes. It allows the use of Bayesian networks with an MCMC sampler with less iterations. The proposed method has always converged earlier than the traditional MCMC scheme. We observe an improvement in accuracy of the recovered networks for the Gaussian simulated data, but this improvement is absent for both real data and data simulated from ODE.","Bayesian networks are a very promising model for the inference of regulatory networks in Systems Biology."
"Economic growth is often based on the intensification of crop production, energy consumption and urbanization. In many cases, this leads to the degradation of aquatic ecosystems. Modelling water resources and the related identification of key drivers of change are essential to improve and protect water quality in river basins. This study evaluates the potential of Bayesian belief network models to predict the ecological water quality in a typical multifunctional and tropical river basin. Field data, expert knowledge and literature data were used to develop a set of Bayesian belief network models. The developed models were evaluated based on weighted Cohen's Kappa (K-w), percentage of correctly classified instances (CCI) and spherical payoff. On top, a sensitivity analysis and practical simulation tests of the two most reliable models were performed. Cross-validation based on K-w (Model 1: 0.44 +/- 0.08; Model 2: 0.44 +/- 0.11) and CCI (Model 1: 36.3 +/- 2.3; Model 2: 41.6 +/- 2.3) indicated that the performance was reliable and stable. Model 1 comprised of input variables main land use, elevation, sediment type, chlorophyll, flow velocity, dissolved oxygen, and chemical oxygen demand; whereas Model 2 did not include dissolved oxygen and chemical oxygen demand. Although the predictive performance of Model 2 was slightly higher than that of Model 1, simulation outcomes of Model 1 were more coherent. Additionally, more management options could be evaluated with Model I. As the model's ability to simulate management outcomes is of utmost importance in model selection, Model 1 is recommended as a tool to support decision-making in river management. Model predictions and sensitivity analysis indicated that flow velocity is the major variable determining ecological water quality and suggested that construction of additional dams and water abstraction within the basin would have an adverse effect on water quality. Although a case study in a single river basin is presented, the modelling approach can be of general use on any other river basin. (C) 2015 Elsevier B.V. All rights reserved.","The developed models were evaluated based on weighted Cohen's Kappa (K-w), percentage of correctly classified instances (CCI) and spherical payoff."
"We devise a novel inference algorithm to effectively solve the cancer progression model reconstruction problem. Our empirical analysis of the accuracy and convergence rate of our algorithm, CAncer PRogression Inference (CAPRI), shows that it outperforms the state-of-the-art algorithms addressing similar problems. Motivation: Several cancer-related genomic data have become available (e.g. The Cancer Genome Atlas, TCGA) typically involving hundreds of patients. At present, most of these data are aggregated in a cross-sectional fashion providing all measurements at the time of diagnosis. Our goal is to infer cancer 'progression' models from such data. These models are represented as directed acyclic graphs (DAGs) of collections of 'selectivity' relations, where a mutation in a gene A 'selects' for a later mutation in a gene B. Gaining insight into the structure of such progressions has the potential to improve both the stratification of patients and personalized therapy choices. Results: The CAPRI algorithm relies on a scoring method based on a probabilistic theory developed by Suppes, coupled with bootstrap and maximum likelihood inference. The resulting algorithm is efficient, achieves high accuracy and has good complexity, also, in terms of convergence properties. CAPRI performs especially well in the presence of noise in the data, and with limited sample sizes. Moreover CAPRI, in contrast to other approaches, robustly reconstructs different types of confluent trajectories despite irregularities in the data. We also report on an ongoing investigation using CAPRI to study atypical Chronic Myeloid Leukemia, in which we uncovered non trivial selectivity relations and exclusivity patterns among key genomic events.","We devise a novel inference algorithm to effectively solve the cancer progression model reconstruction problem."
"Tunnel squeezing or time-dependent large deformations due to creep are common in tunnels constructed in weak rock masses at large depth or subjected to high horizontal in situ stresses in tectonically active regions. Squeezing can produce tunnel collapses, budget overruns and construction delays, and being able to predict squeezing is therefore important. This study presents a novel application of Bayesian networks (BNs) to predict squeezing. In particular, we employ a Naive Bayes classifier based on five parameters - support stiffness (K), Rock Tunneling Quality Index (Q), tunnel depth (H), tunnel diameter (D), and strength-stress ratio (SSR) - about which information is commonly available at early design stages. The Naive Bayes classifier is \"learned\", using the Expectation Maximization algorithm, with a database of 166 tunneling case histories from 7 countries compiled by the authors which is provided as Supplementary material. Then, the junction Tree algorithm is employed for \"belief updating\"; i.e., to predict the probability of tunnel squeezing for a given set of (probably incomplete) evidence. The model is validated using 10-fold cross-validation and also using an additional set of case-histories that had not been originally employed to learn the network. Results show that, when compared with other available criteria, the error rate of our BN is among the lowest, but with the advantage that it is able to provide predictions even with incomplete data. Results of a sensitivity analysis to assess the importance of input parameters on the squeezing outcome are also presented. And, finally, a web-based implementation of the proposed BN is provided to improve the ease-of-use of our approach. (C) 2015 Elsevier B.V. All rights reserved.","In particular, we employ a Naive Bayes classifier based on five parameters - support stiffness (K), Rock Tunneling Quality Index (Q), tunnel depth (H), tunnel diameter (D), and strength-stress ratio (SSR) - about which information is commonly available at early design stages."
"Background: The Drosophila sex determination hierarchy is a classic example of a transcriptional regulatory hierarchy, with sex-specific isoforms regulating morphology and behavior. We use a structural equation modeling approach, leveraging natural genetic variation from two studies on Drosophila female head tissues - DSPR collection (596 F1-hybrids from crosses between DSPR sub-populations) and CEGS population (75 F1-hybrids from crosses between DGRP/Winters lines to a reference strain w1118) - to expand understanding of the sex hierarchy gene regulatory network (GRN). This approach is completely generalizable to any natural population, including humans. Results: We expanded the sex hierarchy GRN adding novel links among genes, including a link from fruitless (fru) to Sex-lethal (Sxl) identified in both populations. This link is further supported by the presence of fru binding sites in the Sxl locus. 754 candidate genes were added to the pathway, including the splicing factors male-specific lethal 2 and Rm62 as downstream targets of Sxl which are well-supported links in males. Independent studies of doublesex and transformer mutants support many additions, including evidence for a link between the sex hierarchy and metabolism, via Insulin-like receptor. Conclusions: The genes added in the CEGS population were enriched for genes with sex-biased splicing and components of the spliceosome. A common goal of molecular biologists is to expand understanding about regulatory interactions among genes. Using natural alleles we can not only identify novel relationships, but using supervised approaches can order genes into a regulatory hierarchy. Combining these results with independent large effect mutation studies, allows clear candidates for detailed molecular follow-up to emerge.",""
"As many devices equipped with various sensors have recently proliferated, the fusion methods of various information and data from different sources have been studied. Bayesian network is one of the popular methods that solve this problem to cope with the uncertainty and imprecision. However, because a monolithic Bayesian network has high computational and design complexities, it is hard to apply to realistic problems. In this paper, we propose a modular Bayesian network system to extract context information by cooperative inference of multiple modules, which guarantees reliable inference compared to a monolithic Bayesian network without losing its strength like the easy management of knowledge and scalability. The proposed method preserves inter-modular dependencies by virtual linking and has lower computational complexity in complicated environments. The inter-modular d-separation controls local information to be delivered only to relevant modules. We verify that the proposed modular Bayesian network is enough to keep inter-modular causalities in a time-saving manner. This paper shows a possibility that a context-aware system would be easily constructed by mashing up Bayesian network fractions independently designed or leaned in different domains. (C) 2015 Elsevier B.V. All rights reserved.","In this paper, we propose a modular Bayesian network system to extract context information by cooperative inference of multiple modules, which guarantees reliable inference compared to a monolithic Bayesian network without losing its strength like the easy management of knowledge and scalability."
"Identifiability of parameters is an essential property for a statistical model to be useful in most settings. However, establishing parameter identifiability for Bayesian networks with hidden variables remains challenging. In the context of finite state spaces, we give algebraic arguments establishing identifiability of some special models on small directed acyclic graphs (DAGs). We also establish that, for fixed state spaces, generic identifiability of parameters depends only on the Markov equivalence class of the DAG. To illustrate the use of these results, we investigate identifiability for all binary Bayesian networks with up to five variables, one of which is hidden and parental to all observable ones. Surprisingly, some of these models have parameterizations that are generically 4-to-one, and not 2-to-one as label swapping of the hidden states would suggest. This leads to interesting conflict in interpreting causal effects.",""
"Principal component analysis has been widely used in the process industries for the purpose of monitoring abnormal behaviour. The process of reducing dimension is obtained through PCA, while T-tests are used to test for abnormality. Some of the main contributions to the success of PCA is its ability to not only detect problems, but to also give some indication as to where these problems are located. However, PCA and the T-test make use of Gaussian assumptions which may not be suitable in process fault detection. A previous modification of this method is the use of independent component analysis (ICA) for dimension reduction combined with kernel density estimation for detecting abnormality; like PCA, this method points out location of the problems based on linear data-driven methods, but without the Gaussian assumptions. Both ICA and PCA, however, suffer from challenges in interpreting results, which can make it difficult to quickly act once a fault has been detected online. This paper proposes the use of Bayesian networks for dimension reduction which allows the use of process knowledge enabling more intelligent dimension reduction and easier interpretation of results. The dimension reduction technique is combined with multivariate kernel density estimation, making this technique effective for non-linear relationships with non-Gaussian variables. The performance of PCA, ICA and Bayesian networks are compared on data from an industrial scale plant. (C) 2015 ISA. Published by Elsevier Ltd. All rights reserved.",""
"A novel real-time reliability evaluation methodology is proposed by combining root cause diagnosis phase based on Bayesian networks (BNs) and reliability evaluation phase based on dynamic BNs (DBNs). The root cause diagnosis phase exactly locates the root cause of a complex mechatronic system failure in real time to increase diagnostic coverage and is performed through backward analysis of BNs. The reliability evaluation phase calculates the real-time reliability of the entire system by forward inference of DBNs. The application of the proposed methodology is demonstrated using a case of a subsea pipe ram blowout preventer system. The value and the variation trend of real-time system reliability when the faults of components occur are studied; the importance degree sequence of components at different times is also determined using mutual information and belief variance. (C) 2015 ISA. Published by Elsevier Ltd. All rights reserved.","The reliability evaluation phase calculates the real-time reliability of the entire system by forward inference of DBNs."
"Supply disruptions are attracting growing attention. Even in geographically, politically and economically stable locations, companies are exposed to disruptions, because they depend on their suppliers and suppliers' suppliers. The analysis of these disruptions helps mitigate risks: for example, instead of relying on local measures such as safety stock or insurance, a company can introduce new supply contracts or backup risky suppliers. In this article, we analyze risks caused by supplier disruptions by introducing concepts from probabilistic risk assessment (PRA), which is a widely employed methodology for the risk analysis of complex engineering systems. We apply PRA to examine simple networks such as triads analytically, and use simulation to analyze disruption risks in random networks of realistic size. We also illustrate how PRA can support strategic decisions such as whether or not to use single or multiple suppliers; which suppliers are more risky than others; and what impacts the complexity of the supply base has on the reliability of the supplier network.",""
"Accidental releases of hazardous chemicals from process facilities can cause catastrophic consequences. The Bhopal disaster resulting from a combination of inherently unsafe designs and poorly managed operations is a well-known case. Effective risk modeling approaches that provide early warnings are helpful to prevent and control such rare but catastrophic events. Probability estimation of these events is a constant challenge due to the scarcity of directly relevant data. Therefore, precursor-based methods that adopt the Bayesian theorem to update prior judgments on event probabilities using empirical data have been proposed. The updated probabilities are then integrated with consequences of varying severity to produce the risk profile. This paper proposes an operational risk assessment framework, in which a precursor-based Bayesian network approach is used for probability estimation, and loss functions are applied for consequence assessment. The estimated risk profile can be updated continuously given real-time operational data. As process facilities operate, this method integrates a failure-updating mechanism with potential consequences to generate a real-time operational risk profile. The real time risk profile is valuable in activating accident prevention and control strategies. The approach is applied to the Bhopal accident to demonstrate its applicability and effectiveness. (C) 2015 The Institution of Chemical Engineers. Published by Elsevier B.V. All rights reserved.",""
"In this article, Bayesian networks are used to model semiconductor lifetime data obtained from a cyclic stress test system. The data of interest are a mixture of log-normal distributions, representing two dominant physical failure mechanisms. Moreover, the data can be censored due to limited test resources. For a better understanding of the complex lifetime behavior, interactions between test settings, geometric designs, material properties, and physical parameters of the semiconductor device are modeled by a Bayesian network. Statistical toolboxes in MATLAB (R) have been extended and applied to find the best structure of the Bayesian network and to perform parameter learning. Due to censored observations Markov chain Monte Carlo (MCMC) simulations are employed to determine the posterior distributions. For model selection the automatic relevance determination (ARD) algorithm and goodness-of-fit criteria such as marginal likelihoods, Bayes factors, posterior predictive density distributions, and sum of squared errors of prediction (SSEP) are applied and evaluated. The results indicate that the application of Bayesian networks to semiconductor reliability provides useful information about the interactions between the significant covariates and serves as a reliable alternative to currently applied methods.",""
"In this study a Bayesian network (BN) has been built for the study of the objective motility of Tinca tinca spermatozoa (spz). Semen from eight 2-year-old sexually mature male tenchs was obtained and motility analyses were performed at 6-17, 23-34 and 40-51 s after activation, using computer-assisted sperm analysis (CASA) software. Motility parameters rendered by CASA were treated with a two-step cluster analysis. Three well-defined sperm subpopulations were identified, varying the proportion of spermatozoa contained in each cluster with time and male. Cluster, cinematic and time variables were used to build the BN to study the probabilistic relationships among variables and how each variable influenced the final sperm classification into one of three predefined clusters. Both network structure and conditional probabilities were calculated based on the collected data set. Results shown that almost all the variables were directly or indirectly related to each other. By doing probabilistic inference we observed that the cluster distribution corresponded to the definition provided by the cluster analysis. Also, velocity and time variables determined the cluster to which each spermatozoon belonged with a high degree of accuracy. Thus, BNs can be applied in the study of sperm motility. The construction of a BN that include fertility data opens a new way to try to clarify the roles of motility and other sperm quality indicators in fertilization.","Cluster, cinematic and time variables were used to build the BN to study the probabilistic relationships among variables and how each variable influenced the final sperm classification into one of three predefined clusters."
"The first high-temperature-reactor pebble-bed demonstration module (HTR-PM) is under construction currently in China. At the same time, development of a system that is used to support nuclear emergency response is in progress. The supporting system is expected to complete two tasks. The first one is diagnostics of the fault in the reactor based on abnormal sensor measurements obtained. The second one is prognostic of the accident progression based on sensor measurements obtained and operator actions. Both tasks will provide valuable guidance for emergency staff to take appropriate protective actions. Traditional method for the two tasks relies heavily on expert judgment, and has been proven to be inappropriate in some cases, such as Three Mile Island accident. To better perform the two tasks, dynamic Bayesian networks (DBN) is introduced in this paper and a pilot study based on the approach is carried out. DBN is advantageous in representing complex dynamic systems and taking full consideration of evidences obtained to perform diagnostics and prognostics. Pearl's loopy belief propagation (LBP) algorithm is recommended for diagnostics and prognostics in DBN. The DBN model of HTR-PM is created based on detailed system analysis and accident progression analysis. A small break loss of coolant accident (SBLOCA) is selected to illustrate the application of the DBN model of HTR-PM in fault diagnostics (FD) and accident progression prognostics (APP). Several advantages of DBN approach compared with other techniques are discussed. The pilot study lays the foundation for developing the nuclear emergency response supporting system (NERSS) for HTR-PM. (C) 2015 Elsevier B.V. All rights reserved.",""
"This paper presents an open-source software package, rSCA, which is developed based upon a stepwise cluster analysis method and serves as a statistical tool for modeling the relationships between multiple dependent and independent variables. The rSCA package is efficient in dealing with both continuous and discrete variables, as well as nonlinear relationships between the variables. It divides the sample sets of dependent variables into different subsets (or subclusters) through a series of cutting and merging operations based upon the theory of multivariate analysis of variance (MANOVA). The modeling results are given by a cluster tree, which includes both intermediate and leaf subclusters as well as the flow paths from the root of the tree to each leaf subcluster specified by a series of cutting and merging actions. The rSCA package is a handy and easy-to-use tool and is freely available at http://cran.r-project.org/package=rSCA. By applying the developed package to air quality management in an urban environment, we demonstrate its effectiveness in dealing with the complicated relationships among multiple variables in real-world problems.",""
"This paper investigates the use of expert knowledge as a resource for digital soil mapping. To do this, three models of topsoil soil bulk density (D-b) were produced: (i) a random forest model formulated and cross-validated with the limited data available (which served as the benchmark), (ii) a naive Bayesian network (BN) where the conditional probabilities that define the relations between D-b and explanatory landscape variables were derived from expert knowledge rather than data and (iii) a hierarchical' BN where model structure was also defined by expert knowledge. These models were used to generate spatial predictions for mapping topsoil D-b at a landscape scale. The results show that expert knowledge-based models can identify the same spatial trends in soil properties at a landscape scale as state-of-the-art mapping algorithms. This means that they are a viable option for soil mapping applications in areas that have limited empirical data.",""
"Intelligent systems for online fault diagnoses can increase the reliability, safety, and availability of large and complex systems. As an intelligent system, Dynamic Uncertain Causality Graph (DUCG) is a newly presented approach to graphically and compactly represent complex uncertain causalities, and perform probabilistic reasoning, which can be applied in fault diagnoses and other tasks. However, only static evidence was utilized previously. In this paper, the methodology for DUCG to perform fault diagnoses with dynamic evidence is presented. Causality propagations among sequential time slices are avoided. In the case of process systems, the basic failure events are classified as initiating, and non-initiating events. This classification can increase the efficiency of fault diagnoses greatly. Failure rates of initiating events can be used to replace failure probabilities without affecting diagnostic results. Illustrative examples are provided to illustrate the methodology.","In the case of process systems, the basic failure events are classified as initiating, and non-initiating events."
"Inspection and maintenance of concrete bridges is a major cost factor in transportation infrastructure, and there is significant potential for using information gained during inspection to update predictive models of the performance and reliability of such structures. In this context, this paper presents an approach for assessing and updating the reliability of prestressed concrete bridges subjected to chloride-induced reinforcement corrosion. The system deterioration state is determined based on a Dynamic Bayesian Network (DBN) model that considers the spatial variability of the corrosion process. The overall system reliability is computed by means of a probabilistic structural model coupled with the deterioration model. Inspection data are included in the system reliability calculation through Bayesian updating on the basis of the DBN model. As proof of concept, a software prototype is developed to implement the method presented here. The software prototype is applied to a typical highway bridge and the influence of inspection information on the system deterioration state and the structural reliability is quantified taking into account the spatial correlation of the corrosion process. This work is a step towards developing a software tool that can be used by engineering practitioners to perform reliability assessments of ageing concrete bridges and update their reliability with inspection and monitoring data.",""
"The search for complex, nonlinear relationships and causality in data is hindered by the availability of techniques in many domains, including forensic science. Linear multivariable techniques are useful but present some shortcomings. In the past decade, Bayesian approaches have been introduced in forensic science. To date, authors have mainly focused on providing an alternative to classical techniques for quantifying effects and dealing with uncertainty. Causal networks, including Bayesian networks, can help detangle complex relationships in data. A Bayesian network estimates the joint probability distribution of data and graphically displays dependencies between variables and the circulation of information between these variables. In this study, we illustrate the interest in utilizing Bayesian networks for dealing with complex data through an application in clinical forensic science. Evaluating the functional impairment of assault survivors is a complex task for which few determinants are known. As routinely estimated in France, the duration of this impairment can be quantified by days of 'Total Incapacity to Work' ('Incapacit, totale de travail,' ITT). In this study, we used a Bayesian network approach to identify the injury type, victim category and time to evaluation as the main determinants of the 'Total Incapacity to Work' (TIW). We computed the conditional probabilities associated with the TIW node and its parents. We compared this approach with a multivariable analysis, and the results of both techniques were converging. Thus, Bayesian networks should be considered a reliable means to detangle complex relationships in data.",""
"In this paper, we review recent results on audiovisual (AV) fusion. We also discuss some of the challenges and report on approaches to address them. One important issue in AV fusion is how the modalities interact and influence each other. This review will address this question in the context of AV speech processing, and especially speech recognition, where one of the issues is that the modalities both interact but also sometimes appear to desynchronize from each other. An additional issue that sometimes arises is that one of the modalities may be missing at test time, although it is available at training time; for example, it may be possible to collect AV training data while only having access to audio at test time. We will review approaches to address this issue from the area of multiview learning, where the goal is to learn a model or representation for each of the modalities separately while taking advantage of the rich multimodal training data. In addition to multiview learning, we also discuss the recent application of deep learning (DL) toward AV fusion. We finally draw conclusions and offer our assessment of the future in the area of AV fusion.",""
"Objectives: Time to definitive care is important for trauma outcomes, thus many emergency medical services (EMS) systems in the world adopt response times of ambulances as a key performance indicator. The objective of this study is to examine the underlying risk factors that can affect ambulance response times (ART) for trauma incidents, so as to derive interventional measures that can improve the ART. Material and methods: This was a retrospective study based on two years of trauma data obtained from the national EMS operations centre of Singapore. Trauma patients served by the national EMS provider over the period from 1 January 2011 till 31 December 2012 were included. ART was categorized into \"Short\" (<4 min), \"Intermediate\" (4-8 min) and \"Long\" (>8 min) response times. A modelling framework which leveraged on both multinomial logistic (MNL) regression models and Bayesian networks was proposed for the identification of main and interaction effects. Results: Amongst the process-related risk factors, weather, traffic and place of incident were found to be significant. The traffic conditions on the roads were found to have the largest effect the odds ratio (OR) of \"Long\" ART in heavy traffic condition was 12.98 (95% CI: 10.66-15.79) times higher than that under light traffic conditions. In addition, the ORs of \"Long ART\" under \"Heavy Rain\" condition were significantly higher (OR 1.58, 95% CI: 1.26-1.97) than calls responded under \"Fine\" weather. After accounting for confounders, the ORs of \"Long\" ART for trauma incidents at \"Home\" or \"Commercial\" locations were also significantly higher than that for \"Road\" incidents. Conclusion: Traffic, weather and the place of incident were found to be significant in affecting the ART. The evaluation of factors affecting the ART enables the development of effective interventions for reducing the ART. (C) 2015 Elsevier Ltd. All rights reserved.","A modelling framework which leveraged on both multinomial logistic (MNL) regression models and Bayesian networks was proposed for the identification of main and interaction effects."
"The social life cycle impact assessment (SLCIA) incorporates either a type I or type II characterization model. We improved both models by introducing explicit causality by using statistic modeling through development of (1) a quantitative approach to simultaneously identify impact pathways of type II models with multiple impact categories, targeting SLCIA method developers and (2) a new hybrid model to establish causality between inventory indicators and subcategories, targeting social life cycle assessment practitioners. Causality establishments for type II impact pathways and the new hybrid model are the core requirements for this study. We used structural equation modeling (SEM) to identify the impact pathways for type II characterization models, therefore resolving the issues of unobservability and unvalidatibility in type II models. Using country-level data from the World Bank, the method was applied to an example impact pathway at macro-scale. We applied Bayesian networks in our hybrid model to address the issues of relevance and representativeness in type I models, assuming the unobservable social performances of an organization are the causes for observable inventory indicators. The method was applied to a hypothetical example for the stakeholder of the worker at company scale. Temporal precedence (i.e., lag effects) was incorporated into both models. The results from the confirmatory SEM supported our hypotheses that comprised the impact pathway from economic development to health outcomes, which were fully mediated by health expenditures and health access. A 1-year lag between each impact category resulted in the best model fit. Limitations on the data as well as subjective choice of indicators to represent impact categories are subject to criticism. The results from the hybrid model showed that, depending on the likelihood of the inventory indicators, the posterior probability of subcategories either deviated from their prior probability or behaved similarly. The construction of proper conditional probability tables and the choice of probability distribution for the likelihood are major challenges for the hybrid model. This study was the first attempt in using statistic causal models to quantitatively identify unobservable impact pathways of the type II model and to develop a hybrid model for SLCIA. A SEM that incorporates temporal precedence enables identification of impact pathways with multiple unobservable impact categories. The hybrid model using Bayesian networks represents the subcategories in posterior probabilities instead of absolute scores, helping companies to better develop instructions for future management practices.",""
"The limited supply of high quality planting materials for a wide species base is a major reason for the limited success of reforestation programmes or projects in many developing countries. This paper reports the research that was undertaken to improve the supply of high quality planting materials for agro-forestry, tree farming and reforestation in the Philippines. A systems approach was used to identify mechanisms to improve the operational effectiveness of the forest nursery sector. A Bayesian Belief Network of the forest nursery sector was developed to examine the interactions between the key components of the forest nursery sector, identify key leverage points for intervention and explore potential impacts of possible policy interventions. Although improving the operational effectiveness of individual, communal and government nurseries will result in high operational effectiveness of the forest nursery sector, the operational effectiveness of government nurseries is likely to have a negative impact on the market for seedlings from smallholder nurseries i.e. individual and communal nurseries, thus impeding the sustainability of smallholder nurseries. Increasing the supply of high quality germplasm for a wide variety of species, improving the technical capabilities of smallholder nursery operators in seedling production and increasing the market demand of high quality seedlings from smallholder nurseries are the most important requirements for improving the operational effectiveness of the forest nursery sector. However, government nurseries can play a crucial role in improving the effectiveness of the forest nursery sector by diversifying their production to focus on species that are in demand by smallholder farmers and which cannot be supplied by individual or communal nurseries. Failing to do this will result in the current situation continuing in which government nursery sector competes with private and communal nurseries. Crown Copyright (C) 2015 Published by Elsevier Ltd. All rights reserved.",""
"Statistical relational learning (SRL) is concerned with developing formalisms for representing and learning from data that exhibit both uncertainty and complex, relational structure. Most of the work in SRL has focused on modeling and learning from data that only contain discrete variables. As many important problems are characterized by the presence of both continuous and discrete variables, there has been a growing interest in developing hybrid SRL formalisms. Most of these formalisms focus on reasoning and representational issues and, in some cases, parameter learning. What has received little attention is learning the structure of a hybrid SRL model from data. In this paper, we fill that gap and make the following contributions. First, we propose hybrid relational dependency networks (HRDNs), an extension to relational dependency networks that are able to model continuous variables. Second, we propose an algorithm for learning both the structure and parameters of an HRDN from data. Third, we provide an empirical evaluation that demonstrates that explicitly modeling continuous variables results in more accurate learned models than discretizing them prior to learning.",""
"The ability to predict and reason about other people's choices is fundamental to social interaction. We propose that people reason about other people's choices using mental models that are similar to decision networks. Decision networks are extensions of Bayesian networks that incorporate the idea that choices are made in order to achieve goals. In our first experiment, we explore how people predict the choices of others. Our remaining three experiments explore how people infer the goals and knowledge of others by observing the choices that they make. We show that decision networks account for our data better than alternative computational accounts that do not incorporate the notion of goal-directed choice or that do not rely on probabilistic inference. (C) 2015 Elsevier B.V. All rights reserved.","We show that decision networks account for our data better than alternative computational accounts that do not incorporate the notion of goal-directed choice or that do not rely on probabilistic inference."
"The complexity and spatial heterogeneity of ecosystem processes driving ecosystem service delivery require spatially explicit models that take into account the different parameters affecting those processes. Current attempts to model ecosystem service delivery on a broad, regional scale often depend on indicator-based approaches that are generally not able to fully capture the complexity of ecosystem processes. Moreover, they do not allow quantification of uncertainty on their predictions. In this paper, we discuss a QGIS plug-in which promotes the use of Bayesian belief networks for regional modelling and mapping of ecosystem service delivery and associated uncertainties. Different types of specific Bayesian belief network output maps, delivered by the plug-in, are discussed and their decision support capacities are evaluated. This plug-in, used in combination with firmly developed Bayesian belief networks, has the potential to add value to current spatial ecosystem service accounting methods. The plug-in can also be used in other research domains dealing with spatial data and uncertainty. (C) 2015 Elsevier Ltd. All rights reserved.",""
"In the field of computer vision, visual surveillance systems have recently become an important research topic. Growth in this area is being driven by both the increase in the availability of inexpensive computing devices and image sensors as well as the general inefficiency of manual surveillance and monitoring. In particular, the ultimate goal for many visual surveillance systems is to provide automatic activity recognition for events at a given site. A higher level of understanding of these activities requires certain lower-level computer vision tasks to be performed. So in this paper, we propose an intelligent activity recognition model that uses a structure learning method and a classification method. The structure learning method is provided as a K2-learning algorithm that generates Bayesian networks of causal relationships between sensors for a given activity. The statistical characteristics of the sensor values and the topological characteristics of the generated graphs are learned for each activity, and then a neural network is designed to classify the current activity according to the features extracted from the multiple sensor values that have been collected. Finally, the proposed method is implemented and tested by using PETS2013 benchmark data.","So in this paper, we propose an intelligent activity recognition model that uses a structure learning method and a classification method."
"Background: There are increasing efforts to bring high-throughput systems biology techniques to bear on complex animal model systems, often with a goal of learning about underlying regulatory network structures (e.g., gene regulatory networks). However, complex animal model systems typically have significant limitations on cohort sizes, number of samples, and the ability to perform follow-up and validation experiments. These constraints are particularly problematic for many current network learning approaches, which require large numbers of samples and may predict many more regulatory relationships than actually exist. Results: Here, we test the idea that by leveraging the accuracy and efficiency of classifiers, we can construct high-quality networks that capture important interactions between variables in datasets with few samples. We start from a previously-developed tree-like Bayesian classifier and generalize its network learning approach to allow for arbitrary depth and complexity of tree-like networks. Using four diverse sample networks, we demonstrate that this approach performs consistently better at low sample sizes than the Sparse Candidate Algorithm, a representative approach for comparison because it is known to generate Bayesian networks with high positive predictive value. We develop and demonstrate a resampling-based approach to enable the identification of a viable root for the learned tree-like network, important for cases where the root of a network is not known a priori. We also develop and demonstrate an integrated resampling-based approach to the reduction of variable space for the learning of the network. Finally, we demonstrate the utility of this approach via the analysis of a transcriptional dataset of a malaria challenge in a non-human primate model system, Macaca mulatta, suggesting the potential to capture indicators of the earliest stages of cellular differentiation during leukopoiesis. Conclusions: We demonstrate that by starting from effective and efficient approaches for creating classifiers, we can identify interesting tree-like network structures with significant ability to capture the relationships in the training data. This approach represents a promising strategy for inferring networks with high positive predictive value under the constraint of small numbers of samples, meeting a need that will only continue to grow as more high-throughput studies are applied to complex model systems.","Results: Here, we test the idea that by leveraging the accuracy and efficiency of classifiers, we can construct high-quality networks that capture important interactions between variables in datasets with few samples."
"Alarm flooding is a significant problem in the process industries. To solve this problem, a scenario-based early warning system design methodology is proposed. It comprises three steps: (i) scenario identification: events are identified by HAZOP analysis, variables are allocated to the scenario-based group, and the variables states correlated to the scenarios are identified; (ii) model development: Bayesian network of all variables is learned from the process data, and the events nodes are appended according to expert knowledge to construct the Bayesian network model of a scenario-based early warning system; (iii) model implementation: the model is applied online to monitor process, the monitored variables continuously produce evidence, update the events probabilities, find the root causes, and give an events warning message together with the root cause to operators. The methodology implementation and salient points are explained with the help of an easy to follow simple case study.",""
"We study the NP-hard problem of finding a directed acyclic graph (DAG) on a given set of nodes so as to maximize a given scoring function. The problem models the task of inferring a probabilistic network from data, which has been studied extensively in the fields of artificial intelligence and machine learning. Several variants of the problem, where the output DAG is constrained in several ways, are NP-hard as well, for example when the DAG is required to have bounded in-degree, or when it is required to be a polytree. Polynomial-time algorithms are known only for rare special cases, perhaps most notably for branchings, that is, polytrees in which the in-degree of every node is at most one. In this paper, we generalize this polynomial-time result to polytrees that can be turned into a branching by deleting a constant number of arcs. Our algorithm stems from a matroid intersection formulation. As the order of the polynomial time bound depends on the number of deleted arcs, the algorithm does not establish fixed-parameter tractability when parameterized by that number. We show that certain additional constraints on the sought polytree render the problem fixed-parameter tractable. We contrast this positive result by showing that if we parameterize by the number of deleted nodes, a somewhat more powerful parameter, the problem is not fixed-parameter tractable, subject to a complexity-theoretic assumption. (C) 2015 Elsevier B.V. All rights reserved.",""
"Although protection failures have critical influence on the reliability of power systems, the methodology of assessing composite power system reliability including protection failures has not gone far enough yet. In this study, a Bayesian network (BN)-based analytical methodology is proposed for modelling and analysis of the impact of protection system failures on bulk power system reliability. Initially, basic BN model of composite power system reliability is constructed based on its minimal cutsets (MCs) and logical relationships between components, MCs and system failure. Then, different failure modes of protection system and the interactions among components caused by protection system failures are conveniently incorporated into the basic BN model and the reliability calculations. By using the presented method, several restrictive assumptions, implicit in the other methods, can be removed. Moreover, applying BN provides additional capabilities at modelling and analysis levels. The proposed method is applied to the IEEE reliability test system and the results demonstrate that the proposed method is effective and is flexible in applications.",""
"We formalise and present a new generic multifaceted complex system approach for modelling complex business enterprises. Our method has a strong focus on integrating the various data types available in an enterprise which represent the diverse perspectives of various stakeholders. We explain the challenges faced and define a novel approach to converting diverse data types into usable Bayesian probability forms. The data types that can be integrated include historic data, survey data, and management planning data, expert knowledge and incomplete data. The structural complexities of the complex system modelling process, based on various decision contexts, are also explained along with a solution. This new application of complex system models as a management tool for decision making is demonstrated using a railway transport case study. The case study demonstrates how the new approach can be utilised to develop a customised decision support model for a specific enterprise. Various decision scenarios are also provided to illustrate the versatility of the decision model at different phases of enterprise operations such as planning and control.",""
"Knowledge of the production loads and production times is an essential ingredient for making successful production plans and schedules. In steel production, the production loads and the production times are impacted by many uncertainties, which necessitates their prediction via stochastic models. In order to avoid having separate prediction models for planning and for scheduling, it is helpful to develop a single prediction model that allows us to predict both production loads and production times. In this work, Bayesian network models are employed to predict the probability distributions of these variables. First, network structure is identified by maximizing the Bayesian scores that include the likelihood and model complexity. In order to handle large domain of discrete variables, a novel decision-tree structured conditional probability table based Bayesian inference algorithm is developed. We present results for realworld steel production data and show that the proposed models can accurately predict the probability distributions. (C) 2015 Elsevier Ltd. All rights reserved.","In order to handle large domain of discrete variables, a novel decision-tree structured conditional probability table based Bayesian inference algorithm is developed."
"In the safety-critical systems, potential hazard may jeopardize the safety of system, which can even lead to catastrophic accident. The occurrence of accident contains deterministic and non-deterministic causal logic relationships. The former refers to technical aspects, where event tree and fault tree could be applied to analyze. The latter mainly refers to operational aspects. Due to uncertainties and lack of data, Bayesian network becomes the appropriate tool. However; how to determine the conditional probability table is a difficult task. In this paper, a new conditional probability allocation method based on fuzzy logic is proposed to reference posterior probability. Before quantitative analysis, the risk model of influencing factors may be established. Finally, the proposed method is applied to evaluate human error probability.",""
"It is well known that Bayesian network structure learning from data is an NP-hard problem. Learning a correct skeleton of a DAG is the foundation of dependency analysis algorithms for this problem. Considering the unreliability of the high order condition independence (CI) tests and the aim to improve the efficiency of a dependency analysis algorithm, the key steps are to use less number of CI tests and reduce the sizes of condition sets as many as possible. Based on these analyses and inspired by the algorithm HPC, we present an algorithm, named efficient hybrid parents and child (EHPC), for learning the adjacent neighbors of every variable. We proof the validity of the algorithm. Compared with state-of-the-art algorithms, the experimental results show that EHPC can handle large network and has better accuracy with fewer number of condition independence tests and smaller size of conditioning set.",""
"Underground metro tunnels present a popular solution to relieve the pressure of surface transportation systems worldwide. However, tunnel construction inevitably generates soil displacements and deformations, which may affect the safety performance of the surface road operation. This paper presents a systemic dynamic decision approach based on dynamic Bayesian networks (DBNs), aiming to provide guidelines for the dynamic safety analysis of the tunnel-induced road surface damage over time. The potential uncertainty and randomness underlying tunnel construction is modeled by following a discrete-time Markov chain process. A detailed step-by-step procedure is proposed, including risk/hazard identification, and DBN-based predictive and diagnostic analysis. A case study concerning the dynamic safety analysis in the construction of the Wuhan Yangtze Metro Tunnel is presented. Results demonstrate the feasibility of the proposed approach, as well as its application potential. The relationships between the DNB-based and BN-based approaches are further discussed based on the results. The proposed approach can be used as a decision tool to provide support for safety assurance in tunnel construction, and thus increase the likelihood of a successful project in a dynamic environment.",""
"Understanding the relationship between the impact of radiation at the component and system levels is challenging. This paper discusses a hierarchical approach, based on Bayesian theory, to establish a mechanism for determining system health based on the status of, and interactions between, the radiation response of component parts. When the Bayesian network is trained with a combination of experimental data, data from similar parts, simulations, and expert estimates, a quantitative estimate of the Total-Ionizing Dose (TID) response of a system can be obtained. Bayesian networks enable inference about system-level functional performance, the dose exposure, and the sensitivity of different components to TID, thus providing a framework for TID awareness in design and operation of systems. A case study of a robotic system consisting of commercial components is presented.","Bayesian networks enable inference about system-level functional performance, the dose exposure, and the sensitivity of different components to TID, thus providing a framework for TID awareness in design and operation of systems."
"Observations depending on sums of random variables are common throughout many fields; however, no efficient solution is currently known for performing max-product inference on these sums of general discrete distributions (max-product inference can be used to obtain maximum a posteriori estimates). The limiting step to max-product inference is the max-convolution problem (sometimes presented in log-transformed form and denoted as \"infimal convolution,\" \"min-convolution,\" or \"convolution on the tropical semiring\"), for which no O(k log(k)) method is currently known. Presented here is an O(k log(k)) numerical method for estimating the max-convolution of two nonnegative vectors (e.g., two probability mass functions), where k is the length of the larger vector. This numerical max-convolution method is then demonstrated by performing fast max-product inference on a convolution tree, a data structure for performing fast inference given information on the sum of n discrete random variables in O(nk log(nk)log(n)) steps (where each random variable has an arbitrary prior distribution on k contiguous possible states). The numerical max-convolution method can be applied to specialized classes of hidden Markov models to reduce the runtime of computing the Viterbi path from nk(2) to nk log(k), and has potential application to the all-pairs shortest paths problem.","Observations depending on sums of random variables are common throughout many fields; however, no efficient solution is currently known for performing max-product inference on these sums of general discrete distributions (max-product inference can be used to obtain maximum a posteriori estimates)."
"When constructing Bayesian networks with domain experts, network engineers often use the noisy-OR model, and causal interaction models more generally, to alleviate the burden of probability elicitation: the use of such a model serves to reduce the number of probabilities to be elicited on the one hand, and on the other hand forestalls experts having to give assessments for probabilities with compound conditions which they feel are hard to envision. Recently, we have shown that ill-considered use of the noisy-OR model specifically can substantially decrease a network's performance, especially in domains in which causal mechanisms include cancellation effects. Motivated by this observation, we designed a new causal interaction model, with the same engineering advantages as the noisy-OR model, to describe such effects. We detail properties of our intercausal cancellation model, and compare it against existing causal interaction models. We further illustrate the application of our model in the real-world domain of pharmacology. (C) 2015 Elsevier Inc. All rights reserved.",""
"A learning style describes the attitudes and behaviors, which determine an individual's preferred way of learning. Learning styles are particularly important in educational settings since they may help students and tutors become more self-aware of their strengths and weaknesses as learners. The traditional way to identify learning styles is using a test or questionnaire. Despite being reliable, these instruments present some problems that hinder the learning style identification. Some of these problems include students' lack of motivation to fill out a questionnaire and lack of self-awareness of their learning preferences. Thus, over the last years, several approaches have been proposed for automatically detecting learning styles, which aim to solve these problems. In this work, we review and analyze current trends in the field of automatic detection of learning styles. We present the results of our analysis and discuss some limitations, implications and research gaps that can be helpful to researchers working in the field of learning styles.",""
"Intelligent tutoring and personalization are considered as the two most important factors in the research of learning systems and environments. An effective tool that can be used to improve problem-solving ability is an Intelligent Tutoring System which is capable of mimicking a human tutor's actions in implementing a one-to-one personalized and adaptive teaching. In this paper, a novel Flowchart-based Intelligent Tutoring System (FITS) is proposed benefiting from Bayesian networks for the process of decision making so as to aid students in problem-solving activities and learning computer programming. FITS not only takes full advantage of Bayesian networks, but also benefits from a multi-agent system using an automatic text-to-flowchart conversion approach for engaging novice programmers in flowchart development with the aim of improving their problem-solving skills. In the end, in order to investigate the efficacy of FITS in problem-solving ability acquisition, a quasi-experimental design was adopted by this research. According to the results, students in the FITS group experienced better improvement in their problem-solving abilities than those in the control group. Moreover, with regard to the improvement of a user's problem-solving ability, FITS has shown to be considerably effective for students with different levels of prior knowledge, especially for those with a lower level of prior knowledge.",""
"Prediction of coastal vulnerability is of increasing concern to policy makers, coastal managers and other stakeholders. Coastal regions and barrier islands along the Atlantic and Gulf coasts are subject to frequent, large storms, whose waves and storm surge can dramatically alter beach morphology, threaten infrastructure, and impact local economies. Given that precise forecasts of regional hazards are challenging, because of the complex interactions between processes on many scales, a range of probable geomorphic change in response to storm conditions is often more helpful than deterministic predictions. Site-specific probabilistic models of coastal change are reliable because they are formulated with observations so that local factors, of potentially high influence, are inherent in the model. The development and use of predictive tools such as Bayesian Networks in response to future storms has the potential to better inform management decisions and hazard preparation in coastal communities. We present several Bayesian Networks designed to hindcast distinct morphologic changes attributable to the Nor'Ida storm of 2009, at Fire Island, New York. Model predictions are informed with historical system behavior, initial morphologic conditions, and a parameterized treatment of wave climate. We refine a preliminary Bayesian Network by 1) increasing model experience through additional observations, 2) including anthropogenic modification history, and 3) replacing parameterized wave impact values with maximum run-up elevation. Further, we develop and train a pair of generalized models with an additional dataset encompassing a different storm event, which expands the observations beyond our hindcast objective. We compare the skill of the generalized models against the Nor'Ida specific model formulation, balancing the reduced skill with an expectation of increased transferability. Results of Nor'Ida hindcasts ranged in skill from 0.37 to 0.51 and accuracy of 65.0 to 81.9%. (C) 2015 Elsevier B.V. All rights reserved.",""
"Both supply issues and interaction between various stages of a production network are common occurrences in product customization under a delayed differentiation strategy. This paper studies non-decouple systems for product customization in the context of a single-market segment under a delayed differentiation strategy by incorporating supply issues and interaction between various stages of a production network. We analyze two alternatives for customizing two individual products. One alternative is the contractual obligations for raw materials delivery where the supplier is responsible for maintaining certain inventory service levels agreements. The other alternative is the customer demand requirements where the manufacturer is responsible for maintaining certain end-products service levels to prevent customer erosion. We characterize the service level requirements in both scenarios and determine the optimal customization point of the production network where the practitioner can operate at minimum cost. We use a Bayesian Belief Networks method to model interaction between stages of the production network and derive the inventory service level requirements. A mini-case involving the customization of a personal desktop computer is used to illustrate the applicability of this framework. We also compare the benefits of delayed product differentiation under non-decouple systems and the traditional decouple systems and provide insights into how firms can choose the right strategy to effectively compete. Published by Elsevier B.V.",""
"Smart environments are able to support users during their daily life. For example, smart energy systems can be used to support energy saving by controlling devices, such as lights or displays, depending on context information, such as the brightness in a room or the presence of users. However, proactive decisions should also match the users' preferences to maintain the users' trust in the system. Wrong decisions could negatively influence the users' acceptance of a system and at worst could make them abandon the system. In this paper, a trust-based model, called User Trust Model (UTM), for automatic decision-making is proposed, which is based on Bayesian networks. The UTM's construction, the initialization with empirical data gathered in an online survey, and its integration in an office setting are described. Furthermore, the results of a live study and a live survey analyzing the users' experience and acceptance are presented.",""
"Recognizing handwritten mathematics is a challenging classification problem, requiring simultaneous identification of all the symbols comprising an input as well as the complex two-dimensional relationships between symbols and subexpressions. Because of the ambiguity present in handwritten input, it is often unrealistic to hope for consistently perfect recognition accuracy. We present a system which captures all recognizable interpretations of the input and organizes them in a parse forest from which individual parse trees may be extracted and reported. If the top-ranked interpretation is incorrect, the user may request alternates and select the recognition result they desire. The tree extraction step uses a novel probabilistic tree scoring strategy in which a Bayesian network is constructed based on the structure of the input, and each joint variable assignment corresponds to a different parse tree. Parse trees are then reported in order of decreasing probability. Two accuracy evaluations demonstrate that the resulting recognition system is more accurate than previous versions (which used non-probabilistic methods) and other academic math recognizers. (C) 2015 Elsevier Ltd. All rights reserved.","Recognizing handwritten mathematics is a challenging classification problem, requiring simultaneous identification of all the symbols comprising an input as well as the complex two-dimensional relationships between symbols and subexpressions."
"Provision of network infrastructure to meet rising network peak demand is increasing the cost of electricity. Addressing this demand is a major imperative for Australian electricity agencies. The network peak demand model reported in this paper provides a quantified decision support tool and a means of understanding the key influences and impacts on network peak demand. An investigation of the system factors impacting residential consumers' peak demand for electricity was undertaken in Queensland, Australia. Technical factors, such as the customers' location, housing construction and appliances, were combined with social factors, such as household demographics, culture, trust and knowledge, and Change Management Options (CMOs) such as tariffs, price, managed supply, etc., in a conceptual 'map' of the system. A Bayesian network was used to quantify the model and provide insights into the major influential factors and their interactions. The model was also used to examine the reduction in network peak demand with different market-based and government interventions in various customer locations of interest and investigate the relative importance of instituting programs that build trust and knowledge through well designed customer-industry engagement activities. The Bayesian network was implemented via a spreadsheet with a tickbox interface. The model combined available data from industry-specific and public sources with relevant expert opinion. The results revealed that the most effective intervention strategies involve combining particular CMOs with associated education and engagement activities. The model demonstrated the importance of designing interventions that take into account the interactions of the various elements of the socio-technical system. The options that provided the greatest impact on peak demand were Off-Peak Tariffs and Managed Supply and increases in the price of electricity. The impact in peak demand reduction differed for each of the locations and highlighted that household numbers, demographics as well as the different climates were significant factors. It presented possible network peak demand reductions which would delay any upgrade of networks, resulting in savings for Queensland utilities and ultimately for households. The use of this systems approach using Bayesian networks to assist the management of peak demand in different modelled locations in Queensland provided insights about the most important elements in the system and the intervention strategies that could be tailored to the targeted customer segments.",""
"Contingency plans are essential in guiding the response to marine oil spills. However, they are written before the pollution event occurs so must contain some degree of assumption and prediction and hence may be unsuitable for a real incident when it occurs. The use of Bayesian networks in ecology, environmental management, oil spill contingency planning and post-incident analysis is reviewed and analysed to establish their suitability for use as real-time environmental decision support systems during an oil spill response. It is demonstrated that Bayesian networks are appropriate for facilitating the re-assessment and re-validation of contingency plans following pollutant release, thus helping ensure that the optimum response strategy is adopted. This can minimise the possibility of sub-optimal response strategies causing additional environmental and socioeconomic damage beyond the original pollution event. (C) 2015 Elsevier Ltd. All rights reserved.",""
"A causal model is an abstract representation of a physical system as a directed acyclic graph (DAG), where the statistical dependencies are encoded using a graphical criterion called 'd-separation'. Recent work by Wood and Spekkens shows that causal models cannot, in general, provide a faithful representation of quantum systems. Since d-separation encodes a form of Reichenbach's common cause principle (RCCP), whose validity is questionable in quantum mechanics, we propose a generalized graph separation rule that does not assume the RCCP. We prove that the new rule faithfully captures the statistical dependencies between observables in a quantum network, encoded as a DAG, and reduces to d-separation in a classical limit.",""
"The Life Quality Index (LQI) is a rational way to establish a relationship among the resources utilized to improve human safety and the expected fatalities that can be avoided by safety improvement. This article uses the LQI approach to quantify the social benefits of a number of safety management plans for a railway facility such as level crossing (LC). We apply influence diagrams (IDs), which are the extensions of Bayesian Networks, to model and assess the life safety risks. In IDs, problems of probabilistic inference, economics-based utility values, and decision alternatives are combined and optimized. The optimal decision, which maximizes total benefits to society, is obtained for the LC. As low as reasonably practicable (ALARP) case, and is a widely accepted risk acceptance criteria in the railway industry. According to the ALARP, there exists a so-called tolerable region between the regions of intolerable and negligible risks. In the tolerable region, risk is undertaken only if a benefit is desired. To quantify socioeconomic benefits, one needs to have an additional risk acceptance criterion such as LQI. In this article we apply and discuss the advantages of the LQI and the IDs for a number of safety management plans for railway LCs.","In IDs, problems of probabilistic inference, economics-based utility values, and decision alternatives are combined and optimized."
"The global economic structure, with its decentralized production and the consequent increase in freight traffic all over the world, creates considerable problems and challenges for the freight transport sector. This situation has led shipping to become the most suitable and cheapest way to transport goods. Thus, ports are configured as nodes with critical importance in the logistics supply chain as a link between two transport systems, sea and land. Increase in activity at seaports is producing three undesirable effects: increasing road congestion, lack of open space in port installations and a significant environmental impact on seaports. These adverse effects can be mitigated by moving part of the activity inland. Implementation of dry ports is a possible solution and would also provide an opportunity to strengthen intermodal solutions as part of an integrated and more sustainable transport chain, acting as a link between road and railway networks. In this sense, implementation of dry ports allows the separation of the links of the transport chain, thus facilitating the shortest possible routes for the lowest capacity and most polluting means of transport. Thus, the decision of where to locate a dry port demands a thorough analysis of the whole logistics supply chain, with the objective of transferring the largest volume of goods possible from road to more energy efficient means of transport, like rail or short-sea shipping, that are less harmful to the environment. However, the decision of where to locate a dry port must also ensure the sustainability of the site. Thus, the main goal of this article is to research the variables influencing the sustainability of dry port location and how this sustainability can be evaluated. With this objective, in this paper we present a methodology for assessing the sustainability of locations by the use of Multi-Criteria Decision Analysis (MCDA) and Bayesian Networks (BNs). MCDA is used as a way to establish a scoring, whilst BNs were chosen to eliminate arbitrariness in setting the weightings using a technique that allows us to prioritize each variable according to the relationships established in the set of variables. In order to determine the relationships between all the variables involved in the decision, giving us the importance of each factor and variable, we built a K2 BN algorithm. To obtain the scores of each variable, we used a complete cartography analysed by ArcGIS. Recognising that setting the most appropriate location to place a dry port is a geographical multidisciplinary problem, with significant economic, social and environmental implications, we consider 41 variables (grouped into 17 factors) which respond to this need. As a case of study, the sustainability of all of the 10 existing dry ports in Spain has been evaluated. In this set of logistics platforms, we found that the most important variables for achieving sustainability are those related to environmental protection, so the sustainability of the locations requires a great respect for the natural environment and the urban environment in which they are framed.",""
"Bayesian network (BN) is an efficient graphical method that uses directed acyclic graphs (DAG) to provide information about a set of data. BNs consist of nodes and arcs (or edges) where nodes represent variables and arcs represent relations and influences between nodes. Interest in organic food has been increasing in the world during the last decade. The same trend is also valid in Turkey. Although there are numerous studies that deal with customer perception of organic food and customer characteristics, none of them used BNs. Thus, this study, which shows a new application area of BNs, aims to reveal the perception and characteristics of organic food buyers. In this work, a survey is designed and applied in seven different organic bazaars in Turkey. Afterwards, BNs are constructed with the data gathered from 611 organic food consumers. The findings match with the previous studies as factors such as health, environmental factors, food availability, product price, consumers' income and trust to organization are found to influence consumers effectively.",""
"Large-magnitude earthquakes can damage high-voltage transformers, trigger power flow disruption and impact the economy and society. However, methods that enable large transformer vulnerability assessment in a practical and rigorous way are scarce. This paper proposes a probabilistic framework using Bayesian belief networks (BBNs) to predict the damage of high-voltage transformers subjected to seismic events. This framework incorporates major causes of transformer vulnerability at once, such as liquefaction, rocking response of the transformer, and interactions with interconnected equipment, which are otherwise commonly studied in isolation. To demonstrate the application of the framework, the paper elaborates on each step of the BBN framework, which is then validated with historical empirical data. Furthermore, the value of the proposed method is illustrated with high-voltage transformers in substations of the electric value BC Hydro in British Columbia, Canada. The paper also offers a sensitivity analysis that evaluates the effects of input variables on transformer damage. The proposed framework is simple to perform in practice, and the results are expected to support decisions on mitigation measures, seismic risk management, and to provide a step towards modelling the vulnerability of entire electrical substations.",""
"The paper describes the supervised method approach to identifying vessel anomaly behaviour. The vessel anomaly behaviour is determined by learning from self-reporting maritime systems based on the Automatic Identification System (AIS). The MS is a real world vessel reporting data system, which has been recently made compulsory by the International Convention for the Safety of Life and Sea (SOLAS) for vessels over 300 gross tons and most commercial vessels such as cargo ships, passenger vessels, tankers, etc. In this paper, we describe the use of Bayesian networks (BNs) approach to identify the behaviour of the vessel of interest. The BNs is a machine learning technique based on probabilistic theory that represents a set of random variables and their conditional independencies via directed acyclic graph (DAG). Previous studies showed that the BNs have important advantages compared to other machine learning techniques. Among them are that expert knowledge can be included in the BNs model, and that humans can understand and interpret the BNs model more readily. This work proves that the BNs technique is applicable to the identification of vessel anomaly behaviour.",""
"To meet the real-time diagnosis requirements of complex systems, this paper presents a novel fault diagnosis framework based on dynamic fault tree and arithmetic circuit. It pays attention to meeting two challenges: model development and real-time reasoning. Specifically, we use a dynamic fault tree to model dynamic fault modes and calculate some quantitative parameters using algebraic technique and Bayesian network (BN) in order to avoid the state space explosion problem. Furthermore, we compile a BN into an arithmetic circuit to obtain answers to probabilistic queries by evaluating and differentiating the arithmetic circuit. In addition, we incorporate sensors information into diagnosis process and propose the schemes on how to update the diagnostic importance factor and the minimal cut sequences. Finally, the example of a train-ground communication system is used to demonstrate the proposed method.",""
"The global trend of transformation and loss of wetlands through conversion to other land uses has deleterious effects on surrounding ecosystems, and there is a resultant increasing need for the conservation and preservation of wetlands. Improved mapping of wetland locations is critical to achieving objective regional conservation goals, which depends on accurate spatial knowledge. Current approaches to mapping wetlands through the classification of satellite imagery typically under-represents actual wetland area; the importance of ancillary data in improving accuracy in mapping wetlands is therefore recognised. In this study, we compared two approaches - Bayesian networks and logistic regression - to predict the likelihood of wetland occurrence in KwaZulu-Natal, South Africa. Both approaches were developed using the same data set of environmental surrogate predictors. We compared and verified model outputs using an independent test data set, with analyses including receiver operating characteristic curves and area under the curve (AUC). Both models performed similarly (AUC>0.84), indicating the suitability of a likelihood approach for ancillary data for wetland mapping. Results indicated that high wetland probability areas in the final model outputs correlated well with known wetland systems and wetland-rich areas in KwaZulu-Natal. We conclude that predictive models have the potential to improve the accuracy of wetland mapping in South Africa by serving as valuable ancillary data.","Current approaches to mapping wetlands through the classification of satellite imagery typically under-represents actual wetland area; the importance of ancillary data in improving accuracy in mapping wetlands is therefore recognised."
"We present an accelerated probabilistic learning concept and its prototype implementation for mining heterogeneous Earth observation images, e.g., multispectral images, synthetic aperture radar (SAR) images, image time series, or geographical information systems (GIS) maps. The system prototype combines, at pixel level, the unsupervised clustering results of different features, extracted from heterogeneous satellite images and geographical information resources, with user-defined semantic annotations in order to calculate the posterior probabilities that allow the final probabilistic searches. The system is able to learn different semantic labels based on a newly developed Bayesian networks algorithm and allows different probabilistic retrieval methods of all semantically related images with only a few user interactions. The new algorithm reduces the computational cost, overperforming existing conventional systems, under certain conditions, by several orders of magnitude. The achieved speed-up allows the introduction of new feature models improving the learning capabilities of knowledge-driven image information mining systems and opening them to Big Data environments.",""
"A new approach for uncertain causality representation and probabilistic reasoning named as dynamic uncertain causality graph (DUCG) was presented previously, in which only the discrete variables and certain evidence were addressed. In this paper, the free mixtures of discrete and continuous variables as well as uncertain evidence are addressed. The general idea to deal with continuous variables is to transform them into fuzzy discrete variables along with their corresponding uncertain causalities, and then treat them as ordinary discrete variables, which involves how to deal with fuzzy evidence. It is pointed out that uncertain evidence is either: 1) fuzzy evidence that is an observed certain value of a continuous variable falling into a fuzzy area across two or more fuzzy discrete states of a variable or 2) soft evidence that can only be understood as a probability distribution over the states of a variable. The algorithm for utilizing uncertain evidence in inference is presented, in which uncertain evidence is treated as a virtual child variable of the observed variable without changing the knowledge and inference algorithm encoded in DUCG. It is proved that the two types of uncertain evidence are the same in nature and can be treated indiscriminatingly. Moreover, this method dealing with fuzzy evidence in DUCG can be used for failure forecasting of systems. Examples are provided to illustrate the methodology.","The algorithm for utilizing uncertain evidence in inference is presented, in which uncertain evidence is treated as a virtual child variable of the observed variable without changing the knowledge and inference algorithm encoded in DUCG."
"A multitude of different probabilistic programming languages exists today, all extending a traditional programming language with primitives to support modeling of complex, structured probability distributions. Each of these languages employs its own probabilistic primitives, and comes with a particular syntax, semantics and inference procedure. This makes it hard to understand the underlying programming concepts and appreciate the differences between the different languages. To obtain a better understanding of probabilistic programming, we identify a number of core programming concepts underlying the primitives used by various probabilistic languages, discuss the execution mechanisms that they require and use these to position and survey state-of-the-art probabilistic languages and their implementation. While doing so, we focus on probabilistic extensions of logic programming languages such as Prolog, which have been considered for over 20 years.","Each of these languages employs its own probabilistic primitives, and comes with a particular syntax, semantics and inference procedure."
"Fault diagnosis for a solar assisted heat pump (SAHP) system in the presence of incomplete data and expert knowledge is discussed in this article. A method for parameter learning of Bayesian networks (BNs) from incomplete data based on the back-propagation (BP) neural network and maximum likelihood estimation (MLE), which is called BP-MLE method, is presented. The BP neural network is utilized to impute the missing data and the complete data sets are addressed with MLE to obtain the parameters of BN. A method for parameter estimation under incomplete expert knowledge based on BP neural networks and fuzzy set theory is also presented, which is called BP-FS method. Similarly, the missing information is imputed by the trained BP neural network. Fuzzy set theory is employed to quantify the parameters of BN based on complete qualitative expert knowledge. The presented methods are applied to parameter learning of diagnostic BN for a SAHP system with incomplete simulation data and expert knowledge. The developed BN can perform fault diagnosis with complete or incomplete symptoms. (C) 2015 Elsevier Ltd. All rights reserved.",""
"This paper outlines the design and development of an intelligent tutoring technique to personalize the navigation of individual users in the course content and generate advice to students. Based on that, a framework for adaptive e-learning, the e-Learning Guide system (eLGuide), is implemented and an empirical evaluation of the prototype is conducted to assess the possibility of its integration with web-based learning systems. The system is tested in a real setting with the SQLA Database Language course comprising postgraduate students of the computer science program, all new to database systems. The evaluation study shows a positive impact on learning outcomes. The results of the experimental study allowed us to conclude that eLGuide is a useful framework, which can be employed in a web-based learning environment to support students as well as teachers in a better way. (c) 2015 Wiley Periodicals, Inc. Comput Appl Eng Educ 23:542-555, 2015; View this article online at ; DOI",""
"Altered interplay between gut mucosa and microbiota during treated HIV infection may possibly contribute to increased bacterial translocation and chronic immune activation, both of which are predictors of morbidity and mortality. Although a dysbiotic gut microbiota has recently been reported in HIV + individuals, the metagenome gene pool associated with HIV infection remains unknown. The aim of this study is to characterize the functional gene content of gut microbiota in HIV + patients and to define the metabolic pathways of this bacterial community, which is potentially associated with immune dysfunction. We determined systemic markers of innate and adaptive immunity in a cohort of HIV-infected individuals on successful antiretroviral therapy without comorbidities and in healthy non-HIV-infected subjects. Metagenome sequencing revealed an altered functional profile, with enrichment of the genes involved in various pathogenic processes, lipopolysaccharide biosynthesis, bacterial translocation, and other inflammatory pathways. In contrast, we observed depletion of genes involved in amino acid metabolism and energy processes. Bayesian networks showed significant interactions between the bacterial community, their altered metabolic pathways, and systemic markers of immune dysfunction. This study reveals altered metabolic activity of microbiota and provides novel insight into the potential host-microbiota interactions driving the sustained inflammatory state in successfully treated HIV-infected patients.",""
"We present a continuous variable Bayesian networks modeling framework that integrates the graphical representation of a Bayesian networks model with empirical model-developing approach. Our model retains the Bayesian networks model's graphical representation of hypothesized causal connections among important variables and employs conventional statistical modeling approaches for establishing functional relationships among these variables. The modeling framework avoids discretizing continuous variables and the resulting models can be updated over time when new data are available or updated using local data to develop a site-specific model. We illustrate the modeling approach using a data for establishing nutrient criteria in streams and rivers in Ohio, U.S.A. (C) 2015 Elsevier Ltd. All rights reserved.",""
"replace with: Currently, the impacts of wide-scale implementation of photovoltaic (PV) technology are evaluated in terms of such indicators as rated capacity, energy output or return on investment. However, as PV markets mature, consideration of additional impacts (such as electricity transmission and distribution infrastructure or socio-economic factors) is required to evaluate potential costs and benefits of wide-scale PV in relation to specific policy objectives. This study describes a hybrid GIS spatio-temporal modelling approach integrating probabilistic analysis via a Bayesian technique to evaluate multi-scale/multi-domain impacts of PV. First, a wide-area solar resource modelling approach utilising GIS-based dynamic interpolation is presented and the implications for improved impact analysis on electrical networks are discussed. Subsequently, a GIS-based analysis of PV deployment in an area of constrained electricity network capacity is presented, along with an impact analysis of specific policy implementation upon the spatial distribution of increasing PV penetration. Finally, a Bayesian probabilistic graphical model for assessment of socio-economic impacts of domestic PV at high penetrations is demonstrated. Taken together, the results show that integrated spatio-temporal probabilistic assessment supports multi-domain analysis of the impacts of PV, thereby providing decision makers with a tool to facilitate deliberative and systematic evidence-based policy making incorporating diverse stakeholder perspectives.",""
"In time-series environments, uncertain knowledge among variables in a time slice can be represented and modeled by a Bayesian network (BN). In this paper, we are to achieve the global uncertain knowledge during a period of time for decision-making or action selection by fussing or combining the participating uncertainties of multiple time slices consistently while satisfying the demands of high efficiency and instantaneousness. We adopt qualitative probabilistic network (QPN), the qualitative abstraction of BN, as the underlying framework of modeling and fusing time-series uncertain knowledge. The BNs in continuous time slices constitute time-series BNs, from which we derive time-series QPNs. Taking time-series BNs as input, we propose a QPN-based approach to fuse time-series uncertainties in line with temporal specialties. First, for each time slice, we enhance the implied QPN by augmenting interval-valued weights derived from the corresponding BN, and then obtain the QPN with weighted influences, denoted EQPN (Enhanced Qualitative Probabilistic Network), which provides a quantitative and conflict-free basis for fusing uncertain knowledge. Then, we give the method for fusing the graphical structures of time-series EQPNs based on the concept of Markov equivalence. Following, we give a superposition method for fusing qualitative influences of time-series EQPNs. Experimental results show that our method is not only efficient, but also effective. Meanwhile, the simulation results when applying time-series EQPNs and the fusion algorithm to a robotic system show that our method is applicable in realistic intelligent situations.",""
"Models for predicting the risk of cardiovascular (CV) events based on individual patient characteristics are important tools for managing patient care. Most current and commonly used risk prediction models have been built from carefully selected epidemiological cohorts. However, the homogeneity and limited size of such cohorts restrict the predictive power and generalizability of these risk models to other populations. Electronic health data (EHD) from large health care systems provide access to data on large, heterogeneous, and contemporaneous patient populations. The unique features and challenges of EHD, including missing risk factor information, non-linear relationships between risk factors and CV event outcomes, and differing effects from different patient subgroups, demand novel machine learning approaches to risk model development. In this paper, we present a machine learning approach based on Bayesian networks trained on EHD to predict the probability of having a CV event within 5 years. In such data, event status may be unknown for some individuals, as the event time is right-censored due to disenrollment and incomplete follow-up. Since many traditional data mining methods are not well-suited for such data, we describe how to modify both modeling and assessment techniques to account for censored observation times. We show that our approach can lead to better predictive performance than the Cox proportional hazards model (i.e., a regression-based approach commonly used for censored, time-to-event data) or a Bayesian network with ad hoc approaches to right-censoring. Our techniques are motivated by and illustrated on data from a large US Midwestern health care system."," a regression-based approach commonly used for censored, time-to-event data) or a Bayesian network with ad hoc approaches to right-censoring."
"Background: Quantitative microbial risk assessment (QMRA) is the current method of choice for determining the risk to human health from exposure to microorganisms of concern. However, current approaches are often constrained by the availability of required data, and may not be able to incorporate the many varied factors that influence this risk. Systems models, based on Bayesian networks (BNs), are emerging as an effective complementary approach that overcomes these limitations. Objectives: This article aims to provide a comparative evaluation of the capabilities and challenges of current QMRA methods and BN models, and a scoping review of recent published articles that adopt the latter for microbial risk assessment. Pros and cons of systems approaches in this context are distilled and discussed. Methods: A search of the peer-reviewed literature revealed 15 articles describing BNs used in the context of QMRAs for foodborne and waterborne pathogens. These studies were analysed in terms of their application, uses and benefits in QMRA. Discussion: The applications were notable in their diversity. BNs were used to make predictions, for scenario assessment, risk minimisation, to reduce uncertainty and to separate uncertainty and variability. Most studies focused on a segment of the exposure pathway, indicating the broad potential for the method in other QMRA steps. BNs offer a number of useful features to enhance QMRA, including transparency, and the ability to deal with poor quality data and support causal reasoning. Conclusion: The method has significant untapped potential to describe the complex relationships between microbial environmental exposures and health. Crown Copyright (C) 2015 Published by Elsevier Ltd. All rights reserved.",""
"The use of Bayesian Belief Networks (BBNs) in risk analysis (and in particular Human Reliability Analysis, HRA) is fostered by a number of features, attractive in fields with shortage of data and consequent reliance on subjective judgments:. the intuitive graphical representation, the possibility of combining diverse sources of information, the use the probabilistic framework to characterize uncertainties. In HRA, BBN applications are steadily increasing, each emphasizing a different BBN feature or a different HRA aspect to improve. This paper aims at a critical review of these features as well as at suggesting research needs. Five groups of BBN applications are analysed: modelling of organizational factors, analysis of the relationships among failure influencing factors, BBN-based extensions of existing HRA methods, dependency assessment among human failure events, assessment of situation awareness. Further, the paper analyses the process for building BBNs and in particular how expert judgment is used in the assessment of the BBN conditional probability distributions. The gaps identified in the review suggest the need for establishing more systematic frameworks to integrate the different sources of information relevant for HRA (cognitive models, empirical data, and expert judgment) and to investigate algorithms to avoid elicitation of many relationships via expert judgment. (C) 2015 Elsevier Ltd. All rights reserved.",""
"This paper proposes a framework for risk analysis of maritime transportation systems, where risk analysis is understood as a tool for argumentative decision support. Uncertainty is given a more prominent role than in the current state of art in the maritime transportation application area, and various tools are presented for analyzing uncertainty. A two-stage risk description is applied. In the first stage, Bayesian Network (BN) modeling is applied for probabilistic risk quantification. The model functions as a communication and argumentation tool, serving as an aid to thinking in a qualitative evidence and assumption effect assessment. The evidence assessment is used together with a sensitivity analysis to select alternative hypotheses for the risk quantification, while the assumption effect assessment is used to convey an argumentation beyond the model. Based on this, a deliberative uncertainty judgment is made in the second risk analysis stage, which is supplemented with a global strength of evidence assessment. The framework is applied to a case study of oil spill from tanker collisions, aimed at response capacity planning and ecological risk assessment. The BN-model is a proactive and transferable tool for assessing the occurrence of various spill sizes in a sea area. While the case study uses evidence specific to the Gulf of Finland, the model and risk analysis approach can be applied to other areas. Based on evaluation criteria and tests for the risk model and risk analysis, it is found that the model is a plausible representation of tanker collision oil spill risk. (C) 2015 The Authors. Published by Elsevier Ltd.",""
"The exploration and development of oil and gas resources located in extreme and harsh offshore environments are characterized with high safety risk and drilling cost. Some of these resources would be either uneconomical if extracted using conventional overbalanced drilling due to increased drilling problems and prolonged non-productive time, or too risky to adopt underbalanced drilling technique. Seeking new ways to reduce drilling cost and minimize risks has led to the development of managed pressure drilling techniques. Managed pressure drilling methods address the drawbacks of conventional overbalanced and underbalanced drilling techniques. As managed pressure drilling techniques are evolving, there are many unanswered questions related to safety and operating pressure regime. This study investigates the safety and operational issues of constant bottom-hole pressure drilling technique which is used in managed pressure drilling compared to conventional overbalanced drilling. The study first uses bow-tie models to map safety challenges and operating pressure regimes in constant bottom-hole pressure drilling technique. Due to the difficulties in modeling dependencies and updating the belief on the operational data, the bow-ties are mapped into Bayesian networks. The Bayesian networks are thoroughly analyzed to assess the safety critical elements of constant bottom-hole pressure drilling techniques and their safe operating pressure regime. (C) 2015 Elsevier Ltd. All rights reserved.",""
"The physicochemical changes that occur in poultry egg during storage, make a reduction in its quality. The present research investigates the possibility of the nondestructive classification and quality inspection of eggs using dielectric detection technique in the range of radio frequency. Several machine learning (ML) techniques were developed for freshness detection including artificial neural networks (ANN), Bayesian networks (BNs), decision trees (DTs) and support vector machines (SVMs). Among ANNs, the ANN with topology of 62-18-6 gave a perfect capability to predict the class of freshness for all samples with accuracy of 100%. Also all types of BNs represented excellent results with Kappa statistic of 1 and overall accuracy of 100%. From developed SVMs, the SVM with polynomial kernel function gave the best results with Kappa value of I and accuracy of 100%. Among DTs, LMT tree had the highest Kappa value (0.846) and the highest accuracy (87.5%) compared to other DT approaches. Different ML methods were used to predict air cell height. Among ANNs the 24-12-1 structure with R value of 0.817, among DTs the MSP tree with R value of 0.906 and among SVMs the RBF form with R value of 0.920 had the highest value of correlation coefficient and the lowest standard error of 0.452. (C) 2015 Elsevier Ltd. All rights reserved.","The present research investigates the possibility of the nondestructive classification and quality inspection of eggs using dielectric detection technique in the range of radio frequency."
"Bayesian networks have become popular for modeling probabilistic relationships between entities. As their structure can also be given a causal interpretation about the studied system, they can be used to learn, for example, regulatory relationships of genes or proteins in biological networks and pathways. Inference of the Bayesian network structure is complicated by the size of the model structure space, necessitating the use of optimization methods or sampling techniques, such Markov Chain Monte Carlo (MCMC) methods. However, convergence of MCMC chains is in many cases slow and can become even a harder issue as the dataset size grows. We show here how to improve convergence in the Bayesian network structure space by using an adjustable proposal distribution with the possibility to propose a wide range of steps in the structure space, and demonstrate improved network structure inference by analyzing phosphoprotein data from the human primary T cell signaling network.","Inference of the Bayesian network structure is complicated by the size of the model structure space, necessitating the use of optimization methods or sampling techniques, such Markov Chain Monte Carlo (MCMC) methods."
"Motivation: Proteins are responsible for a multitude of vital tasks in all living organisms. Given that a protein's function and role are strongly related to its subcellular location, protein location prediction is an important research area. While proteins move from one location to another and can localize to multiple locations, most existing location prediction systems assign only a single location per protein. A few recent systems attempt to predict multiple locations for proteins, however, their performance leaves much room for improvement. Moreover, such systems do not capture dependencies among locations and usually consider locations as independent. We hypothesize that a multi-location predictor that captures location inter-dependencies can improve location predictions for proteins. Results: We introduce a probabilistic generative model for protein localization, and develop a system based on it-which we call MDLoc-that utilizes inter-dependencies among locations to predict multiple locations for proteins. The model captures location inter-dependencies using Bayesian networks and represents dependency between features and locations using a mixture model. We use iterative processes for learning model parameters and for estimating protein locations. We evaluate our classifier MDLoc, on a dataset of single- and multi-localized proteins derived from the DBMLoc dataset, which is the most comprehensive protein multi-localization dataset currently available. Our results, obtained by using MDLoc, significantly improve upon results obtained by an initial simpler classifier, as well as on results reported by other top systems.","We evaluate our classifier MDLoc, on a dataset of single- and multi-localized proteins derived from the DBMLoc dataset, which is the most comprehensive protein multi-localization dataset currently available."
"Background: Cell biology research is fundamentally limited by the number of intracellular components, particularly proteins, that can be co-measured in the same cell. Therefore, cell-to-cell heterogeneity in unmeasured proteins can lead to completely different observed relations between the same measured proteins. Attempts to infer such relations in a heterogeneous cell population can yield uninformative average relations if only one underlying biochemical network is assumed. To address this, we developed a method that recursively couples an iterative unmixing process with a Bayesian analysis of each unmixed subpopulation. Results: Our approach enables to identify the number of distinct cell subpopulations, unmix their corresponding observations and resolve the network structure of each subpopulation. Using simulations of the MAPK pathway upon EGF and NGF stimulations we assess the performance of the method. We demonstrate that the presented method can identify better than clustering approaches the number of subpopulations within a mixture of observations, thus resolving correctly the statistical relations between the proteins. Conclusions: Coupling the unmixing of multiplexed observations with the inference of statistical relations between the measured parameters is essential for the success of both of these processes. Here we present a conceptual and algorithmic solution to achieve such coupling and hence to analyze data obtained from a natural mixture of cell populations. As the technologies and necessity for multiplexed measurements are rising in the systems biology era, this work addresses an important current challenge in the analysis of the derived data.","Conclusions: Coupling the unmixing of multiplexed observations with the inference of statistical relations between the measured parameters is essential for the success of both of these processes."
"The Engineering Department of the Spanish Civil Guard has been using automatic speaker recognition systems for forensic purposes providing likelihood ratios since 2004. They are quantitatively much more modest than in the DNA field. In this context, it is essential a suitable calculation of the prior odds to figure out the posterior odds once the comparison result is expressed as likelihood ratio. These odds are under the responsibility of a Judge, and many consider unlikely that they can be quantitatively calculated in real cases. However, our experience defending in Court over 500 speaker recognition expert reports allows us to suggest how the expert may support Judges from a technical point of view to assess the odds. Technical support as referred should be preferentially provided in the preliminary investigation stage, after the expert report being issued by the laboratory, as in the course of oral hearings it is much more difficult for those who are not familiar with the new paradigm. It can be initiated upon request by the Examining Judge or any of the litigant parties. We consider this practice favourable to the equality of arms principle. The use of Bayesian networks is proposed to provide inferential assistance to the Judge when assessing the prior odds. An example of the explanation above is provided by the case of the terrorist attack against Madrid-Barajas Airport Terminal 4 perpetrated in December 2006.",""
"Recent cancer sequencing studies provide a wealth of somatic mutation data from a large number of patients. One of the most intriguing and challenging questions arising from this data is to determine whether the temporal order of somatic mutations in a cancer follows any common progression. Since we usually obtain only one sample from a patient, such inferences are commonly made from cross-sectional data from different patients. This analysis is complicated by the extensive variation in the somatic mutations across different patients, variation that is reduced by examining combinations of mutations in various pathways. Thus far, methods to reconstruct tumor progression at the pathway level have restricted attention to known, a priori defined pathways. In this work we show how to simultaneously infer pathways and the temporal order of their mutations from cross-sectional data, leveraging on the exclusivity property of driver mutations within a pathway. We define the pathway linear progression model, and derive a combinatorial formulation for the problem of finding the optimal model from mutation data. We show that with enough samples the optimal solution to this problem uniquely identifies the correct model with high probability even when errors are present in the mutation data. We then formulate the problem as an integer linear program (ILP), which allows the analysis of datasets from recent studies with large numbers of samples. We use our algorithm to analyze somatic mutation data from three cancer studies, including two studies from The Cancer Genome Atlas (TCGA) on large number of samples on colorectal cancer and glioblastoma. The models reconstructed with our method capture most of the current knowledge of the progression of somatic mutations in these cancer types, while also providing new insights on the tumor progression at the pathway level.","Since we usually obtain only one sample from a patient, such inferences are commonly made from cross-sectional data from different patients."
"A Multiregression Dynamic Model (MDM) is a class of multivariate time series that represents various dynamic causal processes in a graphical way. One of the advantages of this class is that, in contrast to many other Dynamic Bayesian Networks, the hypothesised relationships accommodate conditional conjugate inference. We demonstrate for the first time how straightforward it is to search over all possible connectivity networks with dynamically changing intensity of transmission to find the Maximum a Posteriori Probability (MAP) model within this class. This search method is made feasible by using a novel application of an Integer Programming algorithm. The efficacy of applying this particular class of dynamic models to this domain is shown and more specifically the computational efficiency of a corresponding search of 11-node Directed Acyclic Graph (DAG) model space. We proceed to show how diagnostic methods, analogous to those defined for static Bayesian Networks, can be used to suggest embellishment of the model class to extend the process of model selection. All methods are illustrated using simulated and real resting-state functional Magnetic Resonance Imaging (fMRI) data.","A Multiregression Dynamic Model (MDM) is a class of multivariate time series that represents various dynamic causal processes in a graphical way."
"The fate choice of human embryonic stem cells (hESCs) is controlled by complex signaling milieu synthesized by diverse chemical factors in the growth media. Prevalence of crosstalks and interactions between parallel pathways renders any analysis probing the process of fate transition of hESCs elusive. This work presents an important step in the evaluation of network level interactions between signaling molecules controlling endoderm lineage specification from hESCs using a statistical network identification algorithm. Network analysis was performed on detailed signaling dynamics of key molecules from TGF-beta/SMAD, PI3K/AKT and MAPK/ERK pathways under two common endoderm induction conditions. The results show the existence of significant crosstalk interactions during endoderm signaling and they identify differences in network connectivity between the induction conditions in the early and late phases of signaling dynamics. Predicted networks elucidate the significant effect of modulation of AKT mediated crosstalk leading to the success of PI3K inhibition in inducing efficient endoderm from hESCs in combination with TGF-beta/SMAD signaling.",""
"In order to meet the demand of testability analysis and evaluation for complex equipment under a small sample test in the equipment life cycle, the hierarchical hybrid testability modeling and evaluation method (HHTME), which combines the testability structure model (TSM) with the testability Bayesian networks model (TBNM), is presented. Firstly, the testability network topology of complex equipment is built by using the hierarchical hybrid testability modeling method. Secondly, the prior conditional probability distribution between network nodes is determined through expert experience. Then the Bayesian method is used to update the conditional probability distribution, according to history test information, virtual simulation information and similar product information. Finally, the learned hierarchical hybrid testability model (HHTM) is used to estimate the testability of equipment. Compared with the results of other modeling methods, the relative deviation of the HHTM is only 0.52%, and the evaluation result is the most accurate.",""
"In this paper, we propose a novel implicit video emotion tagging approach by exploring the relationships between videos' common emotions, subjects' individualized emotions and subjects' outer facial expressions. First, head motion and face appearance features are extracted. Then, the spontaneous facial expressions of subjects are recognized by Bayesian networks. After that, the relationships between the outer facial expressions, the inner individualized emotions and the video's common emotions are captured by another Bayesian network, which can be used to infer the emotional tags of videos. To validate the effectiveness of our approach, an emotion tagging experiment is conducted on the NVIE database. The experimental results show that head motion features improve the performance of both facial expression recognition and emotion tagging, and that the captured relations between the outer facial expressions, the inner individualized emotions and the common emotions improve the performance of common and individualized emotion tagging.",""
"Grasping and manipulating everyday objects in a goal-directed manner is an important ability of a service robot. The robot needs to reason about task requirements and ground these in the sensorimotor information. Grasping and interaction with objects are challenging in real-world scenarios, where sensorimotor uncertainty is prevalent. This paper presents a probabilistic framework for the representation and modeling of robot-grasping tasks. The framework consists of Gaussian mixture models for generic data discretization, and discrete Bayesian networks for encoding the probabilistic relations among various task-relevant variables, including object and action features as well as task constraints. We evaluate the framework using a grasp database generated in a simulated environment including a human and two robot hand models. The generative modeling approach allows the prediction of grasping tasks given uncertain sensory data, as well as object and grasp selection in a task-oriented manner. Furthermore, the graphical model framework provides insights into dependencies between variables and features relevant for object grasping.",""
"This study proposes a novel method for the construction of efficient and convenient Bayesian networks (BNs) and influence diagrams regarding medical problems based on fuzzy rules. The general methodology that was developed is able to address decisions based on fuzzy medical rules that connect symptoms with the severity/rating scale of a disease. These fuzzy rules are rich enough to cover a large variety of medical decisions. The method overcomes the major disadvantage of Bayesian nets, that is, the need of a vast amount of subjective probabilities. Instead of filling conditional probability tables in BNs, physicians report their knowledge in the form of fuzzy rules. The knowledge of these rules after defuzzification is transformed according to certain equations into probabilities. This becomes possible, categorizing the rules into certain types. For each type of rule, a mathematical expression is determined, which sets correctly the conditional probabilities that relate a symptom with its child severity. A particular example of assessing pulmonary infections and making decisions on severity degree was explored, implementing a decision support system (DSS) to show the functionality of the proposed methodology. The developed front-end DSS was evaluated and proved capable to drive fair decisions close to those obtained by physicians.",""
"Winter navigation is a complex but common operation in north-European sea areas. In Finnish waters, the smooth flow of maritime traffic and safety of vessel navigation during the winter period are managed through the Finnish-Swedish winter navigation system (FSWNS). This article focuses on accident risks in winter navigation operations, beginning with a brief outline of the FSWNS. The study analyses a hazard identification model of winter navigation and reviews accident data extracted from four winter periods. These are adopted as a basis for visualizing the risks in winter navigation operations. The results reveal that experts consider ship independent navigation in ice conditions the most complex navigational operation, which is confirmed by accident data analysis showing that the operation constitutes the type of navigation with the highest number of accidents reported. The severity of the accidents during winter navigation is mainly categorized as less serious. Collision is the most typical accident in ice navigation and general cargo the type of vessel most frequently involved in these accidents. Consolidated ice, ice ridges and ice thickness between 15 and 40 cm represent the most common ice conditions in which accidents occur. Thus, the analysis presented in this article establishes the key elements for identifying the operation types which would benefit most from further safety engineering and safety or risk management development. (C) 2015 Elsevier Ltd. All rights reserved.",""
"Dependable sensor data are vital in complex systems, which rely on a suite of sensors for control as well as condition monitoring. With any unanticipated deviations in sensor values, the challenge is to determine if the anomalies are the result of one or more flawed sensors or if it is indicative of a potentially more serious system-level fault. This paper describes a methodology using Bayesian networks to distinguish between sensor and process faults as well as faults involving multiple sensors or processes. A review of existing methodologies is presented first, followed by a description of the sensor/process fault detection and isolation (SPFDI) algorithm, its limitations and corresponding mitigating strategies. Discussions are also provided on the potential for false alarms and real-time updates of the system model based on validated sensor data. Factors that affect the algorithm such as the effect of network structure, sensor characteristics, effect of discretization, etc., are discussed. This is followed by details of implementation of the algorithm on an electromechanical actuator (EMA) test bed.",""
"Probabilistic causal interaction models have become quite popular among Bayesian-network engineers as elicitation of all probabilities required often proves the main bottleneck in building a real-world network with domain experts. The best-known interaction models are the noisy-OR model and its generalisations. These models in essence are parameterised conditional probability tables for which just a limited number of parameter probabilities are required. The models assume specific properties of intercausal interaction and cannot be applied uncritically. Given their clear engineering advantages however, they are subject to ill-considered use. This paper demonstrates that such ill-considered use can result in poorly calibrated output probabilities from a Bayesian network. By studying, in an analytical way, the propagation effects of noisy-OR calculated probability values, we identify conditions under which use of the model can be harmful for a network's performance. These conditions demonstrate that use of the noisy-OR model for mere pragmatic reasons is sometimes warranted, even when the model's underlying assumptions are not met in reality. (C) 2015 Elsevier Inc. All rights reserved.",""
"Rivers are complex systems for which it is hard to make reliable assessments of causes and responses to impairments. We present a holistic risk-based framework for river ecosystem assessment integrating all potential intervening processes and functions. Risk approaches allow us to deal with uncertainty both in the construction of indicators for magnitude of stressors and in the inference of environmental processes and their impairment. Yet, here we go further than simply replacing uncertainty by a risk factor. We introduce a more accurate and rigorous notion of risk with a transcription of uncertainty in causal relationships in probability distributions for the magnitude of impairment and the weight of different descriptors, with an associated confidence in the diagnostic. We discuss how Bayesian belief networks and Bayesian hierarchical inference allow us to deal with this risk concept to predict impairments and potential recovery of river ecosystems. We developed a comprehensive approach for river ecosystem assessment, which offers an appealing tool to facilitate diagnosis of the likely causes of impairment and predict future conditions. The ability of the risk approaches to integrate multi-scale quantitative and qualitative descriptors in the identification of multiple stressor sources and pathways in the stream network, and their impairment of specific processes and structures is illustrated for the national-level risk analysis for hydromorphology and pesticide pollution. Not only does the risk-based framework provide a more complete picture of environmental impairments, but it also offers a comprehensive, user-friendly tool to instruct the decision process.","Risk approaches allow us to deal with uncertainty both in the construction of indicators for magnitude of stressors and in the inference of environmental processes and their impairment."
"Bayesian Networks are increasingly popular methods of modeling uncertainty in artificial intelligence and machine learning. A Bayesian Network consists of a directed acyclic graph in which each node represents a variable and each arc represents probabilistic dependency between two variables. Constructing a Bayesian Network from data is a learning process that consists of two steps: learning structure and learning parameter. Learning a network structure from data is the most difficult task in this process. This paper presents a new algorithm for constructing an optimal structure for Bayesian Networks based on optimization. The algorithm has two major parts. First, we define an optimization model to find the better network graphs. Then, we apply an optimization approach for removing possible cycles from the directed graphs obtained in the first part which is the first of its kind in the literature. The main advantage of the proposed method is that the maximal number of parents for variables is not fixed a priory and it is defined during the optimization procedure. It also considers all networks including cyclic ones and then choose a best structure by applying a global optimization method. To show the efficiency of the algorithm, several closely related algorithms including unrestricted dependency Bayesian Network algorithm, as well as, benchmarks algorithms SVM and C4.5 are employed for comparison. We apply these algorithms on data classification; data sets are taken from the UCI machine learning repository and the LIBSVM.","We apply these algorithms on data classification; data sets are taken from the UCI machine learning repository and the LIBSVM."
"Managers and operators of major hazard facilities make complex decisions as a part of their daily work activity. These decisions are made against the background potential for a major accident. Such decisions may be required to account for daily changes in a large number of factors including plant condition and performance, operational status, knowledge and experience of personnel, interactions with other activities, and the effectiveness of processes. The information involved in the decision comes from multiple sources and may be difficult to assess. Technical risk assessments provide a useful picture of major accident risk but some widely accepted approaches suffer from some significant problems which limit their value as tools for operational decision making. The article describes investigations into an approach that addresses how these difficulties may be addressed in day-to-day assessments. It describes a method and tool in which risks can be monitored in real-time and so enable safer decision making. The method is applicable to the assessment of a wide range of major accident hazard scenarios. The article will describe how the tool addresses problems in some alternative approaches. A significant feature of the approach is its ability to identify the most probable causes of risk. The speed of the assessment points to its potential use in real-time detection and control systems. The method employs a Bayesian net to perform the risk assessment. Bayesian nets have been used to aid decision making in many different situations and industries, but have received relatively little attention as risk assessment and decision tools in major hazard industries. The article will include a description of the benefits offered by this technology as well as a view of its limitations. (c) 2014 American Institute of Chemical Engineers Process Saf Prog 34: 183-190, 2015",""
"In this paper, we study Bayesian network (BN) for form identification based on partially filled fields. It uses electronic ink-tracing files without having any information about form structure. Given a form format, the ink-tracing files are used to build the BN by providing the possible relationships between corresponding fields using conditional probabilities, that goes from individual fields up to the complete model construction. To simplify the BN, we sub-divide a single form into three different areas: header, body and footer, and integrate them together, where we study three fundamental BN learning algorithms: Naive, Peter & Clark and maximum weighted spanning tree. Under this framework, we validate it with a real-world industrial problem i.e., electronic note-taking in form processing. The approach provides satisfactory results, attesting the interest of BN for exploiting the incomplete form analysis problems, in particular.",""
"Natural image processing and understanding encompasses hundreds of different algorithms. Each algorithm generates best results for a particular set of input features and configurations of the objects/regions in the input image (environment). To obtain the best possible result of processing in a reliable manner, we propose an algorithm selection approach that selects the best algorithm for a each input image. The proposed algorithm selection starts by first selecting an algorithm using low level features such as color intensity, histograms, spectral coefficients or so and a user given context if available. The resulting high-level image description is analyzed for logical inconsistencies (contradictions) and image regions that must be processed using a different algorithm are selected. The high-level description and the optional user-given context are used by a Bayesian Network to estimate the cause of the error in the processing. The same Bayesian Network also generates new candidate algorithm for each region containing the contradiction in an iterative manner. This iterative selection stops when the high-level inconsistencies are all resolved or no more different algorithms can be selected. We also show that when inconsistencies can be detected, our framework is able to improve high-level description when compared with single algorithms. In order for such complex and iterative processing being computationally tractable we also introduce a hardware platform based on reconfigurable VLSI that is well suited as the platform of the proposed approach. We show that the algorithm selected approach is ideally suited for either a hybrid type VLSI processor or for a Logic-In-Memory processing platform.",""
"This article addresses the problem of robustly estimating the dynamic state of a mechanism from a set of noisy sensor measurements. We start with a rigorous treatment of the problem from the perspective of graphical models, a popular formalism in the fields of statistical inference and machine learning. The modeling power of such a formalism is demonstrated by showing how the sequential estimation of a mechanism state with an extended Kalman filter (EKF), often used in previous works, becomes just one of the possible solutions. As an interesting alternative, we derive the formulation of a sequential Monte Carlo (SMC) filter, also known as a particle filter (PF), suitable for online tracking the state of a rigid mechanism. We validate our ideas with both simulated and real datasets. Moreover, we prove the usefulness of the particle filtering solution for real-work applications due to its unmatched capability of automatically inferring the initial states of the mechanism along with its \"assembly configuration\" or \"branch\" if several ones are possible, a feature not matched by any previously proposed state observer in the multibody literature.","We start with a rigorous treatment of the problem from the perspective of graphical models, a popular formalism in the fields of statistical inference and machine learning."
"In data-mining algorithms contingency tables are frequently built from ADtrees, as ADtrees have been demonstrated to be an efficient data structure for caching sufficient statistics. This paper introduces three modifications. The first two use a one-dimensional array and a hash map for representing contingency tables, and the third uses the non-recursive approach to build contingency tables from sparse ADtrees. We implement algorithms to construct contingency tables with a two-dimensional array, a tree, a one-dimensional array, and a hash map using recursion and non-recursive approaches in Python. We empirically test these algorithms in five aspects with a large number of randomly generated datasets. We also apply the modified algorithms to Bayesian networks learning and test the performance improvements using three real-life datasets. We demonstrate experimentally that all three of these modifications improve algorithm performance. The improvements are more significant with higher arities and larger arity values.",""
"The use of expert systems can be helpful to improve the transparency and repeatability of assessments in areas of risk analysis with limited data available. In this field, human reliability analysis (HRA) is no exception, and, in particular, dependence analysis is an HRA task strongly based on analyst judgement. The analysis of dependence among Human Failure Events refers to the assessment of the effect of an earlier human failure on the probability of the subsequent ones. This paper analyses and compares two expert systems, based on Bayesian Belief Networks and Fuzzy Logic (a Fuzzy Expert System, FES), respectively. The comparison shows that a BBN approach should be preferred in all the cases characterized by quantifiable uncertainty in the input (i.e. when probability distributions can be assigned to describe the input parameters uncertainty), since it provides a satisfactory representation of the uncertainty and its output is directly interpretable for use within PSA. On the other hand, in cases characterized by very limited knowledge, an analyst may feel constrained by the probabilistic framework, which requires assigning probability distributions for describing uncertainty. In these cases, the FES seems to lead to a more transparent representation of the input and output uncertainty. (C) 2015 Elsevier Ltd. All rights reserved.",""
"The material and modeling parameters that drive structural reliability analysis for marine structures are subject to a significant uncertainty. This is especially true when time-dependent degradation mechanisms such as structural fatigue cracking are considered. Through inspection and monitoring, information such as crack location and size can be obtained to improve these parameters and the corresponding reliability estimates. Dynamic Bayesian Networks (DBNs) are a powerful and flexible tool to model dynamic system behavior and update reliability and uncertainty analysis with life cycle data for problems such as fatigue cracking. However, a central challenge in using DBNs is the need to discretize certain types of continuous random variables to perform network inference while still accurately tracking low-probability failure events. Most existing discretization methods focus on getting the overall shape of the distribution correct, with less emphasis on the tail region. Therefore, a novel scheme is presented specifically to estimate the likelihood of low-probability failure events. The scheme is an iterative algorithm which dynamically partitions the discretization intervals at each iteration. Through applications to two stochastic crack-growth example problems, the algorithm is shown to be robust and accurate. Comparisons are presented between the proposed approach and existing methods for the discretization problem. (C) 2015 Elsevier Ltd. All rights reserved.","However, a central challenge in using DBNs is the need to discretize certain types of continuous random variables to perform network inference while still accurately tracking low-probability failure events."
"Mining is an economic sector with a high number of accidents. Mines are hazardous places and workers can suffer a wide variety of injuries. Utilizing a database composed of almost 70,000 occupational accidents and fatality reports corresponding to the decade 2003-2012 in the Spanish mining sector, the paper analyzes the main causes of those accidents. To carry out the study, powerful statistical tools have been applied, such as Bayesian classifiers, decision trees or contingency tables, among other data mining techniques. Statistical analyses have been performed using Weka software and behavioral patterns based on certain rules have been obtained. From these rules, some conclusions are extracted which can help to develop suitable prevention policies to reduce injuries and fatalities. (C) 2015 Elsevier Ltd. All rights reserved.","To carry out the study, powerful statistical tools have been applied, such as Bayesian classifiers, decision trees or contingency tables, among other data mining techniques."
"This paper proposes a novel approach to select the individual classifiers to take part in a Multiple-Classifier System. Individual classifier selection is a key step in the development of multi-classifiers. Several works have shown the benefits of fusing complementary classifiers. Nevertheless, the selection of the base classifiers to be used is still an open question, and different approaches have been proposed in the literature. This work is based on the selection of the appropriate single classifiers by means of an evolutionary algorithm. Different base classifiers, which have been chosen from different classifier families, are used as candidates in order to obtain variability in the classifications given. Experimental results carried out with 20 databases from the UCI Repository show how adequate the proposed approach is; Stacked Generalization multi-classifier has been selected to perform the experimental comparisons. (C) 2015 Elsevier B.V. All rights reserved.","This paper proposes a novel approach to select the individual classifiers to take part in a Multiple-Classifier System."
"One of the most common problems faced by planners, whether in industry or government, is optimisation-finding the optimal solution to a problem. Even a one percent improvement in a solution can make a difference of millions of dollars in some cases. Traditionally optimisation problems are solved by analytic means or exact optimisation methods. Today, however, many optimisation problems in the design of embedded architectures involve complex combinatorial systems that make such traditional approaches unsuitable or intractable. Genetic algorithms, instead, tackle these kind of problems by finding good solutions in a reasonable amount of time. Their successful application, however, relies on algorithm parameters which are problem dependent, and usually even depend on the problem instance at hand. To address this issue, we propose an adaptive parameter control method for genetic algorithms, which adjusts parameters during the optimisation process. The central aim of this work is to assist practitioners in solving complex combinatorial optimisation problems by adapting the optimisation strategy to the problem being solved. We present a case study from the automotive industry, which shows the efficiency and applicability of the proposed adaptive optimisation approach. The experimental evaluation indicates that the proposed approach outperforms optimisation methods with pre-tuned parameter values and three prominent adaptive parameter control techniques.",""
"Multi-output inference tasks, such as multi-label classification, have become increasingly important in recent years. A popular method for multi-label classification is classifier chains, in which the predictions of individual classifiers are cascaded along a chain, thus taking into account inter-label dependencies and improving the overall performance. Several varieties of classifier chain methods have been introduced, and many of them perform very competitively across a wide range of benchmark datasets. However, scalability limitations become apparent on larger datasets when modelling a fully cascaded chain. In particular, the methods' strategies for discovering and modelling a good chain structure constitute a major computational bottleneck. In this paper, we present the classifier trellis (CT) method for scalable multi-label classification. We compare CT with several recently proposed classifier chain methods to show that it occupies an important niche: it is highly competitive on standard multi-label problems, yet it can also scale up to thousands or even tens of thousands of labels. (C) 2015 Elsevier Ltd. All rights reserved.","Multi-output inference tasks, such as multi-label classification, have become increasingly important in recent years."
NA,""
"The adequate operation for a number of service distribution networks relies on the effective maintenance and fault management of their underlay DSL infrastructure. Thus, new tools are required in order to adequately monitor and further diagnose anomalies that other segments of the DSL network cannot identify due to the pragmatic issues raised by hardware or software misconfigurations. In this work we present a fundamentally new approach for classifying known DSL-level anomalies by exploiting the properties of novelty detection via the employment of one-class Support Vector Machines (SVMs). By virtue of the imbalance residing in the training samples that consequently lead to problematic prediction outcomes when used within two-class formulations, we adopt the properties of one-class classification and construct models for independently identifying and classifying a single type of a DSL-level anomaly. Given the fact that the greater number of the installed Digital Subscriber Line Access Multiplexers (DSLAMs) within the DSL network of a large European ISP were misconfigured, thus unable to accurately flag anomalous events, we utilize as inference solutions the models derived by the one-class SVM formulations built by the known labels as flagged by the much smaller number of correctly configured DSLAMs in the same network in order to aid the classification aspect against the monitored unlabeled events. By reaching an average over 95% on a number of classification accuracy metrics such as precision, recall and F-score we show that one-class SVM classifiers overcome the biased classification outcomes achieved by the traditional two-class formulations and that they may constitute as viable and promising components within the design of future network fault management strategies. In addition, we demonstrate their superiority over commonly used two-class machine learning approaches such as Decision Trees and Bayesian Networks that has been used in the same context within past solutions. (C) 2015 Elsevier B.V. All rights reserved.","By virtue of the imbalance residing in the training samples that consequently lead to problematic prediction outcomes when used within two-class formulations, we adopt the properties of one-class classification and construct models for independently identifying and classifying a single type of a DSL-level anomaly."
"The growth of maritime oil transportation in the Gulf of Finland (GoF), North-Eastern Baltic Sea, increases environmental risks by increasing the probability of oil accidents. By integrating the work of a multidisciplinary research team and information from several sources, we have developed a probabilistic risk assessment application that considers the likely future development of maritime traffic and oil transportation in the area and the resulting risk of environmental pollution. This metamodel is used to compare the effects of two preventative management actions on the tanker collision probabilities and the consequent risk. The resulting risk is evaluated from four different perspectives. Bayesian networks enable large amounts of information about causalities to be integrated and utilized in probabilistic inference. Compared with the baseline period of 2007-2008, the worst-case scenario is that the risk level increases 4-fold by the year 2015. The management measures are evaluated and found to decrease the risk by 4-13%, but the utility gained by their joint implementation would be less than the sum of their independent effects. In addition to the results concerning the varying risk levels, the application provides interesting information about the relationships between the different elements of the system.","Bayesian networks enable large amounts of information about causalities to be integrated and utilized in probabilistic inference."
"In the past decade, population genetics has gained tremendous success in identifying genetic variations that are statistically relevant to renal diseases and kidney function. However, it is challenging to interpret the functional relevance of the genetic variations found by population genetics studies. In this review, we discuss studies that integrate multiple levels of data, especially transcriptome profiles and phenotype data, to assign functional roles of genetic variations involved in kidney function. Furthermore, we introduce state-of-the-art machine learning algorithms, Bayesian networks, support vector machines, and Gaussian process regression, which have been applied successfully to integrating genetic, regulatory, and clinical information to predict clinical outcomes. These methods are likely to be deployed successfully in the nephrology field in the near future. (C) 2015 Elsevier Inc. All rights reserved.","Furthermore, we introduce state-of-the-art machine learning algorithms, Bayesian networks, support vector machines, and Gaussian process regression, which have been applied successfully to integrating genetic, regulatory, and clinical information to predict clinical outcomes."
"How can we be sure that sufficient safeguards are in place and safety level is acceptable? As we heard Prof. Nancy Leveson stating at last year's MKOPSC symposium, even with all components functioning, dysfunctional component interaction can still be a cause of mishap. Human factor expert, Prof. Erik Hollnagel, asserts it in even stronger terms: the Efficiency-Thoroughness Trade-off principle, or rather dilemma, contends that one can hardly do it perfectly well. Perfect thoroughness, certainly in complex situations, requires an amount of time with which efficiency will be in conflict. For improved situational awareness, sufficient resilience, and adequate risk control, we must adopt a top-down system approach. Hazard scenarios possible in the system, with all its entangled interactions of hardware, procedures, and humans shall be identified bottom-up and causal relations made clear. Fortunately, in recent years two potentially helpful tools have become available: Blended Hazid, a vastly improved, heavily computerized system approach making use of HazOp and FMEA, and Bayesian networks, a tool to model cause effect structures allowing inclusion of uncertainty information. Bayesian networks as an infrastructure enable also the use of indicator values to relate the result of safety management effectiveness, which expresses itself as safety attitude of employees, competence, workload, and motivation, with their effects on error and failure probability. This paper will explain the directions these developments are advancing and the openings they provide for further process safety research and risk assessment, which when applied will result in improved process risk control. (c) 2014 Elsevier Ltd. All rights reserved.",""
"This paper presents the results of research on the use of smartphone sensors (namely, GPS and accelerometers), geospatial information (points of interest, such as bus stops and train stations) and machine learning (ML) to sense mobility contexts. Our goal is to develop techniques to continuously and automatically detect a smartphone user's mobility activities, including walking, running, driving and using a bus or train, in real-time or near-real-time (<5 s). We investigated a wide range of supervised learning techniques for classification, including decision trees (DT), support vector machines (SVM), naive Bayes classifiers (NB), Bayesian networks (BN), logistic regression (LR), artificial neural networks (ANN) and several instance-based classifiers (KStar, LWLand IBk). Applying ten-fold cross-validation, the best performers in terms of correct classification rate (i.e., recall) were DT (96.5%), BN (90.9%), LWL (95.5%) and KStar (95.6%). In particular, the DT-algorithm RandomForest exhibited the best overall performance. After a feature selection process for a subset of algorithms, the performance was improved slightly. Furthermore, after tuning the parameters of RandomForest, performance improved to above 97.5%. Lastly, we measured the computational complexity of the classifiers, in terms of central processing unit (CPU) time needed for classification, to provide a rough comparison between the algorithms in terms of battery usage requirements. As a result, the classifiers can be ranked from lowest to highest complexity (i.e., computational cost) as follows: SVM, ANN, LR, BN, DT, NB, IBk, LWL and KStar. The instance-based classifiers take considerably more computational time than the non-instance-based classifiers, whereas the slowest non-instance-based classifier (NB) required about five-times the amount of CPU time as the fastest classifier (SVM). The above results suggest that DT algorithms are excellent candidates for detecting mobility contexts in smartphones, both in terms of performance and computational complexity.","We investigated a wide range of supervised learning techniques for classification, including decision trees (DT), support vector machines (SVM), naive Bayes classifiers (NB), Bayesian networks (BN), logistic regression (LR), artificial neural networks (ANN) and several instance-based classifiers (KStar, LWLand IBk)."
"When two or more crimes show specific similarities, such as a very distinct modus operandi, the probability that they were committed by the same offender becomes of interest. This probability depends on the degree of similarity and distinctiveness. We show how Bayesian networks can be used to model different evidential structures that can occur when linking crimes, and how they assist in understanding the complex underlying dependencies. That is, how evidence that is obtained in one case can be used in another and vice versa. The flip side of this is that the intuitive decision to \"unlink\" a case in which exculpatory evidence is obtained leads to serious overestimation of the strength of the remaining cases. (C) 2014 The Chartered Society of Forensic Sciences. Published by Elsevier Ireland Ltd. All rights reserved.",""
"To construct an accurate probability density function for lead time demand in inventory management models, a mixture of polynomials (MOPs) distributions is estimated using B-spline functions from empirical data on demand per unit time. This is accomplished by summarizing the empirical observations into separate datasets for each lead time value. A mixture distribution approach is then applied to model lead time demand in a continuous review inventory system. Inventoiy policies can be determined without knowledge of the underlying demand and/or lead time distributions. An improvement to a mixture distribution approach that models the lead time demand distribution with a mixture of truncated exponentials (MTEs) distribution is also presented, and the MOP and MTE techniques are compared. Both methods provide reasonable accuracy, but the MOP approach requires lower computational time to determine optimal inventory policies. The mixture distribution approach is also compared with solutions calculated using optimization through simulation and by compiling a discrete, empirical lead time demand distribution. (C) 2015 Elsevier B.V. All rights reserved.",""
"Transfer learning aims to provide a framework to utilize previously-acquired knowledge to solve new but similar problems much more quickly and effectively. In contrast to classical machine learning methods, transfer learning methods exploit the knowledge accumulated from data in auxiliary domains to facilitate predictive modeling consisting of different data patterns in the current domain. To improve the performance of existing transfer learning methods and handle the knowledge transfer process in real-world systems, computational intelligence has recently been applied in transfer learning. This paper systematically examines computational intelligence-based transfer learning techniques and clusters related technique developments into four main categories: (a) neural network-based transfer learning; (b) Bayes-based transfer learning; (c) fuzzy transfer learning, and (d) applications of computational intelligence-based transfer learning. By providing state-of-the-art knowledge, this survey will directly support researchers and practice-based professionals to understand the developments in computational intelligence-based transfer learning research and applications. (C) 2015 Elsevier B.V. All rights reserved.",""
"An experience sampling method (ESM) study on 40 volunteers was conducted to explore the environmental factors and psychological conditions related to involuntary musical imagery (INMI) in everyday life. Participants reported 6 times per day for one week on their INMI experiences, relevant contextual information and associated environmental conditions. The resulting data was modeled with Bayesian networks and led to insights into the interplay of factors related to INMI experiences. The activity that a person is engaged was found to play an important role in the experience of mind wandering, which in turn enables the experience of INMI. INMI occurrence is independent of the time of the day while the INMI trigger affects the subjective evaluation of the INMI experience. The results are compared to findings from earlier studies based on retrospective surveys and questionnaires and highlight the advantage of ESM techniques in research on spontaneous experiences like INMI. (C) 2015 Elsevier Inc. All rights reserved.",""
"Functional diagnosis for complex systems can be a very time-consuming and expensive task, trying to identify the source of an observed misbehavior. We propose an automatic incremental diagnostic methodology and CAD flow, based on data mining (DM). It is a model-based approach that incrementally determines the tests to be executed to isolate the faulty component, aiming at minimizing the total number of executed tests, without compromising 100% diagnostic accuracy. The DM engine allows for shorter test sequences with respect to other reasoning-based solutions (e.g., Bayesian belief networks), not requiring complex pre and post-conditions management. Experimental results on a large set of synthetic examples and on three industrial boards substantiate the quality of the proposed approach.",""
"The paradigm of pervasive computing is gaining more and more attention nowadays, thanks to the possibility of obtaining precise and continuous monitoring. Ease of deployment and adaptivity are typically implemented by adopting autonomous and cooperative sensory devices; however, for such systems to be of any practical use, reliability and fault tolerance must be guaranteed, for instance by detecting corrupted readings amidst the huge amount of gathered sensory data. This paper proposes an adaptive distributed Bayesian approach for detecting outliers in data collected by a wireless sensor network; our algorithm aims at optimizing classification accuracy, time complexity and communication complexity, and also considering externally imposed constraints on such conflicting goals. The performed experimental evaluation showed that our approach is able to improve the considered metrics for latency and energy consumption, with limited impact on classification accuracy.","This paper proposes an adaptive distributed Bayesian approach for detecting outliers in data collected by a wireless sensor network; our algorithm aims at optimizing classification accuracy, time complexity and communication complexity, and also considering externally imposed constraints on such conflicting goals."
"Particle filter (PF) is a method dedicated to posterior density estimations using weighted samples whose elements are called particles. In particular, this approach can be applied to object tracking in video sequences in complex situations and, in this paper, we focus on articulated object tracking, i.e., objects that can be decomposed as a set of subparts. One of PF's crucial step is a resampling step in which particles are resampled to avoid degeneracy problems. In this paper, we propose to exploit mathematical properties of articulated objects to swap conditionally independent subparts of the particles in order to generate new particle sets. We then introduce a new resampling method called Combinatorial Resampling that resamples over the particle set resulting from all the \"admissible\" swappings, the so-called combinatorial set. In essence, combinatorial resampling (CR) is quite similar to the combination of a crossover operator and a usual resampling, but there exists a fundamental difference between CR and the use of crossover operators: we prove that CR is sound, i.e., in a Bayesian framework, it is guaranteed to represent without any bias the posterior densities of the states over time. By construction, the particle sets produced by CR better represent the density to estimate over the whole state space than the original set and, therefore, CR produces higher quality samples. Unfortunately, the combinatorial set is generally of an exponential size and, therefore, to be scalable, we show how it can be implicitly constructed and resampled from, thus resulting in both an efficient and effective resampling scheme. Finally, through experimentations both on challenging synthetic and real video sequences, we also show that our resampling method outperforms all classical resampling methods both in terms of the quality of its results and in terms of computation times.",""
"Bayesian Belief Networks (BBNs) are being increasingly used to develop a range of predictive models and risk assessments for ecological systems. Ecological BBNs can be applied to complex catchment and water quality issues, integrating multiple spatial and temporal variables within social, economic and environmental decision making processes. This paper reviews the essential components required for ecologists to design a best-practice predictive BBN in an ecological risk assessment (ERA) framework for aquatic ecosystems, outlining: (1) how to create a BBN for an aquatic ERA?; (2) what are the challenges for aquatic ecologists in adopting the best-practice applications of BBNs to ERAs?; and (3) how can BBNs in ERAs influence the science/management interface into the future? The aims of this paper are achieved using three approaches. The first is to demonstrate the best-practice development of BBNs in aquatic sciences using a simple nutrient model. The second is to discuss the limitations and challenges aquatic ecologists encounter when applying BBNs to ERAs. The third is to provide a framework for integrating best-practice BBNs into ERAs and the management of aquatic ecosystems. A quantitative review of the application and development of BBNs in aquatic science from 2002 to 2014 was conducted to identify areas where continued best-practice development is required. We outline a best-practice framework for the integration of BBNs into ERAs and study of complex aquatic systems. (C) 2015 Elsevier Ltd. All rights reserved.",""
"Wildfires will continue to reach people and property regardless of management effort in the landscape. House-based strategies are therefore required to complement the landscape strategies in order to reduce the extent of house loss. Here we use a Bayesian Network approach to quantify the relative influence of preventative and suppressive management strategies on the probability of house loss in Australia. Community education had a limited effect on the extent to which residents prepared their property hence a limited effect on the reduction in risk of house loss, however hypothetically improving property preparedness did reduce the risk of house loss. Increasing expenditure on suppression resources resulted in a greater reduction in the risk of loss than preparedness. This increase had an interaction effect with increasing the distance between vegetation and the houses. The extent to which any one action can be implemented is limited by social, environmental and economic factors. (C) 2015 Published by Elsevier Ltd.",""
"With the increasing use of multimedia data in communication technologies, the idea of employing visual information in automatic speech recognition (ASR) has recently gathered momentum. In conjunction with the acoustical information, the visual data enhances the recognition performance and improves the robustness of ASR systems in noisy and reverberant environments. In audio-visual systems, dynamic weighting of audio and video streams according to their instantaneous confidence is essential for reliably and systematically achieving high performance. In this paper, we present a complete framework that allows blind estimation of dynamic stream weights for audio-visual speech recognition based on coupled hidden Markov models (CHMMs). As a stream weight estimator, we consider using multilayer perceptrons and logistic functions to map multidimensional reliability measure features to audiovisual stream weights. Training the parameters of the stream weight estimator requires numerous input-output tuples of reliability measure features and their corresponding stream weights. We estimate these stream weights based on oracle knowledge using an expectation maximization algorithm. We define 31-dimensional feature vectors that combine model-based and signal-based reliability measures as inputs to the stream weight estimator. During decoding, the trained stream weight estimator is used to blindly estimate stream weights. The entire framework is evaluated using the Grid audio-visual corpus and compared to state-of-the-art stream weight estimation strategies. The proposed framework significantly enhances the performance of the audio-visual ASR system in all examined test conditions.",""
"Quality of Experience (QoE) as an aggregate of Quality of Service (QoS) and human user-related metrics will be the key success factor for current and future mobile computing systems. QoE measurement and prediction are complex tasks as they may involve a large parameter space such as location, delay, jitter, packet loss, and user satisfaction just to name a few. These tasks necessitate the development of practical context-aware QoE models that efficiently determine relationships between user context and QoE parameters. In this paper, we propose, develop, and validate a novel decision-theoretic approach called CaQoEM for QoE modelling, measurement, and prediction. We address the challenge of QoE measurement and prediction where each QoE parameter can be measured on a different scale and may involve different units of measurement. CaQoEM is context-aware and uses Bayesian networks and utility theory to measure and predict users' QoE under uncertainty. We validate CaQoEM using extensive experimentation, user studies and simulations. The results soundly demonstrate that CaQoEM correctly measures range-defined QoE using a bipolar scale. For QoE prediction, an overall accuracy of 98.93% was achieved using 10-fold cross validation in multiple diverse network conditions such as vertical handoffs, wireless signal fading and wireless network congestion.",""
"In this paper, we propose deep transfer learning for classification of Gaussian networks with time-delayed regulations. To ensure robust signaling, most real world problems from related domains have inherent alternate pathways that can be learned incrementally from a stable form of the baseline. In this paper, we leverage on this characteristic to address the challenges of complexity and scalability. The key idea is to learn high dimensional network motifs from low dimensional forms through a process of transfer learning. In contrast to previous work, we facilitate positive transfer by introducing a triangular inequality constraint, which provides a measure for the feasibility of mapping between different motif manifolds. Network motifs from different classes of Gaussian networks are used collectively to pre-train a deep neural network governed by a Lyapunov stability condition. The proposed framework is validated on time series data sampled from synthetic Gaussian networks and applied to a real world dataset for the classification of basketball games based on skill level. We observe an improvement in the range of [15-25]% in accuracy and a saving in the range of [25-600]% in computational cost on synthetic as well as realistic networks with time-delays when compared to existing state-of-the-art approaches. In addition, new insights into meaningful offensive formations in the Basketball games can be derived from the deep network. (C) 2014 Elsevier B.V. All rights reserved.","In this paper, we propose deep transfer learning for classification of Gaussian networks with time-delayed regulations."
"A generative model for modelling maritime vessel behaviour is proposed. The model is a novel variant of the dynamic Bayesian network (DBN). The proposed DBN is in the form of a switching linear dynamic system (SLDS) that has been extended into a larger DBN. The application of synthetic data fabrication of maritime vessel behaviour is considered. Behaviour of various vessels in a maritime piracy situation is simulated. A means to integrate information from context based external factors that influence behaviour is provided. Simulated observations of the vessels kinematic states are generated. The generated data may be used for the purpose of developing and evaluating counter-piracy methods and algorithms. A novel methodology for evaluating and optimising behavioural models such as the proposed model is presented. The log-likelihood, cross entropy, Bayes factor and the Bhattacharyya distance measures are applied for evaluation. The results demonstrate that the generative model is able to model both spatial and temporal datasets. (C) 2014 Elsevier B.V. All rights reserved.",""
"This paper presents a, novel probabilistic modeling and inference method that is computationally more efficient than Bayesian networks (BNs). This method is applicable to systems with Continuous variables. It is also applicable to systems with discrete variables that have an adequately high number of states. This work, indeed, represents an application of the rolling pin method introduced in our earlier paper [Mohsent Ahooyi.et al., Ind. Ehg. Chem. Res, DOI: 10.1021/ie50584q]. Unlike BNs, the method does not require knowledge of the causal relationships among the variables, because the rolling pin method Models jointprobabilities without causal factonzation of the distributions. Furtherrnore, learning and inference steps are performed much faster by this method than by BNs, and the method enables one to perform local inference ovot any set of query variables; it allows for probabilities to be calculated over regions with no historical data, and it prevents information loss and an increase in the computational cost, both due to discretization of continuous variables. The application and performance of the method are shown through two examples.","This paper presents a, novel probabilistic modeling and inference method that is computationally more efficient than Bayesian networks (BNs)."
"Dynamic gene-regulatory networks are complex since the interaction patterns between their components mean that it is impossible to study parts of the network in separation. This holistic character of gene-regulatory networks poses a real challenge to any type of modelling. Graphical models are a class of models that connect the network with a conditional independence relationships between random variables. By interpreting these random variables as gene activities and the conditional independence relationships as functional non-relatedness, graphical models have been used to describe gene-regulatory networks. Whereas the literature has been focused on static networks, most time-course experiments are designed in order to tease out temporal changes in the underlying network. It is typically reasonable to assume that changes in genomic networks are few, because biological systems tend to be stable. We introduce a new model for estimating slow changes in dynamic gene-regulatory networks, which is suitable for high-dimensional data, e.g. time-course microarray data. Our aim is to estimate a dynamically changing genomic network based on temporal activity measurements of the genes in the network. Our method is based on the penalized likelihood with l(1)-norm, that penalizes conditional dependencies between genes as well as differences between conditional independence elements across time points. We also present a heuristic search strategy to find optimal tuning parameters. We re-write the penalized maximum likelihood problem into a standard convex optimization problem subject to linear equality constraints. We show that our method performs well in simulation studies. Finally, we apply the proposed model to a time-course T-cell dataset.",""
"Saltwater intrusion problems have been usually tackled through analytical models because of its simplicity, easy implementation and low computational cost. Most of these models are based on the sharp-interface approximation and the Ghyben-Herzberg relation, which neglects mixing of fresh water and seawater and implicitly assumes that salt water remains static. This paper provides insight into the validity of a sharp-interface approximation defined from a steady state solution when applied to transient seawater intrusion problems. The validation tests have been performed on a 3D unconfined synthetic aquifer, which include spatial and temporal distribution of recharge and pumping wells. Using a change of variable, the governing equation of the steady state sharp-interface problem can be written with the same structure of the steady confined groundwater flow equation as a function of a single potential variable (phi). We propose to approach also the transient problem solving a single potential equation (using also the phi variable) with the same structure of the confined groundwater flow equation. It will allow solving the problem by using the classical MODFLOW code. We have used the parameter estimation model PEST to calibrate the parameters of the transient sharp-interface equation. We show how after the calibration process, the sharp-interface approach may provide accurate enough results when applied to transient problems and improve the steady state results, thus avoiding the need of implementing a density-dependent model and reducing the computational cost. This has been proved by comparing results with those obtained using the finite difference numerical code SEAWAT for solving the coupled partial differential equations of flow and density-dependent transport. The comparison was performed in terms of piezometric heads, seawater penetration, transition zone width and critical pumping rates. Copyright (c) 2014 John Wiley & Sons, Ltd.",""
"The purpose of this study is to design and develop a probabilistic network for detecting errors in radiotherapy plans for use at the time of initial plan verification. Our group has initiated a multi-pronged approach to reduce these errors. We report on our development of Bayesian models of radiotherapy plans. Bayesian networks consist of joint probability distributions that define the probability of one event, given some set of other known information. Using the networks, we find the probability of obtaining certain radiotherapy parameters, given a set of initial clinical information. A low probability in a propagated network then corresponds to potential errors to be flagged for investigation. To build our networks we first interviewed medical physicists and other domain experts to identify the relevant radiotherapy concepts and their associated interdependencies and to construct a network topology. Next, to populate the network's conditional probability tables, we used the Hugin Expert software to learn parameter distributions from a subset of de-identified data derived from a radiation oncology based clinical information database system. These data represent 4990 unique prescription cases over a 5 year period. Under test case scenarios with approximately 1.5% introduced error rates, network performance produced areas under the ROC curve of 0.88, 0.98, and 0.89 for the lung, brain and female breast cancer error detection networks, respectively. Comparison of the brain network to human experts performance (AUC of 0.90 +/- 0.01) shows the Bayes network model performs better than domain experts under the same test conditions. Our results demonstrate the feasibility and effectiveness of comprehensive probabilistic models as part of decision support systems for improved detection of errors in initial radiotherapy plan verification procedures.",""
"Bell's theorem shows that quantum mechanical correlations can violate the constraints that the causal structure of certain experiments impose on any classical explanation. It is thus natural to ask to which degree the causal assumptions-e.g., locality or measurement independence-have to be relaxed in order to allow for a classical description of such experiments. Here we develop a conceptual and computational framework for treating this problem. We employ the language of Bayesian networks to systematically construct alternative causal structures and bound the degree of relaxation using quantitative measures that originate from the mathematical theory of causality. The main technical insight is that the resulting problems can often be expressed as computationally tractable linear programs. We demonstrate the versatility of the framework by applying it to a variety of scenarios, ranging from relaxations of the measurement independence, locality, and bilocality assumptions, to a novel causal interpretation of Clauser-Horne-Shimony-Holt inequality violations.",""
"Evaluating whether a disease outbreak has occurred based on limited information in medical records is inherently a probabilistic problem. This paper presents a methodology for consistently analysing the probability that a disease targeted by a surveillance system has appeared in the population, based on the medical records of the individuals within the target population, using a Bayesian network. To enable the system to produce a probability density function of the fraction of the population that is infected, a mathematically consistent conjoining of Bayesian networks and particle filters is used. This approach is tested against the default algorithm of ESSENCE Desktop Edition (which adaptively uses Poisson, exponentially weighted moving average and linear regression techniques as needed), and is shown, for the simulated test data used, to give significantly shorter detection times at false alarm rates of practical interest. This methodology shows promise to greatly improve detection times for outbreaks in populations where timely electronic health records are available for data-mining. Crown Copyright (C) 2015 Published by Elsevier Ltd. All rights reserved.","This approach is tested against the default algorithm of ESSENCE Desktop Edition (which adaptively uses Poisson, exponentially weighted moving average and linear regression techniques as needed), and is shown, for the simulated test data used, to give significantly shorter detection times at false alarm rates of practical interest."
"Background The grades of recommendation, assessment, development and evaluation (GRADE) approach is widely implemented in systematic reviews, health technology assessment and guideline development organisations throughout the world. A key advantage to this approach is that it aids transparency regarding judgments on the quality of evidence. However, the intricacies of making judgments about research methodology and evidence make the GRADE system complex and challenging to apply without training. Methods We have developed a semi-automated quality assessment tool (SAQAT) vertical bar based on GRADE. This is informed by responses by reviewers to checklist questions regarding characteristics that may lead to unreliability. These responses are then entered into the Bayesian network to ascertain the probabilities of risk of bias, inconsistency, indirectness, imprecision and publication bias conditional on review characteristics. The model then combines these probabilities to provide a probability for each of the GRADE overall quality categories. We tested the model using a range of plausible scenarios that guideline developers or review authors could encounter. Results Overall, the model reproduced GRADE judgements for a range of scenarios. Potential advantages over standard assessment are use of explicit and consistent weightings for different review characteristics, forcing consideration of important but sometimes neglected characteristics and principled downgrading where small but important probabilities of downgrading are accrued across domains. Conclusions Bayesian networks have considerable potential for use as tools to assess the validity of research evidence. The key strength of such networks lies in the provision of a statistically coherent method for combining probabilities across a complex framework based on both belief and evidence. In addition to providing tools for less experienced users to implement reliability assessment, the potential for sensitivity analyses and automation may be beneficial for application and the methodological development of reliability tools.",""
"Detailed and timely information on crop area, production and yield is important for the assessment of environmental impacts of agriculture, for the monitoring of the land use and management practices, and for food security early warning systems. A machine learning approach is proposed to model crop rotations which can predict with good accuracy, at the beginning of the agricultural season, the crops most likely to be present in a given field using the crop sequence of the previous 3-5 years. The approach is able to learn from data and to integrate expert knowledge represented as first-order logic rules. Its accuracy is assessed using the French Land Parcel Information System implemented in the frame of the EU's Common Agricultural Policy. This assessment is done using different settings in terms of temporal depth and spatial generalization coverage. The obtained results show that the proposed approach is able to predict the crop type of each field, before the beginning of the crop season, with an accuracy as high as 60%, which is better than the results obtained with current approaches based on remote sensing imagery. (C) 2015 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).",""
"The prediction of total egg production (TEP) potential in poultry is an important task to aid optimized management decisions in commercial enterprises. The objective of the present study was to compare different modeling approaches for prediction of TEP in meat type quails (Coturnix coturnix coturnix) using phenotypes such as weight, weight gain, egg production and egg quality measurements. Phenotypic data on 30 traits from two lines (L1, n = 180; and L2, n = 205) of quail were modeled to predict TEP. Prediction models included multiple linear regression and artificial neural network (ANN). Moreover, Bayesian network (BN) and a stepwise approach were used as variable selection methods. BN results showed that TEP is independent from other earlier expressed traits when conditioned on egg production from 35 to 80 days of age (EP1). In addition, the prediction accuracy was much lower when EP1 was not included in the model. The best predictive model was ANN, after feature selection, showing prediction correlations of r = 0.792 and r = 0.714 for L1 and L2, respectively. In conclusion, machine learning methods may be useful, but reasonable prediction accuracies are obtained only when partial egg production measurements are included in the model.","Prediction models included multiple linear regression and artificial neural network (ANN)."
"Reconstructing biological networks using high-throughput technologies has the potential to produce condition-specific interactomes. But are these reconstructed networks a reliable source of biological interactions? Do some network inference methods offer dramatically improved performance on certain types of networks? To facilitate the use of network inference methods in systems biology, we report a large-scale simulation study comparing the ability of Markov chain Monte Carlo (MCMC) samplers to reverse engineer Bayesian networks. The MCMC samplers we investigated included foundational and state-of-the-art Metropolis-Hastings and Gibbs sampling approaches, as well as novel samplers we have designed. To enable a comprehensive comparison, we simulated gene expression and genetics data from known network structures under a range of biologically plausible scenarios. We examine the overall quality of network inference via different methods, as well as how their performance is affected by network characteristics. Our simulations reveal that network size, edge density, and strength of gene-to-gene signaling are major parameters that differentiate the performance of various samplers. Specifically, more recent samplers including our novel methods outperform traditional samplers for highly interconnected large networks with strong gene-to-gene signaling. Our newly developed samplers show comparable or superior performance to the top existing methods. Moreover, this performance gain is strongest in networks with biologically oriented topology, which indicates that our novel samplers are suitable for inferring biological networks. The performance of MCMC samplers in this simulation framework can guide the choice of methods for network reconstruction using systems genetics data.","But are these reconstructed networks a reliable source of biological interactions? Do some network inference methods offer dramatically improved performance on certain types of networks? To facilitate the use of network inference methods in systems biology, we report a large-scale simulation study comparing the ability of Markov chain Monte Carlo (MCMC) samplers to reverse engineer Bayesian networks."
"The complex system required for carbon capture and storage (CCS) encompasses numerous sub-systems with inter-dependencies and large parameter uncertainties that propagate throughout the system. Exploring and understanding these inter-dependencies and uncertainties is invaluable for developing robust risk information. Bayesian Networks (BN), a decision support tool, are being increasingly used in the broader risk assessment community and show promise for use in CCS. BNs explore the dependencies and uncertainties within a system and have the potential to provide a better understanding of risk than more traditional tools such as logic trees or other less integrated approaches. Working with experts from within the Cooperative Research Centre for Greenhouse Gas Technologies (CO2CRC), we have developed a generic BN structure for the storage sub-system of CCS which can be used to guide the development of BNs for other CCS applications and for use in both diagnostic and predictive analysis. This bi-directionality provides one of the more important benefits of BNs; it allows for (1) traditional bottom-up risk assessment where the likely consequences based on the expected state of the system can be calculated and also (2) top-down, or outcome oriented risk, where the state of the system leading to a particular outcome, such as the likelihood of 2% leakage in 1000 years, is determined. This allows for a comprehensive sensitivity analysis which highlights important contributors to the risk and also where additional knowledge may benefit the project and reduce uncertainty. A robust expert elicitation procedure, for both the development of the network structure and the determination of event probabilities, is an integral part of the use of any such BN tool in CCS. Finally, we show the direct application of a smaller CCS BN by the CO2CRC. (C) 2015 Elsevier Ltd. All rights reserved.",""
"For safe and efficient exploitation of ice-covered waters, knowledge about ship performance in ice is crucial. The literature describes numerical and semi-empirical models that characterize ship speed in ice. These however often fail to account for the joint effect of the ice conditions on ship's speed. Moreover, they omit the effect of ice compression. The latter, when combined with the presence of ridges, can significantly limit the capabilities of an ice-strengthened ship, and potentially bring her to a halt, even if the actual ice conditions are within the design range for the given ship. This paper introduces two probabilistic, data-driven models that predict a ship's speed and the situations where a ship is likely to get stuck in ice based on the joint effect of ice features such as the thickness and concentration of level ice, ice ridges, rafted ice, moreover ice compression is considered. To develop the models, two full-scale datasets were utilized. First, the dataset about the performance of a selected ship in ice is acquired from the automatic identification system. Second, the dataset containing numerical description of the ice field is obtained from a numerical ice model HELMI, developed in the Finnish Meteorological Institute. The collected datasets describe a single and unassisted trip of an ice-strengthened bulk carrier between two Finnish ports in the presence of challenging ice conditions, which varied in time and space. The relations between ship performance and the ice conditions were established using Bayesian networks and selected learning algorithms. The obtained results show good prediction power of the models. This means, on average 80% for predicting the ship's speed within specified bins, and above 90% for predicting cases where a ship may get stuck in ice. (C) 2015 The Authors. Published by Elsevier B.V.",""
"The contention of this paper is that many social science research problems are too \"wicked\" to be suitably studied using conventional statistical and regression-based methods of data analysis. This paper argues that an integrated geospatial approach based on methods of machine learning is well suited to this purpose. Recognizing the intrinsic wickedness of traffic safety issues, such approach is used to unravel the complexity of traffic crash severity on highway corridors as an example of such problems. The support vector machine (SVM) and coactive neuro-fuzzy inference system (CANFIS) algorithms are tested as inferential engines to predict crash severity and uncover spatial and non-spatial factors that systematically relate to crash severity, while a sensitivity analysis is conducted to determine the relative influence of crash severity factors. Different specifications of the two methods are implemented, trained, and evaluated against crash events recorded over a 4-year period on a regional highway corridor in Northern Iran. Overall, the SVM model outperforms CANFIS by a notable margin. The combined use of spatial analysis and artificial intelligence is effective at identifying leading factors of crash severity, while explicitly accounting for spatial dependence and spatial heterogeneity effects. Thanks to the demonstrated effectiveness of a sensitivity analysis, this approach produces comprehensive results that are consistent with existing traffic safety theories and supports the prioritization of effective safety measures that are geographically targeted and behaviorally sound on regional highway corridors.","The contention of this paper is that many social science research problems are too \"wicked\" to be suitably studied using conventional statistical and regression-based methods of data analysis."
"Designing an engineering system that is both environmentally and economically sustainable is a challenging task. Designers need to cope with socio-technical uncertainties and design systems to provide high performance during long lifecycles. Flexibility in engineering design provides ways to address such challenges by making engineering systems changeable in the face of uncertainty. It is difficult, however, to identify suitable system elements for designing flexibility, especially when subjected to multiple sources of uncertainty and complex interdependency between socio-technical and systems elements. This paper considers embedding flexibility into the engineering design as a mechanism to ensure better sustainability and to improve economic performance in long-term lifecycles. The main contribution is a novel methodology to identify valuable opportunities to embed flexibility as a way to deal pro-actively with uncertainty in market and environment. The proposed methodology integrates Bayesian network into engineering system design to effectively model complex change propagation in the flexibility identification process. It helps structure concept generation activities by identifying candidate areas to embed flexibility in the system. It compares favorably to other concept generation methods (e.g., prompting, brainstorming) that require modeling and evaluation of a large number of concepts generated in order to identify the ones offering better performance. It differs from other flexibility enabler identification methods by considering indirect as well as direct dependencies, in addition to the probabilistic nature and risk resulting from possible changes. Another contribution is the demonstration application of the proposed methodology through the analysis of a waste-to-energy technology in Singapore based on anaerobic digestion. Results show that the expected net present value of the flexible design concepts provides more than 10 % improvement over a fixed benchmark design in terms of economic lifecycle performance. This design is conducive of better economic sustainability via additional power generation and better use of resources. Results also indicate that the flexible design can reduce downside risks and capitalize on upside opportunities significantly.",""
"Maintenance is an important activity in industry. It is performed either to revive a machine/component or to prevent it from breaking down. Different strategies have evolved through time, bringing maintenance to its current state: condition-based and predictive maintenances. This evolution was due to the increasing demand of reliability in industry. The key process of condition-based and predictive maintenances is prognostics and health management, and it is a tool to predict the remaining useful life of engineering assets. Nowadays, plants are required to avoid shutdowns while offering safety and reliability. Nevertheless, planning a maintenance activity requires accurate information about the system/component health state. Such information is usually gathered by means of independent sensor nodes. In this study, we consider the case where the nodes are interconnected and form a wireless sensor network. As far as we know, no research work has considered such a case of study for prognostics. Regarding the importance of data accuracy, a good prognostics requires reliable sources of information. This is why, in this paper, we will first discuss the dependability of wireless sensor networks, and then present a state of the art in prognostic and health management activities. (C) 2014 Elsevier B.V. All rights reserved.",""
"There has been much interest in reconstructing bi-directional regulatory networks linking the circadian clock to metabolism in plants. A variety of reverse engineering methods from machine learning and computational statistics have been proposed and evaluated. The emphasis of the present paper is on combining models in a model ensemble to boost the network reconstruction accuracy, and to explore various model combination strategies to maximize the improvement. Our results demonstrate that a rich ensemble of predictors outperforms the best individual model, even if the ensemble includes poor predictors with inferior individual reconstruction accuracy. For our application to metabolomic and transcriptomic time series from various mutagenesis plants grown in different light-dark cycles we also show how to determine the optimal time lag between interactions, and we identify significant interactions with a randomization test. Our study predicts new statistically significant interactions between circadian clock genes and metabolites in Arabidopsis thaliana, and thus provides independent statistical evidence that the regulation of metabolism by the circadian clock is not uni-directional, but that there is a statistically significant feedback mechanism aiming from metabolism back to the circadian clock.",""
"For many supervised learning applications, additional information, besides the labels, is often available during training, but not available during testing. Such additional information, referred to the privileged information, can be exploited during training to construct a better classifier. In this paper, we propose a Bayesian network (BN) approach for learning with privileged information. We propose to incorporate the privileged information through a three-node BN. We further mathematically evaluate different topologies of the three-node BN and identify those structures, through which the privileged information can benefit the classification. Experimental results on handwritten digit recognition, spontaneous versus posed expression recognition, and gender recognition demonstrate the effectiveness of our approach.","Such additional information, referred to the privileged information, can be exploited during training to construct a better classifier."
"Recent advances in the areas of pervasive computing, data mining, and machine learning offer unique opportunities to provide health monitoring and assistance for individuals facing difficulties to live independently in their homes. Several components have to work together to provide health monitoring for smart home residents including, but not limited to, activity recognition, activity discovery, activity prediction, and prompting system. Compared to the significant research done to discover and recognize activities, less attention has been given to predict the future activities that the resident is likely to perform. Activity prediction components can play a major role in design of a smart home. For instance, by taking advantage of an activity prediction module, a smart home can learn context-aware rules to prompt individuals to initiate important activities. In this paper, we propose an activity prediction model using Bayesian networks together with a novel two-step inference process to predict both the next activity features and the next activity label. We also propose an approach to predict the start time of the next activity which is based on modeling the relative start time of the predicted activity using the continuous normal distribution and outlier detection. To validate our proposed models, we used real data collected from physical smart environments.","In this paper, we propose an activity prediction model using Bayesian networks together with a novel two-step inference process to predict both the next activity features and the next activity label."
"Gears are widely used in machines to transmit torque from one shaft to another shaft and to change the speed of a power source. Gear failure is one of the major causes for mechanical transmission system breakdown. Therefore, early gear faults must be immediately detected prior to its failure. Once early gear faults are diagnosed, gear remaining useful life (RUL) should be estimated to prevent any unexpected gear failure. In this paper, an intelligent prognostic system is developed for gear performance degradation assessment and RUL estimation. For gear performance degradation assessment, which aims to monitor current gear health condition, first, the frequency spectrum of gear acceleration error signal is mathematically analyzed to design a high-order complex Comblet for extracting gear fault related signatures. Then, two health indicators called heath indicator 1 and health indicator 2 are constructed to detect early gear faults and assess gear performance degradation, respectively, using two individual dynamic Bayesian networks. For gear RUL estimation, which aims to predict future gear health condition, a general sequential Monte Carlo algorithm is applied to iteratively infer gear failure probability density function (FPDF), which is used to predict gear residual lifetime. One case study is investigated to illustrate how the developed prognostic system works. The vibration data collected from a gearbox accelerated life test are used in this paper, where the gearbox started from a brand-new state, and ran until gear tooth failure. The results show that the developed prognostic system is able to detect early gear faults, track gear performance degradation, and predict gear RUL.",""
"Bayesian network structure learning algorithms with limited data are being used in domains such as systems biology and neuroscience to gain insight into the underlying processes that produce observed data. Learning reliable networks from limited data is difficult; therefore, transfer learning can improve the robustness of learned networks by leveraging data from related tasks. Existing transfer learning algorithms for Bayesian network structure learning give a single maximum a posteriori estimate of network models. Yet, many other models may be equally likely, and so a more informative result is provided by Bayesian structure discovery. Bayesian structure discovery algorithms estimate posterior probabilities of structural features, such as edges. We present transfer learning for Bayesian structure discovery which allows us to explore the shared and unique structural features among related tasks. Efficient computation requires that our transfer learning objective factors into local calculations, which we prove is given by a broad class of transfer biases. Theoretically, we show the efficiency of our approach. Empirically, we show that compared to single-task learning, transfer learning is better able to positively identify true edges. We apply the method to whole-brain neuroimaging data.",""
"This paper presents a new method to recognize posed and spontaneous expressions through modeling their spatial patterns. Gender and expression categories are employed as privileged information to further improve the recognition. The proposed approach includes three steps. First, geometric features about facial shape and Action Unit variations are extracted from the differences between apex and onset facial images to capture the spatial facial variation. Second, statistical hypothesis testings are conducted to explore the differences between posed and spontaneous expressions using the defined geometric features from three aspects: all samples, samples given the gender information, and samples given expression categories. Third, several Bayesian networks are built to capture posed and spontaneous spatial facial patterns respectively given gender and expression categories. The statistical analysis results on the USTC-NVIE and SPOS databases both demonstrate the effectiveness of the proposed geometric features. The recognition results on the USTC-NVIE database indicate that the privileged information of gender and expression can help model the spatial patterns caused by posed and spontaneous expressions. The recognition results on both databases outperform those of the state of the art.",""
"In this paper we propose several approximation algorithms for the problems of full and partial abductive inference in Bayesian belief networks. Full abductive inference is the problem of finding the most probable state assignments to all non-evidence variables in the network while partial abductive inference is the problem of finding the most probable state assignments for a subset of the non-evidence variables in the network, called the explanation set. We developed several multi-swarm algorithms based on the overlapping swarm intelligence framework to find approximate solutions to these problems. For full abductive inference a swarm is associated with each node in the network. For partial abductive inference, a swarm is associated with each node in the explanation set and each node in the Markov blankets of the explanation set variables. Each swarm learns the value assignments for the variables in the Markov blanket associated with that swarm's node. Swarms learning state assignments for the same variable compete for inclusion in the final solution.","In this paper we propose several approximation algorithms for the problems of full and partial abductive inference in Bayesian belief networks."
"Bayesian networks (BNs) are a popular tool in natural resource management but are limited when dealing with ecological assemblage data and when discretizing continuous variables. We present a method that addresses these challenges using a BN model developed for the Upper Murrumbidgee River Catchment (south-eastern Australia). A selection process was conducted to choose the taxa from the whole macroinvertebrate assemblage that were incorporated in the BN as endpoints. Furthermore, two different approaches to the discretization of continuous predictor variables for the BN were compared. One approach used Threshold Indicator Taxa Analysis (TITAN) which estimates the thresholds based on the biological community. The other approach used was the expert opinion. The TITAN-based discretizations provided comparable predictions to expert opinion-based discretizations but in combining statistical rigor and ecological relevance, offer a novel and objective approach to the discretization. The TITAN-based method may be used together with expert opinion. (C) 2014 Elsevier Ltd. All rights reserved.",""
"University education is crucial for cultural and economic growth. Thus, the academic mission recognizes the achievement of both institutional and social objectives, and research provides the basis for the systematic creation of knowledge and the development of human capital. Universities attempt to manage a global system with a holistic vision based on data and facts and oriented to the continuous improvement of its effectiveness and efficiency. The goal is achieved by implementing a monitoring system based both on internal and external performances. As a consequence, it is necessary to consider both students perspective regarding needs, expectations, level of satisfaction and loyalty and internal key performance indicators. This paper proposes the use of Bayesian networks for jointly monitoring internal and external performance of a Master's programme of an Italian University in a holistic approach. A Bayesian network is estimated using a learning algorithm able to analyze the association structure among mixed ordinal and nominal variables. Various scenarios are evaluated thanks to efficient computational algorithms of Bayesian networks. (C) 2014 Elsevier Ltd. All rights reserved.",""
"Objectives Hip fractures commonly result in permanent disability, institutionalization or death in elderly. Existing hip-fracture predicting tools are underused in clinical practice, partly due to their lack of intuitive interpretation. By use of a graphical layer, Bayesian network models could increase the attractiveness of fracture prediction tools. Our aim was to study the potential contribution of a causal Bayesian network in this clinical setting. A logistic regression was performed as a standard control approach to check the robustness of the causal Bayesian network approach. Setting EPIDOS is a multicenter study, conducted in an ambulatory care setting in five French cities between 1992 and 1996 and updated in 2010. The study included 7598 women aged 75 years or older, in which fractures were assessed quarterly during 4 years. A causal Bayesian network and a logistic regression were performed on EPIDOS data to describe major variables involved in hip fractures occurrences. Results Both models had similar association estimations and predictive performances. They detected gait speed and mineral bone density as variables the most involved in the fracture process. The causal Bayesian network showed that gait speed and bone mineral density were directly connected to fracture and seem to mediate the influence of all the other variables included in our model. The logistic regression approach detected multiple interactions involving psychotropic drug use, age and bone mineral density. Conclusion Both approaches retrieved similar variables as predictors of hip fractures. However, Bayesian network highlighted the whole web of relation between the variables involved in the analysis, suggesting a possible mechanism leading to hip fracture. According to the latter results, intervention focusing concomitantly on gait speed and bone mineral density may be necessary for an optimal prevention of hip fracture occurrence in elderly people.","A logistic regression was performed as a standard control approach to check the robustness of the causal Bayesian network approach."
"Structural health monitoring (SHM) is challenged by massive data storage pressure and structural fault location. In response to these issues, a bio-inspired memory model that is embedded with a causality reasoning function is proposed for fault location. First, the SHM data for processing are divided into three temporal memory areas to control data volume reasonably. Second, the inherent potential of the causal relationships in structural state monitoring is mined. Causality and dependence indices are also proposed to establish the mechanism of quantitative description of the reason and result events. Third, a mechanism of causality reasoning is developed for the reason and result events to locate faults in a SHM system. Finally, a deformation experiment conducted on a steel spring plate demonstrates that the proposed model can be applied to real-time acquisition, compact data storage, and system fault location in a SHM system. Moreover, the model is compared with some typical methods based on an experimental benchmark dataset.",""
"Multidrug-resistant enterococci are considered crucial drivers for the dissemination of antimicrobial resistance determinants within and beyond a genus. These organisms may pass numerous resistance determinants to other harmful pathogens, whose multiple resistances would cause adverse consequences. Therefore, an understanding of the coexistence epidemiology of resistance genes is critical, but such information remains limited. In this study, our first objective was to determine the prevalence of principal resistance phenotypes and genes among Enterococcus faecalis isolated from retail chicken domestic products collected throughout Japan. Subsequent analysis of these data by using an additive Bayesian network (ABN) model revealed the co-appearance patterns of resistance genes and identified the associations between resistance genes and phenotypes. The common phenotypes observed among E. faecalis isolated from the domestic products were the resistances to oxytetracycline (58.4%), dihydrostreptomycin (50.4%), and erythromycin (37.2%), and the gene tet(L) was detected in 46.0% of the isolates. The ABN model identified statistically significant associations between tet(L) and erm(B), tet(L) and ant(6)-Ia, ant(6)-Ia and aph(3')IIIa, and aph(3')-IIIa and erm(B), which indicated that a multiple-resistance profile of tetracycline, erythromycin, streptomycin, and kanamycin is systematic rather than random. Conversely, the presence of tet(O) was only negatively associated with that of erm(B) and tet (M), which suggested that in the presence of tet(O), the aforementioned multiple resistance is unlikely to be observed. Such heterogeneity in linkages among genes that confer the same phenotypic resistance highlights the importance of incorporating genetic information when investigating the risk factors for the spread of resistance. The epidemiological factors that underlie the persistence of systematic multiple-resistance patterns warrant further investigations with appropriate adjustments for ecological and bacteriological factors.",""
"Paucity of data on rare species is a common problem, preventing the use of most approaches to model development and evaluation. This study demonstrates how models can be developed and different forms of evaluation can be performed despite a lack of sufficient data, by presenting a habitat suitability model for the rare Astacopsis gouldi, the giant freshwater crayfish. We use a Bayesian network approach that readily incorporates incomplete data and allows for the evaluation of uncertainties. To supplement the limited field data on A. gouldi, expert knowledge was elicited through surveys designed to provide probability values that described the strength of relationships between the habitat suitability of the species and three variables - elevation, upstream riparian condition and geomorphic condition - and credible intervals around those values. A series of 18 alternative models were developed based on the same model structure but parameterised using different sources - expert judgement, field data or a combination of the two. The models were evaluated by estimating and comparing their performance accuracy and sensitivity analysis results, and in assessing the assumptions underpinning each of the models. Using performance accuracy as a measure, the data-based and combined expert- and data-based models performed better than the expert-based models. The sensitivity analysis results show that geomorphic condition was the most influential variable in the majority of models and that elevation had minimal influence on the occurrence of A. gouldi. Overall the models were found to have large predictive uncertainties, although the modelling process itself revealed insights into the habitat suitability of the species and identified key knowledge and data gaps for future monitoring, management and research. (C) 2014 Elsevier B.V. All rights reserved.",""
"With the rapid development of Web 2.0 applications, social media have increasingly become a major factor influencing the purchase decisions of customers. Longitudinal individual and engagement behavioral data generated on social media sites post challenges to integrate diverse heterogeneous data to improve prediction performance in customer response modeling. In this study, a hierarchical ensemble learning framework is proposed for behavior-aware user response modeling using diverse heterogeneous data. In the framework, a general-purpose data transformation and feature extraction strategy is developed to transform the heterogeneous high-dimensional multi-relational datasets into customer-centered high-order tensors and to extract attributes. An improved hierarchical multiple kernel support vector machine (H-MK-SVM) is developed to integrate the external, tag and keyword, individual behavioral and engagement behavioral data for feature selection from multiple correlated attributes and for ensemble learning in user response modeling. The subagging strategy is adopted to deal with large-scale imbalanced datasets. Computational experiments using a real-world microblog database were conducted to investigate the benefits of integrating diverse heterogeneous data. Computational results show that the improved H-MK-SVM using longitudinal individual behavioral data exhibits superior performance over some commonly used methods using aggregated behavioral data and the improved H-MK-SVM using engagement behavioral data performs better than using only the external and individual behavioral data. (C) 2014 Elsevier B.V. All rights reserved.",""
"This paper presents a new hybrid approach for learning Bayesian networks (BNs) based on artificial bee colony algorithm and particle swarm optimization. Firstly, an unconstrained optimization problem is established, which can provide a smaller search space. Secondly, the definition and encoding of the basic mathematical elements of our algorithm are given, and the basic operations are designed, which provide guarantee of convergence. Thirdly, from a known original Bayesian network with probabilistic logic sampling, full samples for the training set and testing set are generated, and then the structure of Bayesian network is learned from complete training set by using our method. The simulation experimental results show that our method is effective.",""
"Graphical models are widely used to study biological networks. Interventions on network nodes are an important feature of many experimental designs for the study of biological networks. In this paper we put forward a causal variant of dynamic Bayesian networks (DBNs) for the purpose of modeling time-course data with interventions. The models inherit the simplicity and computational efficiency of DBNs but allow interventional data to be integrated into network inference. We show empirical results, on both simulated and experimental data, that demonstrate the need to appropriately handle interventions when interventions form part of the design.","The models inherit the simplicity and computational efficiency of DBNs but allow interventional data to be integrated into network inference."
"Gas lifting is a common practice in the oil industry. Using an appropriate gas lift injection rate can ensure that the desired oil production rate would be achieved. In the case of an oil field, the problem of distributing a certain amount of the available gas among a number of wells is formally known as a gas lift allocation problem. In this paper, a multi-objective optimization algorithm, based on the Gaussian Bayesian Networks and the Gaussian kernels, is proposed in order to determine the best injection points, considering multiple objective functions. Firstly, the problem is solved in a similar approach to the previous literature with similar gas lift data and similar function approximation method, to compare the performance of the proposed algorithm with the older ones. Thereafter, an extended problem is discussed, with minimizing the water production as a new optimization criterion. The developed multi-objective scheme is capable of handling and optimizing a gas-lift problem with several constraints and conflicting objectives such as controlling the gas usage and increasing the oil production, whereas in the conventional single-objective optimizations, any alteration in the constraints demands a new optimization process and often there is no place for considering an additional objective in the gas-lift allocation problem. The results obtained by the proposed optimization algorithm significantly overcame those reported in the previous similar literature. For a single-objective fifty-six well problem, the results exhibited 16.24% improvement in the total oil production. (C) 2015 Elsevier B.V. All rights reserved.",""
"Assessing environmental risks on large dams is a challenging task. This paper describes a study on a novel and comprehensive application of Bayesian Networks (BNs) on the Abolabbas dam in Iran. Bayesian networks are based on probability theory and provide a powerful tool for structuring conceptualizations of the interactions between variables with uncertainties. Firstly, the interaction-based structure of variables is developed using the graphical model. Then, the Bayesian Network input variables, which affect the risk in two categories (\"hazards index\" and \"consequences index\"), are determined and the relations between different variables are modeled. The probability values for the risk levels are derived from a novel fuzzy set analysis. The results show that the environmental risk of the Abolabbas dam is considered at a high level with 12.8 percent probability. Also, the sensitivity analysis is used to find out the most effective variables on the environmental risk of the dam site. Finally certain important action plans are suggested to reduce and control the risk which represents a novel way for the risk reduction.",""
"A tantalizing question in cellular physiology is whether the cellular state and environmental conditions can be inferred by the expression signature of an organism. To investigate this relationship, we created an extensive normalized gene expression compendium for the bacterium Escherichia coli that was further enriched with meta-information through an iterative learning procedure. We then constructed an ensemble method to predict environmental and cellular state, including strain, growth phase, medium, oxygen level, antibiotic and carbon source presence. Results show that gene expression is an excellent predictor of environmental structure, with multi-class ensemble models achieving balanced accuracy between 70.0% (+/- 3.5%) to 98.3%(+/- 2.3%) for the various characteristics. Interestingly, this performance can be significantly boosted when environmental and strain characteristics are simultaneously considered, as a composite classifier that captures the inter-dependencies of three characteristics (medium, phase and strain) achieved 10.6% (+/- 1.0%) higher performance than any individual models. Contrary to expectations, only 59% of the top informative genes were also identified as differentially expressed under the respective conditions. Functional analysis of the respective genetic signatures implicates a wide spectrum of Gene Ontology terms and KEGG pathways with condition-specific information content, including iron transport, transferases, and enterobactin synthesis. Further experimental phenotypic-to-genotypic mapping that we conducted for knock-out mutants argues for the information content of top-ranked genes. This work demonstrates the degree at which genome-scale transcriptional information can be predictive of latent, heterogeneous and seemingly disparate phenotypic and environmental characteristics, with far-reaching applications.","Interestingly, this performance can be significantly boosted when environmental and strain characteristics are simultaneously considered, as a composite classifier that captures the inter-dependencies of three characteristics (medium, phase and strain) achieved 10."
"Marine spatial planning (MSP) requires spatially explicit environmental risk assessment (ERA) frameworks with quantitative or probabilistic measures of risk, enabling an evaluation of spatial management scenarios. ERAs comprise the steps of risk identification, risk analysis, and risk evaluation. A review of ERAs in in the context of spatial management revealed a synonymous use of the concepts of risk, vulnerability and impact, a need to account for uncertainty and a lack of a clear link between risk analysis and risk evaluation. In a case study, we addressed some of the identified gaps and predicted the risk of changing the current state of benthic disturbance by bottom trawling due to future MSP measures in the German EEZ of the North Sea. We used a quantitative, dynamic, and spatially explicit approach where we combined a Bayesian belief network with GIS to showcase the steps of risk characterization, risk analysis, and risk evaluation. We distinguished 10 benthic communities and 6 international fishing fleets. The risk analysis produced spatially explicit estimates of benthic disturbance, which was computed as a ratio between relative local mortality by benthic trawling and the recovery potential after a trawl event. Results showed great differences in spatial patterns of benthic disturbance when accounting for different environmental impacts of the respective fleets. To illustrate a risk evaluation process, we simulated a spatial shift of the international effort of two beam trawl fleets, which are affected the most by future offshore wind development. The Bayesian belief network (BN) model was able to predict the proportion of the area where benthic disturbance likely increases. In conclusion, MSP processes should embed ERA frameworks which allow for the integration of multiple risk assessments and the quantification of related risks as well as uncertainties at a common spatial scale.",""
"We describe two novel Markov chain Monte Carlo approaches to computing estimates of parameters concerned with healthcare-associated infections. The first approach frames the discrete time, patient level, hospital transmission model as a Bayesian network, and exploits this framework to improve greatly on the computational efficiency of estimation compared with existing programs. The second approach is in continuous time and shares the same computational advantages. Both methods have been implemented in programs that are available from the authors. We use these programs to show that time discretization can lead to statistical bias in the underestimation of the rate of transmission of pathogens. We show that the continuous implementation has similar running time to the discrete implementation, has better Markov chain mixing properties, and eliminates the potential statistical bias. We, therefore, recommend its use when continuous-time data are available.",""
"We propose a new quantum Bayesian Network model in order to compute probabilistic inferences in decisionmaking scenarios. The application of a quantum paradigm to decision making generates interference effects that influence probabilistic inferences. These effects do not exist in a classical setting and constitute a major issue in the decision process, because they generate quantum parameters that highly increase with the amount of uncertainty of the problem. To automatically compute these quantum parameters, we propose a heuristic inspired by Jung's Synchronicity principle. Synchronicity can be defined by a significant coincidence that appears between a mental state and an event occurring in the external world. It is the occurrence of meaningful, but not causally connected events. We tested our quantum Bayesian Network together with the Synchronicity inspired heuristic in empirical experiments related to categorization/ decision in which the law of total probability was being violated. Results showed that the proposed quantum model was able to simulate the observed empirical findings from the experiments. We then applied our model to a more general scenario and showed the differences between classical and quantum inferences in a Lung Cancer medical diagnosis Bayesian Network.","We propose a new quantum Bayesian Network model in order to compute probabilistic inferences in decisionmaking scenarios."
"Three experiments examined children's and adults' abilities to use statistical and temporal information to distinguish between common cause and causal chain structures. In Experiment 1, participants were provided with conditional probability information and/or temporal information and asked to infer the causal structure of a 3-variable mechanical system that operated probabilistically. Participants of all ages preferentially relied on the temporal pattern of events in their inferences, even if this conflicted with statistical information. In Experiments 2 and 3, participants observed a series of interventions on the system, which in these experiments operated deterministically. In Experiment 2, participants found it easier to use temporal pattern information than statistical information provided as a result of interventions. In Experiment 3, in which no temporal pattern information was provided, children from 6- to 7-years-old, but not younger children, were able to use intervention information to make causal chain judgments, although they had difficulty when the structure was a common cause. The findings suggest that participants, and children in particular, may find it more difficult to use statistical information than temporal pattern information because of its demands on information processing resources. However, there may also be an inherent preference for temporal information.","Participants of all ages preferentially relied on the temporal pattern of events in their inferences, even if this conflicted with statistical information."
"Networks offer a flexible framework to represent and analyse the complex interactions between components of cellular systems. In particular gene networks inferred from expression data can support the identification of novel hypotheses on regulatory processes. In this review we focus on the use of gene network analysis in the study of heart development. Understanding heart development will promote the elucidation of the aetiology of congenital heart disease and thus possibly improve diagnostics. Moreover, it will help to establish cardiac therapies. For example, understanding cardiac differentiation during development will help to guide stem cell differentiation required for cardiac tissue engineering or to enhance endogenous repair mechanisms. We introduce different methodological frameworks to infer networks from expression data such as Boolean and Bayesian networks. Then we present currently available temporal expression data in heart development and discuss the use of network-based approaches in published studies. Collectively, our literature-based analysis indicates that gene network analysis constitutes a promising opportunity to infer therapy-relevant regulatory processes in heart development. However, the use of network-based approaches has so far been limited by the small amount of samples in available datasets. Thus, we propose to acquire high-resolution temporal expression data to improve the mathematical descriptions of regulatory processes obtained with gene network inference methodologies. Especially probabilistic methods that accommodate the intrinsic variability of biological systems have the potential to contribute to a deeper understanding of heart development.","Thus, we propose to acquire high-resolution temporal expression data to improve the mathematical descriptions of regulatory processes obtained with gene network inference methodologies."
"The relationship between ecological impact and ecosystem structure is often strongly nonlinear, so that small increases in impact levels can cause a disproportionately large response in ecosystem structure. Nonlinear ecosystem responses can be difficult to predict because locally relevant data sets can be difficult or impossible to obtain. Bayesian networks (BN) are an emerging tool that can help managers to define ecosystem relationships using a range of data types from comprehensive quantitative data sets to expert opinion. We show how a simple BN can reveal nonlinear dynamics in seagrass ecosystems using ecological relationships sourced from the literature. We first developed a conceptual diagram by cataloguing the ecological responses of seagrasses to a range of drivers and impacts. We used the conceptual diagram to develop a BN populated with values sourced from published studies. We then applied the BN to show that the amount of initial seagrass biomass has a mitigating effect on the level of impact a meadow can withstand without loss, and that meadow recovery can often require disproportionately large improvements in impact levels. This mitigating effect resulted in the middle ranges of impact levels having a wide likelihood of seagrass presence, a situation known as bistability. Finally, we applied the model in a case study to identify the risk of loss and the likelihood of recovery for the conservation and management of seagrass meadows in Moreton Bay, Queensland, Australia. We used the model to predict the likelihood of bistability in 23 locations in the Bay. The model predicted bistability in seven locations, most of which have experienced seagrass loss at some stage in the past 25 years providing essential information for potential future restoration efforts. Our results demonstrate the capacity of simple, flexible modeling tools to facilitate collation and synthesis of disparate information. This approach can be adopted in the initial stages of conservation programs as a low-cost and relatively straightforward way to provide preliminary assessments of nonlinear dynamics in ecosystems.",""
"Progression of atherosclerotic process constitutes a serious and quite common condition due to accumulation of fatty materials in the arterial wall, consequently posing serious cardiovascular complications. In this paper, we assemble and analyze a multitude of heterogeneous data in order to model the progression of atherosclerosis (ATS) in coronary vessels. The patient's medical record, biochemical analytes, monocyte information, adhesion molecules, and therapy-related data comprise the input for the subsequent analysis. As indicator of coronary lesion progression, two consecutive coronary computed tomography angiographies have been evaluated in the same patient. To this end, a set of 39 patients is studied using a twofold approach, namely, baseline analysis and temporal analysis. The former approach employs baseline information in order to predict the future state of the patient (in terms of progression of ATS). The latter is based on an approach encompassing dynamic Bayesian networks whereby snapshots of the patient's status over the follow-up are analyzed in order to model the evolvement of ATS, taking into account the temporal dimension of the disease. The quantitative assessment of our work has resulted in 93.3% accuracy for the case of baseline analysis, and 83% overall accuracy for the temporal analysis, in terms of modeling and predicting the evolvement of ATS. It should be noted that the application of the SMOTE algorithm for handling class imbalance and the subsequent evaluation procedure might have introduced an overestimation of the performance metrics, due to the employment of synthesized instances. The most prominent features found to play a substantial role in the progression of the disease are: diabetes, cholesterol and cholesterol/HDL. Among novel markers, the CD11b marker of leukocyte integrin complex is associated with coronary plaque progression.",""
"Within the last years, probabilistic causality has become a very active research topic in artificial intelligence and statistics communities. Due to its high impact in various applications involving reasoning tasks, machine learning researchers have proposed a number of techniques to learn Causal Bayesian Networks. Within the existing works in this direction, few studies have explicitly considered the role that decisional guidance might play to alternate between observational and experimental data processing. In this paper, we go further by introducing a serendipitous strategy to elucidate semantic background knowledge provided by the domain ontology to learn the causal structure of Bayesian Networks. We also complement our contribution with an enrichment process by which it will be possible to reuse these causal discoveries, support the evolving character of the semantic background and make an ontology evolution. Finally, the proposed method will be validated through simulations and real data analysis. (C) 2014 Elsevier B.V. All rights reserved.",""
"Credal nets are probabilistic graphical models which extend Bayesian nets to cope with sets of distributions. An algorithm for approximate credal network updating is presented. The problem in its general formulation is a multilinear optimization task, which can be linearized by an appropriate rule for fixing all the local models apart from those of a single variable. This simple idea can be iterated and quickly leads to accurate inferences. A transformation is also derived to reduce decision making in credal networks based on the maximality criterion to updating. The decision task is proved to have the same complexity of standard inference, being NPPP-complete for general credal nets and NP-complete for polytrees. Similar results are derived for the E-admissibility criterion. Numerical experiments confirm a good performance of the method. (C) 2014 Elsevier Inc. All rights reserved.","This simple idea can be iterated and quickly leads to accurate inferences."
"In this paper we study how different theoretical concepts of Bayesian networks have been extended to chain graphs. Today there exist mainly three different interpretations of chain graphs in the literature. These are the Lauritzen-Wermuth-Frydenberg, the Andersson-Madigan-Perlman and the multivariate regression interpretations. The different chain graph interpretations have been studied independently and over time different theoretical concepts have been extended from Bayesian networks to also work for the different chain graph interpretations. This has however led to confusion regarding what concepts exist for what interpretation. In this article we do therefore study some of these concepts and how they have been extended to chain graphs as well as what results have been achieved so far. More importantly we do also identify when the concepts have not been extended and contribute within these areas. Specifically we study the following theoretical concepts: Unique representations of independence models, the split and merging operators, the conditions for when an independence model representable by one chain graph interpretation can be represented by another chain graph interpretation and finally the extension of Meek's conjecture to chain graphs. With our new results we give a coherent overview of how each of these concepts is extended for each of the different chain graph interpretations. (C) 2014 Elsevier Inc. All rights reserved.","These are the Lauritzen-Wermuth-Frydenberg, the Andersson-Madigan-Perlman and the multivariate regression interpretations."
"In the past decade, there have been important strategic advances relative to pathobiological modeling as well as clinical management for oral mucositis caused by cancer therapies. Prior to the 1990s, research in this field was conducted by a relatively small number of basic and clinical investigators. Increasing interest among researchers and clinicians over the past twenty years has produced a synergistic outcome characterized by a number of key dynamics, including novel discovery models for pathobiology, increased experience in designing and conducting clinical trials, and creation of international collaborations among cancer care professionals who in turn have modeled clinical care paradigms based on state-of-the-science evidence. This maturation of the science and its clinical translation has positioned investigators and oncology providers to further accelerate both the foundational research and the clinical modeling for patient management in the years ahead. The stage is now set to further capitalize upon optimizing the interactions across this interface, with the goal of strategically enhancing management of patients with cancer at risk for this toxicity while reducing the cost of cancer care.",""
"Directed acyclic graphs are the basic representation of the structure underlying Bayesian networks, which represent multivariate probability distributions. In many practical applications, such as the reverse engineering of gene regulatory networks, not only the estimation of model parameters but the reconstruction of the structure itself is of great interest. As well as for the assessment of different structure learning algorithms in simulation studies, a uniform sample from the space of directed acyclic graphs is required to evaluate the prevalence of certain structural features. Here we analyse how to sample acyclic digraphs uniformly at random through recursive enumeration, an approach previously thought too computationally involved. Based on complexity considerations, we discuss in particular how the enumeration directly provides an exact method, which avoids the convergence issues of the alternative Markov chain methods and is actually computationally much faster. The limiting behaviour of the distribution of acyclic digraphs then allows us to sample arbitrarily large graphs. Building on the ideas of recursive enumeration based sampling we also introduce a novel hybrid Markov chain with much faster convergence than current alternatives while still being easy to adapt to various restrictions. Finally we discuss how to include such restrictions in the combinatorial enumeration and the new hybrid Markov chain method for efficient uniform sampling of the corresponding graphs.",""
"The survival of rare animals is an important concern in an environmental impact assessment. However, it is very difficult to quantitatively predict the possible effect that a development project has on rare animals, and there is a heavy reliance on expert knowledge and judgment. In order to improve the credibility of expert judgment, this study uses Bayesian belief networks (BBN) to visually represent expert knowledge and to clearly explain the inference process. For the case study, the primary difficulty is in determining a large amount of conditional probabilities in the BBN, because there is a lack of sufficient data concerning rare animals. Therefore, a new method that uses fuzzy logic to systematically generate these probabilities is proposed. The combination of the BBN and the fuzzy logic system is used to assess the possible future population status of the Pheasant-tailed jacana and the associated probabilities, which have been affected by the construction of the Taiwan High-Speed Rail. The analysis shows that a restoration program would successfully preserve the species, because in the restoration area, the BBN model predicts that there is a 75.49 % probability that the species will flourish in the future.","In order to improve the credibility of expert judgment, this study uses Bayesian belief networks (BBN) to visually represent expert knowledge and to clearly explain the inference process."
"The theory of causal independence is frequently used to facilitate the assessment of the probabilistic parameters of discrete probability distributions of complex Bayesian networks. Although it is possible to include continuous parameters in Bayesian networks as well, such parameters could not, so far, be modeled by means of causal-independence theory, as a theory of continuous causal independence was not available. In this paper, such a theory is developed and generalized such that it allows merging continuous with discrete parameters based on the characteristics of the problem at hand. This new theory is based on the discovered relationship between the theory of causal independence and convolution in probability theory, discussed in detail for the first time in this paper. Furthermore, the new theory is used as a basis to develop a relational theory of probabilistic interactions. It is also illustrated how this new theory can be used in connection with special probability distributions. (C) 2014 Wiley Periodicals, Inc.",""
"Mixtures of polynomials (MoPs) are a nonparametric density estimation technique especially designed for hybrid Bayesian networks with continuous and discrete variables. Algorithms to learn one- and multidimensional (marginal) MoPs from data have recently been proposed. In this paper, we introduce two methods for learning MoP approximations of conditional densities from data. Both approaches are based on learning MoP approximations of the joint density and the marginal density of the conditioning variables, but they differ as to how the MoP approximation of the quotient of the two densities is found. We illustrate and study the methods using data sampled from known parametric distributions, and demonstrate their applicability by learning models based on real neuroscience data. Finally, we compare the performance of the proposed methods with an approach for learning mixtures of truncated basis functions (MoTBFs). The empirical results show that the proposed methods generally yield models that are comparable to or significantly better than those found using the MoTBF-based method. (C) 2014 Wiley Periodicals, Inc.",""
"In this paper, we discuss some practical issues that arise in solving hybrid Bayesian networks that include deterministic conditionals for continuous variables. We show how exact inference can become intractable even for small networks due to the difficulty in handling deterministic conditionals (for continuous variables). We propose some strategies for carrying out the inference task using mixtures of polynomials (MOPs) and mixtures of truncated exponentials. MOPs can be defined on hypercubes or hyperrhombuses. We compare these two methods. A key strategy is to reapproximate large potentials with potentials consisting of fewer pieces and lower degrees/number of terms. We discuss several methods for reapproximating potentials. We illustrate our methods in a practical application consisting of solving a stochastic program evaluation and review technique (PERT) network. (C) 2014 Wiley Periodicals, Inc.","We show how exact inference can become intractable even for small networks due to the difficulty in handling deterministic conditionals (for continuous variables)."
"Learning Bayesian networks is known to be an NP-hard problem, and this, combined with the growing interest in learning models from high-dimensional domains, leads to the necessity of finding more efficient learning algorithms. Recent papers have proposed constrained approaches of successfully and widely used local search algorithms, such as Hill Climbing. One of these algorithms families, called constrained Hill Climbing (CHC), greatly improves upon the efficiency of the original approach, obtaining models with slightly lower quality but maintaining their theoretical properties. In this paper, we propose three different modifications to the most scalable version of these algorithms, fast constrained Hill Climbing, to improve the quality of its output by relaxing the constraints imposed to include some diversification in the search process. The aim of these new approaches is to adjust the trade-off between efficiency and accuracy of the algorithm, as they do not modify its complexity and only imply a few more search iterations. We perform an intensive experimental evaluation of the modifications proposed with an extensive comparison between the original algorithms and the new modifications covering several scenarios with quite large data sets. Available code and data for further use of the algorithms presented in this paper and experiment replication can be available at .http://simd.albacete.org/supplements/FastCHC.html. (C) 2014 Wiley Periodicals, Inc.",""
"Rlatecursive probability trees (RPTs) offer a flexible framework for representing the probabilistic information in probabilistic graphical models. This structure is able to provide a compact representation of the distribution it encodes by specifying most of the types of independencies that can be found in a probability distribution. The real benefit of this representation heavily depends on the ability of learning such independencies from data. In this paper, we expand our approach at learning RPTs from data by extending an existing greedy methodology for retrieving small RPTs from probabilistic potentials. We test the performance of the algorithm by learning from different databases, both real and handcrafted, and we compare the performance for different databases sizes. (C) 2014 Wiley Periodicals, Inc.",""
"The paper presents the application of a Technical Mapping and tacit knowledge elicitation in industry in order to promote the modeling of tacit knowledge to explicit and represent it in the form of production rules for use in manufacturing processes. The technique was applied with the involved people in the lithographic process in a Metallurgical Company located in southern Brazil. Knowledge of two production coordinators were modeled. For the process of knowledge acquisition and mapping of attributes and values to feed the knowledge base of an expert system, were used quality tools such as Brainstorming, Pareto Chart and Ishikawa Diagram associated with knowledge elicitation techniques such as unstructured interview, rating chips, observation technique, limitation of information and protocol analysis. Quality tools and techniques of knowledge elicitation were systematized to promote process mapping and the elicitation of tacit knowledge, with the aim of representing knowledge by means of production rules. We constructed two knowledge bases with the same methods of production, one in a non-probabilistic expert system (knowledge-based system) and the other in a probabilistic expert system (Bayesian networks) in order to perform comparisons and simulations of the results found. Expert systems perform systematic analysis from the answers given by those involved in lithographic labels process while the defect is identified in order to support the user in diagnosing the root cause of the failure process. From simulations of changes in process variables was possible to prove the hypothesis of the use of probabilistic expert system as industrial support tool in preventing the occurrence of defects in the process and result in a productivity gain. (C) 2014 Elsevier Ltd. All rights reserved.",""
"Background: Cancer progression is caused by the sequential accumulation of mutations, but not all orders of accumulation are equally likely. When the fixation of some mutations depends on the presence of previous ones, identifying restrictions in the order of accumulation of mutations can lead to the discovery of therapeutic targets and diagnostic markers. The purpose of this study is to conduct a comprehensive comparison of the performance of all available methods to identify these restrictions from cross-sectional data. I used simulated data sets (where the true restrictions are known) but, in contrast to previous work, I embedded restrictions within evolutionary models of tumor progression that included passengers (mutations not responsible for the development of cancer, known to be very common). This allowed me to assess, for the first time, the effects of having to filter out passengers, of sampling schemes (when, how, and how many samples), and of deviations from order restrictions. Results: Poor choices of method, filtering, and sampling lead to large errors in all performance measures. Having to filter passengers lead to decreased performance, especially because true restrictions were missed. Overall, the best method for identifying order restrictions were Oncogenetic Trees, a fast and easy to use method that, although unable to recover dependencies of mutations on more than one mutation, showed good performance in most scenarios, superior to Conjunctive Bayesian Networks and Progression Networks. Single cell sampling provided no advantage, but sampling in the final stages of the disease vs. sampling at different stages had severe effects. Evolutionary model and deviations from order restrictions had major, and sometimes counterintuitive, interactions with other factors that affected performance. Conclusions: This paper provides practical recommendations for using these methods with experimental data. It also identifies key areas of future methodological work and, in particular, it shows that it is both possible and necessary to embed assumptions about order restrictions and the nature of driver status within evolutionary models of cancer progression to evaluate the performance of inferential approaches.",""
"Fault tree analysis (FTA) is a very prominent method to analyze the risks related tosafety and economically critical assets, like power plants, airplanes, data centers and web shops. FTA methods comprise of a wide variety of modeling and analysis techniques, supported by a wide range of software tools. This paper surveys over 150 papers on fault tree analysis, providing an in-depth overview of the state-of-the-art in FTA. Concretely, we review standard fault trees, as well as extensions such as dynamic FT, repairable FT, and extended FT. For these models, we review both qualitative analysis methods, like cut sets and common cause failures, and quantitative techniques, including a wide variety of stochastic methods to compute failure probabilities. Numerous examples illustrate the various approaches, and tables present a quick overview of results. (C) 2015 Elsevier Inc. All rights reserved.",""
"First of all, and to clarify our purpose, it seems important to say that the work we are presenting here lie within the framework of learner modeling in an adaptive system understood as computational modeling of the learner. we must state also that Bayesian Networks are effective tools for learner modeling under uncertainty. They have been successfully used in many systems, with different objectives, from the assessment of knowledge of the learner to the recognition of the plan followed in problem solving. The main objective of this paper is to develop a Bayesian networks for modeling the learner from the use case diagram of the Unified Modeling Language. To achieve this objective it is necessary first to ask the Why and how we can represent a Learner model using Bayesian networks? How can we go from a dynamic representation of the learner model using UML to a probabilistic representation with Bayesian networks? Is this approach considered experimentally justified? First, we will return to the definitions of the main relationships in the diagram use cases and Bayesian networks, and then we will focus on the development rules on which we have based our work. We then demonstrate how to develop a Bayesian network based on these rules. Finally we will present the formal structure for this consideration. The prototypes and diagrams presented in this work are arguments in favor of our objective. And the network obtained also promotes reusing the learner modeling through similar systems.",""
"In this study, a methodology has been proposed for risk analysis of dust explosion scenarios based on Bayesian network. Our methodology also benefits from a bow-tie diagram to better represent the logical relationships existing among contributing factors and consequences of dust explosions. In this study, the risks of dust explosion scenarios are evaluated, taking into account common cause failures and dependencies among root events and possible consequences. Using a diagnostic analysis, dust particle properties, oxygen concentration, and safety training of staff are identified as the most critical root events leading to dust explosions. The probability adaptation concept is also used for sequential updating and thus learning from past dust explosion accidents, which is of great importance in dynamic risk assessment and management. We also apply the proposed methodology to a case study to model dust explosion scenarios, to estimate the envisaged risks, and to identify the vulnerable parts of the system that need additional safety measures.",""
"Bayesian Networks have been used for the inference of transcriptional regulatory relationships among genes, and are valuable for obtaining biological insights. However, finding optimal Bayesian Network (BN) is NP-hard. Thus, heuristic approaches have sought to effectively solve this problem. In this work, we develop a hybrid search method combining Simulated Annealing with a Greedy Algorithm (SAGA). SAGA explores most of the search space by undergoing a two-phase search: first with a Simulated Annealing search and then with a Greedy search. Three sets of background-corrected and normalized microarray datasets were used to test the algorithm. BN structure learning was also conducted using the datasets, and other established search methods as implemented in BANJO (Bayesian Network Inference with Java Objects). The Bayesian Dirichlet Equivalence (BDe) metric was used to score the networks produced with SAGA. SAGA predicted transcriptional regulatory relationships among genes in networks that evaluated to higher BDe scores with high sensitivities and specificities. Thus, the proposed method competes well with existing search algorithms for Bayesian Network structure learning of transcriptional regulatory networks. (C) 2014 Elsevier Inc. All rights reserved.","Bayesian Networks have been used for the inference of transcriptional regulatory relationships among genes, and are valuable for obtaining biological insights."
"Objective: To develop a probabilistic model for discovering and quantifying determinants of outbreak detection and to use the model to predict detection performance for new outbreaks. Materials and methods: We used an existing software platform to simulate waterborne disease outbreaks of varying duration and magnitude. The simulated data were overlaid on real data from visits to emergency department in Montreal for gastroenteritis. We analyzed the combined data using biosurveillance algorithms, varying their parameters over a wide range. We then applied structure and parameter learning algorithms to the resulting data set to build a Bayesian network model for predicting detection performance as a function of outbreak characteristics and surveillance system parameters. We evaluated the predictions of this model through 5-fold cross-validation. Results: The model predicted performance metrics of commonly used outbreak detection methods with an accuracy greater than 0.80. The model also quantified the influence of different outbreak characteristics and parameters of biosurveillance algorithms on detection performance in practically relevant surveillance scenarios. In addition to identifying characteristics expected a priori to have a strong influence on detection performance, such as the alerting threshold and the peak size of the outbreak, the model suggested an important role for other algorithm features, such as adjustment for weekly patterns. Conclusion: We developed a model that accurately predicts how characteristics of disease outbreaks and detection methods will influence on detection. This model can be used to compare the performance of detection methods under different surveillance scenarios, to gain insight into which characteristics of outbreaks and biosurveillance algorithms drive detection performance, and to guide the configuration of surveillance systems. (C) 2014 Elsevier Inc. All rights reserved.",""
"The interface between terrestrial and aquatic ecosystems contributes to the provision of key ecosystem services including improved water quality and reduced flood risk. We develop an ecological-economic model using a Bayesian Belief Network (BBN) to assess and value the delivery of ecosystem services from riparian buffer strips. By capturing the interactions underlying ecosystem processes and the delivery of services we aim to further the operationalization of ecosystem services approaches. The model is developed through outlining the underlying ecological processes which deliver ecosystem services. Alternative management options and regional locations are used for sensitivity analysis. We identify optimal management options but reveal relatively small differences between impacts of different management options. We discuss key issues raised as a result of the probabilistic nature of the BBN model. Uncertainty over outcomes has implications for the approach to valuation particularly where preferences might exhibit non-linearities or thresholds. The interaction between probabilistic outcomes and the statistical nature of valuation estimates suggests the need for further exploration of sensitivity in such models. Although the BBN is a promising participatory decision support tool, there remains a need to understand the trade-off between realism, precision and the benefits of developing joint understanding of the decision context. (C) 2014 Elsevier B.V. All rights reserved.",""
"A deterministic design philosophy for systems, structures, and components of nuclear facilities, accepted by all stakeholders, has been adopted to address safety concerns. However, over-conservatism makes the design of nuclear facilities, especially research reactors, expensive; therefore, the industry needs to reevaluate its level of over-conservatism to obtain an optimized design. This paper proposes a probabilistic design and optimization approach for instrumentation & control (I&C) systems of research reactors based on sensitivity, availability criteria, and cost. Compared to commercial nuclear power plants, research reactors are sensitive to the cost competitiveness of I&C systems, so optimization of I&C system design is needed. As a case study, we formulated four architecture configurations for a reactor protection system and performed the reliability feature analysis, assessed cost, and computed reliability index. In addition to International Electrotechnical Commission (IEC) and American Society of Mechanical Engineers (ASME) criteria, we introduce the reliability index, a novel parameter describing the correlation between cost and reliability used to determine the point at which an I&C architecture has been optimized. To observe the significance of this study, we compared our results with a benchmark architecture. Our novel probabilistic design and optimization approach produced two optimized I&C architectures with design cost reductions of 30-40% compared to the benchmark architecture, while maintaining safety and availability.",""
"Detecting audio events in Hollywood movies is a complex task due to the presence of variability between the soundtracks of the movies. This inter-movies variability is shown to impair the audio events detection results in a realistic framework. In this article, we propose to model the variability using a factor analysis technique, which we then use to compensate the audio features. The factor analysis compensation is validated using the state-of-the-art system based on multiple audio words sequences and contextual Bayesian networks which we previously developed in Penet et al. (2013). Results obtained on the same publicly available dataset for the detection of gunshots and explosions show an improvement in the handling of the variability, while keeping the robustness capabilities of the previous system. Furthermore, the system is applied to the detection of screams and proves its ability to generalise to other types of events. The obtained results also emphasise the fact that, in addition to modelling variability, adding concepts in the system may also be beneficial for the precision rates.",""
"In this work we deal with the problem of TBox learning from incomplete semantic web data. TBox, or conceptual schema, is the backbone of a Description Logic (DL) ontology, but is always difficult to obtain. Existing approaches either fail in getting correct results under incompleteness or learn results that are not enough to resolve the incompleteness. We propose to transform TBox learning in DL into inference in the extension of Bayesian Description Logic Network (abbreviated as BelNet(+)), whereby the structure in the data is leveraged when evaluating the relationships between two concepts. BelNet(+), integrating the probabilistic inference capability of Bayesian Networks with the logical formalism of DL ontologies Description Logics, supports promising inference. In this paper, we firstly explain the details of BelNet(+) and introduce a TBox learning approach based on BelNet(+). In order to overcome the drawbacks of current evaluation metrics, we then propose a novel evaluation framework conforming to the Open World Assumption (OWA) generally made in the semantic web. Finally the results from empirical studies on comparisons with the state-of-the-art TBox learners verify the effectiveness of our approach. (C) 2014 Elsevier B.V. All rights reserved.","We propose to transform TBox learning in DL into inference in the extension of Bayesian Description Logic Network (abbreviated as BelNet(+)), whereby the structure in the data is leveraged when evaluating the relationships between two concepts."
"Space avionics are the essential capabilities of a spacecraft that guarantee space flight safety and mission success. One of the most important elements developed to deal with the health of the space avionics is the integrated system health management. Fault diagnostics, a safety-critical process in the integrated system health management, has become more complex as the number of avionics systems within the spacecraft has grown, so failure data are now multidimensional, often incomplete, and have cumulatively acquired uncertainties. Therefore, an accurate fault diagnostics model is needed to handle these types of data and ensure information is adequately adapted and efficiently updated. To date, there has been little research focused on efficient and effective space avionics fault diagnostics. This article presents a novel integrated system health management-oriented intelligent diagnostics methodology based on data mining. A numerical example is provided to illustrate the methodology, which demonstrates the significant benefits of data mining for the efficient processing of massive, incomplete data, and the ability of using a robust diagnostic Bayesian network to identify faults with uncertainty in a dynamic environment. The combined approach shows how some limitations can be overcome with an improved diagnostic performance. For application, sensory information must initially be discretized to Boolean values. Data mining is then used to mine for useful association rules and to learn the dynamic Bayesian network structure. After parameter training, the diagnostics is conducted. This methodology can be applied to systems of varying sizes and is flexible enough to accommodate other efficient diagnostic methods.",""
"Cyber security is an emerging safety issue in the nuclear industry, especially in the instrumentation and control (I&C) field. To address the cyber security issue systematically, a model that can be used for cyber security evaluation is required. In this work, a cyber security risk model based on a Bayesian network is suggested for evaluating cyber security for nuclear facilities in an integrated manner. The suggested model enables the evaluation of both the procedural and technical aspects of cyber security, which are related to compliance with regulatory guides and system architectures, respectively. The activity-quality analysis model was developed to evaluate how well people and/or organizations comply with the regulatory guidance associated with cyber security. The architecture analysis model was created to evaluate vulnerabilities and mitigation measures with respect to their effect on cyber security. The two models are integrated into a single model, which is called the cyber security risk model, so that cyber security can be evaluated from procedural and technical viewpoints at the same time. The model was applied to evaluate the cyber security risk of the reactor protection system (RPS) of a research reactor and to demonstrate its usefulness and feasibility. (C) 2014 Elsevier Ltd. All rights reserved.",""
"An important issue in Hazardous Material (hazmat) transportation risk assessment is to evaluate the probability bounds of accidents occurrence, whose values are difficult to be estimated due to its low frequency and the related lack of statistical data. This paper presents an original approach to integrate uncertainty in the quantitative analysis of hazmat transportation accidents. The proposed approach is based on the use of Valuation-Based Systems (VBSs) and belief functions theory. Furthermore, we propose to identify the factors for which the reduction of epistemic uncertainty (imprecision) gives the greatest impact on the uncertainty of the final results by using some proposed measures. The applicability and generality of the proposed approach is demonstrated on a case study. (C) 2014 Elsevier Ltd. All rights reserved.",""
"Genes regulate each other and form a gene regulatory network (GRN) to realise biological functions. Elucidating GRN from experimental data remains a challenging problem in systems biology. Numerous techniques have been developed and sparse linear regression methods become a promising approach to infer accurate GRNs. However, most linear methods are either based on steady-state gene expression data or their statistical properties are not analysed. Here, two sparse penalties, adaptive least absolute shrinkage and selection operator and smoothly clipped absolute deviation, are proposed to infer GRNs from time-course gene expression data based on an auto-regressive model and their Oracle properties are proved under mild conditions. The effectiveness of those methods is demonstrated by applications to in silico and real biological data.","Numerous techniques have been developed and sparse linear regression methods become a promising approach to infer accurate GRNs."
"Over the last years, it has been observed an increasing interest of the finance and business communities in any application tool related to the prediction of credit and bankruptcy risk, probably due to the need of more robust decision-making systems capable of managing and analyzing complex data. As a result, plentiful techniques have been developed with the aim of producing accurate prediction models that are able to tackle these issues. However, the design of experiments to assess and compare these models has attracted little attention so far, even though it plays an important role in validating and supporting the theoretical evidence of performance. The experimental design should be done carefully for the results to hold significance; otherwise, it might be a potential source of misleading and contradictory conclusions about the benefits of using a particular prediction system. In this work, we review more than 140 papers published in refereed journals within the period 2000-2013, putting the emphasis on the bases of the experimental design in credit scoring and bankruptcy prediction applications. We provide some caveats and guidelines for the usage of databases, data splitting methods, performance evaluation metrics and hypothesis testing procedures in order to converge on a systematic, consistent validation standard.",""
"This paper analyzes the significance, interpretation, and quantification of uncertainty in prognostics, with an emphasis on predicting the remaining useful life of engineering systems and components. Prognostics deals with predicting the future behavior of engineering systems, and is affected by various sources of uncertainty. In order to facilitate meaningful prognostics-based decision-making, it is important to analyze how these sources of uncertainty affect prognostics, and thereby, compute the overall uncertainty in the remaining useful life prediction. This paper investigates the classical (frequentist) and subjective (Bayesian) interpretations of uncertainty and their implications on prognostics, and argues that the Bayesian interpretation of uncertainty is more suitable for condition-based prognostics and health monitoring. It is also demonstrated that uncertainty quantification in remaining useful life prediction needs to be approached as an uncertainty propagation problem. Several uncertainty propagation methods are discussed in this context, and the practical challenges involved in such uncertainty quantification are outlined. (C) 2014 Elsevier Ltd. All rights reserved.",""
"The growing need to find proper countermeasures able to protect critical infrastructures from threats has addressed the definition of quantitative methodologies for risk assessment. One of the most difficult aspects in this topic is the evaluation of the effects of attacks. Attacks Trees represent one of the most used formalisms in the modeling of attack scenarios: notwithstanding some extensions have been proposed to enrich the expressiveness of the original formalism, some effort should be spent on their analyzability. This paper defines a transformational approach that translates Attack Trees into Bayesian Networks. The proposed approach can cope with different Attack Trees extensions; moreover, it allows the quantitative evaluation of combined attacks modelled as a set of Attack Trees.",""
"Land-use and land-cover change modeling helps us to understand the driving factors and impacts of human-induced land changes better, and depict likely future development paths. Uncertainty associated with various steps in the modeling process substantially influences the reliability of the results, but until now it has only rarely been addressed. In this study, we explore uncertainty in land-change modeling using a probabilistic approach based on Bayesian belief networks. We apply this approach to a case study of deforestation in the Brazilian Amazon and identify three modeling steps as sources of uncertainty: model structure, variable selection, and data preprocessing. For these three steps, we quantify the uncertainty and the respective impact on the outcome accuracy. The results indicate remarkable uncertainties in each of the steps. We demonstrate that a higher uncertainty in the land-change modeling process does not necessarily lead to a lower accuracy of the modeling outcome. Moreover, we show that the different uncertainty sources only slightly influence the ratio between quantity disagreement and allocation disagreement for the modeling outcome. We conclude that uncertainty is inherent in land-change modeling, and that future studies should address this uncertainty more explicitly to improve the robustness of modeling outcomes for science and decision-making.",""
"In this paper, a hierarchical PO-MOESP subspace identification algorithm for directed acyclic graphs (DAGs) is presented. The state of every node and the structure of the graph are assumed to be unknown. The method computes a hierarchical partition of the DAG by using projection matrices typically used in subspace identification methods applied in cascade. Then, the column space of the observability matrix of every node is sequentially estimated by projecting away the input-output data from the previous levels. A past input-output instrumental variable approach is adopted to deal with the noise. The topology of multilevel DAGs is revealed by dedicated projections applied on every level. Moreover, we provide a brief study of a more general class of DAGs that can be accurately represented by multilevel DAGs of reduced interconnection structure. Finally, three simulation examples are provided to show the effectiveness of the proposed methodology.",""
"The article focuses on the application of the Bayesian networks (BN) technique to problems of personalized medicine. The simple (intuitive) algorithm of BN optimization with respect to the number of nodes using naive network topology is developed. This algorithm allows to increase the BN prediction quality and to identify the most important variables of the network. The parallel program implementing the algorithm has demonstrated good scalability with an increase in the computational cores number, and it can be applied to the large patients database containing thousands of variables. This program is applied for the prediction for the unfavorable outcome of coronary artery disease (CAD) for patients who survived the acute coronary syndrome (ACS). As a result, the quality of the predictions of the investigated networks was significantly improved and the most important risk factors were detected. The significance of the tumor necrosis factor-alpha gene polymorphism for the prediction of the unfavorable outcome of CAD for patients survived after ACS was revealed for the first time.",""
"The development of a condition-based deterioration modelling methodology at bridge group level using Bayesian belief network (BBN) is presented in this paper. BBN is an efficient tool to handle complex interdependencies within elements of engineering systems, by means of conditional probabilities specified on a fixed model structure. The advantages and limitations of the BBN for such applications are reviewed by analysing a sample group of masonry bridges on the UK railway infrastructure network. The proposed methodology is then extended to develop a time dependent deterioration model using a dynamic Bayesian network. The condition of elements within the selected sample of bridges and a set of conditional probabilities for static and time dependent variables, based on inspection experience, are used as input to the models to yield, in probabilistic terms, overall condition-based deterioration profiles for bridge groups. Sensitivity towards various input parameters, as well as underlying assumptions, on the point-in-time performance and the deterioration profile of the group are investigated. Together with results from 'what if' scenarios, the potential of the developed methodology is demonstrated in relation to the specification of structural health monitoring requirements and the prioritisation of maintenance intervention activities.",""
"This chapter describes biological backgrounds of regulatory relationships in living cells, high-throughput experimental technologies, and application of computational approaches in reverse engineering of microRNA (miRNA)-mediated and gene regulatory networks (GRNs). The most commonly used models for GRNs inference based on Boolean networks, Bayesian networks, dynamic Bayesian networks, association networks, novel two-stage model using integration of a priori biological knowledge, differential and difference equations models are detailed and their inference capabilities are compared. The regulatory role of miRNAs and transcription factors (TFs) in miRNAs-mediated regulatory networks is described as well. Additionally, commonly used methods for target prediction of miRNAs and TFs are described as well as most commonly used biological regulatory relationships databases and tools are listed. The mainly validation criteria used for assessment of inferred regulatory networks are explained. Finally, concluding remarks and further directions for miRNA-mediated and GRNs inference are given.","The most commonly used models for GRNs inference based on Boolean networks, Bayesian networks, dynamic Bayesian networks, association networks, novel two-stage model using integration of a priori biological knowledge, differential and difference equations models are detailed and their inference capabilities are compared."
"Fault tolerant technology is often used to improve systems reliability. However, high reliability makes it difficult to get sufficient fault samples, resulting in epistemic uncertainty, which increases significantly challenges in these systems diagnosis. A novel dynamic diagnosis strategy for complex systems is proposed to improve the diagnostic efficiency in the paper, which makes full use of dynamic fault tree, Bayesian networks (BN), fuzzy sets theory, and TOPSIS. Specifically, it uses a dynamic fault tree to model dynamic fault modes and evaluates the failure rates of the basic events using fuzzy sets to deal with epistemic uncertainty. Furthermore, it generates qualitative structure information based on zero-suppressed binary decision diagrams and calculates quantitative parameters provided by reliability analysis using a hybrid approach. Additionally, sensors data are incorporated to update the qualitative information and quantitative parameters. Qualitative information, quantitative parameters, and previous diagnosis result are taken into account to design a new dynamic diagnosis strategy which can locate the fault at the lowest cost. Finally, a case study is given to verify the developed approach and to demonstrate its effectiveness.",""
"Cancer has been characterized as a heterogeneous disease consisting of many different subtypes. The early diagnosis and prognosis of a cancer type have become a necessity in cancer research, as it can facilitate the subsequent clinical management of patients. The importance of classifying cancer patients into high or low risk groups has led many research teams, from the biomedical and the bioinformatics field, to study the application of machine learning (ML) methods. Therefore, these techniques have been utilized as an aim to model the progression and treatment of cancerous conditions. In addition, the ability of ML tools to detect key features from complex datasets reveals their importance. A variety of these techniques, including Artificial Neural Networks (ANNs), Bayesian Networks (BNs), Support Vector Machines (SVMs) and Decision Trees (DTs) have been widely applied in cancer research for the development of predictive models, resulting in effective and accurate decision making. Even though it is evident that the use of ML methods can improve our understanding of cancer progression, an appropriate level of validation is needed in order for these methods to be considered in the everyday clinical practice. In this work, we present a review of recent ML approaches employed in the modeling of cancer progression. The predictive models discussed here are based on various supervised ML techniques as well as on different input features and data samples. Given the growing trend on the application of ML methods in cancer research, we present here the most recent publications that employ these techniques as an aim to model cancer risk or patient outcomes. (C) 2014 Kourou et al. Published by Elsevier B.V. on behalf of the Research Network of Computational and Structural Biotechnology. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/3.0/).",""
"Parcelization and forest fragmentation are of concern for ecological, economic, and social reasons. Efforts to keep large, private forests intact may be supported by a decision-making process that incorporates landowners' objectives and uncertainty. We used structured decision making (SDM) with owners of large, private forests in Macon County, North Carolina. Macon County has little land use regulation and a history of discordant, ineffective attempts to address land use and development. We worked with landowners to define their objectives, identify decision options for forest management, build a Bayesian decision network to predict the outcomes of decisions, and determine the optimal and least-desirable decision options. The optimal forest management options for an average, large, forested property (30 ha property with 22 ha of forest) in Macon County was crown-thinning timber harvest under the Present-Use Value program, in which enrolled property is taxed at the present-use value (growing timber for commercial harvest) rather than full market value. The least-desirable forest management actions were selling 1 ha and personal use of the forest (e.g., trails, firewood) with or without a conservation easement. Landowners reported that they enjoyed participating in the project (85%) and would reconsider what they are currently doing to manage their forest (69%). The decision that landowners initially thought would best meet their objectives did not match results from the decision network. This highlights the usefulness of SDM, which typically has been applied to decision problems involving public resources.",""
"The use of probabilistic risk analysis in jet engines manufacturing process is essential to prevent failure. The objective of this study is to present a probabilistic risk analysis model to analyze the safety of this process. The standard risk assessment normally conducted is inadequate to address the risks. To remedy this problem, the model presented in this paper considers the effects of human, software and calibration reliability in the process. Bayesian Belief Network coupled to a Bow Tie diagram is used to identify potential engine failure scenarios. In this context and to meet this objective, an in depth literature research was conducted to identify the most appropriate modeling techniques and an interview were conducted with experts. As a result of this study, this paper presents a model that combines fault tree analysis, event tree analysis and a Bayesian Belief Networks into a single model that can be used by decision makers to identify critical risk factors in order to allocate resources to improve the safety of the system. The model is delivered in the form of a computer assisted decision tool supported by subject expert estimates.",""
"Detailed and innovative analysis of gene regulatory network structures may reveal novel insights to biological mechanisms. Here we study how gene regulatory network in Saccharomyces cerevisiae can differ under aerobic and anaerobic conditions. To achieve this, we discretized the gene expression profiles and calculated the self-entropy of down-and upregulation of gene expression as well as joint entropy. Based on these quantities the uncertainty coefficient was calculated for each gene triplet, following which, separate gene logic networks were constructed for the aerobic and anaerobic conditions. Four structural parameters such as average degree, average clustering coefficient, average shortest path, and average betweenness were used to compare the structure of the corresponding aerobic and anaerobic logic networks. Five genes were identified to be putative key components of the two energy metabolisms. Furthermore, community analysis using the Newman fast algorithm revealed two significant communities for the aerobic but only one for the anaerobic network. David Gene Functional Classification suggests that, under aerobic conditions, one such community reflects the cell cycle and cell replication, while the other one is linked to the mitochondrial respiratory chain function.","David Gene Functional Classification suggests that, under aerobic conditions, one such community reflects the cell cycle and cell replication, while the other one is linked to the mitochondrial respiratory chain function."
"Active and dynamic fusion for fuzzy and uncertain data have key challenges such as high complexity and difficult to guarantee accuracy, etc. In order to resolve the challenging issues, in this article a selective and incremental data fusion approach based on probabilistic graphical model is proposed. General Bayesian networks are adopted to represent the relationship among the data and fusion result. It purposively selects the most informative and decision-relevant data for fusion based on Markov Blanket in probabilistic graphical model. Meanwhile we present a special incremental learning method for updating the fusion model to reflect the temporal changes of environment. Theoretical analysis and experimental results all demonstrate the proposed method has higher accuracy and lower time complexity than existing state-of-the-art methods.",""
"In this paper we develop a formal dynamic version of Chain Event Graphs (CEGs), a particularly expressive family of discrete graphical models. We demonstrate how this class links to semi-Markov models and provides a convenient generalization of the Dynamic Bayesian Network (DBN). In particular we develop a repeating time-slice Dynamic CEG providing a useful and simpler model in this family. We demonstrate how the Dynamic CEG's graphical formulation exhibits asymmetric conditional independence statements and also how each model can be estimated in a closed form enabling fast model search over the class. The expressive power of this model class together with its estimation is illustrated throughout by a variety of examples that include the risk of childhood hospitalization and the efficacy of a flu vaccine.",""
"We present a place-history-based activity prediction system called Agatha, in order to enable activity-aware mobile services in smart cities. The system predicts a user's potential subsequent activities that are highly likely to occur given a series of information about activities done before or activity-related contextual information such as visit place and time. To predict the activities, we develop a causality-based activity prediction model using Bayesian networks. The basic idea of the prediction is that where a person has been and what he/she has done so far influence what he/she will do next. To show the feasibility, we evaluate the prediction model using the American Time-Use Survey (ATUS) dataset, which includes more than 10,000 people's location and activity history. Our evaluation shows that Agatha can predict users' potential activities with up to 90% accuracy for the top 3 activities, more than 80% for the top 2 activities, and about 65% for the top 1 activity while considering a relatively large number of daily activities defined in the ATUS dataset, that is, 17 activities.",""
"The dropout high rate is a serious problem in E-learning programs. Thus, it is a concern of education administrators and researchers. Predicting the dropout potential of students is a workable solution for preventing dropouts. Based on the analysis of related literature, this study selected students' personal characteristics and academic performance as input attributions. Prediction models were developed using Artificial Neural Network (ANN), Decision Tree (DT) and Bayesian Networks (BNs). A large sample of 62,375 students was utilized in the procedures of model training and testing. The results of each model were presented in a confusion matrix and were analyzed by calculating the rates of accuracy, precision, recall, and F-measure. The results suggested all of the three machine learning methods were effective for student dropout prediction, but DT presented a better performance. Finally, some suggestions were made for future research.",""
"The problem of finding the most probable explanation to a designated set of variables given partial evidence (the MAP problem) is a notoriously intractable problem in Bayesian networks, both to compute exactly and to approximate. It is known, both from theoretical considerations and from practical experience, that low tree-width is typically an essential prerequisite to efficient exact computations in Bayesian networks. In this paper we investigate whether the same holds for approximating MAP. We define four notions of approximating MAP (by value, structure, rank, and expectation) and argue that all of them are intractable in general. We prove that efficient value-approximations, structure-approximations, and rank-approximations of MAP instances with high tree-width will violate the Exponential Time Hypothesis. In contrast, we show that MAP can sometimes be efficiently expectation-approximated, even in instances with high tree-width, if the most probable explanation has a high probability. We introduce the complexity class FERT, analogous to the class FPT, to capture this notion of fixed-parameter expectation-approximability. We suggest a road-map to future research that yields fixed-parameter tractable results for expectation-approximate MAP, even in graphs with high tree-width.",""
"The correct prediction in the transport logistics has vital importance in the adequate means and resource planning and in their optimisation. Up to this date, port planning studies were based mainly on empirical, analytical or simulation models. This paper deals with the possible use of Bayesian networks in port planning. The methodology indicates the work scenario and how the network was built. The network was afterwards used in container terminals planning, with the support provided by the tools of the Elvira code. The main variables were defined and virtual scenarios inferences were realised in order to carry out the analysis of the container terminals scenarios through probabilistic graphical models. Having performed the data analysis on the different terminals and on the considered variables (berth, area, TEU, crane number), the results show the possible relationships between them. Finally, the conclusions show the obtained values on each considered scenario.","The main variables were defined and virtual scenarios inferences were realised in order to carry out the analysis of the container terminals scenarios through probabilistic graphical models."
"Decentralized POMDPs provide an expressive framework for multiagent sequential decision making. However, the complexity of these models-NEXP-Complete even for two agents-has limited their scalability. We present a promising new class of approximation algorithms by developing novel connections between multiagent planning and machine learning. We show how the multiagent planning problem can be reformulated as inference in a mixture of dynamic Bayesian networks (DBNs). This planning-as-inference approach paves the way for the application of efficient inference techniques in DBNs to multiagent decision making. To further improve scalability, we identify certain conditions that are sufficient to extend the approach to multiagent systems with dozens of agents. Specifically, we show that the necessary inference within the expectation-maximization framework can be decomposed into processes that often involve a small subset of agents, thereby facilitating scalability. We further show that a number of existing multiagent planning models satisfy these conditions. Experiments on large planning benchmarks confirm the benefits of our approach in terms of runtime and scalability with respect to existing techniques.","We show how the multiagent planning problem can be reformulated as inference in a mixture of dynamic Bayesian networks (DBNs)."
"We discuss different mathematical models of gene regulatory networks as relevant to the onset and development of cancer. After discussion of alternative modelling approaches, we use a paradigmatic two-gene network to focus on the role played by time delays in the dynamics of gene regulatory networks. We contrast the dynamics of the reduced model arising in the limit of fast mRNA dynamics with that of the full model. The review concludes with the discussion of some open problems.",""
"Purpose: Increase of costs and complexities in organizations beside the increase of uncertainty and risks have led the managers to use the risk management in order to decrease risk taking and deviation from goals. SCRM has a close relationship with supply chain performance. During the years different methods have been used by researchers in order to manage supply chain risk but most of them are either qualitative or quantitative. Supply chain operation reference (SCOR) is a standard model for SCP evaluation which have uncertainty in its metrics. In This paper by combining qualitative and quantitative metrics of SCOR, supply chain performance will be measured by Bayesian Networks. Design/methodology/approach: First qualitative assessment will be done by recognizing uncertain metrics of SCOR model and then by quantifying them, supply chain performance will be measured by Bayesian Networks (BNs) and supply chain operations reference (SCOR) in which making decision on uncertain variables will be done by predictive and diagnostic capabilities. Findings: After applying the proposed method in one of the biggest automotive companies in Iran, we identified key factors of supply chain performance based on SCOR model through predictive and diagnostic capability of Bayesian Networks. After sensitivity analysis, we find out that 'Total cost' and its criteria that include costs of labors, warranty, transportation and inventory have the widest range and most effect on supply chain performance. So, managers should take their importance into account for decision making. We can make decisions simply by running model in different situations. Research limitations/implications: A more precise model consisted of numerous factors but it is difficult and sometimes impossible to solve big models, if we insert all of them in a Bayesian model. We have adopted real world characteristics with our software and method abilities. On the other hand, fewer data exist for some of the performance metrics. Practical implications: Mangers often use simple qualitative metrics for SCRM. However, combining qualitative and quantitative metrics will be more useful. Industries can recognize the important uncertain metrics by predicting supply chain performance and diagnosing possible happenings. Originality/value: This paper proposed a Bayesian method based on SCOR metrics which has the ability to manage supply chain risks and improve supply chain performance. This is the only presented case study for measuring supply chain performance by SCOR metrics.",""
"The first article describing this project presented the three games that the participants played: the Ultimatum Game, the Trust Game and the Public Goods Game. This article describes the study group on the basis of a questionnaire regarding where they study and come from, their social contacts, interest in current issues, views on inequality and outlook on life. A description of the migratory decisions of students is given. In particular, two exploratory methods are used to investigate the data's structure: Bayesian networks and principal component analysis. Bayesian networks are used to illustrate the associations between categorical variables. Principal component analysis is designed to describe latent variables which reflect the associations between numerical variables. We present the results of this analysis and discuss the advantages and disadvantages of these two methods.",""
"In home/office automation applications, pyroelectric infrared (PIR) sensors have been widely used for human presence detection. However, PIR sensors suffer from false-on and false-off problems. In this study, we used multimodal sensors to complement each other in order to improve the detection performance. In addition, we proposed a low-computational-complexity sensor fusion algorithm to infer the status of room occupancy, which is very suitable for manipulation using the sensor nodes of wireless sensor networks. By combining spatial and temporal data through a sensor fusion mechanism, the proposed method can address the missing sensing values problem of PIR sensors, thus improving the accuracy of room occupancy determination. The inference algorithm of sensor fusion was evaluated for the sensor detection accuracy and compared with multisensor fusion using dynamic Bayesian networks (DBNs). The experimental results showed that the detection accuracy of room occupancy was greater than 99%, which was better than that of the DBN-based sensor fusion method.","The inference algorithm of sensor fusion was evaluated for the sensor detection accuracy and compared with multisensor fusion using dynamic Bayesian networks (DBNs)."
"Some of the basic algorithms for learning the structure of Bayesian networks, such as the well-known K2 algorithm, require a prior ordering over the nodes as part of the input. It is well known that the accuracy of the K2 algorithm is highly sensitive to the initial ordering. In this paper, we introduce the aggregation of ordering information provided by multiple experts to obtain a more robust node ordering. In order to reduce the effect of novice participants, the accuracy of each person is used in the aggregation process. The accuracies of participants, not known in advance, are estimated by the expectation maximization algorithm. Any possible contradictions occurred in the resulting aggregation are resolved by modelling the result as a directed graph and avoiding the cycles in this graph. Finally, the topological order of this graph is used as the initial ordering in the K2 algorithm. The experimental results demonstrate the effectiveness of the proposed method in improving the structure learning process.",""
"In this paper, we generalize the noisy-or model. The generalizations are three-fold. First, we allow parents to be multivalued ordinal variables. Second, parents can have both positive and negative influences on their common child. Third, we describe how the suggested generalization can be extended to multivalued child variables. The major advantage of our generalizations is that they require only one parameter per parent. We suggest a model learning method and report results of experiments on the Reuters text classification data. The generalized noisy-or models achieve equal or better performance than the standard noisy-or. An important property of the noisy-or model and of its generalizations suggested in this paper is that it allows more efficient exact inference than logistic regression models do.","We suggest a model learning method and report results of experiments on the Reuters text classification data."
"Accident duration modeling has been considered as a difficult problem due to the variety of information (accident characteristics, traffic and weather information, geometry of the accident location and so on) that should be taken into account to improve predictions and explain the phenomenon. We introduce Fuzzy Rule-Based Systems to model freeway accident duration and cope with the uncertainties and complexities hindering in accident monitoring systems. The models are also compared to classical hazard-based regression models, as well as Multi-Layer Perceptrons. Results show that a Fuzzy Rule-Based System may predict accident duration with fair accuracy using limited information on traffic and weather conditions. Introducing the entire amount of information on accidents to the Fuzzy Rule-Based System leads to reduced modeling accuracy, probably due to the difficulties in converging to a solution. Nevertheless, the Fuzzy Rule-Based System with limited information may predict more accurately than classical hazard based duration models and with comparable accuracy to a Multi-Layer Perceptron, which is presented with information on accident characteristics, traffic and weather conditions, as well as the geometry of the accident location. (C) 2015 Elsevier Ltd. All rights reserved.","The models are also compared to classical hazard-based regression models, as well as Multi-Layer Perceptrons."
"Because robots are physically embodied agents, touch is one of the important modalities through which robots communicate with humans. Among the several factors that affect human-robot interaction, this research focuses on the effect of a user's personality traits on tactile interactions with a robot. Participants interacted freely with a robot and their tactile interaction patterns were analyzed. Several classifiers were used to examine the effect of a participant's degree of extroversion on tactile communication patterns with the robot and our results showed that a user's personality traits affected the way in which they interacted with the robot. Specifically, important features of Bayesian networks, such as the Markov blanket and what-if/goal-seeking power were tested and showed the effect of personality on tactile interaction with respect to where and how participants touched the robot. We also found that, by using Bayesian network classifiers, a user's personality traits can be inferred based on tactile communication patterns.","Several classifiers were used to examine the effect of a participant's degree of extroversion on tactile communication patterns with the robot and our results showed that a user's personality traits affected the way in which they interacted with the robot."
"Purpose - The purpose of this paper is to perform an extensive literature review in the area of decision making for condition-based maintenance (CBM) and identify possibilities for proactive online recommendations by considering real-time sensor data. Based on these, the paper aims at proposing a framework for proactive decision making in the context of CBM. Design/methodology/approach - Starting with the manufacturing challenges and the main principles of maintenance, the paper reviews the main frameworks and concepts regarding CBM that have been proposed in the literature. Moreover, the terms of e-maintenance, proactivity and decision making are analysed and their potential relevance to CBM is identified. Then, an extensive literature review of methods and techniques for the various steps of CBM is provided, especially for prognosis and decision support. Based on these, limitations and gaps are identified and a framework for proactive decision making in the context of CBM is proposed. Findings - In the proposed framework for proactive decision making, the CBM concept is enriched in the sense that it is structured into two components: the information space and the decision space. Moreover, it is extended in a way that decision space is further analyzed according to the types of recommendations that can be provided. Moreover, possible inputs and outputs of each step are identified. Practical implications - The paper provides a framework for CBM representing the steps that need to be followed for proactive recommendations as well as the types of recommendations that can be given. The framework can be used by maintenance management of a company in order to conduct CBM by utilizing real-time sensor data depending on the type of decision required. Originality/value - The results of the work presented in this paper form the basis for the development and implementation of proactive Decision Support System (DSS) in the context of maintenance.",""
"The increasing use of vehicles and the heavy traffic emerged in metropolises have both turn to an important part of people's daily routine. This situation could provide a unique opportunity for some kind of social communication among people. However, this social communication has its requirements and constraints. In this paper, we introduce the notion of SocioCar to develop a transient social vehicular network service. This service inspires the users to communicate quickly by reducing social constraints. In this regard, one of the key points in this paper is the idea to adapt the user identity and its role in rapid communication. Furthermore, we integrate our design with a user social classification method using Bayesian Networks with an aim to effectively highlight the desired users and predict users' characteristics. Through this approach, we investigate the social classification model and conduct a user study to probe the usefulness of social network.","Furthermore, we integrate our design with a user social classification method using Bayesian Networks with an aim to effectively highlight the desired users and predict users' characteristics."
"Inferring the gene regulatory network (GRN) structure from data is an important problem in computational biology. However, it is a computationally complex problem and approximate methods such as heuristic search techniques, restriction of the maximum-number-of-parents (maxP) for a gene, or an optimal search under special conditions are required. The limitations of a heuristic search are well known but literature on the detailed analysis of the widely used maxP technique is lacking. The optimal search methods require large computational time. We report the theoretical analysis and experimental results of the strengths and limitations of the maxP technique. Further, using an optimal search method, we combine the strengths of the maxP technique and the known GRN topology to propose two novel algorithms. These algorithms are implemented in a Bayesian network framework and tested on biological, realistic, and in silico networks of different sizes and topologies. They overcome the limitations of the maxP technique and show superior computational speed when compared to the current optimal search algorithms.",""
"This article clarifies the factual basis for attributing causal responsibility in interpersonal causation scenarios in international criminal law. Such content is obliterated when explaining causal contributions in overdetermined and indeterministic harm scenarios. The resulting gap between individual agency and causal attribution is explained away by reference to values underlying legal liability ('causal minimalism'). Probabilistic causal models can express causal influences in interpersonal causation scenarios, and explicate the objective basis of causal attribution. International Criminal Courts' approaches to indirect and joint perpetration, as well as the notion of causal contribution in joint criminal enterprise are discussed in light of the existing approaches to testing causation in law, as well as with regard to Judea Pearl's notion of causal sustenance. The article concludes that expressing causal contributions in language of probabilities can explain causal intuitions underlying legal liability better than supplanting factual basis of attributing responsibility with normative and policy justifications.",""
"Purpose - A perfect knowledge management (KM) initiative is one that achieves its objectives without any failure during a pre-defined period. However, KM implementation is not perfect in every organization as it requires substantial changes in organizational infrastructures, including culture, structure, and technology. Therefore, the purpose of this paper is to propose a model for assessing the reliability of KM to help organizations evaluate their ability to implement KM successfully by identifying key reliability variables, modeling the complex interaction structure among variables, and determining the probability of failure for each KM capability. Design/methodology/approach - In this study, relevant variables are identified by a thorough analysis of related references in literature. In order to determine the compound structure of complicated interactions among variables, a group-based approach is utilized. Based on the combined cognitive maps, a cognitive network is constructed as a framework for graphically representing the logical relationships between variables and capturing the uncertainty in the dependency among these variables using conditional probabilities. The applicability of the proposed approach and the efficacy of the model was verified and validated with data from a banking institution. Findings - Results show that KM reliability can be defined by the degree to which required KM capabilities, including infrastructure and process capabilities, have the ability to perform as intended in a certain organizational environment. Furthermore, it is demonstrated that reliability assessment of KM through a hybrid approach of fuzzy cognitive map and Bayesian network is possible and useful. Practical implications - The proposed reliability assessment model facilitates the process of understanding why and how failures occur in KM. Moreover, the proposed approach evaluates the probability of success for each variable as well as for the entire KM initiative. Therefore, it can provide insight for managers and executives into the degree of reliability for their existing KM and prevention of failures in vital factors through necessary actions. Originality/value - The suggested approach to KM reliability assessment is a novel method that provides powerful arguments for a more holistic view of KM reliability factors, which is crucial for the successful implementation of KM.",""
"During the past decades, numerous computational approaches have been introduced for inferring the GRNs. PCA-CMI approach achieves the highest precision on the benchmark GRN datasets; however, it does not recover the meaningful edges that may have been deleted in an earlier iterative process. To recover this disadvantage and enhance the precision and robustness of GRNs inferred, we present an ensemble method, named as JRAMF, to infer GRNs from gene expression data by adopting two strategies of resampling and arithmetic mean fusion in this work. The jackknife resampling procedure were first employed to form a series of sub-datasets of gene expression data, then the PCA-CMI was used to generate the corresponding sub-networks from the sub-datasets, and the final GRN was inferred by integrating these sub-networks with an arithmetic mean fusion strategy. Compared with PCA-CMI algorithm, the results show that JRAMF outperforms significantly PCA-CMI method, which has a high and robust performance.",""
"Purpose - Cold chain has become an integral part of the supply chain domain. The purpose of this paper is to consider all the significant factors in a single study. This will result into a better model to study the effectiveness of a cold chain because there has been absence of such an integrated study. Design/methodology/approach - The basis of the factors is justified by performing extensive literature review. Inter relations are drawn based on critical analysis of each factor and its implications on cold chain. Bayesian Network is used to develop the model. Findings - The end result is an established model, depicting the interdependencies of the factors. The model ultimately provides effectiveness of a given cold chain when the corresponding values of factors are put in. Practical implications - The findings will be helpful for government and non-government bodies to analyse the effectiveness of a cold chain. This can be used to increase the performance of different stages in the cold chain. From a business perspective, an investor can analyse the cold chains of various geographies in order to make an investment decision. Originality/value - The value lies in developing and introducing new factors which were not considered in the related literature previously. To identify the inter relations among the factors in order to build a causal model is another contribution of the present paper. This would assist in decision-making process with respect to any given cold chain. It can be applied to any cold chain as proposed model is not specific to a particular country or material.",""
"This paper proposes an approach to solve cost-optimal factored planning problems. Planning consists in organizing actions in order to reach some predefined goal. In factored planning one considers several interacting planning problems and has to design an action plan for each of them. But one must also guarantee that all these local plans are compatible: actions shared among several problems must be jointly performed or jointly rejected. We enrich the problem with the extra requirement that the global plan computed in this modular manner must also minimize the sum of all action costs. A solution is provided to this problem, based on classical message passing algorithms, known as belief propagation in the setting of Bayesian networks. Here, messages carry complex information under the form of weighted (or (min; +)) automata, and all computations are performed with these objects. At the time our first paper on this topic was published, this method was the only one to solve cost-optimal factored planning problems in a modular way. Since then, new approaches were proposed. Experiments on classical benchmarks show that it is a valuable alternative to existing methods.",""
"Agricultural watersheds are affected by changes in climate, land use, agricultural practices, and human demand for energy, food, and water resources. In this context, we analyzed the agricultural, urbanizing Yahara watershed ( size: 1345 km(2), population: 372,000) to assess its responses to multiple changing drivers. We measured recent trends in land use/cover and water quality of the watershed, spatial patterns of 10 ecosystem services, and spatial patterns and nestedness of governance. We developed scenarios for the future of the Yahara watershed by integrating trends and events from the global scenarios literature, perspectives of stakeholders, and models of biophysical drivers and ecosystem services. Four qualitative scenarios were created to explore plausible trajectories to the year 2070 in the watershed's social-ecological system under different regimes: no action on environmental trends, accelerated technological development, strong intervention by government, and shifting values toward sustainability. Quantitative time-series for 2010-2070 were developed for weather and land use/cover during each scenario as inputs to model changes in ecosystem services. Ultimately, our goal is to understand how changes in the social-ecological system of the Yahara watershed, including management of land and water resources, can build or impair resilience to shifting drivers, including climate.",""
"Complex problems often result from the multiple interactions between human activities and ecosystems. The interconnected nature of ecological and social systems should be considered if these \"wicked problems\" are to be addressed. Ecosystem service approaches provide an opportunity to link ecosystem function with social values, but in practice the essential role that social dynamics play in the delivery of outcomes remains largely unexplored. Social factors such as management regimes, power relationships, skills, and values, can dramatically affect the definition and delivery of ecosystem services. Input from a diverse group of stakeholders improves the capacity of ecosystem service approaches to address wicked problems by acknowledging diverse sets of values and accounting for conflicting world views. Participatory modeling can incorporate both social and ecological dynamics into decision making that involves stakeholders, but is itself a complex social undertaking that may not yield precise or predictable outcomes. We explore the efficacy of different types of participatory modeling in relation to the integration of social values into ecosystem services frameworks and the generation of four important elements of social capital needed to address wicked problems: enhancing social learning and capacity building; increasing transparency; mediating power; and building trust. Our findings indicate that mediated modeling, group mapping, and mental/conceptual modeling are likely to generate elements of social capital that can improve ecosystem service frameworks. Participatory simulation, system dynamic modeling, and Bayesian belief networks, if utilized in isolation, were found to have a low likelihood of generating the social capital needed to improve ecosystem services frameworks. Scenario planning, companion modeling, group model building, and participatory mapping all generate a moderate to high level of social capital elements that improve the capacity of ecosystem service frameworks to address wicked problems.",""
"In 2010, M. Studeny, R. Hemmecke, and S. Lindner explored a new algebraic description of graphical models, called characteristic imsets. Compared with standard imsets, characteristic imsets have several advantages: they are still unique vector representatives of conditional independence structures, 0-1 vectors, and more intuitive in terms of graphs than standard imsets. After defining a characteristic imset polytope (cim-polytope) as the convex hull of all characteristic imsets with a given set of nodes, they also showed that a model selection in graphical models, which maximizes a quality criterion, can be converted into a linear programming problem over the cim-polytope. However, in general, for a fixed set of nodes, the cim-polytope can have exponentially many vertices over an exponentially high dimension. Therefore, in this paper, we focus on the family of directed acyclic graphs whose nodes have a fixed order. This family includes diagnosis models described by bipartite graphs with a set of m nodes and a set of n nodes for any m, n is an element of Z(+). We first consider cim-polytopes for all diagnosis models and show that these polytopes are direct products of simplices. Then we give a combinatorial description of all edges and all facets of these polytopes. Finally, we generalize these results to the cim-polytopes for all Bayesian networks with a fixed underlying ordering of nodes with or without fixed (or forbidden) edges.",""
"Fault tolerant technology has greatly improved the reliability of train-ground wireless communication system (TWCS). However, its high reliability caused the lack of sufficient fault data and epistemic uncertainty, which increased significantly challenges in system diagnosis. A novel diagnosis method for TWCS is proposed to deal with these challenges in this paper, which makes the best of reliability analysis, fuzzy sets theory and MADM. Specifically, it adopts dynamic fault tree to model their dynamic fault modes and evaluates the failure rates of the basic events using fuzzy sets theory and expert elicitation to hand epistemic uncertainty. Furthermore, it calculates some quantitative parameters information provided by reliability analysis using algebraic technique and Bayesian network to overcome some disadvantages of the traditional methods. Diagnostic importance factor, sensitivity index and heuristic information values are considered comprehensively to obtain the optimal diagnostic ranking order of TWCS using an improved TOPSIS. The proposed method takes full advantages of the dynamic fault tree for modelling, fuzzy sets theory for handling uncertainty and MADM for the best fault search scheme, which is especially suitable for fault diagnosis of the complex systems.",""
"Purpose - This paper aims to contribute towards understanding how safety knowledge can be elicited from railway experts for the purposes of supporting effective decision-making. Design/methodology/approach - A consortium of safety experts from across the British railway industry is formed. Collaborative modelling of the knowledge domain is used as an approach to the elicitation of safety knowledge from experts. From this, a series of knowledge models is derived to inform decision-making. This is achieved by using Bayesian networks as a knowledge modelling scheme, underpinning a Safety Prognosis tool to serve meaningful prognostics information and visualise such information to predict safety violations. Findings - Collaborative modelling of safety-critical knowledge is a valid approach to knowledge elicitation and its sharing across the railway industry. This approach overcomes some of the key limitations of existing approaches to knowledge elicitation. Such models become an effective tool for prediction of safety cases by using railway data. This is demonstrated using passenger-train interaction safety data. Practical implications - This study contributes to practice in two main directions: by documenting an effective approach to knowledge elicitation and knowledge sharing, while also helping the transport industry to understand safety. Social implications - By supporting the railway industry in their efforts to understand safety, this research has the potential to benefit railway passengers, staff and communities in general, which is a priority for the transport sector. Originality/value - This research applies a knowledge elicitation approach to understanding safety based on collaborative modelling, which is a novel approach in the context of transport.",""
"This work analyzes energy demand in a High-Tech greenhouse and its characterization, with the objective of building and evaluating classification models based on Bayesian networks. The utility of these models resides in their capacity of perceiving relations among variables in the greenhouse by identifying probabilistic dependences between them and their ability to make predictions without the need of observing all the variables present in the model. In this way they provide a useful tool for an energetic control system design. In this paper the acquisition data system used in order to collect the dataset studied is described. The energy demand distribution is analyzed and different discretization techniques are applied to reduce its dimensionality, paying particular attention to their impact on the classification model's performance. A comparison between the different classification models applied is performed.","This work analyzes energy demand in a High-Tech greenhouse and its characterization, with the objective of building and evaluating classification models based on Bayesian networks."
"BACKGROUND: The intrinsic mechanism of multimorbidity is difficult to recognize and prediction and diagnosis are difficult to carry out accordingly. Bayesian networks can help to diagnose multimorbidity in health care, but it is difficult to obtain the conditional probability table (CPT) because of the lack of clinically statistical data. OBJECTIVE: Today, expert knowledge and experience are increasingly used in training Bayesian networks in order to help predict or diagnose diseases, but the CPT in Bayesian networks is usually irrational or ineffective for ignoring realistic constraints especially in multimorbidity. METHODS: In order to solve these problems, an evidence reasoning (ER) approach is employed to extract and fuse inference data from experts using a belief distribution and recursive ER algorithm, based on which evidence reasoning method for constructing conditional probability tables in Bayesian network of multimorbidity is presented step by step. RESULTS: A multimorbidity numerical example is used to demonstrate the method and prove its feasibility and application. Bayesian network can be determined as long as the inference assessment is inferred by each expert according to his/her knowledge or experience. CONCLUSIONS: Our method is more effective than existing methods for extracting expert inference data accurately and is fused effectively for constructing CPTs in a Bayesian network of multimorbidity.","METHODS: In order to solve these problems, an evidence reasoning (ER) approach is employed to extract and fuse inference data from experts using a belief distribution and recursive ER algorithm, based on which evidence reasoning method for constructing conditional probability tables in Bayesian network of multimorbidity is presented step by step."
"Analysing structure of gene networks is an important way to understand regulatory mechanisms of organism at the molecular level. In this work, gene mutual information networks are constructed based on gene expression profiles in prostate tissues with and without cancer. In order to contrast structural difference of normal and diseased networks, curves of four structural parameters are given with the change of thresholds. Then threshold discrimination intervals and discrimination weights are defined. A method of finding structural key genes with significant degree-difference is proposed. The finding of key genes will help the biomedical scientists to further research the pathogenesis of prostate cancer. Finally randomisation test is performed to prove that these structural parameters can distinguish normal and prostate cancer in their structures compared with these results in real data.",""
"Genome-wide Association Studies (GWAS) have resulted in many discovered risk variants for several obesity-related traits. However, before clinical relevance of these discoveries can be achieved, molecular or physiological mechanisms of these risk variants needs to be discovered. One strategy is to perform data mining of phenotypically-rich data sources such as those present in dbGAP (database of Genotypes and Phenotypes) for hypothesis generation. Here we propose a technique that combines the power of existing Bayesian Network (BN) learning algorithms with the statistical rigour of Structural Equation Modelling (SEM) to produce an overall phenotypic network discovery system with optimal properties. We illustrate our method using the analysis of a candidate SNP data set from the AMERICO sample, a multi-ethnic cross-sectional cohort of roughly 300 children with detailed obesity-related phenotypes. We demonstrate our approach by showing genetic mechanisms for three obesity-related SNPs.",""
"Bayesian Multi-nets (BMNs) are a special kind of Bayesian network (BN) classifiers that consist of several local Bayesian networks, one for each predictable class, to model an asymmetric set of variable dependencies given each class value. Deterministic methods using greedy local search are the most frequently used methods for learning the structure of BMNs based on optimizing a scoring function. Ant Colony Optimization (ACO) is a meta-heuristic global search method for solving combinatorial optimization problems, inspired by the behavior of real ant colonies. In this paper, we propose two novel ACO-based algorithms with two different approaches to build BMN classifiers: ABC-Miner(l)(mn) and ABC-Miner(g)(mn). The former uses a local learning approach, in which the ACO algorithm completes the construction of one local BN at a time. The latter uses a global approach, which involves building a complete BMN classifier by each single ant in the colony. We experimentally evaluate the performance of our ant-based algorithms on 33 benchmark classification datasets, where our proposed algorithms are shown to be significantly better than other commonly used deterministic algorithms for learning various Bayesian classifiers in the literature, as well as competitive to other well-known classification algorithms.","Bayesian Multi-nets (BMNs) are a special kind of Bayesian network (BN) classifiers that consist of several local Bayesian networks, one for each predictable class, to model an asymmetric set of variable dependencies given each class value."
"The Connectivity Map (CMAP) project profiled human cancer cell lines exposed to a library of anticancer compounds with the goal of connecting cancer with underlying genes and potential treatments. Since the therapeutic goal of most anticancer drugs is to induce tumor-selective apoptosis, it is critical to understand the specific cell death pathways triggered by drugs. This can help to better understand the mechanism of how cancer cells respond to chemical stimulations and improve the treatment of human tumors. In this study, using CMAP microarray data from breast cancer cell line MCF7, we applied a Gaussian Bayesian network modeling approach and identified apoptosis as a major drug-induced cellular-pathway. We then focused on 13 apoptotic genes that showed significant differential expression across all drug-perturbed samples to reconstruct the apoptosis network. In our predicted subnetwork, 9 out of 15 high-confidence interactions were validated in the literature, and our inferred network captured two major cell death pathways by identifying BCL2L11 and PMAIP1 as key interacting players for the intrinsic apoptosis pathway and TAXBP1 and TNFAIP3 for the extrinsic apoptosis pathway. Our inferred apoptosis network also suggested the role of BCL2L11 and TNFAIP3 as \"gateway\" genes in the drug-induced intrinsic and extrinsic apoptosis pathways.",""
"The aim of this study was to determine the accuracy of Bayesian networks in supporting breast cancer diagnoses. Systematic review and meta-analysis were carried out, including articles and papers published between January 1990 and March 2013. We included prospective and retrospective cross-sectional studies of the accuracy of diagnoses of breast lesions (target conditions) made using Bayesian networks (index test). Four primary studies that included 1,223 breast lesions were analyzed, 89.52% (444/496) of the breast cancer cases and 6.33% (46/727) of the benign lesions were positive based on the Bayesian network analysis. The area under the curve (AUC) for the summary receiver operating characteristic curve (SROC) was 0.97, with a Q* value of 0.92. Using Bayesian networks to diagnose malignant lesions increased the. pretest probability of a true positive from 40.03% to 90.05% and decreased the probability of a false negative to 6.44%. Therefore, our results demonstrated that Bayesian networks provide an accurate and non-invasive method to support breast cancer diagnosis.",""
"In technical systems understood in terms of Agile Systems, the important elements are information flows between all phases of an object existence. Among these information streams computation processes play an important role and can be done automatically and also in a natural way should include consideration of uncertainty. This article presents a model of such a process implemented in a Bayesian network technology. The model allows the prediction of the unit costs of operation of a combine harvester based on the monitoring of dependent variables. The values of the decision variables representing the parameters of the machine's operation and the intensity and the conditions for its operation, are known to an accuracy, which is defined by a probability distribution. The study shows, using inference mechanisms built into the network, how cost simulation studies of various situational options can be carried out.","The study shows, using inference mechanisms built into the network, how cost simulation studies of various situational options can be carried out."
"Event tree/fault tree (E/FT) method is the most recognized probabilistic risk assessment tool for complex large engineering systems, while its classical formalism most often only considers pivotal events (PEs) being independent or time-independent. However, the practical difficulty regarding phased-mission system (PMS) is that the PEs always modelled by fault trees (FTs) are explicit dependent caused by shared basic events, and phase-dependent when the time interval between PEs is not negligible. In this paper, we combine the Bayesian networks (BN) with the E/FT analysis to figure such types of PMS based on the conditional probability to give expression of the phase-dependency, and further expand it by the dynamic Bayesian networks (DBN) to cope with more complex time-dependency such as functional dependency and spares. Then, two detailed examples are used to demonstrate the application of the proposed approach in complex event tree time-dependency analysis.",""
"The model building of Influence Nets, a special instance of Bayesian belief networks, is a time-consuming and laborintensive task. No formal process exists that decision makers/system analyst, who are typically not familiar with the underlying theory and assumptions of belief networks, can use to build concise and easy-to-interpret models. In many cases, the developed model is extremely dense, that is, it has a very high link-to-node ratio. The complexity of a network makes the already intractable task of belief updating more difficult. The problem is further intensified in dynamic domains where the structure of the built model is repeated for multiple time-slices. It is, therefore, desirable to do a post-processing of the developed models and to remove arcs having a negligible influence on the variable(s) of interests. The paper applies sensitivity of arc analysis to identify arcs that can be removed from an Influence Net without having a significant impact on its inferencing capability. A metric is suggested to gauge changes in the joint distribution of variables before and after the arc removal process. The results are benchmarked against the KL divergence metric. An empirical study based on several real Influence Nets is conducted to test the performance of the sensitivity of arc analysis in reducing the model complexity of an Influence Net without causing a significant change in its joint probability distribution.",""
"Statistics is one of the paramount sciences of the 21st century because of the emergence of the value of data as a key asset for development. It is of fundamental importance to deliver statistical methods and tools for the progress of science and sustainable economic growth. Statistics for Innovation (sfi)(2) has been, between 2007 and 2014, a centre of excellence providing new statistical methodology for selected Norwegian industries and service providers. In this paper, we trace the history of (sfi)(2), describe the way it operated and the type of projects it has run over 8 years. We discuss challenges and the way we tried to overcome them. The purpose of this paper is to share our experience for similar centres, outline the advantages and challenges and indicate some recommendations. Copyright (c) 2015 John Wiley & Sons, Ltd.",""
"Marine benthic ecosystems are difficult to monitor and assess, which is in contrast to modern ecosystem-based management requiring detailed information at all important ecological and anthropogenic impact levels. Ecosystem management needs to ensure a sustainable exploitation of marine resources as well as the protection of sensitive habitats, taking account of potential multiple-use conflicts and impacts over large spatial scales. The urgent need for large-scale spatial data on benthic species and communities resulted in an increasing application of distribution modelling (DM). The use of DM techniques enables to employ full spatial coverage data of environmental variables to predict benthic spatial distribution patterns. Especially, statistical DMs have opened new possibilities for ecosystem management applications, since they are straightforward and the outputs are easy to interpret and communicate. Mechanistic modelling techniques, targeting the fundamental niche of species, and Bayesian belief networks are the most promising to further improve DM performance in the marine realm. There are many actual and potential management applications of DMs in the marine benthic environment, these are (i) early warning systems for species invasion and pest control, (ii) to assess distribution probabilities of species to be protected, (iii) uses in monitoring design and spatial management frameworks (e.g. MPA designations), and (iv) establishing long-term ecosystem management measures (accounting for future climate-driven changes in the ecosystem). It is important to acknowledge also the limitations associated with DM applications in a marine management context as well as considering new areas for future DM developments. The knowledge of explanatory variables, for example, setting the basis for DM, will continue to be further developed: this includes both the abiotic (natural and anthropogenic) and the more pressing biotic (e.g. species interactions) aspects of the ecosystem. While the response variables on the other hand are often focused on species presence and some work undertaken on species abundances, it is equally important to consider, e.g. biological traits or benthic ecosystem functions in DM applications. Tools such as DMs are suitable to forecast the possible effects of climate change on benthic species distribution patterns and hence could help to steer present-day ecosystem management.",""
"Inferring Gene Regulatory Networks (GRNs) from gene expression data is a major challenge in systems biology. The Path Consistency (PC) algorithm is one of the popular methods in this field. However, as an order dependent algorithm, PC algorithm is not robust because it achieves different network topologies if gene orders are permuted. In addition, the performance of this algorithm depends on the threshold value used for independence tests. Consequently, selecting suitable sequential ordering of nodes and an appropriate threshold value for the inputs of PC algorithm are challenges to infer a good GRN. In this work, we propose a heuristic algorithm, namely SORDER, to find a suitable sequential ordering of nodes. Based on the SORDER algorithm and a suitable interval threshold for Conditional Mutual Information (CMI) tests, a network inference method, namely the Consensus Network (CN), has been developed. In the proposed method, for each edge of the complete graph, a weighted value is defined. This value is considered as the reliability value of dependency between two nodes. The final inferred network, obtained using the CN algorithm, contains edges with a reliability value of dependency of more than a defined threshold. The effectiveness of this method is benchmarked through several networks from the DREAM challenge and the widely used SOS DNA repair network in Escherichia coli. The results indicate that the CN algorithm is suitable for learning GRNs and it considerably improves the precision of network inference. The source of data sets and codes are available at http://bs.ipm.ir/softwares/CN.","Based on the SORDER algorithm and a suitable interval threshold for Conditional Mutual Information (CMI) tests, a network inference method, namely the Consensus Network (CN), has been developed."
"Decades of hypothesis-driven and/or first-principles research have been applied towards the discovery and explanation of the mechanisms that drive climate phenomena, such as western African Sahel summer rainfall variability. Although connections between various climate factors have been theorized, not all of the key relationships are fully understood. We propose a data-driven approach to identify candidate players in this climate system, which can help explain underlying mechanisms and/or even suggest new relationships, to facilitate building a more comprehensive and predictive model of the modulatory relationships influencing a climate phenomenon of interest. We applied coupled heterogeneous association rule mining (CHARM), Lasso multivariate regression, and dynamic Bayesian networks to find relationships within a complex system, and explored means with which to obtain a consensus result from the application of such varied methodologies. Using this fusion of approaches, we identified relationships among climate factors that modulate Sahel rainfall. These relationships fall into two categories: well-known associations from prior climate knowledge, such as the relationship with the El Nino-Southern Oscillation (ENSO) and putative links, such as North Atlantic Oscillation, that invite further research.","We applied coupled heterogeneous association rule mining (CHARM), Lasso multivariate regression, and dynamic Bayesian networks to find relationships within a complex system, and explored means with which to obtain a consensus result from the application of such varied methodologies."
"Cancer is a somatic evolutionary process characterized by the accumulation of mutations, which contribute to tumor growth, clinical progression, immune escape, and drug resistance development. Evolutionary theory can be used to analyze the dynamics of tumor cell populations and to make inference about the evolutionary history of a tumor from molecular data. We review recent approaches to modeling the evolution of cancer, including population dynamics models of tumor initiation and progression, phylogenetic methods to model the evolutionary relationship between tumor subclones, and probabilistic graphical models to describe dependencies among mutations. Evolutionary modeling helps to understand how tumors arise and will also play an increasingly important prognostic role in predicting disease progression and the outcome of medical interventions, such as targeted therapy.","Evolutionary theory can be used to analyze the dynamics of tumor cell populations and to make inference about the evolutionary history of a tumor from molecular data."
"Causal knowledge plays a crucial role in human thought, but the nature of causal representation and inference remains a puzzle. Can human causal inference be captured by relations of probabilistic dependency, or does it draw on richer forms of representation? This article explores this question by reviewing research in reasoning, decision making, various forms of judgment, and attribution. We endorse causal Bayesian networks as the best normative framework and as a productive guide to theory building. However, it is incomplete as an account of causal thinking. On the basis of a range of experimental work, we identify three hallmarks of causal reasoning-the role of mechanism, narrative, and mental simulation-all of which go beyond mere probabilistic knowledge. We propose that the hallmarks are closely related. Mental simulations are representations over time of mechanisms. When multiple actors are involved, these simulations are aggregated into narratives.","Causal knowledge plays a crucial role in human thought, but the nature of causal representation and inference remains a puzzle."
"We recently proposed two methods for estimating Bayesian networks from high-dimensional non-independent and identically distributed data containing exogenous variables and random effects (Kasza et al., 2012). The first method is fully Bayesian, and the second is \"residual\"-based, accounting for the effects of the exogenous variables by utilizing the notion of restricted maximum likelihood. We describe the methods and compare their performance using the Kullback-Leibler divergence, which provides a natural framework for comparing posterior distributions. In applications where the exogenous variables are not of primary interest, we show that the potential loss of information about parameters of interest is typically small.",""
"The effect of different factors (spawning biomass, environmental conditions) on recruitment is a subject of great importance in the management of fisheries, recovery plans and scenario exploration. In this study, recently proposed supervised classification techniques, tested by the machine-learning community, are applied to forecast the recruitment of seven fish species of North East Atlantic (anchovy, sardine, mackerel, horse mackerel, hake, blue whiting and albacore), using spawning, environmental and climatic data. In addition, the use of the probabilistic flexible naive Bayes classifier (FNBC) is proposed as modelling approach in order to reduce uncertainty for fisheries management purposes. Those improvements aim is to improve probability estimations of each possible outcome (low, medium and high recruitment) based in kernel density estimation, which is crucial for informed management decision making with high uncertainty. Finally, a comparison between goodness-of-fit and generalization power is provided, in order to assess the reliability of the final forecasting models. It is found that in most cases the proposed methodology provides useful information for management whereas the case of horse mackerel is an example of the limitations of the approach. The proposed improvements allow for a better probabilistic estimation of the different scenarios, i.e. to reduce the uncertainty in the provided forecasts. (C) 2014 Elsevier B.V. All rights reserved.","In this study, recently proposed supervised classification techniques, tested by the machine-learning community, are applied to forecast the recruitment of seven fish species of North East Atlantic (anchovy, sardine, mackerel, horse mackerel, hake, blue whiting and albacore), using spawning, environmental and climatic data."
"Bayesian Networks (BN) are probabilistic graphical models used to encode in a compact way a joint probability distribution over a set of random variables. The NP-complete problem of finding the most probable BN structure given the observed data has been largely studied in recent years. In the literature, several complete algorithms have been proposed for the problem; in parallel, several tests for statistical independence between the random variables have been proposed, in order to reduce the size of the search space. In this work, we study how to hybridize the algorithm representing the state-of-the-art in complete search with two types of independence tests, and assess the performance of the two hybrid algorithms in terms of both solution quality and computational time. Experimental results show that hybridization with both types of independence test results in a substantial gain in computational time, against a limited loss in solution quality, and allow us to provide some guidelines on the choice of the test type, given the number of nodes in the network and the sample size.",""
"The response to drug treatment in asthma is a complex trait and is markedly variable even in patients with apparently similar clinical features. Pharmaco-genomics, which is the study of variations of human genome characteristics as related to drug response, can play a role in asthma therapy. Both a traditional candidate-gene approach to conducting genetic association studies and genome-wide association studies have provided an increasing list of genes and variants associated with the three major classes of asthma medications: beta(2)-agonists, inhaled corticosteroids, and leukotriene modifiers. Moreover, a recent integrative, systems-level approach has offered a promising opportunity to identify important pharmacogenomics loci in asthma treatment. However, we are still a long way away from making this discipline directly relevant to patients. The combination of network modeling, functional validation, and integrative omics technologies will likely be needed to move asthma pharmacogenomics closer to clinical relevance.",""
"Currently, two frameworks of causal reasoning compete: Whereas dependency theories focus on dependencies between causes and effects, dispositional theories model causation as an interaction between agents and patients endowed with intrinsic dispositions. One important finding providing a bridge between these two frameworks is that failures of causes to generate their effects tend to be differentially attributed to agents and patients regardless of their location on either the cause or the effect side. To model different types of error attribution, we augmented a causal Bayes net model with separate error sources for causes and effects. In several experiments, we tested this new model using the size of Markov violations as the empirical indicator of differential assumptions about the sources of error. As predicted by the model, the size of Markov violations was influenced by the location of the agents and was moderated by the causal structure and the type of causal variables.",""
"When we consider the fiercely competitive environment in which modern companies operate, highly creative people can be considered strategic assets in furthering companies' competitiveness. This research provided a novel approach to creativity management through scenario analyses that applied the Bayesian network. This article focused particularly on perceptions of individual creativity and asked two questions: how do the processes of creative revelation-exploitation and exploration-contribute to building individual creativity, and how do environmental factors-task complexity, and bureaucratic and supportive cultures-affect individual creativity? The Bayesian network seems appropriate from this perspective because Bayesian network structure addresses the causal relationships between all variables. For the empirical test, we collected questionnaires and applied the Bayesian network to the survey data to extract a set of reliable causal relationships. By performing scenario-based simulations both \"what-if\" and goal-seeking simulations-we found that individual creativity can be managed very effectively by adjusting the related variables in such a way as to maximize that quality. Crown Copyright (C) 2013 Published by Elsevier Ltd. All rights reserved.",""
"Transcriptional regulation plays vital roles in many fundamental biological processes. Reverse engineering of genome-wide regulatory networks from high-throughput transcriptomic data provides a promising way to characterize the global scenario of regulatory relationships between regulators and their targets. In this review, we summarize and categorize the main frameworks and methods currently available for inferring transcriptional regulatory networks from microarray gene expression profiling data. We overview each of strategies and introduce representative methods respectively. Their assumptions, advantages, shortcomings, and possible improvements and extensions are also clarified and commented.",""
"A Bayesian network model was developed to assess the combined influence of nutrient conditions and climate on the occurrence of cyanobacterial blooms within lakes of diverse hydrology and nutrient supply. Physicochemical, biological, and meteorological observations were collated from 20 lakes located at different latitudes and characterized by a range of sizes and trophic states. Using these data, we built a Bayesian network to (1) analyze the sensitivity of cyanobacterial bloom development to different environmental factors and (2) determine the probability that cyanobacterial blooms would occur. Blooms were classified in three categories of hazard (low, moderate, and high) based on cell abundances. The most important factors determining cyanobacterial bloom occurrence were water temperature, nutrient availability, and the ratio of mixing depth to euphotic depth. The probability of cyanobacterial blooms was evaluated under different combinations of total phosphorus and water temperature. The Bayesian network was then applied to quantify the probability of blooms under a future climate warming scenario. The probability of the \"high hazardous\" category of cyanobacterial blooms increased 5% in response to either an increase in water temperature of 0.8 degrees C (initial water temperature above 24 degrees C) or an increase in total phosphorus from 0.01 mg/L to 0.02 mg/L. Mesotrophic lakes were particularly vulnerable to warming. Reducing nutrient concentrations counteracts the increased cyanobacterial risk associated with higher temperatures.","Blooms were classified in three categories of hazard (low, moderate, and high) based on cell abundances."
"Cancer drivers are genomic alterations that provide cells containing them with a selective advantage over their local competitors, whereas neutral passengers do not change the somatic fitness of cells. Cancer-driving mutations are usually discriminated from passenger mutations by their higher degree of recurrence in tumor samples. However, there is increasing evidence that many additional driver mutations may exist that occur at very low frequencies among tumors. This observation has prompted alternative methods for driver detection, including finding groups of mutually exclusive mutations and incorporating prior biological knowledge about gene function or network structure. Dependencies among drivers due to epistatic interactions can also result in low mutation frequencies, but this effect has been ignored in driver detection so far. Here, we present a new computational approach for identifying genomic alterations that occur at low frequencies because they depend on other events. Unlike passengers, these constrained mutations display punctuated patterns of occurrence in time. We test this driver-passenger discrimination approach based on mutation timing in extensive simulation studies, and we apply it to cross-sectional copy number alteration (CNA) data from ovarian cancer, CNA and single-nucleotide variant (SNV) data from breast tumors and SNV data from colorectal cancer. Among the top ranked predicted drivers, we find low-frequency genes that have already been shown to be involved in carcinogenesis, as well as many new candidate drivers. The mutation timing approach is orthogonal and complementary to existing driver prediction methods. It will help identifying from cancer genome data the alterations that drive tumor progression.",""
"Observations from philosophy and psychology heavily favor the Empiricist tenet that many lexical concepts are learned. However, many observations also heavily favor the Nativist tenet that such concepts are representationally atomic. Fodor (The language of thought, 1975, In J. Fodor (Ed.) Representations: Philosophical essays on the foundations of cognitive science, 1981, LOT2: The language of thought revisited, 2008) has famously argued that representationally atomic concepts cannot be learned, at least not learned by hypothesis formation and testing. Concept theorists who want to preserve observations about concept learning have developed acquisition models on which the acquired concepts are either non-atomic or are acquired by a process that doesn't involve hypothesis formation and testing. I offer a model, Baptizing Meanings for Concepts (BMC), in which representationally atomic concepts are learned by hypothesis formation and testing. The concepts are learned by the agent's hypothesizing the existence of a latent/hidden/imperceptible property in objects to explain the objects' perceptible similarities. Once a hidden property is hypothesized, a new atomic mental name is assigned to it, and this atomic name becomes the concept. Any connections between the name and the representations involved in linking the name to its referent are stored as contingent. Further experience may give the agent reason to revise its hypotheses about latent properties as explanations for its observations. I discuss a software robot implementation of the BMC process that uses a Bayesian learning network. The implementation provides an existence proof of the possibility of learning representationally atomic concepts by hypothesis formation and testing.",""
"BACKGROUND AND PURPOSE: Head and neck cancer is common, and understanding the prognosis is an important part of patient management. In addition to the Tumor, Node, Metastasis staging system, tumor biomarkers are becoming more useful in understanding prognosis and directing treatment. We assessed whether MR imaging texture analysis would correctly classify oropharyngeal squamous cell carcinoma according to p53 status. MATERIALS AND METHODS: A cohort of 16 patients with oropharyngeal squamous cell carcinoma was prospectively evaluated by using standard clinical, histopathologic, and imaging techniques. Tumors were stained for p53 and scored by an anatomic pathologist. Regions of interest on MR imaging were selected by a neuroradiologist and then analyzed by using our 2D fast time-frequency transform tool. The quantified textures were assessed by using the subset-size forward-selection algorithm in the Waikato Environment for Knowledge Analysis. Features found to be significant were used to create a statistical model to predict p53 status. The model was tested by using a Bayesian network classifier with 10-fold stratified cross-validation. RESULTS: Feature selection identified 7 significant texture variables that were used in a predictive model. The resulting model predicted p53 status with 81.3% accuracy (P < .05). Cross-validation showed a moderate level of agreement (kappa = 0.625). CONCLUSIONS: This study shows that MR imaging texture analysis correctly predicts p53 status in oropharyngeal squamous cell carcinoma with similar to 80% accuracy. As our knowledge of and dependence on tumor biomarkers expand, MR imaging texture analysis warrants further study in oropharyngeal squamous cell carcinoma and other head and neck tumors.","The model was tested by using a Bayesian network classifier with 10-fold stratified cross-validation."
"We present a new approach to credal networks, which are graphical models that generalise Bayesian networks to deal with imprecise probabilities. Instead of applying the commonly used notion of strong independence, we replace it by the weaker, asymmetrical notion of epistemic irrelevance. We show how assessments of epistemic irrelevance allow us to construct a global model out of given local uncertainty models, leading to an intuitive expression for the so-called irrelevant natural extension of a credal network. In contrast with Cozman [4], who introduced this notion in terms of credal sets, our main results are presented using the language of sets of desirable gambles. This has allowed us to derive some remarkable properties of the irrelevant natural extension, including marginalisation properties and a tight connection with the notion of independent natural extension. Our perhaps most important result is that the irrelevant natural extension satisfies a collection of epistemic irrelevancies that is induced by AD-separation, an asymmetrical adaptation of d-separation. Both AD-separation and the induced collection of irrelevancies are shown to satisfy all graphoid properties except symmetry. (C) 2014 Elsevier Inc. All rights reserved.",""
"Fault diagnosis is a critical task for operators in the context of e-TOM (enhanced Telecom Operations Map) assurance process. Its purpose is to reduce network maintenance costs and to improve availability, reliability and performance of network services. Although necessary, this operation is complex and requires significant involvement of human expertise. The study of the fundamental properties of fault diagnosis shows that the diagnosis process complexity needs to be addressed using more intelligent and efficient approaches. In this paper, we present a hybrid approach that combines Bayesian networks and case-based reasoning in order to overcome the usual limits of fault diagnosis techniques and to reduce human intervention in this process. The proposed mechanism allows the identification of the root cause with a finer precision and a higher reliability. At the same time, it helps to reduce computation time while taking into account the network dynamicity. Furthermore, a study case is presented to show the feasibility and performance of the proposed approach based on a real-world use case: a virtual private network topology. Note to Practitioners-This paper was motivated by the problem of self-diagnosis in communication networks. The root cause identification process used currently consists of testing all of the network metrics without any prior knowledge of the dependency model. This process ignores the causal relationships which may exist between the network components. This paper suggests an approach based on prior modeling dependencies between metrics that compose a network (i.e., packet loss or jitter). The dependency model is achieved through a graph theory technique known as \"Bayesian Network.\" Upon the detection of a failure, the approach considers only the most relevant metrics for the detected fault. The diagnosis consists of inferring the root cause using an algorithm based on the combination of Bayesian networks and case-based reasoning techniques. For the evaluation, observations collected from concrete network monitoring were used. This evaluation shows the efficiency of the proposed approach in terms of automation, speed, accuracy and reliability.",""
"Real-world data are often multifaceted and can be meaningfully clustered in more than one way. There is a growing interest in obtaining multiple partitions of data. In previous work we learnt from data a latent tree model (LTM) that contains multiple latent variables (Chen et al. 2012). Each latent variable represents a soft partition of data and hence multiple partitions result in. The LTM approach can, through model selection, automatically determine how many partitions there should be, what attributes define each partition, and how many clusters there should be for each partition. It has been shown to yield rich and meaningful clustering results. Our previous algorithm EAST for learning LTMs is only efficient enough to handle data sets with dozens of attributes. This paper proposes an algorithm called BI that can deal with data sets with hundreds of attributes. We empirically compare BI with EAST and other more efficient LTM learning algorithms, and show that BI outperforms its competitors on data sets with hundreds of attributes. In terms of clustering results, BI compares favorably with alternative methods that are not based on LTMs.",""
"Managing the uncertainties that arise in disasters - such as a ship or building fire - can be extremely challenging. Previous work has typically focused either on modeling crowd behavior, hazard dynamics, or targeting fully known environments. However, when a disaster strikes, uncertainties about the nature, extent and further development of the hazard is the rule rather than the exception. Additionally, crowds and hazard dynamics are both intertwined and uncertain, making evacuation planning extremely difficult. To address this challenge, we propose a novel spatio-temporal probabilistic model that integrates crowd and hazard dynamics, using ship- and building fire as proof-of-concept scenarios. The model is realized as a dynamic Bayesian network (DBN), supporting distinct kinds of crowd evacuation behavior, being based on studies of physical fire models, crowd psychology models, and corresponding flow models. Simulation results demonstrate that the DBN model allows us to track and forecast the movement of people until they escape, as the hazard develops from time step to time step. Our scheme thus opens up for novel in situ threat mapping and evacuation planning under uncertainty, with applications to emergency response.",""
"Bayesian networks (BNs) are powerful tools for probabilistically simulating natural systems and emulating process models. Cross validation is a technique to avoid overfitting resulting from overly complex BNs. Overfitting reduces predictive skill. Cross-validation for BNs is known but rarely implemented due partly to a lack of software tools designed to work with available BN packages. CVNetica is open-source, written in Python, and extends the Netica software package to perform cross-validation and read, rebuild, and learn BNs from data. Insights gained from cross-validation and implications on prediction versus description are illustrated with: a data-driven oceanographic application; and a model-emulation application. These examples show that overfitting occurs when BNs become more complex than allowed by supporting data and overfitting incurs computational costs as well as causing a reduction in prediction skill. CVNetica evaluates overfitting using several complexity metrics (we used level of discretization) and its impact on performance metrics (we used skill). Published by Elsevier Ltd.",""
"Recent works have highlighted the interest in coastal geographical databases - collected for coastal management purposes - for obtaining insight into current shoreline changes. On La Reunion, a tropical volcanic high island located in the Southern Indian Ocean, a dataset is available which describes shoreline changes, the coastal geomorphology and the presence of anthropic structures. This database is first supplemented with information on the exposure of each coastal segment to energetic waves and to estuarine sediment inputs. To incorporate relative sea-level changes along the coast in the database, levelling data are analysed in combination with GPS, satellite altimetry and sea-level reconstructions. Finally, a method based on Bayesian networks is used to assess the probabilistic relationships between the variables in the database. The results highlight the high degree of dependency between variables: a retrospective model is able to reproduce 81% of the observations of shoreline mobility. Importantly, we report coastal ground motions for La Reunion island of the order of 1 to 2 mm/year along the coast. However, the resulting differing rates of relative sea-level rise do not significantly impact on shoreline changes. Instead, the results suggest a major control of geological processes and local coastal geomorphic settings on shoreline evolution. While any exploration of a coastal database needs to be complemented with human reasoning to interpret the results in terms of physical processes, this study highlights the significance of revisiting other datasets to gain insight into coastal processes and factors causing shoreline changes, including sea-level changes. (C) 2014 The Authors. Published by Elsevier B.V.",""
"Quality management and customer satisfaction evaluation can be difficult tasks to perform when processes involve multiple production lines or provide multichannel services. As a consequence, the top management needs to analyse the problem from different perspectives, to evaluate possible improvement strategies at several levels and to take appropriate decisions. To this aim, we propose to use object-oriented Bayesian networks by which different quality aspects and evaluations can be integrated in a unique framework allowing to analyse improvement strategies in real time. We show, by an application to an internal-customer satisfaction survey, how to combine the perceived quality of different production areas and how to evaluate the impact on the global quality of improvement actions developed in one or more areas.",""
"For most problems in science and engineering we can obtain data sets that describe the observed system from various perspectives and record the behavior of its individual components. Heterogeneous data sets can be collectively mined by data fusion. Fusion can focus on a specific target relation and exploit directly associated data together with contextual data and data about system's constraints. In the paper we describe a data fusion approach with penalized matrix tri-factorization (DFMF) that simultaneously factorizes data matrices to reveal hidden associations. The approach can directly consider any data that can be expressed in a matrix, including those from feature-based representations, ontologies, associations and networks. We demonstrate the utility of DFMF for gene function prediction task with eleven different data sources and for prediction of pharmacologic actions by fusing six data sources. Our data fusion algorithm compares favorably to alternative data integration approaches and achieves higher accuracy than can be obtained from any single data source alone.",""
"Traffic load plays an important role not only in the design of new bridges but also in the reliability assessment of existing structures. Weigh-in-motion systems are used to collect data to determine traffic loads. In this paper, the potential of hybrid nonparametric Bayesian networks (BNs) is demonstrated for modeling the complex data measured by the weigh-in-motion systems. The quantification process provides insight into the statistical buildup of the traffic load. The BN is shown to be a reliable traffic load model for use in bridge design. The model's value is shown with applications for prediction of missing data and calculation of extreme loads. A simulation that includes both a dynamic BN and a static component is performed. The model is able to generate the distribution function of section forces, such as bending moments, generated by multiple vehicles in several lanes. The model presented in this paper should serve as a benchmark for further applications. DOI: 10.1061/(ASCE)BE.1943-5592.0000636. (C) 2014 American Society of Civil Engineers.",""
"Tunnel construction is increasing worldwide in mining and civil engineering. There have been several accidents that resulted in delays, cost overruns, some with more severe consequences. To help minimize these accidents, it is necessary to assess and manage the risks associated with tunnel construction and exploration. A particular type of accident, or undesirable event, which can occur during tunnel construction and operation, is associated with the occurrence of excessive deformations occurring inside the tunnel. This can happen due to deficient design, construction defects, and high in situ stresses or due to specific swelling and squeezing grounds. Deep coal mines where large deformations can occur during and after excavations due to the soft properties of the rock and the high in situ stresses, are particularly vulnerable to this type of event. The associated non-linear problems are related with geomechanical behavior of the rock mass, changes in the geometry of cavities and in some cases with developing surface contacts due to large strains. In this paper, the phenomena involved in large material deformations are analyzed in detail and the basic equations for the Chen's large deformation theory are presented. The application of an FEM based method to simulate large material deformations, the Material Point Method (MPM) to the simulation of large deformation that occurs in tunnels when failure occurs, is also described. An application of MPM to the Jiahe Coal Mine, in China is presented, and the numerical results obtained with MPM compared with solutions using Chen's large deformation theory. Safety considerations about the excessive deformation scenario in tunnels are drawn and a risk assessment methodology with special use of Bayesian networks is proposed. A simplified schematic example was presented,for two case scenarios. (C) 2014 Elsevier Ltd. All rights reserved.",""
"This paper describes how to exploit the modeling features and inference capabilities of dynamic Bayesian networks (DBN), in designing and implementing an innovative approach to fault detection, identification, and recovery (FDIR) for autonomous spacecrafts (e.g., a Mars rover). In particular, issues like partial observability, uncertain system evolution and system-environment interaction, as well as the prediction and mitigation of imminent failures can be naturally addressed by the proposed approach. The DBN framework can augment the modeling and analytical power of standard FDIR methodologies, while still being able to be integrated into the usual system modeling procedures (like, for instance, fault tree analysis). An FDIR cycle composed of the tasks of diagnosis (identification of the current state of the system), prognosis (identification of the future state under the current conditions), and recovery (selection of the best set of actions the autonomous system can perform, in order to avoid critical situations) is introduced and characterized through a DBN model. In particular, by considering the execution of recovery actions in response to either a current or a future abnormal situation, both reactive as well as preventive recovery can be addressed respectively. The proposed approach has been implemented in an on-board software architecture called Anomaly resolution and prognostic health management for autonomy (ARPHA), realized during the VERIFIM study funded by the European Space Agency and jointly performed with Thales/Alenia Italy. We report on some of the results obtained by performing a case study concerning the FDIR analysis of the power supply system of the ExoMars rover, by considering different anomalous and failure simulated scenarios; we conclude that ARPHA is able to properly detect and deal with the simulated problems.","This paper describes how to exploit the modeling features and inference capabilities of dynamic Bayesian networks (DBN), in designing and implementing an innovative approach to fault detection, identification, and recovery (FDIR) for autonomous spacecrafts (e."
"High competitiveness and the emergence of new Information and Communication Technologies in industrial enterprises require a higher understanding and mastering of their operation systems to improve expected performances. In that sense, managers should take decisions about the strategies to be implemented as well as the resources to be used to achieve the target performances. Decisions result either from subjective considerations either from models allowing performances assessment. To help managers in the decision making process, it is necessary to represent industrial systems by means of models to better control them. However, this task underlines two major issues. The first one deals with the development of these models which is time and money consuming for the enterprises. This issue leads the consideration of formalizing generic knowledge by means, for example, of generic patterns, as a relevant solution to support models capitalization. The second issue deals with the degree of confidence of the models regarding to the reality of the industrial systems in order to avoid unrealistic assumptions, decreasing complexity etc. To face these challenges, this paper presents a methodology to represent, in a generic way, the key concepts of an industrial system and the relationships between the concepts materialized by semantic rules. More precisely, this methodology is investigated in the domain of dependability in order to assess performances, from the concepts formalization of both the production system and the maintenance one, based on the maintenance strategies applied. Thus generic patterns are cogent to support knowledge capitalization and reuse for leading to Components Off The Shelf (COTS). Patterns are built on a Probabilistic Relational Model (PRM) and can be instantiated then assembled to form a global model of a specific industrial system. The global model allows simulation step for maintenance strategies assessment helping the decision making process. The feasibility and added-value of this methodology, mainly in terms of patterns capitalization and reuse, are shown on two case studies: a pumping system and a real harvest production system. Moreover, lessons-learned issued from these applications are discussed. (C) 2014 Elsevier Ltd. All rights reserve. .",""
"This paper describes the development of a tool, based on a Bayesian network model, that provides posteriori predictions of operational risk events, aggregate operational loss distributions, and Operational Value-at-Risk, for a structured finance operations unit located within one of Australia's major banks. The Bayesian network, based on a previously developed causal framework, has been designed to model the smaller and more frequent, attritional operational loss events. Given the limited availability of risk factor event information and operational loss data, we rely on the elicitation of subjective probabilities, sourced from domain experts. Parameter sensitivity analysis is performed to validate and check the model's robustness against the beliefs of risk management and operational staff. To ensure that the domain's evolving risk profile is captured through time, a formal approach to organizational learning is investigated that employs the automatic parameter adaption features of the Bayesian network model. A hypothetical case study is then described to demonstrate model adaption and the application of the tool to operational loss forecasting by a business unit risk manager.",""
"DNA is now routinely used in criminal investigations and court cases, although DNA samples taken at crime scenes are of varying quality and therefore present challenging problems for their interpretation. We present a statistical model for the quantitative peak information obtained from an electropherogram of a forensic DNA sample and illustrate its potential use for the analysis of criminal cases. In contrast with most previously used methods, we directly model the peak height information and incorporate important artefacts that are associated with the production of the electropherogram. Our model has a number of unknown parameters, and we show that these can be estimated by the method of maximum likelihood in the presence of multiple unknown individuals contributing to the sample, and their approximate standard errors calculated; the computations exploit a Bayesian network representation of the model. A case example from a UK trial, as reported in the literature, is used to illustrate the efficacy and use of the model, both in finding likelihood ratios to quantify the strength of evidence, and in the deconvolution of mixtures for finding likely profiles of the individuals contributing to the sample. Our model is readily extended to simultaneous analysis of more than one mixture as illustrated in a case example. We show that the combination of evidence from several samples may give an evidential strength which is close to that of a single-source trace and thus modelling of peak height information provides a potentially very efficient mixture analysis.",""
"Inferring the most probable explanation to a set of variables, given a partial observation of the remaining variables, is one of the canonical computational problems in Bayesian networks, with widespread applications in AI and beyond. This problem, known as MAP, is computationally intractable (NP-hard) and remains so even when only an approximate solution is sought. We propose a heuristic formulation of the MAP problem, denoted as Inference to the Most Frugal Explanation (MFE), based on the observation that many intermediate variables (that are neither observed nor to be explained) are irrelevant with respect to the outcome of the explanatory process. An explanation based on few samples (often even a singleton sample) from these irrelevant variables is typically almost as good as an explanation based on (the computationally costly) marginalization over these variables. We show that while MFE is computationally intractable in general (as is MAP), it can be tractably approximated under plausible situational constraints, and its inferences are fairly robust with respect to which intermediate variables are considered to be relevant. (C) 2014 Elsevier B.V. All rights reserved.","We propose a heuristic formulation of the MAP problem, denoted as Inference to the Most Frugal Explanation (MFE), based on the observation that many intermediate variables (that are neither observed nor to be explained) are irrelevant with respect to the outcome of the explanatory process."
"One of the main causes of accidents in safety-critical systems is human error. In order to reduce human errors in the process of handling abnormal situations that are highly complex and mentally taxing activities, operators need to be supported, from a cognitive perspective, in order to reduce their workload, stress, and the consequent error rate. Of the various cognitive activities, a correct understanding of the situation, i.e. situation awareness (SA), is a crucial factor in improving performance and reducing errors. Despite the importance of SA in decision-making in time- and safety-critical situations, the difficulty of SA modeling and assessment means that very few methods have as yet been developed. This study confronts this challenge, and develops an innovative abnormal situation modeling (ASM) method that exploits the capabilities of risk indicators, Bayesian networks and fuzzy logic systems. The risk indicators are used to identify abnormal situations, Bayesian networks are utilized to model them and a fuzzy logic system is developed to assess them. The ASM method can be used in the development of situation assessment decision support systems that underlie the achievement of SA. The performance of the ASM method is tested through a real case study at a chemical plant. (C) 2014 Elsevier Ltd. All rights reserved.",""
"Extending the operating lifetime of ageing technical systems is of great interest for industrial applications. Life extension requires identifying and selecting decision alternatives which allow for a safe and economic operation of the system beyond its design lifetime. This article proposes a dynamic Bayesian network for assessing the life extension of ageing repairable systems. The main objective of the model is to provide decision support based on the system performance during a finite time horizon, which is defined by the life extension period. The model has three main applications: (i) assessing and selecting optimal decision alternatives for the life extension at present time, based on historical data; (ii) identifying and minimizing the factors that have a negative impact on the system performance; and (iii) reassessing and optimizing the decision alternatives during operation throughout the life extension period, based on updating the model with new operational data gathered. A case study illustrates the application of the model for life extension of a real firewater pump system in an oil and gas facility. The case study analyzes three decision alternatives, where preventive maintenance and functional test policies are optimized, and the uncertainty involved in each alternative is computed. (C) 2014 Elsevier Ltd. All rights reserved.",""
"This article presents a concise reliability analysis of network security abstracted from stochastic modeling, reliability, and queuing theories. Network security analysis is composed of threats, their impacts, and recovery of the failed systems. A unique framework with a collection of the key reliability models is presented here to guide the determination of the system reliability based on the strength of malicious acts and performance of the recovery processes. A unique model, called Attack-obstacle model, is also proposed here for analyzing systems with immunity growth features. Most computer science curricula do not contain courses in reliability modeling applicable to different areas of computer engineering. Hence, the topic of reliability analysis is often too diffuse to most computer engineers and researchers dealing with network security. This work is thus aimed at shedding some light on this issue, which can be useful in identifying models, their assumptions and practical parameters for estimating the reliability of threatened systems and for assessing the performance of recovery facilities. It can also be useful for the classification of processes and states regarding the reliability of information systems. Systems with stochastic behaviors undergoing queue operations and random state transitions can also benefit from the approaches presented here. (C) 2014 Elsevier Ltd. All rights reserved.","It can also be useful for the classification of processes and states regarding the reliability of information systems."
"Water resources management is often characterized by conflicts in many arid and semi-arid regions, where agriculture is the main user of groundwater (GW). Conflicts could arise among different decision-makers and stakeholders. Moreover, different policies can interact each other hampering or facilitating their implementation and effectiveness. This contribution describes a new implementation of GeSAP, an integrated modelling tool for enabling local GW management by combining the need for GW protection with socio-economic and behavioural determinants of GW use. GeSAP is based on the involvement of multiple stakeholders and the use of Bayesian Belief Networks (BBN) to simulate and explore their attitude relative to GW exploitation and their responses to the introduction of new protection and agricultural policies. In this work, GeSAP was implemented in the area of the Capitanata Irrigation Users Organization, located in the Apulia region (southern Italy). It was used to simulate the reactions of the main stakeholders involved in GW protection policy implementation and to assess the policy's effectiveness in terms of actual reduction of GW exploitation. Furthermore, the interactions between the GW protection policy and the coming reform of the Common Agricultural Policy (CAP) was investigated. The results of the application proved the capability of the GeSAP tool to assess the actual effectiveness of GW protection policy by investigating how far this policy could be considered acceptable by farmers. In addition, this study demonstrates how the effectiveness of the GW protection policy could be affected by the interaction with the CAP reform. The latter could strongly impact the balance between water demand and availability with the effect of nullifying the positive synergy between CAP and GW protection policy. Although water management issues are not explicitly mentioned among the main scopes of the CAP, this work clearly demonstrates the impact that such policy could have on farmers' decisions on water use. (C) 2014 Elsevier B.V. All rights reserved.",""
"There are several software process models and methodologies such as waterfall, spiral and agile. Even so, the rate of successful software development projects is low. Since software is the major output of software processes, increasing software process management quality should increase the project's chances of success. Organizations have invested to adapt software processes to their environments and the characteristics of projects to improve the productivity and quality of the products. In this paper, we present a procedure to detect problems of processes in software development projects using Bayesian networks. The procedure was successfully applied to Scrum-based software development projects. The research results should encourage the usage of Bayesian networks to manage software processes and increase the rate of successful software development projects. (C) 2014 Elsevier Ltd. All rights reserved.",""
"Mathematical models of natural systems are abstractions of much more complicated processes. Developing informative and realistic models of such systems typically involves suitable statistical inference methods, domain expertise, and a modicum of luck. Except for cases where physical principles provide sufficient guidance, it will also be generally possible to come up with a large number of potential models that are compatible with a given natural system and any finite amount of data generated from experiments on that system. Here we develop a computational framework to systematically evaluate potentially vast sets of candidate differential equation models in light of experimental and prior knowledge about biological systems. This topological sensitivity analysis enables us to evaluate quantitatively the dependence of model inferences and predictions on the assumed model structures. Failure to consider the impact of structural uncertainty introduces biases into the analysis and potentially gives rise to misleading conclusions.","Developing informative and realistic models of such systems typically involves suitable statistical inference methods, domain expertise, and a modicum of luck."
"A commonly used idea in forensic fields is known as the 'hierarchy of propositions'. DNA analysts commonly report at the sub-source level in the hierarchy. This means that they simply comment on the probability of the evidence for the given propositions that consider contributors that lead to a DNA profile and not on the source of specific biological components, not the activity that led to the transfer or the offence that is reported to have occurred. However DNA analysts also commonly report at a level even lower than the sub-source level. In this 'sub-sub-source' level only reference comparisons to components of a mixture are reported. The difference between the sub-source level and sub-sub-source level is the difference between comparing an individual to a mixture as a whole, or comparing them to only one component of a mixture. This idea has been expressed in the past as the 'two trace' problem or the 'factor of two' problem. With the advent of expert systems that can provide a measure of weight of evidence in the form of a likelihood ratio (LR) for any mixture, resolvable or not, the distinction between these two levels becomes more important. In this paper we explore how the LR can be constructed to report correctly at the sub-source level, by taking contributor orders and genotype set orders into account. We include worked examples of the LR calculation to help explain this confusing issue. Crown Copyright (C) 2014 Published by Elsevier Ltd. All rights reserved.",""
"Background: Identifying genetic interactions in data obtained from genome-wide association studies (GWASs) can help in understanding the genetic basis of complex diseases. The large number of single nucleotide polymorphisms (SNPs) in GWASs however makes the identification of genetic interactions computationally challenging. We developed the Bayesian Combinatorial Method (BCM) that can identify pairs of SNPs that in combination have high statistical association with disease. Results: We applied BCM to two late-onset Alzheimer's disease (LOAD) GWAS datasets to identify SNPs that interact with known Alzheimer associated SNPs. We also compared BCM with logistic regression that is implemented in PLINK. Gene Ontology analysis of genes from the top 200 dataset SNPs for both GWAS datasets showed overrepresentation of LOAD-related terms. Four genes were common to both datasets: APOE and APOC1, which have well established associations with LOAD, and CAMK1D and FBXL13, not previously linked to LOAD but having evidence of involvement in LOAD. Supporting evidence was also found for additional genes from the top 30 dataset SNPs. Conclusion: BCM performed well in identifying several SNPs having evidence of involvement in the pathogenesis of LOAD that would not have been identified by univariate analysis due to small main effect. These results provide support for applying BCM to identify potential genetic variants such as SNPs from high dimensional GWAS datasets.","We also compared BCM with logistic regression that is implemented in PLINK."
"Background: Dynamic aspects of gene regulatory networks are typically investigated by measuring system variables at multiple time points. Current state-of-the-art computational approaches for reconstructing gene networks directly build on such data, making a strong assumption that the system evolves in a synchronous fashion at fixed points in time. However, nowadays omics data are being generated with increasing time course granularity. Thus, modellers now have the possibility to represent the system as evolving in continuous time and to improve the models' expressiveness. Results: Continuous time Bayesian networks are proposed as a new approach for gene network reconstruction from time course expression data. Their performance was compared to two state-of-the-art methods: dynamic Bayesian networks and Granger causality analysis. On simulated data, the methods comparison was carried out for networks of increasing size, for measurements taken at different time granularity densities and for measurements unevenly spaced over time. Continuous time Bayesian networks outperformed the other methods in terms of the accuracy of regulatory interactions learnt from data for all network sizes. Furthermore, their performance degraded smoothly as the size of the network increased. Continuous time Bayesian networks were significantly better than dynamic Bayesian networks for all time granularities tested and better than Granger causality for dense time series. Both continuous time Bayesian networks and Granger causality performed robustly for unevenly spaced time series, with no significant loss of performance compared to the evenly spaced case, while the same did not hold true for dynamic Bayesian networks. The comparison included the IRMA experimental datasets which confirmed the effectiveness of the proposed method. Continuous time Bayesian networks were then applied to elucidate the regulatory mechanisms controlling murine T helper 17 (Th17) cell differentiation and were found to be effective in discovering well-known regulatory mechanisms, as well as new plausible biological insights. Conclusions: Continuous time Bayesian networks were effective on networks of both small and large size and were particularly feasible when the measurements were not evenly distributed over time. Reconstruction of the murine Th17 cell differentiation network using continuous time Bayesian networks revealed several autocrine loops, suggesting that Th17 cells may be auto regulating their own differentiation process.",""
"Background: Gene regulatory network (GRN) is a fundamental topic in systems biology. The dynamics of GRN can shed light on the cellular processes, which facilitates the understanding of the mechanisms of diseases when the processes are dysregulated. Accurate reconstruction of GRN could also provide guidelines for experimental biologists. Therefore, inferring gene regulatory networks from high-throughput gene expression data is a central problem in systems biology. However, due to the inherent complexity of gene regulation, noise in measuring the data and the short length of time-series data, it is very challenging to reconstruct accurate GRNs. On the other hand, a better understanding into gene regulation could help to improve the performance of GRN inference. Time delay is one of the most important characteristics of gene regulation. By incorporating the information of time delays, we can achieve more accurate inference of GRN. Results: In this paper, we propose a method to infer time-delayed gene regulation based on cross-correlation and network deconvolution (ND). First, we employ cross-correlation to obtain the probable time delays for the interactions between each target gene and its potential regulators. Then based on the inferred delays, the technique of ND is applied to identify direct interactions between the target gene and its regulators. Experiments on real-life gene expression datasets show that our method achieves overall better performance than existing methods for inferring time-delayed GRNs. Conclusion: By taking into account the time delays among gene interactions, our method is able to infer GRN more accurately. The effectiveness of our method has been shown by the experiments on three real-life gene expression datasets of yeast. Compared with other existing methods which were designed for learning time-delayed GRN, our method has significantly higher sensitivity without much reduction of specificity.","On the other hand, a better understanding into gene regulation could help to improve the performance of GRN inference."
"Network modeling has proven to be a fundamental tool in analyzing the inner workings of a cell. It has revolutionized our understanding of biological processes and made significant contributions to the discovery of disease biomarkers. Much effort has been devoted to reconstruct various types of biochemical networks using functional genomic datasets generated by high-throughput technologies. This paper discusses statistical methods used to reconstruct gene regulatory networks using gene expression data. In particular, we highlight progress made and challenges yet to be met in the problems involved in estimating gene interactions, inferring causality and modeling temporal changes of regulation behaviors. As rapid advances in technologies have made available diverse, large-scale genomic data, we also survey methods of incorporating all these additional data to achieve better, more accurate inference of gene networks. (C) 2014 Elsevier Ltd. All rights reserved.","As rapid advances in technologies have made available diverse, large-scale genomic data, we also survey methods of incorporating all these additional data to achieve better, more accurate inference of gene networks."
"Identification of faults in process systems can be based purely on measurement (e.g. PCA), or can exploit knowledge of process model structure to construct a causal network. This work introduces a method to identify most likely causal network in cases when process model is not known. An incidence matrix, showing location of measurements in the plant network structure, and historical process data are used to identify the optimal causal network structure by means of maximizing Bayesian scores for alternative causal networks. Causal subnetworks, corresponding to subgraphs of the process network, are identified by finding the most probable graph based on highest posterior probability of graph features computed via Markov Chain Monte Carlo simulation. Novel Bayesian contribution indices within the probabilistic graphical network are proposed to identify the potential root-cause variables. Application to Tennessee Eastman Chemical plant demonstrates that the presented method is significantly more accurate than the current methods. (C) 2014 Elsevier Ltd. All rights reserved.",""
"Prediction of foreign exchange (FX) rates is addressed as a binary classification problem in which a continuous time Bayesian network classifier (CTBNC) is developed and used to solve it. An exact algorithm for inference on CTBNC is introduced. The performance of an instance of these classifiers is analysed and compared to that of dynamic Bayesian network by using real tick by tick FX rates. Performance analysis and comparison, based on different metrics such as accuracy, precision, recall and Brier score, evince a predictive power of these models for FX rates at high frequencies. The achieved results also show that the proposed CTBNC is more effective and more efficient than dynamic Bayesian network classifier. In particular, it allows to perform high frequency prediction of FX rates in cases where dynamic Bayesian networks-based models are computationally intractable.","Prediction of foreign exchange (FX) rates is addressed as a binary classification problem in which a continuous time Bayesian network classifier (CTBNC) is developed and used to solve it."
"This paper presents an approach to capacity planning and coordination for the regeneration of complex investment goods. In order to capture the specifics of the regeneration environment, Bayesian networks are utilised to improve the accuracy of the workload forecast and mathematical models are proposed for the long-, medium- and short-term capacity planning and coordination. The long- and medium-term models maximise the total profit through the optimum determination of the quantity of goods to be regenerated in-house or at the sites of external vendors, the number of regenerated goods to be stored and the extent to which penalties should be tolerated for delayed regeneration orders. The short-term model determines the optimal allocation of human and machine resources to different orders. The models are validated through the use of real-world data supplied by an industrial partner. Sensitivity analyses are conducted in order to gain insights into the model. The application of the models leads to significant improvements for the industry partner due to the reduction in regeneration costs as a result of implementing varying plant capacities over the course of a year.",""
"This article presents an iterative six-step risk analysis methodology based on hybrid Bayesian networks (BNs). In typical risk analysis, systems are usually modeled as discrete and Boolean variables with constant failure rates via fault trees. Nevertheless, in many cases, it is not possible to perform an efficient analysis using only discrete and Boolean variables. The approach put forward by the proposed methodology makes use of BNs and incorporates recent developments that facilitate the use of continuous variables whose values may have any probability distributions. Thus, this approach makes the methodology particularly useful in cases where the available data for quantification of hazardous events probabilities are scarce or nonexistent, there is dependence among events, or when nonbinary events are involved. The methodology is applied to the risk analysis of a regasification system of liquefied natural gas (LNG) on board an FSRU (floating, storage, and regasification unit). LNG is becoming an important energy source option and the world's capacity to produce LNG is surging. Large reserves of natural gas exist worldwide, particularly in areas where the resources exceed the demand. Thus, this natural gas is liquefied for shipping and the storage and regasification process usually occurs at onshore plants. However, a new option for LNG storage and regasification has been proposed: the FSRU. As very few FSRUs have been put into operation, relevant failure data on FSRU systems are scarce. The results show the usefulness of the proposed methodology for cases where the risk analysis must be performed under considerable uncertainty.",""
"Theory of graphical models has matured over more than three decades to provide the backbone for several classes of models that are used in a myriad of applications such as genetic mapping of diseases, credit risk evaluation, reliability and computer security. Despite their generic applicability and wide adoption, the constraints imposed by undirected graphical models and Bayesian networks have also been recognized to be unnecessarily stringent under certain circumstances. This observation has led to the proposal of several generalizations that aim at more relaxed constraints by which the models can impose local or context-specific dependence structures. Here we consider an additional class of such models, termed stratified graphical models. We develop a method for Bayesian learning of these models by deriving an analytical expression for the marginal likelihood of data under a specific subclass of decomposable stratified models. A non-reversible Markov chain Monte Carlo approach is further used to identify models that are highly supported by the posterior distribution over the model space. Our method is illustrated and compared with ordinary graphical models through application to several real and synthetic datasets.",""
"Purpose: To identify the factors affecting the surgical decisions of experienced physicians when treating patients with lower urinary tract symptoms that are suggestive of benign prostatic hyperplasia (LUTS/BPH). Methods: Patients with LUTS/BPH treated by two physicians between October 2004 and August 2013 were included in this study. The causal Bayesian network (CBN) model was used to analyze factors influencing the surgical decisions of physicians and the actual performance of surgery. The accuracies of the established CBN models were verified using linear regression (LR) analysis. Results: A total of 1,108 patients with LUTS/BPH were analyzed. The mean age and total prostate volume (TPV) were 66.2 (+/- 7.3, standard deviation) years and 47.3 (+/- 25.4) mL, respectively. Of the total 1,108 patients, 603 (54.4%) were treated by physician A and 505 (45.6%) were treated by physician B. Although surgery was recommended to 699 patients (63.1%), 589 (53.2%) actually underwent surgery. Our CBN model showed that the TPV (R=0.432), treating physician (R=0.370), bladder outlet obstruction (BOO) on urodynamic study (UDS) (R=0.324), and International Prostate Symptom Score (IPSS) question 3 (intermittency; R=0.141) were the factors directly influencing the surgical decision. The transition zone volume (R=0.396), treating physician (R=0.340), and BOO (R=0.300) directly affected the performance of surgery. Compared to the LR model, the area under the receiver operating characteristic curve of the CBN surgical decision model was slightly compromised (0.803 vs. 0.847, P< 0.001), whereas that of the actual performance of surgery model was similar (0.801 vs. 0.820, P=0.063) to the LR model. Conclusions: The TPV, treating physician, BOO on UDS, and the IPSS item of intermittency were factors that directly influenced decision-making in physicians treating patients with LUTS/BPH.","The accuracies of the established CBN models were verified using linear regression (LR) analysis."
"Complex clinical decisions require the decision maker to evaluate multiple factors that may interact with each other. Many clinical studies, however, report 'univariate' relations between a single factor and outcome. Such univariate statistics are often insufficient to provide useful support for complex clinical decisions even when they are pooled using meta-analysis. More useful decision support could be provided by evidence-based models that take the interaction between factors into account. In this paper, we propose a method of integrating the univariate results of a meta-analysis with a clinical dataset and expert knowledge to construct multivariate Bayesian network (BN) models. The technique reduces the size of the dataset needed to learn the parameters of a model of a given complexity. Supplementing the data with the meta-analysis results avoids the need to either simplify the model - ignoring some complexities of the problem - or to gather more data. The method is illustrated by a clinical case study into the prediction of the viability of severely injured lower extremities. The case study illustrates the advantages of integrating combined evidence into BN development: the BN developed using our method outperformed four different data-driven structure learning methods, and a well-known scoring model (MESS) in this domain. (C) 2014 Elsevier Inc. All rights reserved.",""
"In 2008 a runaway chemical reaction caused an explosion at a methomyl unit in West Virginia, USA, killing two employees, injuring eight people, evacuating more than 40,000 residents adjacent to the facility, disrupting traffic on a nearby highway and causing significant business loss and interruption. Although the accident was formally investigated, the role of the situation awareness (SA) factor, i.e., a correct understanding of the situation, and appropriate models to maintain SA, remain unexplained. This paper extracts details of abnormal situations within the methomyl unit and models them into a situational network using dynamic Bayesian networks. A fuzzy logic system is used to resemble the operator's thinking when confronted with these abnormal situations. The combined situational network and fuzzy logic system make it possible for the operator to assess such situations dynamically to achieve accurate SA. The findings show that the proposed structure provides a useful graphical model that facilitates the inclusion of prior background knowledge and the updating of this knowledge when new information is available from monitoring systems. (C) 2014 Elsevier Ltd. All rights reserved.",""
"Bayesian networks are quantitative modeling tools whose applications to the maritime traffic safety context are becoming more popular. This paper discusses the utilization of Bayesian networks in maritime safety modeling. Based on literature and the author's own experiences, the paper studies what Bayesian networks can offer to maritime accident prevention and safety modeling and discusses a few challenges in their application to this context. It is argued that the capability of representing rather complex, not necessarily causal but uncertain relationships makes Bayesian networks an attractive modeling tool for the maritime safety and accidents. Furthermore, as the maritime accident and safety data is still rather scarce and has some quality problems, the possibility to combine data with expert knowledge and the easy way of updating the model after acquiring more evidence further enhance their feasibility. However, eliciting the probabilities from the maritime experts might be challenging and the model validation can be tricky. It is concluded that with the utilization of several data sources, Bayesian updating, dynamic modeling, and hidden nodes for latent variables, Bayesian networks are rather well-suited tools for the maritime safety management and decision-making. (C) 2014 Elsevier Ltd. All rights reserved.",""
"Two studies examined a novel prediction of the causal Bayes net approach to judgments under uncertainty, namely that causal knowledge affects the interpretation of statistical evidence obtained over multiple observations. Participants estimated the conditional probability of an uncertain event (breast cancer) given information about the base rate, hit rate (probability of a positive mammogram given cancer) and false positive rate (probability of a positive mammogram in the absence of cancer). Conditional probability estimates were made after observing one or two positive mammograms. Participants exhibited a causal stability effect: there was a smaller increase in estimates of the probability of cancer over multiple positive mammograms when a causal explanation of false positives was provided. This was the case when the judgments were made by different participants (Experiment 1) or by the same participants (Experiment 2). These results show that identical patterns of observed events can lead to different estimates of event probability depending on beliefs about the generative causes of the observations. (C) 2014 Elsevier B.V. All rights reserved.",""
"The management of complex interacting hydrosystems is challenging if in addition to the physical processes also socio-economic and environmental aspects have to be considered. This causes conflicts of interests among various water actors with mostly contradicting objectives and uncertainties about the consequences of potential management interventions. The objective of this paper is to present a methodological framework to support decision making under uncertainties for the management of complex hydrosystems. The proposed framework conceptualises hydrological and socio-economic interactions by constructing a Bayesian network (BN)-based decision support tool for a typical management problem of agricultural coastal regions. Thereby, the paper demonstrates the value of combining two different commonly used integrated modelling approaches. Coupled domain models are applied to simulate the nonlinearities and feedbacks of a strongly interacting groundwater-agriculture hydrosystem. Afterwards, a BN is used to integrate their results together with empirical knowledge and expert opinions regarding potential management interventions. A prototype application is performed for a coastal arid region, which is affected by saltwater intrusion into a coastal aquifer due to excessive groundwater extraction for irrigated agriculture. It addresses the issues of contradicting management objectives such as sustainable aquifer management vs. profitable agricultural production and the problem of finding appropriate management interventions or policies. Several policy combinations have been analysed regarding their efficiency within different management scenarios in a probabilistic way, which enables decision makers to assess the risks associated with implementing alternative management strategies. In addition, efficient metrics for evaluating performance and uncertainty of the developed task-specific BN are used which underline the reliability of the results.",""
"The noise-assisted method of ensemble empirical mode decomposition represents a significant improvement over the original empirical mode decomposition for eliminating the mode mixing problem. However, the ensemble empirical mode decomposition method will generate some additional problems, including the contamination of the residue noise in the signal reconstruction and the high computational cost. In this work, an improved ensemble empirical mode decomposition method, combining the complementary ensemble empirical mode decomposition and a time-saving ensemble empirical mode decomposition method by over-sampling the investigated signal, is proposed to solve these problems. By using the proposed method, the residue of the added white noise in the signal reconstruction can be eliminated completely by adding white noises in pairs with positive and negative signs to the targeted signal, and the computational cost can be saved drastically by processing the original signal with the cubic spline interpolation technique. Two simulation signals have been used to demonstrate the effectiveness of the proposed method in this article. The analysis results indicate that this method has good performance in eliminating the residue noise and reducing the costing time, which also provides more accurate decomposition results than the original ensemble empirical mode decomposition. Finally, the application to the feature extraction of pressure pulsation signal of hydroelectric generator unit shows that the proposed method has strong practicability.",""
"Non-intrusive appliance load monitoring is the process of disaggregating a household's total electricity consumption into its contributing appliances. In this paper we propose an unsupervised training method for non-intrusive monitoring which, unlike existing supervised approaches, does not require training data to be collected by sub-metering individual appliances, nor does it require appliances to be manually labelled for the households in which disaggregation is performed. Instead, we propose an approach which combines a one-off supervised learning process over existing labelled appliance data sets, with an unsupervised learning method over unlabelled household aggregate data. First, we propose an approach which uses the Tracebase data set to build probabilistic appliance models which generalise to previously unseen households, which we empirically evaluate through cross validation. Second, we use the Reference Energy Disaggregation Data set to evaluate the accuracy with which these general models can be tuned to the appliances within a specific household using only aggregate data. Our empirical evaluation demonstrates that general appliance models can be constructed using data from only a small number of appliances (typically 3-6 appliances), and furthermore that 28-99% of the remaining behaviour which is specific to a single household can be learned using only aggregate data from existing smart meters. (C) 2014 Elsevier B.V. All rights reserved.",""
"Clinical practice guidelines in paper format are still the preferred form of delivery of medical knowledge and recommendations to healthcare professionals. Their current support and development process have well identified limitations to which the healthcare community has been continuously searching solutions. Artificial intelligence may create the conditions and provide the tools to address many, if not all, of these limitations.. This paper presents a comprehensive and up to date review of computer-interpretable guideline approaches, namely Arden Syntax, GLIF, PROforma, Asbru, GLARE and SAGE. It also provides an assessment of how well these approaches respond to the challenges posed by paper-based guidelines and addresses topics of Artificial intelligence that could provide a solution to the shortcomings of clinical guidelines. Among the topics addressed by this paper are expert systems, case-based reasoning, medical ontologies and reasoning under uncertainty, with a special focus on methodologies for assessing quality of information when managing incomplete information. Finally, an analysis is made of the fundamental requirements of a guideline model and the importance that standard terminologies and models for clinical data have in the semantic and syntactic interoperability between a guideline execution engine and the software tools used in clinical settings. It is also proposed a line of research that includes the development of an ontology for clinical practice guidelines and a decision model for a guideline-based expert system that manages non-compliance with clinical guidelines and uncertainty.",""
"The problem of learning the Markov network structure from data has become increasingly important in machine learning, and in many other application fields. Markov networks are probabilistic graphical models, a widely used formalism for handling probability distributions in intelligent systems. This document focuses on a technology called independence-based learning, which allows for the learning of the independence structure of Markov networks from data in an efficient and sound manner, whenever the dataset is sufficiently large, and data is a representative sample of the target distribution. In the analysis of such technology, this work surveys the current state-of-the-art algorithms, discussing its limitations, and posing a series of open problems where future work may produce some advances in the area, in terms of quality and efficiency.",""
"Drinking water security is a life safety issue as an adequate supply of safe water is essential for economic, social and sanitary reasons. Damage to any element of a water system, as well as corruption of resource quality, may have significant effects on the population it serves and on all other dependent resources and activities. As well as an analysis of the reliability of water distribution systems in ordinary conditions, it is also crucial to assess system vulnerability in the event of natural disasters and of malicious or accidental anthropogenic acts. The present work summarizes the initial results of research activities that are underway with the intention of developing a vulnerability assessment methodology for drinking water infrastructures subject to hazardous events. The main aim of the work was therefore to provide decision makers with an effective operational tool which could support them mainly to increase risk awareness and preparedness and, possibly, to ease emergency management. The proposed tool is based on Bayesian Belief Networks (BBN), a probabilistic methodology which has demonstrated outstanding potential to integrate a range of sources of knowledge, a great flexibility and the ability to handle in a mathematically sound way uncertainty due to data scarcity and/or limited knowledge of the system to be managed. The tool was implemented to analyze the vulnerability of two of the most important water supply systems in the Apulia region (southern Italy) which have been damaged in the past by natural hazards. As well as being useful for testing and improving the predictive capabilities of the methodology and for possibly modifying its structure and features, the case studies have also helped to underline its strengths and weaknesses. Particularly, the experiences carried out demonstrated how the use of BBN was consistent with the lack of data reliability, quality and accessibility which are typical of complex infrastructures, such as the water distribution networks. The potential applications and future developments of the proposed tool have been also discussed accordingly.",""
"Probabilistic graphical models such as Bayesian Networks are one of the most powerful structures known by the Computer Science community for deriving probabilistic inferences. However, modern cognitive psychology has revealed that human decisions could not follow the rules of classical probability theory, because humans cannot process large amounts of data in order to make judgments. Consequently, the inferences performed are based on limited data coupled with several heuristics, leading to violations of the law of total probability. This means that probabilistic graphical models based on classical probability theory are too limited to fully simulate and explain various aspects of human decision making. Quantum probability theory was developed in order to accommodate the paradoxical findings that the classical theory could not explain. Recent findings in cognitive psychology revealed that quantum probability can fully describe human decisions in an elegant framework. Their findings suggest that, before taking a decision, human thoughts are seen as superposed waves that can interfere with each other, influencing the final decision. In this work, we propose a new Bayesian Network based on the psychological findings of cognitive scientists. In Computer Science, to the best of our knowledge, there is no quantum like probabilistic system proposed, despite their promising performances. We made experiments with two very well known Bayesian Networks from the literature. The results obtained revealed that the quantum like Bayesian Network can affect drastically the probabilistic inferences, specially when the levels of uncertainty of the network are very high (no pieces of evidence observed). When the levels of uncertainty are very low, then the proposed quantum like network collapses to its classical counterpart. (C) 2014 Elsevier B.V. All rights reserved.","Probabilistic graphical models such as Bayesian Networks are one of the most powerful structures known by the Computer Science community for deriving probabilistic inferences."
"In the reverse supply chain inventory theory, inventory models are concerned with the demand of reusable parts, stock replenishment, ordering cycle, delivery lead time, number of disassembled products, ordering costs. The particularity of these models consists in the occurrence of high uncertainties of the quantity and quality of the returned products and resulting parts. To overcome the problem, an inventory model that incorporates decision variables at proactive and reactive levels is derived and discussed in this paper.",""
"Freshwater ponds deliver a broad range of ecosystem services (ESS). Taking into account this broad range of services to attain cost-effective ESS delivery is an important challenge facing integrated pond management. To assess the strengths and weaknesses of an ESS approach to support decisions in integrated pond management, we applied it on a small case study in Flanders, Belgium. A Bayesian belief network model was developed to assess ESS delivery under three alternative pond management scenarios: intensive fish farming (IFF), extensive fish farming (EFF) and nature conservation management (NCM). A probabilistic cost-benefit analysis was performed that includes both costs associated with pond management practices and benefits associated with ESS delivery. Whether or not a particular ESS is included in the analysis affects the identification of the most preferable management scenario by the model. Assessing the delivery of a more complete set of ecosystem services tends to shift the results away from intensive management to more biodiversity-oriented management scenarios. The proposed methodology illustrates the potential of Bayesian belief networks. BBNs facilitate knowledge integration and their modular nature encourages future model expansion to more encompassing sets of services. Yet, we also illustrate the key weaknesses of such exercises, being that the choice whether or not to include a particular ecosystem service may determine the suggested optimal management practice. (C) 2014 Elsevier Ltd. All rights reserved.",""
"This paper presents a model of maritime safety management and its subareas. Furthermore, the paper links the safety management to the maritime traffic safety indicated by accident involvement, incidents reported by Vessel Traffic Service and the results from Port State Control inspections. Bayesian belief networks are applied as the modeling technique and the model parameters are based on expert elicitation and learning from historical data. The results from this new application domain of a Bayesian network based expert system suggest that, although several its subareas are functioning properly, the current status of the safety management on vessels navigating in the Finnish waters has room for improvement; the probability of zero poor safety management subareas is only 0.13. Furthermore, according to the model a good IT system for the safety management is the strongest safety-management related signal of an adequate overall safety management level. If no deficiencies have been discovered during a Port State Control inspection, the adequacy of the safety management is almost twice as probable as without knowledge on the inspection history. The resulted model could be applied to performing several safety management related queries and it thus provides support for maritime safety related decision making. (C) 2014 Elsevier Ltd. All rights reserved.",""
"This paper considers sequential detection and classification of multiple transient signals from vector observations corrupted with additive noise and multiple types of structured interference. Sparse approximations of observations are found to facilitate computation of the likelihood of each signal model without relying on restrictive assumptions concerning the distribution of observations. Robustness to interference may be incorporated by virtue of the inherent separation capabilities of sparse coding. Each signal model is characterized by a Bayesian Network, which captures the temporal dependency structure among coefficients in successive sparse approximations under the associated hypothesis. Generalized likelihood ratios tests may then be used to perform signal detection and classification during quiescent periods, and quiescent detection whenever a signal is present. The results of applying the proposed method to a national park soundscape analysis problem demonstrate its practical utility for detecting and classifying real acoustical sources present in complex sonic environments.","This paper considers sequential detection and classification of multiple transient signals from vector observations corrupted with additive noise and multiple types of structured interference."
"Decision makers benefit from the utilization of decision-support models in several applications. Obtaining managerial insights is essential to better inform the decision-process. This work offers an in-depth investigation into the structural properties of decision-support models. We show that the input output mapping in influence diagrams, decision trees and decision networks is piecewise multilinear. The conditions under which sensitivity information cannot be extracted through differentiation are examined in detail. By complementing high-order derivatives with finite change sensitivity indices, we obtain a systematic approach that allows analysts to gain a wide range of managerial insights. A well-known case study in the medical sector illustrates the findings. (C) 2014 Elsevier B.V. All rights reserved.",""
"Li-ion batteries are widely used in energy storage systems, electric vehicles, communication systems, etc. The State of Health (SOH) of batteries is of great importance to the safety of these systems. This paper presents a novel online method for the estimation of the SOH of Lithium (Li)-ion batteries based on Dynamic Bayesian Networks (DBNs). The structure of the DBN model is built according to the experience of experts, with the state of charges used as hidden states and the terminal voltages used as observations in the DBN. Parameters of the DBN model are learned based on training data collected through Li-ion battery aging experiments. A forward algorithm is applied for the inference of the DBN model in order to estimate the SOH in real-time. Experimental results show that the proposed method is effective and efficient in estimating the SOH of Li-ion batteries. (C) 2014 Elsevier B.V. All rights reserved.","A forward algorithm is applied for the inference of the DBN model in order to estimate the SOH in real-time."
"This work presents a spatial-component (SC) based approach to aid the diagnosis of Alzheimer's disease (AD) using magnetic resonance images. In this approach, the whole brain image is subdivided in regions or spatial components, and a Bayesian network is used to model the dependencies between affected regions of AD. The structure of relations between affected regions allows to detect neurodegeneration with an estimated performance of 88% on more than 400 subjects and predict neurodegeneration with 80% accuracy, supporting the conclusion that modeling the dependencies between components increases the recognition of different patterns of brain degeneration in AD.",""
"Cortical reorganization following sensory deprivation is characterized by alterations in the connectivity between neurons encoding spared and deprived cortical inputs. The extent to which this alteration depends on Spike Timing Dependent Plasticity (STDP), however, is largely unknown. We quantified changes in the functional connectivity between layer V neurons in the vibrissal primary somatosensory cortex (vSI) (barrel cortex) of rats following sensory deprivation. One week after chronic implantation of a microelectrode array in vSI, sensory-evoked activity resulting from mechanical deflections of individual whiskers was recorded (control data) after which two whiskers on the contralateral side were paired by sparing them while trimming all other whiskers on the rat's mystacial pad. The rats' environment was then enriched by placing novel objects in the cages to encourage exploratory behavior with the spared whiskers. Sensory-evoked activity in response to individual stimulation of spared whiskers and adjacent re-grown whiskers was then recorded under anesthesia 1-2 days and 6-7 days post-trimming (plasticity data). We analyzed spike trains within 100 ms of stimulus onset and confirmed previously published reports documenting changes in receptive field sizes in the spared whisker barrels. We analyzed the same data using Dynamic Bayesian Networks (DBNs) to infer the functional connectivity between the recorded neurons. We found that DBNs inferred from population responses to stimulation of each of the spared whiskers exhibited graded increase in similarity that was proportional to the pairing duration. A significant early increase in network similarity in the spared-whisker barrels was detected 1-2 days post pairing, but not when single neuron responses were examined during the same period. These results suggest that rapid reorganization of cortical neurons following sensory deprivation may be mediated by an STDP mechanism.",""
"Interneuron classification is an important and long-debated topic in neuroscience. A recent study provided a data set of digitally reconstructed interneurons classified by 42 leading neuroscientists according to a pragmatic classification scheme composed of five categorical variables, namely, of the interneuron type and four features of axonal morphology. From this data set we now learned a model which can classify interneurons, on the basis of their axonal morphometric parameters, into these five descriptive variables simultaneously. Because of differences in opinion among the neuroscientists, especially regarding neuronal type, for many interneurons we lacked a unique, agreed-upon classification, which we could use to guide model learning. Instead, we guided model learning with a probability distribution over the neuronal type and the axonal features, obtained, for each interneuron, from the neuroscientists' classification choices. We conveniently encoded such probability distributions with Bayesian networks, calling them label Bayesian networks (LBNs), and developed a method to predict them. This method predicts an LBN by forming a probabilistic consensus among the LBNs of the interneurons most similar to the one being classified. We used 18 axonal morphometric parameters as predictor variables, 13 of which we introduce in this paper as quantitative counterparts to the categorical axonal features. We were able to accurately predict interneuronal LBNs. Furthermore, when extracting crisp (i.e., non-probabilistic) predictions from the predicted LBNs, our method outperformed related work on interneuron classification. Our results indicate that our method is adequate for multi-dimensional classification of interneurons with probabilistic labels. Moreover, the introduced morphometric parameters are good predictors of interneuron type and the four features of axonal morphology and thus may serve as objective counterparts to the subjective, categorical axonal features.","Interneuron classification is an important and long-debated topic in neuroscience."
"Cognitive psychological research focuses on causal learning and reasoning while cognitive anthropological and social science research tend to focus on systems of beliefs. Our aim was to explore how these two types of research can inform each other. Cognitive psychological theories (causal model theory and causal Bayes nets) were used to derive predictions for systems of causal beliefs. These predictions were then applied to lay theories of depression as a specific test case. A systematic literature review on causal beliefs about depression was conducted, including original, quantitative research. Thirty-six studies investigating 13 non-Western and 32 Western cultural groups were analyzed by classifying assumed causes and preferred forms of treatment into common categories. Relations between beliefs and treatment preferences were assessed. Substantial agreement between cultural groups was found with respect to the impact of observable causes. Stress was generally rated as most important. Less agreement resulted for hidden, especially supernatural causes. Causal beliefs were clearly related to treatment preferences in Western groups, while evidence was mostly lacking for non-Western groups. Overall predictions were supported, but there were considerable methodological limitations. Pointers to future research, which may combine studies on causal beliefs with experimental paradigms on causal reasoning, are given.",""
"Bayesian networks provide a powerful tool for reasoning about probabilistic causation, used in many areas of science. They are, however, intrinsically classical. In particular, Bayesian networks naturally yield the Bell inequalities. Inspired by this connection, we generalize the formalism of classical Bayesian networks in order to investigate non-classical correlations in arbitrary causal structures. Our framework of 'generalized Bayesian networks' replaces latent variables with the resources of any generalized probabilistic theory, most importantly quantum theory, but also, for example, Popescu-Rohrlich boxes. We obtain three main sets of results. Firstly, we prove that all of the observable conditional independences required by the classical theory also hold in our generalization; to obtain this, we extend the classical d-separation theorem to our setting. Secondly, we find that the theory-independent constraints on probabilities can go beyond these conditional independences. For example we find that no probabilistic theory predicts perfect correlation between three parties using only bipartite common causes. Finally, we begin a classification of those causal structures, such as the Bell scenario, that may yield a separation between classical, quantum and general-probabilistic correlations.","Finally, we begin a classification of those causal structures, such as the Bell scenario, that may yield a separation between classical, quantum and general-probabilistic correlations."
"Purpose: To identify non-invasive clinical parameters to predict urodynamic bladder outlet obstruction (BOO) in patients with benign prostatic hyperplasia (BPH) using causal Bayesian networks (CBN). Subjects and Methods: From October 2004 to August 2013, 1,381 eligible BPH patients with complete data were selected for analysis. The following clinical variables were considered: age, total prostate volume (TPV), transition zone volume (TZV), prostate specific antigen (PSA), maximum flow rate (Qmax), and post-void residual volume (PVR) on uroflowmetry, and International Prostate Symptom Score (IPSS). Among these variables, the independent predictors of BOO were selected using the CBN model. The predictive performance of the CBN model using the selected variables was verified through a logistic regression (LR) model with the same dataset. Results: Mean age, TPV, and IPSS were 6.2 (+/- 7.3, SD) years, 48.5 (+/- 25.9) ml, and 17.9 (+/- 7.9), respectively. The mean BOO index was 35.1 (+/- 25.2) and 477 patients (34.5%) had urodynamic BOO (BOO index >= 40). By using the CBN model, we identified TPV, Qmax, and PVR as independent predictors of BOO. With these three variables, the BOO prediction accuracy was 73.5%. The LR model showed a similar accuracy (77.0%). However, the area under the receiver operating characteristic curve of the CBN model was statistically smaller than that of the LR model (0.772 vs. 0.798, p = 0.020). Conclusions: Our study demonstrated that TPV, Qmax, and PVR are independent predictors of urodynamic BOO.","The predictive performance of the CBN model using the selected variables was verified through a logistic regression (LR) model with the same dataset."
"This study investigated the use of Bayesian Networks (BNs) for left ventricular assist device (LVAD) therapy; a treatment for end-stage heart failure that has been steadily growing in popularity over the past decade. Despite this growth, the number of LVAD implants performed annually remains a small fraction of the estimated population of patients who might benefit from this treatment. We believe that this demonstrates a need for an accurate stratification tool that can help identify LVAD candidates at the most appropriate point in the course of their disease. We derived BNs to predict mortality at five endpoints utilizing the Interagency Registry for Mechanically Assisted Circulatory Support (INTERMACS) database: containing over 12,000 total enrolled patients from 153 hospital sites, collected since 2006 to the present day, and consisting of approximately 230 pre-implant clinical variables. Synthetic minority oversampling technique (SMOTE) was employed to address the uneven proportion of patients with negative outcomes and to improve the performance of the models. The resulting accuracy and area under the ROC curve (%) for predicted mortality were 30 day: 94.9 and 92.5; 90 day: 84.2 and 73.9; 6 month: 78.2 and 70.6; 1 year: 73.1 and 70.6; and 2 years: 71.4 and 70.8. To foster the translation of these models to clinical practice, they have been incorporated into a web-based application, the Cardiac Health Risk Stratification System (CHRiSS). As clinical experience with LVAD therapy continues to grow, and additional data is collected, we aim to continually update these BN models to improve their accuracy and maintain their relevance. Ongoing work also aims to extend the BN models to predict the risk of adverse events post-LVAD implant as additional factors for consideration in decision making.",""
"Risk analysis in seaports plays an increasingly important role in ensuring port operation reliability, maritime transportation safety and supply chain distribution resilience. However, the task is not straightforward given the challenges, including that port safety is affected by multiple factors related to design, installation, operation and maintenance and that traditional risk assessment methods such as quantitative risk analysis cannot sufficiently address uncertainty in failure data. This paper develops an advanced Failure Mode and Effects Analysis (FMEA) approach through incorporating Fuzzy Rule-Based Bayesian Networks (FRBN) to evaluate the criticality of the hazardous events (HEs) in a container terminal. The rational use of the Degrees of Belief (DoB) in a fuzzy rule base (FRB) facilitates the implementation of the new method in Container Terminal Risk Evaluation (CTRE) in practice. Compared to conventional FMEA methods, the new approach integrates FRB and BN in a complementary manner, in which the former provides a realistic and flexible way to describe input failure information while the latter allows easy updating of risk estimation results and facilitates real-time safety evaluation and dynamic risk-based decision support in container terminals. The proposed approach can also be tailored for wider application in other engineering and management systems, especially when instant risk ranking is required by the stakeholders to measure, predict and improve their system safety and reliability performance.",""
"Land use change results from frequent, independent actions by decision-makers working in isolation, often with a focus on a single land use. In order to develop integrated land use policies that encourage sustainable outcomes, scientists and practitioners must understand the specific drivers of land use change across mixed land use types and ownerships, and must consider the combined influences of biophysical, economic, and social factors that affect land use decisions. In this analysis of two large watersheds covering a total of 1.9 million hectares in Maine, USA, we co-developed with groups of stakeholders land use suitability models that integrated four land uses: economic development, ecosystem protection, forestry, and agriculture. We elicited stakeholder knowledge to: (1) identify generalized drivers of land use change; (2) construct Bayesian network models of suitability for each of the four land uses based on site-level factors that affect land use decisions; and (3) identify thresholds of suitability for each factor and give relative weights to each factor. We then applied 12 distinct Bayesian models using 99 spatially explicit, empirical socio-economic and biophysical datasets to predict spatially the suitability for each of our four land uses on a 30 m x 30 m pixel basis across 1.9 million hectares. We evaluated both the stakeholder engagement process and the land use suitability maps. Results demonstrated the potential efficacy of these models for strategic land use planning, but also revealed that trade-offs occur when stakeholder knowledge is used to augment limited empirical data. First, stakeholder-derived Bayesian land use models can provide decision-makers with relevant insights about the factors affecting land use change. Unfortunately, these models are not easily validated for predictive purposes. Second, integrating stakeholders throughout different phases of the modeling process provides a flexible framework for developing localized or generalizable land use models depending on the scope of stakeholder knowledge and available empirical data. The potential downside is that this can lead to more complex models than anticipated. The trade-offs between model rigor and relevance suggest an adaptive management approach to modeling is needed to improve the integration of stakeholder knowledge into robust land use models. (C) 2014 Elsevier B.V. All rights reserved.",""
"Case retrieval constitutes an interesting area of research which contributes to the evolution of several domains. The similarity measure module is a fundamental step in the retrieval process which affects remarkably on a retrieval system. In this context, we suggest in this paper a similarity measure applied to brain tumor cases retrieval. The rationale behind the proposed measure consists in quantifying the diagnosis correspondence followed by a clinician while comparing two medical cases. Our idea is characterized by the use of the Bayesian inference in the formulation of the proposed measure. The Bayesian network is applied in the classification task and it describes the decision-making process of a radiologist facing a tumor. The proposed similarity algorithm is based essentially on graph correspondence based on signature nodes comparison from the Bayesian classifiers. experiments were directed to compare the performance of the proposed similarity measure method with classical methods of similarity quantification. The performance indices of our proposition are promising.","Our idea is characterized by the use of the Bayesian inference in the formulation of the proposed measure."
"We introduce a flexible copula-based semi-parametric test of financial contagion that is capable of capturing structural shifts in the transmission channel of shocks across a network of financial markets beyond the increase in the intensity of time-varying dependence. We illustrate the capabilities of the proposed test using returns of stock, money, sovereign debt, and foreign exchange markets of seven Latin-American countries, and test for the presence of pure contagion effects for each major financial crisis that affected the Mercosur region between 1994 and 2001. Besides strong evidence in favor of time-varying market interdependence, we cannot rule out the presence of pure contagion effects in the stock market transmission channel associated with the Mexican, Asian, and Russian financial crises.",""
"Spirtes, Glymour and Scheines [Causation, Prediction, and Search (1993) Springer] described a pointwise consistent estimator of the Markov equivalence class of any causal structure that can be represented by a directed acyclic graph for any parametric family with a uniformly consistent test of conditional independence, under the Causal Markov and Causal Faithfulness assumptions. Robins et al. [Biometrika 90 (2003) 491-515], however, proved that there are no uniformly consistent estimators of Markov equivalence classes of causal structures under those assumptions. Subsequently, Kalisch and Buhlmann [J. Mach. Learn. Res. 8 (2007) 613-636] described a uniformly consistent estimator of the Markov equivalence class of a linear Gaussian causal structure under the Causal Markov and Strong Causal Faithfulness assumptions. However, the Strong Faithfulness assumption may be false with high probability in many domains. We describe a uniformly consistent estimator of both the Markov equivalence class of a linear Gaussian causal structure and the identifiable structural coefficients in the Markov equivalence class under the Causal Markov assumption and the considerably weaker k-Triangle-Faithfulness assumption.",""
"Accident modelling is a methodology used to relate the causes and effects of events that lead to accidents. This modelling effectively seeks to answer two main questions: (i) Why does an accident occur, and (ii) How does it occur. This paper presents a review of accident models that have been developed for the chemical process industry with in-depth analyses of a class of models known as dynamic sequential accident models (DSAMs). DSAMs are sequential models with a systematic procedure to utilise precursor data to estimate the posterior risk profile quantitatively. DSAM also offers updates on the failure probabilities of accident barriers and the prediction of future end states. Following a close scrutiny of these methodologies, several limitations are noted and discussed, and based on these insights, future work is suggested to enhance and improve this category of models further. (C) 2014 Elsevier Ltd. All rights reserved.",""
"Enterohemorrhagic Escherichia coli (EHEC) O157 are important foodborne pathogens whose major reservoir are asymptomatic cattle. There is evidence suggesting that nonpathogenic E. coli and bacteriophages in the gastro-intestinal tract can influence the pathogenicity of EHEC O157. The factors contributing to the onset and persistence of shedding EHEC O157 in cattle are not completely elucidated. This study used Bayesian network analysis to identify genetic markers of generic E. coli associated with shedding of EHEC O157 in cattle from data generated during an oral experimental challenge study in 4 groups of 6 steers inoculated with three different EHEC O157 strains. The quantification of these associations was accomplished using mixed effects logistic regression. The results showed that the concurrent presence of generic E. coli carrying the prophage marker R4-N and the virulence marker stx2 increased the odds of the onset of EHEC O157 shedding. The presence of prophage markers z2322 and X011C increased, while C1.N decreased the odds of shedding EHEC O157 two days later. A significant antagonist interaction effect between the presence of the virulence marker stx2 on the day of shedding EHEC O157 and two days before shedding was also found. In terms of the persistence of EHEC O157 shedding, the presence of prophage marker R4-N (OR = 16, and 95% confidence interval (CI): 1.1, 252) was found to increase the odds of stopping EHEC O157 shedding, whereas prophage marker C1.N (OR = 0.16, CI: 0.03, 0.7) and the enterohemolysin gene hly (OR = 0.03, CI: 0.001, 0.8) were found to significantly decrease the odds of stopping EHEC O157 shedding. In conclusion, the study found that the presence of certain genetic markers in the generic E. coli genome can influence the pathogenicity of EHEC O157. (C) 2014 Elsevier B.V. All rights reserved.","The quantification of these associations was accomplished using mixed effects logistic regression."
"Marine spatial management is challenged by complex situations in European countries where multiple stakeholder interests and many management options have to be balanced. EU policy initiatives such as the proposed Marine Spatial Planning Directive, are in different ways targeting area allocation in European waters. In this circumstance, EU marine management needs assessments based on a satisfactory evaluation framework design that can ensure a transparent treatment of different types of information including interests, values, and facts. The main goal of this article is to introduce an evaluation framework applicable to marine management in European countries. This so-called CoExist framework maps out different types of relevant knowledge to assess future possibilities for use or no-use of marine areas and links this with appropriate management measures. The CoExist framework is based on the principles of ensuring transparent treatment of different types of information as well as appropriate stakeholder representation which can ensure legitimacy. Empirical findings in six European case studies have been obtained while conducting the CoExist framework. Applying the basic principles of the CoExist framework when planning future management directions of the coexistence of multiple activities in the long-run will expectedly affect the ecological and social-cultural goals by counterbalancing the economic ones.",""
"Dynamic accident modeling for a gas gathering station is implemented to prevent high-sulfur natural gas leakage and develop equipment inspection strategy. The progress of abnormal event occurring in the gas gathering station is modeled by the combination of fault tree and event sequence diagram, based on accident causal chain theory, i.e. the progress is depicted as sequential failure of safety barriers, then, the occurrence probability of the consequence of abnormal event is predicted. Consequences of abnormal events are divided into accidents and accident precursors which include incidents, near misses and so on. The Bayesian theory updates failure probability of safety barrier when a new observation (i.e. accident precursors or accidents data) arrives. Bayesian network then correspondingly updates failure probabilities of basic events of the safety barriers with the ability of abductive reasoning. Consequence occurrence probability is also updated. The results show that occurrence probability trend of different consequences and failure probability trend of safety barriers and basic events of the safety barriers can be obtained using this method. In addition, the critical basic events which play an important role in accidents occurrence are also identified. All of these provide useful information for the maintenance and inspection of the gas gathering station. (C) 2013 The Institution of Chemical Engineers. Published by Elsevier B.V. All rights reserved.",""
"Intelligent Tutoring Systems (ITS) are computer programs that model learners' psychological states to provide individualized instruction. They have been developed for diverse subject areas (e.g., algebra, medicine, law, reading) to help learners acquire domain-specific, cognitive and metacognitive knowledge. A meta-analysis was conducted on research that compared the outcomes from students learning from ITS to those learning from non-ITS learning environments. The meta-analysis examined how effect sizes varied with type of ITS, type of comparison treatment received by learners, type of learning outcome, whether knowledge to be learned was procedural or declarative, and other factors. After a search of major bibliographic databases, 107 effect sizes involving 14,321 participants were extracted and analyzed. The use of ITS was associated with greater achievement in comparison with teacher-led, large-group instruction (g = .42), non-ITS computer-based instruction (g = .57), and textbooks or workbooks (g = .35). There was no significant difference between learning from ITS and learning from individualized human tutoring (g = -.11) or small-group instruction (g = .05). Significant, positive mean effect sizes were found regardless of whether the ITS was used as the principal means of instruction, a supplement to teacher-led instruction, an integral component of teacher-led instruction, or an aid to homework. Significant, positive effect sizes were found at all levels of education, in almost all subject domains evaluated, and whether or not the ITS provided feedback or modeled student misconceptions. The claim that ITS are relatively effective tools for learning is consistent with our analysis of potential publication bias.",""
"Enhanced Qualitative Probabilistic Network (QPN) is to make qualitative network more applicable by reducing ambiguity in a qualitative way. To reduce ambiguity in the basic QPN inference, we propose an enhanced QPN based on qualitative mutual information (QMI), named QMIQPN. Firstly, we give a strict definition of QMI. Secondly, based on the definition, we present the formalism of QMIQPN. Specifically, we take QMI as the strength of qualitative influence in QMIQPN, the qualitative influence with strength differs from the previous work, which additional expressiveness of the enhancement does not come at the expense of the property of symmetry of influence. Thirdly, we analyze several relative properties of qualitative influences with strengths. Furthermore, we improve the Sign-propagation Algorithm to reduce ambiguity and discuss its complexity. Finally, by experiments on several databases, we analyze the performance of QMIQPN. Theoretic analysis and experimental results illustrate that QMIQPN is qualitative and efficient, yet allows for reducing some ambiguities upon QPN inference. (C) 2014 Elsevier B.V. All rights reserved.","To reduce ambiguity in the basic QPN inference, we propose an enhanced QPN based on qualitative mutual information (QMI), named QMIQPN."
"Missing values are a common occurrence in a number of real world databases, and statistical methods have been developed to deal with this problem, referred to as missing data imputation. In the detection and prediction of incipient faults in power transformers using dissolved gas analysis (DGA), the problem of missing values is significant and has resulted in inconclusive decision-making. This study proposes an efficient nonparametric iterative imputation method named FINNIM, which comprises of three components: 1) the imputation ordering; 2) the imputation estimator; and 3) the iterative imputation. The relationship between gases and faults, and the percentage of missing values in an instance are used as a basis for the imputation ordering; whereas the plausible values for the missing values are estimated from k-nearest neighbor instances in the imputation estimator, and the iterative imputation allows complete and incomplete instances in a DGA dataset to be utilized iteratively for imputing all the missing values. Experimental results on both artificially inserted and actual missing values found in a few DGA datasets demonstrate that the proposed method outperforms the existing methods in imputation accuracy, classification performance, and convergence criteria at different missing percentages.","Experimental results on both artificially inserted and actual missing values found in a few DGA datasets demonstrate that the proposed method outperforms the existing methods in imputation accuracy, classification performance, and convergence criteria at different missing percentages."
"In order to overcome the limitations of traditional methods in uncertainty analysis, a modified Bayesian network (BN), which is called evidence network (EN), was proposed with evidence theory to handle epistemic uncertainty in probabilistic risk assessment (PRA). Fault trees (FTs) and event trees (ETs) were transformed into an EN which is used as a uniform framework to represent accident scenarios. Epistemic uncertainties of basic events in PRA were presented in evidence theory form and propagated through the network. A case study of a highway tunnel risk analysis was discussed to demonstrate the proposed approach. Frequencies of end states are obtained and expressed by belief and plausibility measures. The proposed approach addresses the uncertainties in experts' knowledge and can be easily applied to uncertainty analysis of FTs/ETs that have dependent events.",""
"In computer vision tasks such as, for example, object recognition, semantically accurate segmentation of a particular object of interest (OOI) is a critical step. Due to the OOI consisting of visually different fragments, traditional segmentation algorithms that are based on the identification of homogeneous regions usually do not perform well. In order to narrow this gap between low-level visual features and high-level semantics, some recent methods employ machine learning to generate more accurate models of the OOI. The main contribution of this paper is the inclusion of spatial relationships among the OOI fragments into the model. For this purpose, we employ Bayesian networks as a probabilistic approach for learning the spatial relationships which, in turn, becomes evidence that is used for the process of segmenting future instances of the OOI. The algorithm presented in this paper also uses multiple instance learning to obtain prototypical descriptions of each fragment of the OOI based on low-level visual features. The experimental results on both artificial and real image datasets indicate that the addition of spatial relationships improves segmentation performance.",""
"It is always wise to keep the frequently used information by various users in the cache memory. This makes the users feel that their needed information is available almost immediately. Apart from experiencing less access delay, caching process also helps in better bandwidth utilization and load reduction in the origin server. Pre-fetching is the process of fetching few of the Web pages in advance by assuming that those pages will be needed by the user in the near future. Combining pre-fetching and caching techniques results in experiencing much less access delay and much better bandwidth utilization. Many works have been reported in the literature, separately for Web caching techniques and pre-fetching of Web pages. In this paper, pre-fetching technique that uses clustering is combined with SVM (support vector machine)-LRU algorithm, a machine learning method for Web proxy caching. By using real-time data, it is demonstrated that the latter approach will be advantageous than clustering-based pre-fetching technique using traditional LRU-based caching policy. The efficiency of our proposed method is also compared with caching using Bayesian networks and neuro-fuzzy system.",""
"Photocrosslinked polyacrylic acid (PAA-HEMA) hydrogels are a promising candidate for use in dermatological patch adhesives. To gain further knowledge about the properties of this gel, we investigated the T-1 relaxation time and the diffusion coefficient (D) of water in the hydrogels using magnetic resonance (MR) imaging. Hydrogels with different formulations and process factors were prepared and tested. The observed data were analyzed by ANOVA, which clarified the mode of action of the formulation and process factors based on these MR parameters. Various gel properties (i.e., gel fraction, swelling capacity, gel strength, and water-retention ability) were also measured, followed by a Bayesian network (BN) analysis. The BN allowed us to summarize well the relationships between the formulation and process factors, MR parameters, and gel properties. T-1 was associated with the swelling and water-retention properties of the hydrogel, whereas D was associated with gel formation and gel strength. Furthermore, this study clarified that T-1 and D mostly represented the hydration and water-compartmentalization effects of the hydrogel, respectively. In conclusion, the state of water seems to play an important role in the properties of the PAA-HEMA hydrogel. (c) 2014 Wiley Periodicals, Inc. and the American Pharmacists Association J Pharm Sci 103:3532-3541, 2014",""
"Multi-modal context-aware systems can provide user-adaptive services, but it requires complicated recognition models with larger resources. The limitations to build optimal models and infer the context efficiently make it difficult to develop practical context-aware systems. We developed a multi-modal context-aware system with various wearable sensors including accelerometers, gyroscopes, physiological sensors, and data gloves. The system used probabilistic models to handle the uncertain and noisy time-series sensor data. In order to construct the efficient probabilistic models, this paper uses an evolutionary algorithm to model structure and EM algorithm to determine parameters. The trained models are selectively inferred based on a semantic network which describes the semantic relations of the contexts and sensors. Experiments with the real data collected show the usefulness of the proposed method.",""
"In this paper, we focus on the concept classes C-N induced by Bayesian networks. The relationship between two-dimensional values induced by these concept classes is studied, one of which is the VC-dimension of the concept class C-N; denoted as VCdim(N), and other is the smallest dimensional of Euclidean spaces into which C-N can be embedded, denoted as Edim(N). As a main result, we show that the two-dimensional values are equal for the Bayesian networks with n <= 4 variables, called the VE-dimension for that Bayesian networks.",""
"A maritime accident involving an oil tanker may lead to large scale mortality or reductions in populations of coastal species due to oil. The ecological value at stake is the biota on the coast, which are neither uniformly nor randomly distributed. We used an existing oil spill simulation model, an observation database of threatened species, and a valuation method and developed a software system for assessing the spatially distributed ecological risk posed by oil shipping. The approach links a tanker accident model to a set of oil spill simulations and further to a spatial ecological value data set. The tanker accident model is a Bayesian network and thus we present a case of using a Bayesian network in geographic analysis. A case in the Gulf of Finland is used for illustration of the methodology. The method requires and builds on an extensive data collection and generation effort and modeling. The main difference of our work to earlier works on using a Bayesian network in geospatial setting is that in our case the Bayesian network was used to compute the probabilities of spatial scenarios directly in a global sense while in earlier works Bayesian networks have been used for each location separately to obtain global results. The result was a software system that was used by a distributed research team. (C) 2014 Elsevier Ltd. All rights reserved.",""
"Sensing and responding to the environment are two essential functions that all biological organisms need to master for survival and successful reproduction. Developmental processes are marshalled by a diverse set of signalling and control systems, ranging from systems with simple chemical inputs and outputs to complex molecular and cellular networks with non-linear dynamics. Information theory provides a powerful and convenient framework in which such systems can be studied; but it also provides the means to reconstruct the structure and dynamics of molecular interaction networks underlying physiological and developmental processes. Here we supply a brief description of its basic concepts and introduce some useful tools for systems and developmental biologists. Along with a brief but thorough theoretical primer, we demonstrate the wide applicability and biological application-specific nuances by way of different illustrative vignettes. In particular, we focus on the characterisation of biological information processing efficiency, examining cell-fate decision making processes, gene regulatory network reconstruction, and efficient signal transduction experimental design. (C) 2014 Elsevier Ltd. All rights reserved.",""
"Understanding the mechanisms that enable growth and survival of an organism while driving it to the full range of its adaptation is fundamental to the issues of biodiversity and evolution, particularly regarding global climatic changes. Here we report the Illumina RNA-sequencing (RNA-seq) and de nova assembly of the blue mussel Mytilus edulis transcriptome during early development. This study is based on high-throughput data, which associates genome-wide differentially expressed transcript (DET) patterns with early activation of developmental processes. Approximately 50,383 high-quality contigs were assembled. Over 8000 transcripts were associated with functional proteins from public databases. Coding and non-coding genes served to design customized microarrays targeting every developmental stage, which encompass major transitions in tissue organization. Consequently, multi-processing pattern exploration protocols applied to 3633 DETs helped discover 12 unique coordinated eigengenes supposedly implicated in various physiological and morphological changes that larvae undergo during early development. Moreover, dynamic Bayesian networks (DBNs) provided key insights to understand stage-specific molecular mechanisms activated throughout ontogeny. In addition, delayed and contemporaneous interactions between DETs were coerced with 16 relevant regulators that interrelated in non-random genetic regulatory networks (GRNs). Genes associated with mechanisms of neural and muscular development have been characterized and further included in dynamic networks necessary in growth and functional morphology. This is the first large-scale study being dedicated to M. edulis throughout early ontogeny. Integration between RNA-seq and microarray data enabled a high-throughput exploration of hidden processes essential in growth and survival of microscopic mussel larvae. Our integrative approach will support a holistic understanding of systems biology and will help establish new links between environmental assessment and functional development of marine bivalves. Crown Copyright (C) 2014 Published by Elsevier B.V. All rights reserved.",""
"Background/purposeOptical and parametric skin imaging methods which can efficiently identify invisible sub-skin features or subtle changes in skin layers are very important for accurate optical skin modelling. In this study, a hybrid method is introduced that helps develop a parametric optical skin model by utilizing interdisciplinary techniques including light back-scatter analysis, laser speckle imaging, image-texture analysis and Bayesian inference methods. The model aims to detect subtle skin changes and hence very early signs of skin abnormalities/diseases. MethodsLight back-scatter and laser speckle image textural analysis are applied onto the normal and abnormal skin regions (lesions) to generate set of attributes/parameters. These are then optimized by Bayesian inference method in order to build an optimized parametric model. ResultsThe attributes selected by Bayesian inference method in the optimization stage were used to build an optimized model and then successfully verified. It was clearly proven that Bayesian inference based optimization process yields good results to build an optimized skin model. ConclusionThe outcome of this study clearly shows the applicability of this hybrid method in the analysis of skin features and is therefore expected to lead development of non-invasive and low-cost instrument for early detection of skin changes.","In this study, a hybrid method is introduced that helps develop a parametric optical skin model by utilizing interdisciplinary techniques including light back-scatter analysis, laser speckle imaging, image-texture analysis and Bayesian inference methods."
"This study develops the hybrid models of dynamic multidimensional efficiency classification. By integrating data envelopment analysis (DEA), naive Bayesian networks (NBN) and dynamic Bayesian networks( DBN), this work proposes a five-step design for efficiency classification: (1) performance evaluation with DEA model, (2) efficiency discretization, (3) intra-period classification by NBN, (4) inter-period classification by DBN, (5) testing and validation. Due to the Markovian property of the dynamic models, the inter-period dependency is assumed invariant over time. In data-driven parameter learning, the fuzzy parameters for incorporating the variation in dynamic dependencies are introduced. We conduct an empirical case study of higher education in Taiwan to demonstrate the usability of this design. (C) 2014 Elsevier B.V. All rights reserved.","This study develops the hybrid models of dynamic multidimensional efficiency classification."
"The coral reefs and seagrass habitats in the Great Barrier Reef World Heritage Area (GBRWHA) are vulnerable to physical disturbances, including the anchoring of vessels. Both the anchor being deployed and retrieved, as well as the movement of the attaching rode, can cause damage to corals and seagrasses. Understanding the contributing processes that influence the deployment of anchors can assist with managing anchor damage in the GBRWHA, particularly in the context of climate change. Providing a spatial description of the vulnerability, rather than just a list of factors, requires the incorporation of social, geophysical and ecological factors. An integrated GIS-Bayesian Belief Network was utilised to combine 19 spatial datasets, 6 spatial models and expert opinion. The base scale was set to match the 250 m lattice interval of the Great Barrier Reef digital elevation model. With approximately 5 million data points the model was able to spatially describe the likelihood of damage from anchor deployment across the GBRWHA. While only 19% of the GBRWHA is considered susceptible to anchor damage, the assessment indicates that coral reefs and seagrass meadows adjacent to population centres and in particular close to islands are highly vulnerable. Comparisons with coral reef health surveys (Eye on the Reef Program) and detailed anchorage records from a scientific research vessel indicate the model is robust despite extensive use of disparate spatial data and expert opinion. The effect of each node in the Bayesian Belief Network on the anchor vulnerability beliefs was measured by standard variance reduction and this found that anchor site familiarity and accessibility were the dominant influences aside from the presence of sensitive habitat. Visualisation of the model outputs, including the intermediate stages, provided additional qualitative evaluation. Enhancing the vulnerability assessment to describe every location in the GBRWHA will contribute to the development of policy and governance mechanisms whilst supporting focused monitoring of sites vulnerable to anchor damage. (C) 2014 Elsevier Ltd. All rights reserved.",""
"The debate on probabilistic measures of coherence flourishes for about 15 years now. Initiated by papers that have been published around the turn of the millennium, many different proposals have since then been put forward. This contribution is partly devoted to a reassessment of extant coherence measures. Focusing on a small number of reasonable adequacy constraints I show that (i) there can be no coherence measure that satisfies all constraints, and that (ii) subsets of these adequacy constraints motivate two different classes of coherence measures. These classes do not coincide with the common distinction between coherence as mutual support and coherence as relative set-theoretic overlap. Finally, I put forward arguments to the effect that for each such class of coherence measures there is an outstanding measure that outperforms all other extant proposals. One of these measures has recently been put forward in the literature, while the other one is based on a novel probabilistic measure of confirmation.",""
"Electricity and heat generation are key contributors to global emissions of greenhouse gases (GHG). In this paper, specific attention is paid to renewable energy technologies (RETs) for electricity and heat generation and reviews current understanding and estimates of life cycle GHG emissions from a range of renewable electricity and heat generation technologies. Comprehensive literature reviews for each RET were carried out. The 79 studies reviewed involved the life cycle assessment (LCA) of renewable electricity and heat generation based on onshore and offshore winds, hydropower, marine technologies (wave power and tidal energy), geothermal, photovoltaic (PV), solar thermal, biomass, waste, and heat pumps. The study demonstrates the variability of existing LCA studies (results) in tracking GHG emissions for electricity and heat generation from RETs. This review has shown that the lowest GHG emissions were associated with offshore wind technologies (mean life cycle GHG emissions could be 5.3-13 g CO2 eq/kWh). Results compared with GHG estimates by fossil fuel heat and electricity indicated that life cycle GHG emissions are comparatively higher in conventional sources as compared to renewable sources with the exception of nuclear-based power electricity generation. In this present study, considering renewable energy sources, waste treatment and dedicated biomass technologies (DBTs) were found to potentially have high GHG emissions based on the feedstock, selected boundary and the inputs required for their production. The study identifies additional impacts associated with renewable electricity and heat technologies, points out the effectiveness of life cycle analysis (LCA) as a tool for assessing environmental impacts of renewable energy sources and concludes with opportunities for improvement in the future. (C) 2014 Elsevier Ltd. All rights reserved.",""
"The reconstruction of pedigrees from genetic marker data is relevant to a wide range of applications. Likelihood-based approaches aim to find the pedigree structure that gives the highest probability to the observed data. Existing methods either entail an exhaustive search and are hence restricted to small numbers of individuals, or they take a more heuristic approach and deliver a solution that will probably have high likelihood but is not guaranteed to be optimal. By encoding the pedigree learning problem as an integer linear program we can exploit efficient optimisation algorithms to construct pedigrees guaranteed to have maximal likelihood for the standard situation where we have complete marker data at unlinked loci and segregation of genes from parents to offspring is Mendelian. Previous work demonstrated efficient reconstruction of pedigrees of up to about 100 individuals. The modified method that we present here is not so restricted: we demonstrate its applicability with simulated data on a real human pedigree structure of over 1600 individuals. It also compares well with a very competitive approximate approach in terms of solving time and accuracy. In addition to identifying a maximum likelihood pedigree, we can obtain any number of pedigrees in decreasing order of likelihood. This is useful for assessing the uncertainty of a maximum likelihood solution and permits model averaging over high likelihood pedigrees when this would be appropriate. More importantly, when the solution is not unique, as will often be the case for large pedigrees, it enables investigation into the properties of maximum likelihood pedigree estimates which has not been possible up to now. Crucially, we also have a means of assessing the behaviour of other approximate approaches which all aim to find a maximum likelihood solution. Our approach hence allows us to properly address the question of whether a reasonably high likelihood solution that is easy to obtain is practically as useful as a guaranteed maximum likelihood solution. The efficiency of our method on such large problems bodes well for extensions beyond the standard setting where some pedigree members may be latent, genotypes may be measured with error and markers may be linked. (C) 2014 Elsevier Inc. All rights reserved.",""
"Extensive resources are allocated to managing vertebrate pests, yet spatial understanding of pest threats, and how they respond to management, is limited at the regional scale where much decision-making is undertaken. We provide regional-scale spatial models and management guidance for European rabbits (Oryctolagus cuniculus) in a 260,791 km(2) region in Australia by determining habitat suitability, habitat susceptibility and the effects of the primary rabbit management options (barrier fence, shooting and baiting and warren ripping) or changing predation or disease control levels. A participatory modelling approach was used to develop a Bayesian network which captured the main drivers of suitability and spread, which in turn was linked spatially to develop high resolution risk maps. Policy-makers, rabbit managers and technical experts were responsible for defining the questions the model needed to address, and for subsequently developing and parameterising the model. Habitat suitability was determined by conditions required for warren-building and by above-ground requirements, such as food and harbour, and habitat susceptibility by the distance from current distributions, habitat suitability, and the costs of traversing habitats of different quality. At least one-third of the region had a high probability of being highly suitable (support high rabbit densities), with the model supported by validation. Habitat susceptibility was largely restricted by the current known rabbit distribution. Warren ripping was the most effective control option as warrens were considered essential for rabbit persistence. The anticipated increase in disease resistance was predicted to increase the probability of moderately suitable habitat becoming highly suitable, but not increase the at-risk area. We demonstrate that it is possible to build spatial models to guide regional-level management of vertebrate pests which use the best available knowledge and capture fine spatial-scale processes.",""
"This paper presents a systemic decision support approach for safety risk analysis under uncertainty in tunnel construction. Fuzzy Bayesian Networks (FBN) is used to investigate causal relationships between tunnel-induced damage and its influential variables based upon the risk/hazard mechanism analysis. Aiming to overcome limitations on the current probability estimation, an expert confidence indicator is proposed to ensure the reliability of the surveyed data for fuzzy probability assessment of basic risk factors. A detailed fuzzy-based inference procedure is developed, which has a capacity of implementing deductive reasoning, sensitivity analysis and abductive reasoning. The \"3 sigma criterion\" is adopted to calculate the characteristic values of a triangular fuzzy number in the probability fuzzification process, and the a-weighted valuation method is adopted for defuzzification. The construction safety analysis progress is extended to the entire life cycle of risk-prone events, including the pre-accident, during-construction continuous and post-accident control. A typical hazard concerning the tunnel leakage in the construction of Wuhan Yangtze Metro Tunnel in China is presented as a case study, in order to verify the applicability of the proposed approach. The results demonstrate the feasibility of the proposed approach and its application potential. A comparison of advantages and disadvantages between FBN and fuzzy fault tree analysis (FFTA) as risk analysis tools is also conducted. The proposed approach can be used to provide guidelines for safety analysis and management in construction projects, and thus increase the likelihood of a successful project in a complex environment. (C) 2014 Elsevier Ltd. All rights reserved.","A detailed fuzzy-based inference procedure is developed, which has a capacity of implementing deductive reasoning, sensitivity analysis and abductive reasoning."
"Streaming data are relevant to finance, computer science, and engineering while they are becoming increasingly important to medicine and biology. Continuous time Bayesian network classifiers are designed for analyzing multivariate streaming data when time duration of event matters. Structural and parametric learning for the class of continuous time Bayesian network classifiers are considered in the case where complete data is available. Conditional log-likelihood scoring is developed for structural learning on continuous time Bayesian network classifiers. Performance of continuous time Bayesian network classifiers learned when combining conditional log-likelihood scoring and Bayesian parameter estimation are compared with that achieved by continuous time Bayesian network classifiers when learning is based on marginal log-likelihood scoring and to that achieved by dynamic Bayesian network classifiers. Classifiers are compared in terms of accuracy and computation time. Comparison is based on numerical experiments where synthetic and real data are used. Results show that conditional log-likelihood scoring combined with Bayesian parameter estimation outperforms marginal log-likelihood scoring. Conditional log-likelihood scoring becomes even more effective when the amount of available data is limited. Continuous time Bayesian network classifiers outperform in terms of computation time and accuracy dynamic Bayesian network on synthetic and real data sets. (C) 2014 Elsevier Inc. All rights reserved.","Continuous time Bayesian network classifiers are designed for analyzing multivariate streaming data when time duration of event matters."
"This paper presents a control system, based on artificial intelligence technologies, that implements multiple intelligences. This system aims to support and improve automatic telecontrol of solar power plants, by either automatically triggering actuators or dynamically giving recommendations to human operators. For this purpose, the development of a MultiAgent System is combined with a variety of inference systems, such as Expert Systems, Neural Networks, and Bayesian Networks. This diversity of intelligent technologies is shown to result in an efficient way to mimic the reasoning process in human operators. (C) 2014 Elsevier Ltd. All rights reserved.","For this purpose, the development of a MultiAgent System is combined with a variety of inference systems, such as Expert Systems, Neural Networks, and Bayesian Networks."
"We present a novel hybrid algorithm for Bayesian network structure learning, called H2PC. It first reconstructs the skeleton of a Bayesian network and then performs a Bayesian-scoring greedy hill-climbing search to orient the edges. The algorithm is based on divide-and-conquer constraint-based subroutines to learn the local structure around a target variable. We conduct two series of experimental comparisons of H2PC against Max-Min Hill-Climbing (MMHC), which is currently the most powerful state-of-the-art algorithm for Bayesian network structure learning. First, we use eight well-known Bayesian network benchmarks with various data sizes to assess the quality of the learned structure returned by the algorithms. Our extensive experiments show that H2PC outperforms MMHC in terms of goodness of fit to new data and quality of the network structure with respect to the true dependence structure of the data. Second, we investigate H2PC's ability to solve the multi-label learning problem. We provide theoretical results to characterize and identify graphically the so-called minimal label powersets that appear as irreducible factors in the joint distribution under the faithfulness condition. The multi-label learning problem is then decomposed into a series of multi-class classification problems, where each multi-class variable encodes a label powerset. H2PC is shown to compare favorably to MMHC in terms of global classification accuracy over ten multi-label data sets covering different application domains. Overall, our experiments support the conclusions that local structural learning with H2PC in the form of local neighborhood induction is a theoretically well-motivated and empirically effective learning framework that is well suited to multi-label learning. The source code (in R) of H2PC as well as all data sets used for the empirical tests are publicly available. (C) 2014 Elsevier Ltd. All rights reserved.","The multi-label learning problem is then decomposed into a series of multi-class classification problems, where each multi-class variable encodes a label powerset."
"Wildfires can pose a significant risk to people and property. Billions of dollars are spent investing in fire management actions in an attempt to reduce the risk of loss. One of the key areas where money is spent is through fuel treatment - either fuel reduction (prescribed fire) or fuel removal (fuel breaks). Individual treatments can influence fire size and the maximum distance travelled from the ignition and presumably risk, but few studies have examined the landscape level effectiveness of these treatments. Here we use a Bayesian Network model to examine the relative influence of the built and natural environment, weather, fuel and fuel treatments in determining the risk posed from wildfire to the wildland-urban interface. Fire size and distance travelled was influenced most strongly by weather, with exposure to fires most sensitive to changes in the built environment and fire parameters. Natural environment variables and fuel load all had minor influences on fire size, distance travelled and exposure of assets. These results suggest that management of fuels provided minimal reductions in risk to assets and adequate planning of the changes in the built environment to cope with the expansion of human populations is going to be vital for managing risk from fire under future climates.",""
"Background: The complexity of biological data related to the genetic origins of tumour cells, originates significant challenges to glean valuable knowledge that can be used to predict therapeutic responses. In order to discover a link between gene expression profiles and drug responses, a computational framework based on Consensus p-Median clustering is proposed. The main goal is to simultaneously predict (in silico) anticancer responses by extracting common patterns among tumour cell lines, selecting genes that could potentially explain the therapy outcome and finally learning a probabilistic model able to predict the therapeutic responses. Results: The experimental investigation performed on the NCI60 dataset highlights three main findings: (1) Consensus p-Median is able to create groups of cell lines that are highly correlated both in terms of gene expression and drug response; (2) from a biological point of view, the proposed approach enables the selection of genes that are strongly involved in several cancer processes; (3) the final prediction of drug responses, built upon Consensus p-Median and the selected genes, represents a promising step for predicting potential useful drugs. Conclusion: The proposed learning framework represents a promising approach predicting drug response in tumour cells.",""
"The need for smallholder farmers to adapt their practices to a changing climate is well recognised, particularly in Africa. The cost of adapting to climate change in Africa is estimated to be $20 to $30 billion per year, but the total amount pledged to finance adaptation falls significantly short of this requirement. The difficulty of assessing and monitoring when adaptation is achieved is one of the key barriers to the disbursement of performance-based adaptation finance. To demonstrate the potential of Bayesian Belief Networks for describing the impacts of specific activities on climate change resilience, we developed a simple model that incorporates climate projections, local environmental data, information from peer-reviewed literature and expert opinion to account for the adaptation benefits derived from Climate-Smart Agriculture activities in Malawi. This novel approach allows assessment of vulnerability to climate change under different land use activities and can be used to identify appropriate adaptation strategies and to quantify biophysical adaptation benefits from activities that are implemented. We suggest that multiple-indicator Bayesian Belief Network approaches can provide insights into adaptation planning for a wide range of applications and, if further explored, could be part of a set of important catalysts for the expansion of adaptation finance.",""
"Bayesian networks are a type of probabilistic graphical models lie at the intersection between statistics and machine learning. They have been shown to be powerful tools to encode dependence relationships among the variables of a domain under uncertainty. Thanks to their generality, Bayesian networks can accommodate continuous and discrete variables, as well as temporal processes. In this paper we review Bayesian networks and how they can be learned automatically from data by means of structure learning algorithms. Also, we examine how a user can take advantage of these networks for reasoning by exact or approximate inference algorithms that propagate the given evidence through the graphical structure. Despite their applicability in many fields, they have been little used in neuroscience, where they have focused on specific problems, like functional connectivity analysis from neuroimaging data. Here we survey key research in neuroscience where Bayesian networks have been used with different aims: discover associations between variables, perform probabilistic reasoning over the model, and classify new observations with and without supervision. The networks are learned from data of any kind -morphological, electrophysiological, -omics and neuroimaging-, there by broadening the scope-molecular, cellular, structural, functional, cognitive and medical- of the brain aspects to be studied.","Also, we examine how a user can take advantage of these networks for reasoning by exact or approximate inference algorithms that propagate the given evidence through the graphical structure."
"Existing techniques to reconstruct tree models of progression for accumulative processes, such as cancer, seek to estimate causation by combining correlation and a frequentist notion of temporal priority. In this paper, we define a novel theoretical framework called CAPRESE (CAncer PRogression Extraction with Single Edges) to reconstruct such models based on the notion of probabilistic causation defined by Suppes. We consider a general reconstruction setting complicated by the presence of noise in the data due to biological variation, as well as experimental or measurement errors. To improve tolerance to noise we define and use a shrinkage-like estimator. We prove the correctness of our algorithm by showing asymptotic convergence to the correct tree under mild constraints on the level of noise. Moreover, on synthetic data, we show that our approach outperforms the state-of-the-art, that it is efficient even with a relatively small number of samples and that its performance quickly converges to its asymptote as the number of samples increases. For real cancer datasets obtained with different technologies, we highlight biologically significant differences in the progressions inferred with respect to other competing techniques and we also show how to validate conjectured biological relations with progression models.",""
"Resilience refers to the ability to recover from illness or adversity. At the cell, tissue, organ and whole-organism levels, the response to perturbations such as infections and injury involves the acute inflammatory response, which in turn is connected to and controlled by changes in physiology across all organ systems. When coordinated properly, inflammation can lead to the clearance of infection and healing of damaged tissues. However, when either overly or insufficiently robust, inflammation can drive further cell stress, tissue damage, organ dysfunction and death through a feed-forward process of inflammation! damage! inflammation. To address this complexity, we have obtained extensive datasets regarding the dynamics of inflammation in cells, animals and patients, and created data-driven and mechanistic computational simulations of inflammation and its recursive effects on tissue, organ and whole-organism (patho) physiology. Through this approach, we have discerned key regulatory mechanisms, recapitulated in silico key features of clinical trials for acute inflammation and captured diverse, patient-specific outcomes. These insights may allow for the determination of individual-specific tolerances to illness and adversity, thereby defining the role of inflammation in resilience.",""
"Reliability modeling and troubleshooting reasoning involving complex component interactions in complex systems are an active research topic and a critical challenge to be overcome in decision support. In this paper, we propose an innovative concept of decision support methodology for system failure diagnosis and prognosis in complex systems. Advanced causal structure, incorporating domain and engineering knowledge, and a new Bayesian network (BN) representation of system structure and component interaction are proposed. Based on the BN representation, a Bayesian framework is developed to analyze and fuse the multisource information from different hierarchical levels of a system. This capability supports higher-fidelity modeling and assessing of the reliability of the components, the subsystems, and the system as a whole. The feasibility of our advanced causal structure approach has been proven with implementation using test data acquired from electromechanical actuator systems. A case study is successfully conducted to demonstrate the effectiveness of the proposed methodology. The proposed decision support process in integrated system health management will enable enhancements in flight safety and condition-based maintenance by increasing availability and mission effectiveness while reducing maintenance costs.",""
"Knowledge engineering, being a branch of artificial intelligence, offers a variety of methods for elicitation and structuring of knowledge in a given domain. Only a few of them (ontologies and semantic nets, event/probability trees, Bayesian belief networks and event bushes) are known to volcanologists. Meanwhile, the tasks faced by volcanology and the solutions found so far favor a much wider application of knowledge engineering, especially tools for handling dynamic knowledge. This raises some fundamental logical and mathematical problems and requires an organizational effort, but may strongly improve panel discussions, enhance decision support, optimize physical modeling and support scientific collaboration. (C) 2014 Elsevier B.V. All rights reserved.",""
"We talk about dynamic reliability when the reliability parameters of the system, such as the failure rates, vary according to the current state of the system. In this article, several versions of a benchmark on dynamic reliability taken from the literature are examined. Each version deals with particular aspects such as state-dependent failure rates, failure on demand, and repair. In dynamic reliability evaluation, the complete behavior of the system has to be taken into account, instead of the only failure propagation as in fault tree analysis. To this aim, we exploit dynamic Bayesian networks and the software tool RADYBAN (Reliability Analysis with DYnamic BAyesian Networks), with the goal of computing the system unreliability. Because of the coherence between the results returned by dynamic Bayesian network analysis and those obtained by means of other methods, together with the possibility to compute diagnostic indices, we propose dynamic Bayesian network and RADYBAN to be a valid approach to dynamic reliability evaluation.",""
"The authors of this article have developed six probabilistic causal models for critical risks in tunnel works. The details of the models' development and evaluation were reported in two earlier publications of this journal. Accordingly, as a remaining step, this article is focused on the investigation into the use of these models in a real case study project. The use of the models is challenging given the need to provide information on risks that usually are both project and context dependent. The latter is of particular concern in underground construction projects. Tunnel risks are the consequences of interactions between site-and project-specific factors. Large variations and uncertainties in ground conditions as well as project singularities give rise to particular risk factors with very specific impacts. These circumstances mean that existing risk information, gathered from previous projects, is extremely difficult to use in other projects. This article considers these issues and addresses the extent to which prior risk-related knowledge, in the form of causal models, as the models developed for the investigation, can be used to provide useful risk information for the case study project. The identification and characterization of the causes and conditions that lead to failures and their interactions as well as their associated probabilistic information is assumed to be risk-related knowledge in this article. It is shown that, irrespective of existing constraints on using information and knowledge from past experiences, construction risk-related knowledge can be transferred and used from project to project in the form of comprehensive models based on probabilistic-causal relationships. The article also shows that the developed models provide guidance as to the use of specific remedial measures by means of the identification of critical risk factors, and therefore they support risk management decisions. Similarly, a number of limitations of the models are discussed.",""
"Bayesian networks (BNs) have become a popular method of assessing environmental impacts of water management. However, spatial attributes that influence ecological processes are rarely included in BN models. We demonstrate the benefits of combining two-dimensional hydrodynamic and BN modeling frameworks to explicitly incorporate the spatial variability within a system. The impacts of two diversion scenarios on riparian vegetation recruitment at the Gila River, New Mexico, USA, were evaluated using a coupled modeling framework. We focused on five individual sites in the Upper Gila basin. Our BN model incorporated key ecological drivers based on the recruitment box conceptual model, including the timing of seed availability, floodplain inundation, river recession rate, and groundwater depths. Results indicated that recruitment potential decreased by >20% at some locations within each study site, relative to existing conditions. The largest impacts occurring along dynamic fluvial landforms, such as side channels and sand bars. Reductions in recruitment potential varied depending on the diversion scenario. Our unique approach allowed us to evaluate recruitment consequences of water management scenarios at a fine spatial scale, which not only helped differentiate impacts at distinct channel locations but also was useful for informing stakeholders of possible ecological impacts. Our findings also demonstrate that minor changes to river flow may have large ecological implications. Key Points <list id=\"wrcr21155-list-0001\" list-type=\"bulleted\"> <list-item id=\"wrcr21155-li-0001\">Spatial hydrodynamic variability is important for riparian recruitment <list-item id=\"wrcr21155-li-0002\">Bayesian network models explicitly include uncertainties of ecological processes <list-item id=\"wrcr21155-li-0003\">Small water management changes can have large impacts on riparian processes <doi origin=\"wiley\" registered=\"yes\">10.1002/(ISSN)1944-7973</doi",""
"This article considers cloud service composition from a decision analysis perspective. Traditional QoS-aware composition techniques usually consider the qualities available at the time of the composition because compositions are usually immediately consumed. This is fundamentally different in the cloud environment where the cloud service composition typically lasts for a relatively long period of time. The two most important drivers when composing cloud service are the long-term nature of the composition and the economic motivation for outsourcing tasks to the cloud. We propose an economic model, which we represent as a Bayesian network, to select and compose cloud services. We then leverage influence diagrams to model the cloud service composition. We further extend the traditional influence diagram problem to a hybrid one and adopt an extended Shenoy-Shafer architecture to solve such hybrid influence diagrams that include deterministic chance nodes. In addition, analytical and simulation results are presented to show the performance of the proposed composition approach.",""
"In order to make wind energy more competitive, the big expenses for operation and maintenance must be reduced. Consistent decisions that minimize the expected costs can be made based on risk-based methods. Such methods have been implemented for maintenance planning for oil and gas structures, but for offshore wind turbines, the conditions are different and the methods need to be adjusted accordingly. This paper gives an overview of various approaches to solve the decision problem: methods with decision rules based on observed variables, a method with decision rules based on the probability of failure, a method based on limited memory influence diagrams and a method based on the partially observable Markov decision process. The methods with decision rules based on observed variables are easy to use, but can only take the most recent observation into account, when a decision is made. The other methods can take more information into account, and especially, the method based on the Markov decision process is very flexible and accurate. A case study shows that the Markov decision process and decision rules based on the probability of failure are equally good and give lower costs compared to decision rules based on observed variables.",""
"Programmable Logic Controllers (PLC) are widely used in industry. The reliability of the PLC is vital to many critical applications. This paper presents a novel approach to the symbolic analysis of PLC systems. The approach includes, (1) calculating the uncertainty characterization of the PLC system, (2) abstracting the PLC system as a Hidden Markov Model, (3) solving the Hidden Markov Model with domain knowledge, (4) combining the solved Hidden Markov Model and the uncertainty characterization to form a regular Markov model, and (5) utilizing probabilistic model checking to analyze properties of the Markov model. This framework provides automated analysis of both uncertainty calculations and performance measurements, without the need for expensive simulations. A case study of an industrial, automated PLC system demonstrates the effectiveness of our work.",""
"It is very crucial for the byproduct gas system in steel industry to perform an accurate and timely scheduling, which enables to reasonably utilize the energy resources and effectively reduce the production cost of enterprises. In this study, a novel data-driven-based dynamic scheduling thought is proposed for the realtime gas scheduling, in which a probability relationship described by a Bayesian network is modeled to determine the adjustable gas users that impact on the gas tanks level, and to give their scheduling amounts online by maximizing the posterior probability of the users' operational statuses. For the practical applicability, the obtained scheduling solution can be further verified by a recurrent neural network reported in literature. To indicate the effectiveness of the proposed data-driven scheduling method, the real gas flow data coming from a steel plant in China are employed, and the experimental results indicate that the proposed method can provide real-time and scientific gas scheduling solution for the energy system of steel industry.",""
"Automation or selective automation is adopted as a solution to most productivity problems in the hard disk drive (HDD) industry as the industry continues to grow at a 40% compounded annual growth rate. An automated production line for manufacturing the head gimbal assembly (HGA) has been developed as part of the automation solution. In the automated HGA production line, a solder jet ball (SJB) soldering station connects the suspension circuit to the slider body. We propose a Bayesian approach to automated optical inspection (AOI) of the SJB joint in the HGA process, implementing Tree Augmented Naive Bayes Network (TAN-BN) plus check classifier in-situ using GeNIe/SMILE within the inspection software. The system is further enhanced with a result checker, achieving an overall accuracy of 91.52% with 660 production parts in a blind test.","We propose a Bayesian approach to automated optical inspection (AOI) of the SJB joint in the HGA process, implementing Tree Augmented Naive Bayes Network (TAN-BN) plus check classifier in-situ using GeNIe/SMILE within the inspection software."
"We propose Bayesian Network based methods for estimating the cycle by cycle queue length distribution of a signalized intersection. Queue length here is defined as the number of vehicles in a cycle which have experienced significant delays. The data input to the methods are sample travel times from mobile traffic sensors collected between an upstream location and a downstream location of the intersection. The proposed methods first classify traffic conditions and sample scenarios to seven cases. BN models are then developed for each case. The methods are tested using data from NGSIM, a field experiment, and microscopic traffic simulation. The results are satisfactory compared with two specific queue length estimation methods previously developed in the literature. (C) 2014 Elsevier Ltd. All rights reserved.",""
"Bayesian networks (BN) have been increasingly used for habitat suitability modeling of threatened species due to their potential to construct robust models with limited survey data. However, previous applications of this approach have only occurred in countries where human and budget resources are highly available, but the highest concentrations of threatened vertebrates globally are located in the tropics where resources are much more limited. cthe effectiveness of Bayesian networks in generating habitat suitability models in Thailand, a biodiversity-rich country where the knowledge base is typically sparse for a wide range of threatened species. The Bayesian network approach was used to generate habitat suitability maps for 52 threatened vertebrate species in Thailand, using a range of evidence types, from relatively well-documented species with good local knowledge to poorly documented species, with few local experts. Published information and expert knowledge were used to define habitat requirements. Focal species were categorized into 22 groups based on known habitat preferences, and then habitat suitability models were constructed with outcomes represented spatially. Models had a consistent structure with three major components: potential habitat, known range, and threat level. Model classification sensitivity was tested using presence-only field data for 21 species. Habitat models for 12 species were relatively sensitive (>70% congruency between observed and predicted locations), three were moderately congruent, and six were poor. Classification sensitivity tended to be high for bird models and moderate for mammals, whereas sensitivity for reptiles was low, presumably reflecting the relatively poor knowledge base for reptiles in the region. Bayesian network models show significant potential for biodiversity-rich regions with scarce resources, although they require further refinement and testing. It is possible that one detailed ecological study is sufficient to develop a model with reasonable sensitivity, but BN models for species groups with no quantitative data continue to be problematic.","Model classification sensitivity was tested using presence-only field data for 21 species."
"Rationale, aims and objectivesA prototype diagnostic clinical decision support system (CDSS) was developed to assist primary care clinicians (general practitioners) in clinical decision making, aimed at reducing diagnostic errors. The prototype CDSS showed some promise with high levels of validity and reliability; however, issues regarding the underlying Bayesian belief network (BBN), small sample size and use of radiological imaging as a gold standard measure were highlighted that required further investigation before considering clinical testing. MethodsThe prototype CDSS was reviewed and updated based on computer science literature and expert (orthopaedic consultant) opinion. The updated CDSS was tested by comparing its diagnostic outcome against the diagnosis of 93 case studies as determined by expert opinion combined with arthroscopy findings or radiological imaging. ResultsThe updated CDSS showed significant high levels of sensitivity (91%), specificity (98%), positive likelihood ratio (53.12) and negative likelihood ratio (0.08) with a kappa value of 0.88 to a confidence level of 99% compared with expert diagnosis combined with arthroscopy findings or radiological imaging. ConclusionsThe results suggest that the updated CDSS has addressed the issues highlighted from the initial research while maintaining high levels of validity and reliability. The updated CDSS is now ready for clinical testing.",""
"Striving for a probabilistic explication of coherence, scholars proposed a distinction between agreement and striking agreement. In this paper I argue that only the former should be considered a genuine concept of coherence. In a second step the relation between coherence and reliability is assessed. I show that it is possible to concur with common intuitions regarding the impact of coherence on reliability in various types of witness scenarios by means of an agreement measure of coherence. Highlighting the need to separate the impact of coherence and specificity on reliability it is finally shown that a recently proposed vindication of the Shogenji measure qua measure of coherence vanishes.",""
"Uncertainty of cost items is an important aspect of complex projects. Cost uncertainty analysis aims to help decision makers to understand and model different factors affecting funding exposure and ultimately estimate the cost of project. The common practice in cost uncertainty analysis includes breaking the project into cost items and probabilistically capturing the uncertainty of each item. Dependencies between these items are important and if not considered properly may influence the accuracy of cost estimation. However these dependencies are seldom examined and there are theoretical and practical obstacles in modeling them. This paper proposes a quantitative assessment framework integrating the inference process of Bayesian networks (BN) to the traditional probabilistic risk analysis. BNs provide a framework for presenting causal relationships and enable probabilistic inference among a set of variables. The new approach explicitly quantifies uncertainty in project cost and also provides an appropriate method for modeling complex relationships in a project, such as common causal factors, formal use of experts' judgments, and learning from data to update previous beliefs and probabilities. The capabilities of the proposed approach are explained by a simple example. (C) 2014 Elsevier Ltd. APM and IPMA. All rights reserved.","This paper proposes a quantitative assessment framework integrating the inference process of Bayesian networks (BN) to the traditional probabilistic risk analysis."
"In this paper, the authors propose a five-step approach to the problem of identifying semantic correspondences between attributes of two database schemas. It is one of the key challenges in many database applications such as data integration and data warehousing. The authors' research is focused on uninterpreted schema matching, where the column names and column values are uninterpreted or unreliable. The approach implements Bayesian networks, Pearson's correlation and mutual information to identify inter-attribute dependencies. Additionally, the authors propose an extension to their algorithm that allows the user to manually enter the known mappings to improve the automated matching results. The five-step approach also allows data privacy preservation. The authors' evaluation experiments show that the proposed approach enhances the current set of schema matching techniques.",""
"This paper considers the problem of learning multinomial distributions from a sample of independent observations. The Bayesian approach usually assumes a prior Dirichlet distribution about the probabilities of the different possible values. However, there is no consensus on the parameters of this Dirichlet distribution. Here, it will be shown that this is not a simple problem, providing examples in which different selection criteria are reasonable. To solve it the Imprecise Dirichlet Model (IDM) was introduced. But this model has important drawbacks, as the problems associated to learning from indirect observations. As an alternative approach, the Imprecise Sample Size Dirichlet Model (ISSDM) is introduced and its properties are studied. The prior distribution over the parameters of a multinomial distribution is the basis to learn Bayesian networks using Bayesian scores. Here, we will show that the ISSDM can be used to learn imprecise Bayesian networks, also called credal networks when all the distributions share a common graphical structure. Some experiments are reported on the use of the ISSDM to learn the structure of a graphical model and to build supervised classifiers. (C) 2013 Elsevier Inc. All rights reserved.","Some experiments are reported on the use of the ISSDM to learn the structure of a graphical model and to build supervised classifiers."
"In this paper, we use Bayesian Belief Networks (BBNs) to construct a knowledge model for pipe breaks in a water zone. To the authors' knowledge, this is the first attempt to model drinking water distribution system pipe breaks using BBNs. Development of expert systems such as BBNs for analyzing drinking water distribution system data is not only important for pipe break prediction, but is also a first step in preventing water loss and water quality deterioration through the application of machine learning techniques to facilitate data-based distribution system monitoring and asset management. Due to the difficulties in collecting, preparing, and managing drinking water distribution system data, most pipe break models can be classified as \"statistical-physical\" or \"hypothesis-generating.\" We develop the BBN with the hope of contributing to the \"hypothesis-generating\" class of models, while demonstrating the possibility that BBNs might also be used as \"statistical-physical\" models. Our model is learned from pipe breaks and covariate data from a mid-Atlantic United States (U.S.) drinking water distribution system network. BBN models are learned using a constraint-based method, a score-based method, and a hybrid method. Model evaluation is based on log-likelihood scoring. Sensitivity analysis using mutual information criterion is also reported. While our results indicate general agreement with prior results reported in pipe break modeling studies, they also suggest that it may be difficult to select among model alternatives. This model uncertainty may mean that more research is needed for understanding whether additional pipe break risk factors beyond age, break history, pipe material, and pipe diameter might be important for asset management planning. (C) 2014 Elsevier Ltd. All rights reserved.","Due to the difficulties in collecting, preparing, and managing drinking water distribution system data, most pipe break models can be classified as \"statistical-physical\" or \"hypothesis-generating."
"We propose a scoring criterion, named mixture-based factorized conditional log-likelihood (mfCLL), which allows for efficient hybrid learning of mixtures of Bayesian networks in binary classification tasks. The learning procedure is decoupled in foreground and background learning, being the foreground the single concept of interest that we want to distinguish from a highly complex background. The overall procedure is hybrid as the foreground is discriminatively learned, whereas the background is generatively learned. The learning algorithm is shown to run in polynomial time for network structures such as trees and consistent kappa-graphs. To gauge the performance of the mfCLL scoring criterion, we carry out a comparison with state-of-the-art classifiers. Results obtained with a large suite of benchmark datasets show that mfCLL-trained classifiers are a competitive alternative and should be taken into consideration. (C) 2014 Elsevier Ltd. All rights reserved.","We propose a scoring criterion, named mixture-based factorized conditional log-likelihood (mfCLL), which allows for efficient hybrid learning of mixtures of Bayesian networks in binary classification tasks."
"In recent years, pirate attacks against shipping and oil field installations have become more frequent and more serious. This article proposes an innovative solution to the problem of offshore piracy from the perspective of the entire processing chain: from the detection of a potential threat to the implementation of a response. The response to an attack must take into account multiple variables: the characteristics of the threat and the potential target, existing protection tools, environmental constraints, etc. The potential of Bayesian networks is used to manage this large number of parameters and identify appropriate countermeasures. (C) 2014 Elsevier Ltd. All rights reserved.",""
"The majority of Free and Open Source Software (FOSS) developers are mobile and often use different identities in the projects or communities they participate in. These characteristics pose challenges for researchers studying the presence and contributions of developers across multiple repositories. In this paper, we present a methodology, employ various statistical measures, and leverage Bayesian networks to study the patterns of contribution of 502 developers in both Version Control System (VCS) and mailing list repositories in 20 GNOME projects. Our findings shows that only a small percentage of developers are contributing to both repositories and this cohort is making more commits than they are posting messages to mailing lists. The implications of these findings for understanding the patterns of contribution in FOSS projects and on the quality of the final product are discussed. (C) 2013 Elsevier B.V. All rights reserved.",""
"Bayesian networks are not well-formulated for continuous variables. The majority of recent works dealing with Bayesian inference are restricted only to special types of continuous variables such as the conditional linear Gaussian model for Gaussian variables. In this context, an exact Bayesian inference algorithm for clusters of continuous variables which may be approximated by independent component analysis models is proposed. The complexity in memory space is linear and the overfitting problem is attenuated, while the inference time is still exponential. Experiments for multibiometric score fusion with quality estimates are conducted, and it is observed that the performances are satisfactory compared to some known fusion techniques.","The majority of recent works dealing with Bayesian inference are restricted only to special types of continuous variables such as the conditional linear Gaussian model for Gaussian variables."
"Recent multivariate neuroimaging studies have revealed aging-related alterations in brain structural networks. However, the sensory/motor networks such as the auditory, visual and motor networks, have obtained much less attention in normal aging research. In this study, we used Gaussian Bayesian networks (BN), an approach investigating possible inter-regional directed relationship, to characterize aging effects on structural associations between core brain regions within each of these structural sensory/motor networks using volumetric MRI data. We then further examined the discriminability of BN models for the young (N = 109; mean age = 22.73 years, range 20-28) and old (N = 82; mean age = 74.37 years, range 60-90) groups. The results of the BN modeling demonstrated that structural associations exist between two homotopic brain regions from the left and right hemispheres in each of the three networks. In particular, compared with the young group, the old group had significant connection reductions in each of the three networks and lesser connection numbers in the visual network. Moreover, it was found that the aging related BN models could distinguish the young and old individuals with 90.05,73.82, and 88.48% accuracy for the auditory, visual, and motor networks, respectively. Our findings suggest that BN models can be used to investigate the normal aging process with reliable statistical power. Moreover, these differences in structural inter-regional interactions may help elucidate the neuronal mechanism of anatomical changes in normal aging.",""
"This paper proposes an efficient approach based on Bayesian networks for designing the cover of CEMI-based concrete structures for a prescribed durability under carbonation exposure. The approach combines the use of a carbonation model, whose correlated parameters are the nodes of a Bayesian network, and an experimental investigation on accelerated carbonation tests under high pressure (50% atmospheric pressure), the results of which allow the distributions to be updated. Several updates are performed at 5 days, 7 days and 10 days of exposure. The reliability-based design of cover is subsequently carried out using posterior distributions selected on the basis of the Bayes factor and the maximum likelihood criteria. Percentile values of the carbonation depth resulting from measurement and output of the model are compared, underlining the interest of the approach, which is valuable for practical considerations in engineering design. (C) 2014 Elsevier Ltd. All rights reserved.",""
"Microarrays are commonly used in biology because of their ability to simultaneously measure thousands of genes under different conditions. Due to their structure, typically containing a high amount of variables but far fewer samples, scalable network analysis techniques are often employed. In particular, consensus approaches have been recently used that combine multiple microarray studies in order to find networks that are more robust. The purpose of this paper, however, is to combine multiple microarray studies to automatically identify subnetworks that are distinctive to specific experimental conditions rather than common to them all. To better understand key regulatory mechanisms and how they change under different conditions, we derive unique networks from multiple independent networks built using glasso which goes beyond standard correlations. This involves calculating cluster prediction accuracies to detect the most predictive genes for a specific set of conditions. We differentiate between accuracies calculated using cross-validation within a selected cluster of studies (the intra prediction accuracy) and those calculated on a set of independent studies belonging to different study clusters (inter prediction accuracy). Finally, we compare our method's results to related state-of-the art techniques. We explore how the proposed pipeline performs on both synthetic data and real data (wheat and Fusarium). Our results show that subnetworks can be identified reliably that are specific to subsets of studies and that these networks reflect key mechanisms that are fundamental to the experimental conditions in each of those subsets.",""
"Graphical models are widely used to make inferences concerning interplay in multivariate systems. In many applications, data are collected from multiple related but nonidentical units whose underlying networks may differ but are likely to share features. Here we present a hierarchical Bayesian formulation for joint estimation of multiple networks in this nonidentically distributed setting. The approach is general: given a suitable class of graphical models, it uses an exchangeability assumption on networks to provide a corresponding joint formulation. Motivated by emerging experimental designs in molecular biology, we focus on time-course data with interventions, using dynamic Bayesian networks as the graphical models. We introduce a computationally efficient, deterministic algorithm for exact joint inference in this setting. We provide an upper bound on the gains that joint estimation offers relative to separate estimation for each network and empirical results that support and extend the theory, including an extensive simulation study and an application to proteomic data from human cancer cell lines. Finally, we describe approximations that are still more computationally efficient than the exact algorithm and that also demonstrate good empirical performance.","Graphical models are widely used to make inferences concerning interplay in multivariate systems."
"Fault-Trees (FT) and Event-Trees (ET) are useful analytic tools for the assessment of reliability and safety of complex technical systems, and occupational health-safety systems (OHSS), as well. In this work we broaden and expand our previous study regarding the features of FT/ET methods. To clarify this further, in the present article we statistically analyzed the results of a literature survey, concentrated on FT/ET techniques applied in risk assessment (RA) of OHSSs, in order to (i) depict the subsistent situation of their application in various occupational fields, and (ii) enhance their handling and usage in RA of OHSS. The paper consists of two parts, including: (i) a literature survey (for years 2000-2012), concentrated on the main categories of FT/ET techniques concerning OHSS RA, and (ii) an examination and statistical analysis of the corresponding scientific papers published by thirteen representative scientific journals of Elsevier_B.V. and IEEE_Inc. The review shows that: (a) FT/ET techniques are classified into three basic categories (qualitative, quantitative, hybrid), (b) in risk assessment of occupational worksites, FT/ET application is not quite expanded and has not been extensively incorporated in the main RA methodologies of OHSSs, despite their significance, (c) the papers with FT/ETs constitute a very small part of the literature (similar to 0.71%), (d) the qualitative methods present the highest relative occurrence-frequency (59%), and (e) the field of \"Industry\" concentrates the highest percentage of the papers (34%).","The review shows that: (a) FT/ET techniques are classified into three basic categories (qualitative, quantitative, hybrid), (b) in risk assessment of occupational worksites, FT/ET application is not quite expanded and has not been extensively incorporated in the main RA methodologies of OHSSs, despite their significance, (c) the papers with FT/ETs constitute a very small part of the literature (similar to 0."
"ABC-Miner is a Bayesian classification algorithm based on the Ant colony optimization (ACO) metaheuristic. The algorithm learns Bayesian network Augmented Naive-Bayes (BAN) classifiers, where the class node is the parent of all the nodes representing the input variables. However, this assumes the existence of a dependency relationship between the class variable and all the input variables, and this relationship is always a type of \"causal\" (rather than \"effect\") relationship, which restricts the flexibility of the algorithm to learn. In this paper, we extended the ABC-Miner algorithm to be able to learn the Markov blanket of the class variable. Such a produced model has a more flexible Bayesian network classifier structure, where it is not necessary to have a (direct) dependency relationship between the class variable and each of the input variables, and the dependency between the class and the input variables varies from \"causal\" to \"effect\" relationships. In this context, we propose two algorithms: ABC-Miner+(1), in which the dependency relationships between the class and the input variables are defined in a separate phase before the dependency relationships among the input variables are defined, and ABC-Miner+(2), in which the two types of dependency relationships in the Markov blanket classifier are discovered in a single integrated process. Empirical evaluations on 33 UCI benchmark datasets show that our extended algorithms outperform the original version in terms of predictive accuracy, model size and computational time. Moreover, they have shown a very competitive performance against other well-known classification algorithms in the literature.","ABC-Miner is a Bayesian classification algorithm based on the Ant colony optimization (ACO) metaheuristic."
"Motivation: Cancer cell genomes acquire several genetic alterations during somatic evolution from a normal cell type. The relative order in which these mutations accumulate and contribute to cell fitness is affected by epistatic interactions. Inferring their evolutionary history is challenging because of the large number of mutations acquired by cancer cells as well as the presence of unknown epistatic interactions. Results: We developed Bayesian Mutation Landscape (BML), a probabilistic approach for reconstructing ancestral genotypes from tumor samples for much larger sets of genes than previously feasible. BML infers the likely sequence of mutation accumulation for any set of genes that is recurrently mutated in tumor samples. When applied to tumor samples from colorectal, glioblastoma, lung and ovarian cancer patients, BML identifies the diverse evolutionary scenarios involved in tumor initiation and progression in greater detail, but broadly in agreement with prior results.",""
"Forensic DNA casework is currently regarded as one of the most important types of forensic evidence, and important decisions in intelligence and justice are based on it. However, errors occasionally occur and may have very serious consequences. In other domains, error rates have been defined and published. The forensic domain is lagging behind concerning this transparency for various reasons. In this paper we provide definitions and observed frequencies for different types of errors at the Human Biological Traces Department of the Netherlands Forensic Institute (NFI) over the years 2008-2012. Furthermore, we assess their actual and potential impact and describe how the NFI deals with the communication of these numbers to the legal justice system. We conclude that the observed relative frequency of quality failures is comparable to studies from clinical laboratories and genetic testing centres. Furthermore, this frequency is constant over the five-year study period. The most common causes of failures related to the laboratory process were contamination and human error. Most human errors could be corrected, whereas gross contamination in crime samples often resulted in irreversible consequences. Hence this type of contamination is identified as the most significant source of error. Of the known contamination incidents, most were detected by the NFI quality control system before the report was issued to the authorities, and thus did not lead to flawed decisions like false convictions. However in a very limited number of cases crucial errors were detected after the report was issued, sometimes with severe consequences. Many of these errors were made in the post-analytical phase. The error rates reported in this paper are useful for quality improvement and benchmarking, and contribute to an open research culture that promotes public trust. However, they are irrelevant in the context of a particular case. Here case-specific probabilities of undetected errors are needed. These should be reported, separately from the match probability, when requested by the court or when there are internal or external indications for error. It should also be made clear that there are various other issues to consider, like DNA transfer. Forensic statistical models, in particular Bayesian networks, may be useful to take the various uncertainties into account and demonstrate their effects on the evidential value of the forensic DNA results. (C) 2014 Elsevier Ireland Ltd. All rights reserved.",""
"Models for genome-wide prediction and association studies usually target a single phenotypic trait. However, in animal and plant genetics it is common to record information on multiple phenotypes for each individual that will be genotyped. Modeling traits individually disregards the fact that they are most likely associated due to pleiotropy and shared biological basis, thus providing only a partial, confounded view of genetic effects and phenotypic interactions. In this article we use data from a Multiparent Advanced Generation Inter-Cross (MAGIC) winter wheat population to explore Bayesian networks as a convenient and interpretable framework for the simultaneous modeling of multiple quantitative traits. We show that they are equivalent to multivariate genetic best linear unbiased prediction (GBLUP) and that they are competitive with single-trait elastic net and single-trait GBLUP in predictive performance. Finally, we discuss their relationship with other additive-effects models and their advantages in inference and interpretation. MAGIC populations provide an ideal setting for this kind of investigation because the very low population structure and large sample size result in predictive models with good power and limited confounding due to relatedness.","Finally, we discuss their relationship with other additive-effects models and their advantages in inference and interpretation."
"We proposed a novel measure of mutual information known as Integration to Segregation (I2S) explaining the relationship between two features. We investigated its nontrivial characteristics while comparing its performance in terms of class imbalance measures. We have shown that I2S possesses characteristics useful in identifying sink and source (parent) in a conventional directed acyclic graph in structure learning technique such as Bayesian Belief Network. We empirically indicated that identifying sink and its parent using conventional scoring function is not much impressive in maximizing discriminant function because it is unable to identify; best topology. However, I2S is capable of significantly maximizing discriminant function with the potential of identifying the network topology in structure learning.",""
"A major challenge limiting the practical adoption of Bayesian networks for diagnosis in manufacturing systems is the difficulty of constructing the models from expert knowledge. A key possibility for tackling this limitation is believed to be through utilising the available sources of design information that is readily available as part of the engineering design process. Some of the most notable sources of such design information include formal domain models such as product-process-equipment design ontologies which are becoming a widely accepted mean for formally capturing and communicating design information. This makes these ontologies a valuable knowledge source for automatic and semi-automatic generation of Bayesian networks, instead of the entirely expert-driven traditional approach. However, design ontologies lack on the fault-related information side as they are primarily aimed at capturing the intended behaviour of the designed system. To bridge this gap, we propose integrating failure mode and effect analysis (FMEA) information into design ontologies and using the resulting integral models for the generation of Bayesian diagnostic networks. We also propose a method for the generation process and demonstrate the validity of the approach with an industrial case study.",""
"To specify a Bayesian network, a local distribution in the form of a conditional probability table, often of an effect conditioned on its n causes, needs to be acquired, one for each non-root node. Since the number of parameters to be assessed is generally exponential in n, improving the efficiency is an important concern in knowledge engineering. Non-impeding noisy-AND (NIN-AND) tree causal models reduce the number of parameters to being linear in n, while explicitly expressing both reinforcing and undermining interactions among causes. The key challenge in NIN-AND tree modeling is the acquisition of the NIN-AND tree structure. In this paper, we formulate a concise structure representation and an expressive causal interaction function of NIN-AND trees. Building on these representations, we propose two structural acquisition methods, which are applicable to both elicitation-based and machine learning-based acquisitions. Their accuracy is demonstrated through experimental evaluations.",""
"Background and Objective: The management of rare tumors is difficult because of limited information on natural history. Our objective was to describe a long-term comprehensive prospective database with the assumption that with careful attention to patient, predisposing tumor and treatment variables, valuable knowledge could be obtained that could guide management. Methods: In July of 1982, we began a prospective database of all adult patients admitted to our institution for a surgical procedure for soft tissue sarcoma. Patients were included if they had primary, locally recurrent or metastatic disease undergoing a surgical procedure. Results: Over 3 decades, we entered 10,000 patients into our prospective soft tissue sarcoma database. Data were entered on a weekly or biweekly schedule with full participation of a multidisciplinary team and a dedicated sarcoma pathologist. Extensive information is available from this database. In this article, we describe distribution by site, histopathology, sex, size, and grade. We utilize this information along with outcome data for local recurrence, distant recurrence, disease specific, and overall survival. The value of molecular diagnosis is illustrated. Conclusions: Continuous prospective long-term databases are important to obtain knowledge particularly for rare tumors. Such data can be a rich resource for the development of prognostic indicators including nomograms and can be analyzed by Bayesian Belief Networks. These long-term data linked to collection of tumor and germ-line tissue at the time of an initial procedure will remain a resource for future decades.",""
"Introduction: Oral mucositis remains a common, symptomatically devastating toxicity of common drug and radiation regimens used to treat cancer. The current limited treatment options are likely to be supplemented by an active pipeline in which innovative compounds are being developed which target the underlying biological pathways associated with the condition. Areas covered: A systematic literature search to identify English-language articles published from January 1995 to December 2013 was performed using the PubMed/MEDLINE and Google databases. A supplemental search was conducted of the ClinTrials database. Searches were driven by the following keywords: oral mucositis, mucositis, stomatitis, radiation therapy toxicity, chemotherapy toxicity, and risk prediction. Expert opinion: Discovery and implementation of effective interventions for oral mucositis have been difficult. To date, in the U.S. only palifermin (Kepivance) have been approved and its use is currently confined to patients undergoing stem cell transplant, a very small portion of individuals at risk for mucositis. Fortunately, with the identification of a number of mechanistic pathways which underlie the mucositis' pathogenesis, a rich and diverse pipeline of both topical and systemic compounds are in various stages of pre-clinical and clinical development.",""
"Green spaces in the living environment may provide a meeting place and support social contacts. When people get older they, in general, are less mobile and have more limited activity spaces. At the same time they are faced with smaller social networks due to social and health related changes. Green spaces in their direct living environment are therefore important to support their needs. The aim of this study was to better understand the nature of the relationship between various types of green spaces in the direct living environment and the extent and nature of social contacts of the aging generation, taking into account socio-demographics and other physical and social environmental characteristics. Data for this study were obtained from a survey about living surroundings from a national representative sample of 1501 persons in the age category of 60 years and over in the Netherlands conducted in 2009. The survey included both subjective and objective measurements of the direct living environment of the respondents. Specifically, a Bayesian belief network was used to formulate and estimate the direct and indirect relationships between the selected variables. Results show that social contacts among neighbors are mainly influenced by the availability of trees and grass and the perceived level of green. Green spaces support social contacts in the neighborhood. However, the safety and maintenance of the green spaces are also important; high quality green spaces support social contacts between neighbors and strengthen communities for the aging population. (C) 2014 Elsevier B.V. All rights reserved.",""
"Objectives: To assess referee bias with respect to fouls and penalty kicks awarded by taking explanatory factors into consideration. Design: We present a novel Bayesian network model for assessing referee bias with respect to fouls and penalty kicks awarded. The model is applied to the 2011-12 English Premier League season. Method: Unlike previous studies, the model takes into consideration explanatory factors which, if ignored, can lead to biased assessments of referee bias. For example, a team may be awarded more penalties simply because it attacks more, not because referees are biased in its favour. Hence, we incorporate causal factors such as possession, time spent in the opposition penalty box, etc. prior to estimating the degree of penalty kicks bias. Results: We found fairly strong referee bias, based on penalty kicks awarded, in favour of certain teams when playing at home. Specifically, the two teams (Manchester City and Manchester United) who finished first and second appear to have benefited from bias that cannot be fully justified by the explanatory factors. Conversely Arsenal, a team of similar popularity and wealth and who finished third, benefited least of all 20 teams from referee bias at home with respect to penalty kicks awarded. Conclusions: Among our conclusions are that, in contrast to many previous studies, being the home team does not in itself result in positive referee bias. More importantly, the model is able to explain significant discrepancies of penalty kicks bias into non-significant after accounting for the explanatory factors. (C) 2014 Elsevier Ltd. All rights reserved.",""
"In this paper, we propose novel hybrid approaches to annotate videos in valence and arousal spaces by using users' electroencephalogram (EEG) signals and video content. Firstly, several audio and visual features are extracted from video clips and five frequency features are extracted from each channel of the EEG signals. Secondly, statistical analyses are conducted to explore the relationships among emotional tags, EEG and video features. Thirdly, three Bayesian Networks are constructed to annotate videos by combining the video and EEG features at independent feature-level fusion, decision-level fusion and dependent feature-level fusion. In order to evaluate the effectiveness of our approaches, we designed and conducted the psychophysiological experiment to collect data, including emotion-induced video clips, users' EEG responses while watching the selected video clips, and emotional video tags collected through participants' self-report after watching each clip. The experimental results show that the proposed fusion methods outperform the conventional emotional tagging methods that use either video or EEG features alone in both valence and arousal spaces. Moreover, we can narrow down the semantic gap between the low-level video features and the users' high-level emotional tags with the help of EEG features.",""
"Ecosystem-based management is widely regarded as a method to improve the way we manage our coastal marine resources and ecosystems. Effective ecosystem-based management relies upon synthesizing our scientific knowledge and transferring this knowledge into management actions. Integrated ecosystem assessment is a framework to conduct this scientific synthesis and transfer information to resource managers. Portions of the framework were applied to build consensus on the focal ecosystem components and processes that are characteristic of a sustainable South Florida coastal ecosystem that is producing ecosystem services at the level society desires. Consensus was developed through facilitated meetings that aimed to conceptualize the ecosystem, develop ecosystem indicators, and conduct risk analysis. Resource managers, researchers, academics, and non-governmental organizations participated in these meetings and contributed to the synthesis of science and a myriad of science communications to transfer information to decision makers and the public. A proof of concept Bayesian Belief Network was developed to explore integrating the results of this assessment into an interactive management scenario evaluation tool. The four year effort resulted in the development of a research and management coordination network in South Florida that should provide the foundation for implementing ecosystem-based resource management across multiple agencies. (C) 2014 Elsevier Ltd. All rights reserved.",""
"Model reduction techniques have been widely used in modeling of high-dimensional stochastic input in uncertainty quantification tasks. However, the probabilistic modeling of random variables projected into reduced-order spaces presents a number of computational challenges. Due to the curse of dimensionality, the underlying dependence relationships between these random variables are difficult to capture. In this work, a probabilistic graphical model based approach is employed to learn the dependence by running a number of conditional independence tests using observation data. Thus a probabilistic model of the joint PDF is obtained and the PDF is factorized into a set of conditional distributions based on the dependence structure of the variables. The estimation of the joint PDF from data is then transformed to estimating conditional distributions under reduced dimensions. To improve the computational efficiency, a polynomial chaos expansion is further applied to represent the random field in terms of a set of standard random variables. This technique is combined with both linear and nonlinear model reduction methods. Numerical examples are presented to demonstrate the accuracy and efficiency of the probabilistic graphical model based stochastic input models. (C) 2014 Elsevier Inc. All rights reserved.",""
"Learning from observation (LfO), also known as learning from demonstration, studies how computers can learn to perform complex tasks by observing and thereafter imitating the performance of a human actor. Although there has been a significant amount of research in this area, there is no agreement on a unified terminology or evaluation procedure. In this paper, we present a theoretical framework based on Dynamic-Bayesian Networks (DBNs) for the quantitative modeling and evaluation of LfO tasks. Additionally, we provide evidence showing that: (1) the information captured through the observation of agent behaviors occurs as the realization of a stochastic process (and often not just as a sample of a state-to-action map); (2) learning can be simplified by introducing dynamic Bayesian models with hidden states for which the learning and model evaluation tasks can be reduced to minimization and estimation of some stochastic similarity measures such as crossed entropy. (C) 2014 Elsevier Ltd. All rights reserved.",""
"A novel Score-based Physarum Learner algorithm for learning Bayesian Network structure from data is introduced and shown to outperform common score based structure learning algorithms for some benchmark data sets. The Score-based Physarum Learner first initializes a fully connected Physarum-Maze with random conductances. In each Physarum Solver iteration, the source and sink nodes are changed randomly, and the conductances are updated. Connections exceeding a predefined conductance threshold are considered as Bayesian Network edges, and the score of the connected nodes are examined in both directions. A positive or negative feedback is given to the edge conductance based on the calculated scores. Due to randomness in selecting connections for evaluation, an ensemble of Score-based Physarum Learner is used to build the final Bayesian Network structure. (c) 2014 Elsevier Ltd. All rights reserved.",""
"The evaluation of the initial direction and velocity of the fragments generated in the fragmentation of a vessel due to internal pressure is an important information in the assessment of damage caused by fragments, in particular within the quantitative risk assessment (QRA) of chemical and process plants. In the present study an approach is proposed to the identification and validation of probability density functions (pdfs) for the initial direction of the fragments. A detailed review of a large number of past accidents provided the background information for the validation procedure. A specific method was developed for the validation of the proposed pdfs. Validated pdfs were obtained for both the vertical and horizontal angles of projection and for the initial velocity of the fragments. (C) 2014 Elsevier B.V. All rights reserved.",""
"In the context of genetics and breeding research on multiple phenotypic traits, reconstructing the directional or causal structure between phenotypic traits is a prerequisite for quantifying the effects of genetic interventions on the traits. Current approaches mainly exploit the genetic effects at quantitative trait loci (QTLs) to learn about causal relationships among phenotypic traits. A requirement for using these approaches is that at least one unique QTL has been identified for each trait studied. However, in practice, especially for molecular phenotypes such as metabolites, this prerequisite is often not met due to limited sample sizes, high noise levels and small QTL effects. Here, we present a novel heuristic search algorithm called the QTL+phenotype supervised orientation (QPSO) algorithm to infer causal directions for edges in undirected phenotype networks. The two main advantages of this algorithm are: first, it does not require QTLs for each and every trait; second, it takes into account associated phenotypic interactions in addition to detected QTLs when orienting undirected edges between traits. We evaluate and compare the performance of QPSO with another state-of-the-art approach, the QTL-directed dependency graph (QDG) algorithm. Simulation results show that our method has broader applicability and leads to more accurate overall orientations. We also illustrate our method with a real-life example involving 24 metabolites and a few major QTLs measured on an association panel of 93 tomato cultivars. Matlab source code implementing the proposed algorithm is freely available upon request.",""
"We introduce a methodology for sensitivity analysis of evidence variables in Gaussian Bayesian networks. Knowledge of the posterior probability distribution of the target variable in a Bayesian network, given a set of evidence, is desirable. However, this evidence is not always determined; in fact, additional information might be requested to improve the solution in terms of reducing uncertainty. In this study we develop a procedure, based on Shannon entropy and information theory measures, that allows us to prioritize information according to its utility in yielding a better result. Some examples illustrate the concepts and methods introduced. (C) 2014 Elsevier Inc. All rights reserved.",""
"Our work aims at developing or expliciting bridges between Bayesian networks (BNs) and Natural Exponential Families, by proposing discrete exponential Bayesian networks as a generalization of usual discrete ones. We introduce a family of prior distributions which generalizes the Dirichlet prior applied on discrete Bayesian networks, and then we determine the overall posterior distribution. Subsequently, we develop the Bayesian estimators of the parameters, and a new score function that extends the Bayesian Dirichlet score for BN structure learning. Our goal is to determine empirically in which contexts some of our discrete exponential BNs (Poisson deBNs) can be an effective alternative to usual BNs for density estimation. (C) 2014 Published by Elsevier B.V.",""
"The paper presents a new approach for modeling important geological elements, such as reservoir, trap, and source, in a unified statistical model. This joint modeling of these geological variables is useful for reliable prospect evaluation, and provides a framework for consistent decision making under uncertainty. A Bayesian network (BN), involving different kinds of dependency structures, is used to model the correlation within the various geological elements and to couple the elements. On the basis of the constructed network, an optimal sequential exploration strategy is established with dynamic programming (DP). This strategy is useful for selecting the first prospect to explore and for making the decisions that should follow, depending on the outcome of the first well. A risk-neutral decision maker will continue exploring new wells as long as the expected profit is positive. The model and choice of exploration strategy are tailored to a case study represented by five prospects in a salt basin, but they will also be useful for other contexts. For the particular case study, we show how the strategy clearly depends on the exploration and development cost and the expected volumes and recovery factors. The most lucrative prospect tends to be selected first, but the sequential decisions depend on the outcome of the exploration well in this first prospect.",""
"Several existing methods have been shown to consistently estimate causal direction assuming linear or some form of nonlinear relationship and no latent confounders. However, the estimation results could be distorted if either assumption is violated. We develop an approach to determining the possible causal direction between two observed variables when latent confounding variables are present. We first propose a new linear non-Gaussian acyclic structural equation model with individual-specific effects that are sometimes the source of confounding. Thus, modeling individual-specific effects as latent variables allows latent confounding to be considered. We then propose an empirical Bayesian approach for estimating possible causal direction using the new model. We demonstrate the effectiveness of our method using artificial and real-world data.",""
"Code clones have always been a double edged sword in software development. On one hand, it is a very convenient way to reuse existing code, and to save coding effort. On the other hand, since developers may need to ensure consistency among cloned code segments, code clones can lead to extra maintenance effort and even bugs. Recently studies on the evolution of code clones show that only some of the code clones experience consistent changes during their evolution history. Therefore, if we can accurately predict whether a code clone will experience consistent changes, we will be able to provide useful recommendations to developers on leveraging the convenience of some code cloning operations, while avoiding other code cloning operations to reduce future consistency maintenance effort. In this paper, we define a code cloning operation as consistency-maintenance-required if its generated code clones experience consistent changes in the software evolution history, and we propose a novel approach that automatically predicts whether a code cloning operation requires consistency maintenance at the time point of performing copy-and-paste operations. Our insight is that whether a code cloning operation requires consistency maintenance may relate to the characteristics of the code to be cloned and the characteristics of its context. Based on a number of attributes extracted from the cloned code and the context of the code cloning operation, we use Bayesian Networks, a machine-learning technique, to predict whether an intended code cloning operation requires consistency maintenance. We evaluated our approach on four subjects-two large-scale Microsoft software projects, and two popular open-source software projects-under two usage scenarios: 1) recommend developers to perform only the cloning operations predicted to be very likely to be consistency-maintenance-free, and 2) recommend developers to perform all cloning operations unless they are predicted very likely to be consistency-maintenance-required. In the first scenario, our approach is able to recommend developers to perform more than 50 percent cloning operations with a precision of at least 94 percent in the four subjects. In the second scenario, our approach is able to avoid 37 to 72 percent consistency-maintenance-required code clones by warning developers on only 13 to 40 percent code clones, in the four subjects.",""
"Touch-sensitive surfaces have become a predominant input medium for computing devices. In particular, multitouch capability of these devices has given rise to developing rich interaction vocabularies for \"real\" direct manipulation of user interfaces. However, the richness and flexibility of touch interaction often comes with significant complexity for programming these behaviors. Particularly, finger touches, though intuitive, are imprecise and lead to ambiguity. Touch input often involves coordinated movements of multiple fingers as opposed to the single pointer of a traditional WIMP interface. It is challenging in not only detecting the intended motion carried out by these fingers but also in determining the target objects being manipulated due to multiple focus points. Currently, developers often need to build touch behaviors by dealing with raw touch events that is effort consuming and error-prone. In this article, we present Touch, a tool that allows developers to easily specify their desired touch behaviors by demonstrating them live on a touch-sensitive device or selecting them from a list of common behaviors. Developers can then integrate these touch behaviors into their application as resources and via an API exposed by our runtime framework. The integrated tool support enables developers to think and program optimistically about how these touch interactions should behave, without worrying about underlying complexity and technical details in detecting target behaviors and invoking application logic. We discuss the design of several novel inference algorithms that underlie these tool supports and evaluate them against a multitouch dataset that we collected from end users. We also demonstrate the usefulness of our system via an example application.","We discuss the design of several novel inference algorithms that underlie these tool supports and evaluate them against a multitouch dataset that we collected from end users."
"Bayesian Belief Network (BBN) is an appealing classification model for learning causal and noncausal dependencies among a set of query variables. It is a challenging task to learning BBN structure from observational data because of pool of large number of candidate network structures. In this study, we have addressed the issue of goodness of data fitting versus model complexity. While doing so, we have proposed discriminant function which is non-parametric, free of implicit assumptions but delivering better classification accuracy in structure learning. The contribution in this study is twofold, first contribution (discriminant function) is in BBN structure learning and second contribution is for Decision Stump classifier. While designing the novel discriminant function, we analyzed the underlying relationship between the characteristics of data and accuracy of decision stump classifier. We introduced a meta characteristic measure AMfDS (herein known as Affinity Metric for Decision Stump) which is quite useful in prediction of classification accuracy of Decision Stump. AMfDS requires a single scan of the dataset.","Bayesian Belief Network (BBN) is an appealing classification model for learning causal and noncausal dependencies among a set of query variables."
"Members of health social networks may be susceptible to privacy leaks by the amount of information they leave behind. The threat to privacy increases when members of these networks reuse their pseudonyms in other social networks. The risk of re-identifying users from such networks requires quantitative estimates to evaluate its magnitude. The estimates will enable managers and members of health social communities to take corrective measures. We introduce a new re-identification attack, the social network attack, that takes advantage of the fact that users reuse their pseudonyms. To demonstrate the attack, we establish links between MedHelp and Twitter (two popular social networks) based on matching pseudonyms. We used Bayesian networks to model the re-identification risk and used stylometric techniques to identify the strength of the links. On the basis of our model 7-11. 8% of the MedHelp members in the sample population who reused their pseudonyms in Twitter were re-identifiable compared with 1% who did not. The risk estimates were measured at the 5% risk threshold. Our model was able to re-identify users with a sensitivity of 41% and specificity of 96%. The potential for re-identification increases as more data is accumulated from these profiles, which makes the threat of re-identification more serious.",""
"Muscle fiber images play an important role in the medical diagnosis and treatment of many muscular diseases. The number of nuclei in skeletal muscle fiber images is a key biomarker of the diagnosis of muscular dystrophy. In nuclei segmentation one primary challenge is to correctly separate the clustered nuclei. In this article, we developed an image processing pipeline to automatically detect, segment, and analyze nuclei in microscopic image of muscle fibers. The pipeline consists of image pre-processing, identification of isolated nuclei, identification and segmentation of clustered nuclei, and quantitative analysis. Nuclei are initially extracted from background by using local Otsu's threshold. Based on analysis of morphological features of the isolated nuclei, including their areas, compactness, and major axis lengths, a Bayesian network is trained and applied to identify isolated nuclei from clustered nuclei and artifacts in all the images. Then a two-step refined watershed algorithm is applied to segment clustered nuclei. After segmentation, the nuclei can be quantified for statistical analysis. Comparing the segmented results with those of manual analysis and an existing technique, we find that our proposed image processing pipeline achieves good performance with high accuracy and precision. The presented image processing pipeline can therefore help biologists increase their throughput and objectivity in analyzing large numbers of nuclei in muscle fiber images. (C) 2014 Wiley Periodicals, Inc.",""
"Population aging has been occurring as a global phenomenon with heterogeneous consequences in both developed and developing countries. Neurodegenerative diseases, such as Alzheimer's Disease (AD), have high prevalence in the elderly population. Early diagnosis of this type of disease allows early treatment and improves patient quality of life. This paper proposes a Bayesian network decision model for supporting diagnosis of dementia, AD and Mild Cognitive Impairment (MCI). Bayesian networks are well-suited for representing uncertainty and causality, which are both present in clinical domains. The proposed Bayesian network was modeled using a combination of expert knowledge and data-oriented modeling. The network structure was built based on current diagnostic criteria and input from physicians who are experts in this domain. The network parameters were estimated using a supervised learning algorithm from a dataset of real clinical cases. The dataset contains data from patients and normal controls from the Duke University Medical Center (Washington, USA) and the Center for Alzheimer's Disease and Related Disorders (at the Institute of Psychiatry of the Federal University of Rio de Janeiro, Brazil). The dataset attributes consist of predisposal factors, neuropsychological test results, patient demographic data, symptoms and signs. The decision model was evaluated using quantitative methods and a sensitivity analysis. In conclusion, the proposed Bayesian network showed better results for diagnosis of dementia, AD and MCI when compared to most of the other well-known classifiers. Moreover, it provides additional useful information to physicians, such as the contribution of certain factors to diagnosis. (C) 2014 Elsevier Ltd. All rights reserved.","In conclusion, the proposed Bayesian network showed better results for diagnosis of dementia, AD and MCI when compared to most of the other well-known classifiers."
"We recently introduced the concept of a Hyperbolic Dirac Net (HDN) for medical inference on the grounds that, while the traditional Bayes Net (BN) is popular in medicine, it is not suited to that domain: there are many interdependencies such that any \"node\" can be ultimately conditional upon itself. A traditional BN is a directed acyclic graph by definition, while the HDN is a bidirectional general graph closer to a diffuse \"field\" of influence. Cycles require bidirectionality; the HDN uses a particular type of imaginary number from Dirac's quantum mechanics to encode it. Comparison with the BN is made alongside a set of recipes for converting a given BN to an HDN, also adding cycles that do not usually require reiterative methods. This conversion is called the P-method. Conversion to cycles can sometimes be difficult, but more troubling was that the original BN had probabilities needing adjustment to satisfy realism alongside the important property called \"coherence\". The more general and simpler K-method, not dependent on the BN, is usually (but not necessarily) derived by data mining, and is therefore also introduced. As discussed, BN developments may converge to an HDN-like concept, so it is reasonable to consider the HDN as a BN extension. (C) 2014 Elsevier Ltd. All rights reserved.","We recently introduced the concept of a Hyperbolic Dirac Net (HDN) for medical inference on the grounds that, while the traditional Bayes Net (BN) is popular in medicine, it is not suited to that domain: there are many interdependencies such that any \"node\" can be ultimately conditional upon itself."
"In this paper the fault detection problem is solved using an alternative methodology based on a fuzzy/Bayesian strategy combining a Bayesian network and the fuzzy set theory. The new important issue in this proposed methodology is to address uncertainties in the input of the Bayesian Network. This contribution is possible since the fuzzy set theory is used as the knowledge representation. To illustrate the technique, the fault detection problem in induction machine stator-winding is considered. Specifically, the faults in the induction machine stator-winding are detected by a state change of stator current. Simulation results are presented to illustrate the advance of the proposed methodology when compared to standard Bayesian network. (C) 2014 Elsevier B.V. All rights reserved.",""
"Canopy leaf area index (LAI) is a quantitative measure of canopy foliar area. LAI values can be derived from Moderate Resolution Imaging Spectroradiometer (MODIS) images. In this paper, MODIS pixels from a heterogeneous forest located in The Netherlands were decomposed using the linear mixture model using class fractions derived from a high-resolution aerial image. Gaussian Bayesian networks (GBNs) were applied to improve the spatio-temporal estimation of LAI by combining the decomposed MODIS images with a spatial version of physiological principles predicting growth (3PG) model output at different moments in time. Results showed that the spatial-temporal output obtained with the GBN was 40% more accurate than the spatial 3PG, with a root-mean-square error below 0.25. We concluded that the GBNs improved the spatial estimation of LAI values of a heterogeneous forest by combining a spatial forest growth model with satellite imagery.",""
"Background: Network inference deals with the reconstruction of molecular networks from experimental data. Given N molecular species, the challenge is to find the underlying network. Due to data limitations, this typically is an ill-posed problem, and requires the integration of prior biological knowledge or strong regularization. We here focus on the situation when time-resolved measurements of a system's response after systematic perturbations are available. Results: We present a novel method to infer signaling networks from time-course perturbation data. We utilize dynamic Bayesian networks with probabilistic Boolean threshold functions to describe protein activation. The model posterior distribution is analyzed using evolutionary MCMC sampling and subsequent clustering, resulting in probability distributions over alternative networks. We evaluate our method on simulated data, and study its performance with respect to data set size and levels of noise. We then use our method to study EGF-mediated signaling in the ERBB pathway. Conclusions: Dynamic Probabilistic Threshold Networks is a new method to infer signaling networks from time-series perturbation data. It exploits the dynamic response of a system after external perturbation for network reconstruction. On simulated data, we show that the approach outperforms current state of the art methods. On the ERBB data, our approach recovers a significant fraction of the known interactions, and predicts novel mechanisms in the ERBB pathway.","Background: Network inference deals with the reconstruction of molecular networks from experimental data."
"This paper proposes Bayesian networks (BNs) that combine polarization corrected temperature (PCT) and scattering index (SI) methods to identify rainfall intensity. To learn BN network structures, meta-heuristic techniques including tabu search (TS), simulated annealing (SA) and genetic algorithm (GA) were empirically evaluated and compared for efficiency. The proposed models were applied to the Tanshui river basin in Taiwan. The meteorological data from the Special Sensor Microwave/Imager (SSM/I) of the National Oceanic and Atmospheric Administration (NOAA) comprises seven passive microwave brightness temperatures, and was used to detect rain rates. The data consisted of 71 typhoons affecting the watershed during 2000-2012. A preliminary analysis using simple meta-heuristic BNs identified the main attributes, namely the brightness temperatures of 19,22,37 and 85 GHz for rainfall retrieval. Based on the preliminary analysis of a simple BN run, the advanced BNs combined with SI and PCT successfully demonstrated improved rain rate retrieval accuracy. To compare the proposed meta-heuristic BNs, the traditional SI method, the SI-based support vector regression model (SI-SVR), and artificial neural network (ANN) were used as benchmarks. The results showed that (1) meta-heuristic BN techniques can be used to identify the vital attributes of the rainfall retrieval problem and their causal relationships and (2) according to a comparison of BNs combined with PCT and SI and artificial intelligence (AI)-based models (SI-SVR and ANN), in heavy, torrential, and pouring rainfall, models of BNs combined with PCT and SI provide a superior retrieval performance than that of AI-based models. Therefore, this study confirms that meta-heuristic BNs combined with PCT and SI is an efficient tool for addressing rainfall retrieval problems. (C) 2014 Elsevier B.V. All rights reserved.","To compare the proposed meta-heuristic BNs, the traditional SI method, the SI-based support vector regression model (SI-SVR), and artificial neural network (ANN) were used as benchmarks."
"High Throughput Biological Data (HTBD) requires detailed analysis methods and from a life science perspective, these analysis results make most sense when interpreted within the context of biological pathways. Bayesian Networks (BNs) capture both linear and nonlinear interactions and handle stochastic events in a probabilistic framework accounting for noise making them viable candidates for HTBD analysis. We have recently proposed an approach, called Bayesian Pathway Analysis (BPA), for analyzing HTBD using BNs in which known biological pathways are modeled as BNs and pathways that best explain the given HTBD are found. BPA uses the fold change information to obtain an input matrix to score each pathway modeled as a BN. Scoring is achieved using the Bayesian-Dirichlet Equivalent method and significance is assessed by randomization via bootstrapping of the columns of the input matrix. In this study, we improve on the BPA system by optimizing the steps involved in \"Data Preprocessing and Discretization'', \"Scoring'', \"Significance Assessment'', and \"Software and Web Application''. We tested the improved system on synthetic data sets and achieved over 98% accuracy in identifying the active pathways. The overall approach was applied on real cancer microarray data sets in order to investigate the pathways that are commonly active in different cancer types. We compared our findings on the real data sets with a relevant approach called the Signaling Pathway Impact Analysis (SPIA).",""
"Laboratory experimentation is increasingly concerned with systems whose dynamical behaviour can be affected by a very large number of variables. Objectives of experimentation on such systems are generally both the optimisation of some experimental responses and efficiency of experimentation in terms of low investment of resources and low impact on the environment. Design and modelling for high dimensional systems with these objectives present hard and challenging problems, to which much current research is devoted. In this paper, we introduce a novel approach based on the evolutionary principle and Bayesian network models. This approach can discover optimum values while testing just a very limited number of experimental points. The very good performance of the approach is shown both in a simulation analysis and biochemical study concerning the emergence of new functional bio-entities. (C) 2014 Elsevier B.V. All rights reserved.",""
"Background: Chronic infection with hepatitis C virus (HCV) is a risk factor for liver diseases such as fibrosis, cirrhosis and hepatocellular carcinoma. HCV genetic heterogeneity was hypothesized to be associated with severity of liver disease. However, no reliable viral markers predicting disease severity have been identified. Here, we report the utility of sequences from 3 HCV 1b genomic regions, Core, NS3 and NS5b, to identify viral genetic markers associated with fast and slow rate of fibrosis progression (RFP) among patients with and without liver transplantation (n = 42). Methods: A correlation-based feature selection (CFS) method was used to detect and identify RFP-relevant viral markers. Machine-learning techniques, linear projection (LP) and Bayesian Networks (BN), were used to assess and identify associations between the HCV sequences and RFP. Results: Both clustering of HCV sequences in LP graphs using physicochemical properties of nucleotides and BN analysis using polymorphic sites showed similarities among HCV variants sampled from patients with a similar RFP, while distinct HCV genetic properties were found associated with fast or slow RFP. Several RFP-relevant HCV sites were identified. Computational models parameterized using the identified sites accurately associated HCV strains with RFP in 70/30 split cross-validation (90-95% accuracy) and in validation tests (85-90% accuracy). Validation tests of the models constructed for patients with or without liver transplantation suggest that the RFP-relevant genetic markers identified in the HCV Core, NS3 and NS5b genomic regions may be useful for the prediction of RFP regardless of transplant status of patients. Conclusions: The apparent strong genetic association to RFP suggests that HCV genetic heterogeneity has a quantifiable effect on severity of liver disease, thus presenting opportunity for developing genetic assays for measuring virulence of HCV strains in clinical and public health settings.",""
"Sewage sludge (SS) amendment of agricultural soils has recently become an issue of great interest because improvements in wastewater treatment systems have increased production of this type of waste. This practice is known to benefit soil and crops. However, the potential contamination of the environmental matrices has been rarely assessed because suitable models and data regarding the diffuse contamination of SS in agricultural fields are limited. By using land classification tools, we can evaluate the suitability of each soil parcel, the nature of the local soil, and the landscape parameters, and thus take more confident decisions regarding the management of this waste. In this study, we describe the development of a land classification tool to determine whether SS is suitable for organically amending a given agricultural soil. Bayesian networks (BNs) were used to assess the suitability of agricultural soils located in Catalonia (northeastern Spain). The case study includes a description of the area and the procedure used to develop the BN model. The results of this evaluation are presented in innovative ways, with spatial representation of the probability results to give a broader understanding of the classification problem.","By using land classification tools, we can evaluate the suitability of each soil parcel, the nature of the local soil, and the landscape parameters, and thus take more confident decisions regarding the management of this waste."
"We have had to wait over 30 years since the naive Bayes model was first introduced in 1960 for the so-called Bayesian network classifiers to resurge. Based on Bayesian networks, these classifiers have many strengths, like model interpretability, accommodation to complex data and classification problem settings, existence of efficient algorithms for learning and classification tasks, and successful applicability in real-world problems. In this article, we survey the whole set of discrete Bayesian network classifiers devised to date, organized in increasing order of structure complexity: naive Bayes, selective naive Bayes, seminaive Bayes, one-dependence Bayesian classifiers, k-dependence Bayesian classifiers, Bayesian network-augmented naive Bayes, Markov blanket-based Bayesian classifier, unrestricted Bayesian classifiers, and Bayesian multinets. Issues of feature subset selection and generative and discriminative structure and parameter learning are also covered.","We have had to wait over 30 years since the naive Bayes model was first introduced in 1960 for the so-called Bayesian network classifiers to resurge."
"Languages that combine aspects of probabilistic representations with aspects of first-order logic are referred to as first-order probabilistic languages (FOPLs). FOPLs can be divided into three categories: rule-based, procedural-based and entity-relation-based languages. This article presents a survey of directed entity-relation- based FOPLs and their associated model construction and inference algorithms.","This article presents a survey of directed entity-relation- based FOPLs and their associated model construction and inference algorithms."
"Bayesian belief networks are finding increasing application in adaptive ecosystem management where data are limited and uncertainty is high. The combined effect of multiple stressors is one area where considerable uncertainty exists. Our study area, the Great Barrier Reef is simultaneously data-rich - concerning the physical and biological environment - and data-poor - concerning the effects of interacting stressors. We used a formal expert-elicitation process to obtain estimates of outcomes associated with a variety of scenarios that combined stressors both within and outside the control of local managers. There was much stronger consensus about certain stressor effects - such as between temperature anomalies and bleaching - than others, such as the relationship between water quality and coral cover. In general, the expert outlook for the Great Barrier Reef is pessimistic, with the potential for climate change effects potentially to overshadow the effects of local management actions. (C) 2014 Elsevier Ltd. All rights reserved.",""
"To make further progress towards a safer industry, process safety performance indicators are indispensable. There are, however, some challenges involved with interpretation of indicator outcomes. By going too far in detail one loses overview, but in not noticing the important detail a false impression of safety may be obtained. Aggregation from a detailed level upward may give relief at this point, but what to do if indicator values do not improve any further? Is there a means to relate indicators to the plant's risk level? The paper will show that when making use of the new technique of Bayesian networks for risk management, progress may be made. It seems possible to relate technical failure rates with risk factors acting over time duration and to take action before something breaks down. While originating in bad design, operation, maintenance, or neglect, these risk factors are influenced in the background by organizational, management, and human factors, which are subject to indicator monitoring. An example will be given of results one can expect when the dependencies are modeled in Bayesian network fashion. Current developments in other areas such as in aviation and offshore platform maintenance appear to be advancing in the same direction. (C) 2013 Elsevier Ltd. All rights reserved.",""
"How do children learn the causal structure of the environment? We first summarize a set of theories from the adult literature on causal learning, including associative models, parameter estimation theories, and causal structure learning accounts, as applicable to developmental science. We focus on causal graphical models as a description of children's causal knowledge, and the implications of this computational description for children's causal learning. We then examine the contributions of explanation and exploration to causal learning from a computational standpoint. Finally, we examine how children might learn causal knowledge from others and how computational and constructivist accounts of causal learning can be integrated. (C) 2014 John Wiley & Sons, Ltd.",""
"The likelihood ratio (LR) is a probabilistic method that has been championed as a 'simple rule' for evaluating the probative value of forensic evidence in court. Intuitively, if the LR is greater than one then the evidence supports the prosecution hypothesis; if the LR is less than one it supports the defence hypothesis, and if the LR is equal to one then the evidence favours neither (and so is considered 'neutral' having no probative value). It can be shown by Bayes' theorem that this simple relationship only applies to pairs of hypotheses for which one is the negation of the other (i.e. to mutually exclusive and exhaustive hypotheses) and is not applicable otherwise. We show how easy it can be - even for evidence experts - to use pairs of hypotheses that they assume are mutually exclusive and exhaustive but are not, and hence to arrive at erroneous conclusions about the value of evidence using the LR. Furthermore, even when mutually exclusive and exhaustive hypotheses are used there are extreme restrictions as to what can be concluded about the probative value of evidence just from a LR. Most importantly, while the distinction between source-level hypotheses (such as defendant was/was not at the crime scene) and offence-level hypotheses (defendant is/is not guilty) is well known, it is not widely understood that a LR for evidence about the former generally has no bearing on the LR of the latter. We show for the first time (using Bayesian networks) the full impact of this problem, and conclude that it is only the LR of the offence level hypotheses that genuinely determines the probative value of the evidence. We investigate common scenarios in which evidence has a LR of one but still has significant probative value (i.e. is not neutral as is commonly assumed). As illustration we consider the ramifications of these points for the case of Barry George. The successful appeal against his conviction for the murder of Jill Dando was based primarily on the argument that the firearm discharge residue (FDR) evidence, assumed to support the prosecution hypothesis at the original trial, actually had a LR equal to one and hence was 'neutral'. However, our review of the appeal transcript shows numerous examples of the problems with the use of hypotheses identified above. We show that if one were to follow the arguments recorded in the Appeal judgement verbatim, then contrary to the Appeal conclusion, the probative value of the FDR evidence may not have been neutral as was concluded. (C) 2013 The Chartered Society of Forensic Sciences. Published by Elsevier Ireland Ltd. All rights reserved.",""
"Computerized neuropsychological tests offered several advantages for large epidemiological studies to assess child neuropsychological development. We aimed to evaluate the psychometric properties and criterion validity of 2 computerized tests (n-back and attentional network task [ANT]) used to assess the working memory and attention function, respectively. As part of the BREATHE (BRain dEvelopment and Air polluTion ultrafine particles in scHool childrEn) project, we evaluated the neuropsychological development of 2,904 children between 7 to 9 years of age. The main outcomes of the n-back test were d' scores and hit reaction time (RT) (HRT). The outcomes measured for ANT were incorrect responses, omissions, alerting, orienting, and conflict. We also collected data of child's sex, age, school achievement, ADHD symptomatology, behavioral problems, and maternal education. We observed that the d' scores and HRT showed acceptable internal consistency, reasonable factorial structure, as well as good criterion validity and statistical dependencies. Regarding the ANT, incorrect responses, omissions, and conflict score had acceptable criterion validity although the internal consistency of the ANT was low. We strongly recommend the use of these tests in environmental epidemiological studies as valid, objective, and easy-to-apply measures of child neuropsychological development.",""
"Identifying effective adaptation strategies for coastal communities dependent on marine resources and impacted by climate change can be difficult due to the dynamic nature of marine ecosystems. The task is more difficult if current and predicted shifts in social and economic trends are considered. Information about social and economic change is often limited to qualitative data. A combination of qualitative and quantitative models provide the flexibility to allow the assessment of current and future ecological and socio-economic risks and can provide information on alternative adaptations. Here, we demonstrate how stakeholder input, qualitative models and Bayesian belief networks (BBNs) can provide semi-quantitative predictions, including uncertainty levels, for the assessment of climate and non-climate-driven change in a case study community. Issues are identified, including the need to increase the capacity of the community to cope with change. Adaptation strategies are identified that alter positive feedback cycles contributing to a continued decline in population, local employment and retail spending. For instance, the diversification of employment opportunities and the attraction of new residents of different ages would be beneficial in preventing further population decline. Some impacts of climate change can be combated through recreational bag or size limits and monitoring of popular range-shifted species that are currently unmanaged, to reduce the potential for excessive removal. Our results also demonstrate that combining BBNs and qualitative models can assist with the effective communication of information between stakeholders and researchers. Furthermore, the combination of techniques provides a dynamic, learning-based, semi-quantitative approach for the assessment of climate and socio-economic impacts and the identification of potential adaptation strategies.",""
"Many factors affect the magnitude of nutrient losses from dairy farm systems. Bayesian Networks (BNs) are an alternative to conventional modeling that can evaluate complex multifactor problems using forward and backward reasoning. A BN of annual total phosphorus (TP) exports was developed for a hypothetical dairy farm in the south Otago region of New Zealand and was used to investigate and integrate the effects of different management options under contrasting rainfall and drainage regimes. Published literature was consulted to quantify the relationships that underpin the BN, with preference given to data and relationships derived from the Otago region. In its default state, the BN estimated loads of 0.34 +/- 0.42 kg TP ha(-1) for overland flow and 0.30 +/- 0.19 kg TP ha(-1) for subsurface flow, which are in line with reported TP losses in overland flow (0-1.1 kg TP ha(-1)) and in drainage (0.15-2.2 kg TP ha(-1)). Site attributes that cannot be managed, like annual rainfall and the average slope of the farm, were found to affect the loads of TP lost from dairy farms. The greatest loads (13.4 kg TP ha(-1)) were predicted to occur with above-average annual rainfall (970 mm), where irrigation of farm dairy effluent was managed poorly, and where Olsen P concentrations were above pasture requirements (60 mg kg(-1)). Most of this loading was attributed to contributions from overland flow. This study demonstrates the value of using a BN to understand the complex interactions between site variables affecting P loss and their relative importance.",""
"We study whether taxi companies can simultaneously save petroleum and money by transitioning to electric vehicles. We propose a process to compute the return on investment of transitioning a taxi corporation's fleet to electric vehicles. We use Bayesian data analysis to infer the revenue changes associated with the transition. We do not make any assumptions about the vehicles' mobility patterns; instead, we use a time-series of GPS coordinates of the company's existing petroleum-based vehicles to derive our conclusions. As a case study, we apply our process to a major taxi corporation, Yellow Cab San Francisco (YCSF). Using current prices, we find that transitioning their fleet to battery electric vehicles and plug-in hybrid electric vehicles is profitable for the company. Furthermore, given that gasoline prices in San Francisco are only 5.4 % higher than the rest of the United States, but electricity prices are 75 % higher; taxi companies with similar practices and mobility patterns in other cities are likely to profit more than YCSF by transitioning to electric vehicles.",""
"Bovine respiratory disease (BRD) continues to be the primary cause of morbidity and mortality in feedyard cattle. Accurate identification of those animals that will not finish the production cycle normally following initial treatment for BRD would provide feedyard managers with opportunities to more effectively manage those animals. Our objectives were to assess the ability of different classification algorithms to accurately predict an individual calf's outcome based on data available at first identification of and treatment for BRD and also to identify characteristics of calves where predictive models performed well as gauged by accuracy. Data from 23 feedyards in multiple geographic locations within the U.S. from 2000 to 2009 representing over one million animals were analyzed to identify animals clinically diagnosed with BRD and treated with an antimicrobial. These data were analyzed both as a single dataset and as multiple datasets based on individual feedyards and partitioned into training, testing, and validation datasets. Classifiers were trained and optimized to identify calves that did not finish the production cycle with their cohort. Following classifier training, accuracy was evaluated using validation data. Analysis was also done to identify sub-groups of calves within populations where classifiers performed better compared to other sub-groups. Accuracy of individual classifiers varied by dataset. The accuracy of the best performing classifier by dataset ranged from a low of 63% in one dataset up to 95% in a different dataset. Sub-groups of calves were identified within some datasets where accuracy of a classifiers were greater than 98%; however these accuracies must be interpreted in relation to the prevalence of the class of interest within those populations. We found that by pairing the correct classifier with the data available, accurate predictions could be made that would provide feedlot managers with valuable information. (C) 2014 Elsevier B.V. All rights reserved.","Our objectives were to assess the ability of different classification algorithms to accurately predict an individual calf's outcome based on data available at first identification of and treatment for BRD and also to identify characteristics of calves where predictive models performed well as gauged by accuracy."
"Previous work has shown that predictions can be mediated by mechanistic beliefs. The present study shows that such mediation only occurs in the face of contradictory, and not corroborative, evidence. In four experiments, we presented participants with causal statements describing a common-cause structure (E(1)a dagger C -> E-2). Then we informed them of the states of C and E-1 and asked them to judge the likelihood of E-2. In Experiments 1 and 2, we manipulated whether the mechanisms supporting the two effects were the same or different, and whether the evidence presented confirmed or contradicted the participants' expectations. The relation between the mechanisms only influenced predictions when evidence contradicted the expectations, but not when it was consistent. In Experiments 3 and 4, we used a common-cause structure with identical mechanisms. We manipulated the order in which predictions were made. When confirmatory predictions were made before contradictory predictions, mechanistic modulation was not observed in the confirmatory case. In contrast, the modulation was found when confirmatory predictions were made after contradictory ones. The results support the contradiction hypothesis that causal structure is revised during prediction, but only in the face of unexpected evidence.",""
"Modelling environmental systems becomes a challenge when dealing directly with continuous and discrete data simultaneously. The aim in regression is to give a prediction of a response variable given the value of some feature variables. Multiple linear regression models, commonly used in environmental science, have a number of limitations: (1) all feature variables must be instantiated to obtain a prediction, and (2) the inclusion of categorical variables usually yields more complicated models. Hybrid Bayesian networks are an appropriate approach to solve regression problems without such limitations, and they also provide additional advantages. This methodology is applied to modelling landscape-socioeconomy relationships for different types of data (continuous, discrete or hybrid). Three models relating socioeconomy and landscape are proposed, and two scenarios of socioeconomic change are introduced in each one to obtain a prediction. This proposal can be easily applied to other areas in environmental modelling. (C) 2014 Elsevier Ltd. All rights reserved.","The aim in regression is to give a prediction of a response variable given the value of some feature variables."
"Models of ecosystem management typically measure the benefits of ecosystem services in terms of ecological or biophysical variables, which are influenced by management decisions and biophysical/ ecological conditions. This study uses farmers' expected benefits of ecosystem services as input variables to model their decision between planting rice, annual crops or perennial crops. Based on the theory of planned behavior, a Bayesian network is constructed to model crop choice depending on attitudes toward the ecosystem services of biomass production, reduction of soil erosion, and water quality improvement. The relative importance of these decision-making criteria is quantified using the Analytical Hierarchy Process. Results indicate that Bayesian networks can use socio-psychological measurements to model decision-making. Especially as an extension to biophysical or economic models, they can serve as a powerful tool for grasping the more abstract socio-psychological dimensions of benefits of ecosystem services, and how they translate into the decisions of ecosystem managers. (c) 2014 Elsevier Ltd. All rights reserved.",""
"By means of an integration of decision theory and probabilistic models, we explore and develop methods for improving data privacy. Our work encompasses disclosure control tools in statistical databases and privacy requirements prioritization; in particular we propose a Bayesian approach for the on-line auditing in Statistical Databases and Pairwise Comparison Matrices for privacy requirements prioritization. The first approach is illustrated by means of examples in the context of statistical analysis on the census and medical data, where no salary (resp. no medical information), that could be related to a specific employee (resp. patient), must be released; the second approach is illustrated by means of examples, such as an e-voting system and an e-banking service that have to satisfy privacy requirements in addition to functional and security ones. Several fields in the social sciences, economics and engineering will benefit from the advances in this research area: e-voting, e-government, e-commerce, e-banking, e-health, cloud computing and risk management are a few examples of applications for the findings of this research.",""
"The genetic characterization of unbalanced mixed stains remains an important area where improvement is imperative. In fact, using the standard tools of forensic DNA profiling (i. e., STR markers), the profile of the minor contributor in mixed DNA stains cannot be successfully detected if its quantitative share of DNA is less than 10% of the mixed trace. This is due to the fact that the major contributor's profile \"masks'' that of the minor contributor. Besides known remedies to this problem, such as Y-STR analysis, a new compound genetic marker that consists of a Deletion/Insertion Polymorphism (DIP) linked to a Short Tandem Repeat (STR) polymorphism, has recently been developed and proposed [1]. These novel markers are called DIP-STR markers. This paper compares, from a statistical and forensic perspective, the potential usefulness of these novel DIP-STR markers (i) with traditional STR markers in cases of moderately unbalanced mixtures, and (ii) with Y-STR markers in cases of female-male mixtures. This is done through a comparison of the distribution of 100,000 likelihood ratio values obtained using each method on simulated mixtures. This procedure is performed assuming, in turn, the prosecution's and the defence's point of view. (C) 2014 Elsevier Ireland Ltd. All rights reserved.",""
"System failures, for example in electrical power systems, can have catastrophic impact on human life and high-cost missions. Due to an electrical fire in Swissair flight 111 on September 2, 1998, all 229 passengers and crew on board sadly lost their lives. A battery failure most likely took place on the Mars Global Surveyor, which unfortunately last communicated with Earth and thus ended its mission on November 2, 2006. Fault diagnosis techniques that seek to hinder similar accidents in the future are being developed in this article. We present comprehensive fault diagnosis methods for dynamic and hybrid domains with uncertainty, and validate them using electrical power system data. Our approach relies on the use of Bayesian networks, which model the electrical power system, compiled to arithmetic circuits. We handle in an integrated way varying fault dynamics (both persistent and intermittent faults), fault progression (both abrupt and drift faults), and fault behavior cardinality (both discrete and continuous behaviors). Our work has resulted in a software system for fault diagnosis, ProDiagnose, that has been the top performer in three of the four international diagnostics competitions in which it participated. In this paper we comprehensively present our methods as well as novel and extensive experimental results on data from a NASA electrical power system. (C) 2014 Elsevier Inc. All rights reserved.",""
"One of the hardest challenges in building a realistic Bayesian Network (BN) model is to construct the node probability tables (NPTs). Even with a fixed predefined model structure and very large amounts of relevant data, machine learning methods do not consistently achieve great accuracy compared to the ground truth when learning the NPT entries (parameters). Hence, it is widely believed that incorporating expert judgments can improve the learning process. We present a multinomial parameter learning method, which can easily incorporate both expert judgments and data during the parameter learning process. This method uses an auxiliary BN model to learn the parameters of a given BN. The auxiliary BN contains continuous variables and the parameter estimation amounts to updating these variables using an iterative discretization technique. The expert judgments are provided in the form of constraints on parameters divided into two categories: linear inequality constraints and approximate equality constraints. The method is evaluated with experiments based on a number of well-known sample BN models (such as Asia, Alarm and Hailfinder) as well as a real-world software defects prediction BN model. Empirically, the new method achieves much greater learning accuracy (compared to both state-of-the-art machine learning techniques and directly competing methods) with much less data. For example, in the software defects BN for a sample size of 20 (which would be considered difficult to collect in practice) when a small number of real expert constraints are provided, our method achieves a level of accuracy in parameter estimation that can only be matched by other methods with much larger sample sizes (320 samples required for the standard machine learning method, and 105 for the directly competing method with constraints). (C) 2014 Elsevier Inc. All rights reserved.",""
"This paper proposes a new Probabilistic Graphical Model (PGM) to incorporate the scene, event object interaction, and the event temporal contexts into Dynamic Bayesian Networks (DBNs) for event recognition in surveillance videos. We first construct the baseline event DBNs for modeling the events from their own appearance and kinematic observations, and then augment the DBN with contexts to improve its event recognition performance. Unlike the existing context methods, our model incorporates various contexts simultaneously into one unified model. Experiments on real scene surveillance datasets with complex backgrounds show that the contexts can effectively improve the event recognition performance even under great challenges like large intra-class variations and low image resolution. (C) 2013 Elsevier B. V. All rights reserved.",""
"Validation is an important issue in the development and application of Bayesian Belief Network (BBN) models, especially when the outcome of the model cannot be directly observed. Despite this, few frameworks for validating BBNs have been proposed and fewer have been applied to substantive real-world problems. In this paper we adopt the approach by Pitchforth and Mengersen (2013), which includes nine validation tests that each focus on the structure, discretisation, parameterisation and behaviour of the BBNs included in the case study. We describe the process and result of implementing a validation framework on a model of a real airport terminal system with particular reference to its effectiveness in producing a valid model that can be used and understood by operational decision makers. In applying the proposed validation framework we demonstrate the overall validity of the Inbound Passenger Facilitation Model as well as the effectiveness of the validity framework itself. (C) 2014 Elsevier Ltd. All rights reserved.",""
"We develop a class of neural networks derived from probabilistic models posed in the form of Bayesian networks. Making biologically and technically plausible assumptions about the nature of the probabilistic models to be represented in the networks, we derive neural networks exhibiting standard dynamics that require no training to determine the synaptic weights, that perform accurate calculation of the mean values of the relevant random variables, that can pool multiple sources of evidence, and that deal appropriately with ambivalent, inconsistent, or contradictory evidence. (C) 2014 Elsevier B.V. All rights reserved.",""
"Performing exact inference on Bayesian networks is known to be # P-hard. Typically approximate inference techniques are used instead to sample from the distribution on query variables given the values e of evidence variables. Classically, a single unbiased sample is obtained from a Bayesian network on n variables with at most m parents per node in time O(nmP(e)(-1)), depending critically on P(e), the probability that the evidence might occur in the first place. By implementing a quantum version of rejection sampling, we obtain a square-root speedup, taking O(n2(m)P(e)(-1/2)) time per sample. We exploit the Bayesian network's graph structure to efficiently construct a quantum state, a q-sample, representing the intended classical distribution, and also to efficiently apply amplitude amplification, the source of our speedup. Thus, our speedup is notable as it is unrelativized-we count primitive operations and require no blackbox oracle queries.","Performing exact inference on Bayesian networks is known to be # P-hard."
"Combat system effectiveness simulation (CoSES) needs to model both the physical aspect (i.e. physics modelling) and intelligent aspect (i.e. decision modelling) of combat systems. Combat platform decision-making has several characteristics such as cognition, diversity, agility, uncertainty and higher abstraction level, which bring tough challenges for decision model design, implementation and optimization. In this paper, we propose a domain-specific modelling approach which develops friendly modelling environments for model design, we design code generation mechanisms to transform domain-specific decision models to Python code which is supported by a Python script framework to implement decision models and we present a Bayesian network-based statistical analysis method on simulation output data to optimize the decision model. The case study shows that the proposed modelling and optimization approach effectively supports CoSES with decision models of higher efficiency and increased effectiveness.",""
"Black fly along the Orange River are major pests of livestock and labour-intensive agriculture, causing annual estimated industry losses in excess of US$30 million. The problem is attributed to winter high flows, with the main pest species being Simulium chutteri Lewis, 1965, although Simulium damnosum Theobald, 1903 and Simulium impukane de Meillon, 1936 may also be periodically problematic. During 2011, black fly outbreaks along the middle Orange River were perceived by farmers to have worsened and attributed to S. impukane. Here, we investigate the likelihood of this being the case, using a weight-of-evidence approach incorporating ecohydrological data. Results showed that it is unlikely that the 2011 outbreaks were caused by S. impukane, and more likely that the main outbreak cause remains S. chutteri. Sustained high flows and turbidity levels favour S. chutteri species over the other species of black fly, while flow conditions for a species such as S. impukane were favourable for 1% of the time only. However, during periods of lower flow and lower turbidity, other species of black fly may be favoured and contribute towards periodic outbreaks. We conclude that black fly control should focus on management issues around the control programme.",""
"We consider the problem of learning causal directed acyclic graphs from an observational joint distribution. One can use these graphs to predict the outcome of interventional experiments, from which data are often not available. We show that if the observational distribution follows a structural equation model with an additive noise structure, the directed acyclic graph becomes identifiable from the distribution under mild conditions. This constitutes an interesting alternative to traditional methods that assume faithfulness and identify only the Markov equivalence class of the graph, thus leaving some edges undirected. We provide practical algorithms for finitely many samples, RESIT (regression with subsequent independence test) and two methods based on an independence score. We prove that RESIT is correct in the population setting and provide an empirical evaluation.","We provide practical algorithms for finitely many samples, RESIT (regression with subsequent independence test) and two methods based on an independence score."
"Understanding the interrelationship of environmental and biological factors that influence population growth rates of invasive Sus. scrota (Wild Pig) is a requisite for population management of the species. Such information can be used to evaluate various types of population control to ensure that the most cost-effective damage-abatement methods are used. We developed a sex- and age-structured model to simulate general population dynamics of Wild Pigs in Texas. Our objectives were to estimate potential statewide Wild Pig population-growth rates for Texas, identify model parameters that most influenced population trajectories, and compare resulting model predictions with ancillary population-trend data. Our Wild Pig simulation model estimated a mean annual growth rate of 0.32 (SE = 0.01), and stochastic model projections of Wild Pig population sizes ranged from 3.6 million to 16.9 million after 5 years. To evaluate parameter sensitivity, we recast our simulation results into a Bayesian belief network, and evaluated input-parameter influence based on variance reduction using Shannon's measure of mutual information. Our results indicated that the most influential model parameters within our simulation were number of litters per female and number of piglets recruited into the population, while adult and juvenile survival had little influence on Wild Pig population size within our simulations. Overall, our results suggest that natural resource managers should focus efforts towards reducing Wild Pig reproductive success; as opposed to attempting to increase adult mortality; when conducting Wild Pig population-control campaigns.",""
"We examine the role of a common cognitive heuristic in unsupervised learning of Bayesian probability networks from data. Human beings perceive a larger association between causal than diagnostic relationships. This psychological principal can be used to orient the arcs within Bayesian networks by prohibiting the direction that is less predictive. The heuristic increased predictive accuracy by an average of 0.51 % percent, a small amount. It also increased total agreement between different network learning algorithms (Max Spanning Tree, Taboo, EQ, SopLeq, and Taboo Order) by 25 %. Prior to use of the heuristic, the multiple raters Kappa between the algorithms was 0.60 (95 % confidence interval, CI, from 0.53 to 0.67) indicating moderate agreement among the networks learned through different algorithms. After the use of the heuristic, the multiple raters Kappa was 0.85 (95% CI from 0.78 to 0.92). There was a statistically significant increase in agreement between the five algorithms (alpha < 0.05). These data suggest that the heuristic increased agreement between networks learned through use of different algorithms, without loss of predictive accuracy. Additional research is needed to see if findings persist in other data sets and to explain why a heuristic used by humans could improve construct validity of mathematical algorithms.",""
"In this article we introduce modern statistical machine learning and bioinformatics approaches that have been used in learning statistical relationships from big data in medicine and behavioral science that typically include clinical, genomic (and proteomic) and environmental variables. Every year, data collected from biomedical and behavioral science is getting larger and more complicated. Thus, in medicine, we also need to be aware of this trend and understand the statistical tools that are available to analyze these datasets. Many statistical analyses that are aimed to analyze such big datasets have been introduced recently. However, given many different types of clinical, genomic, and environmental data, it is rather uncommon to see statistical methods that combine knowledge resulting from those different data types. To this extent, we will introduce big data in terms of clinical data, single nucleotide polymorphism and gene expression studies and their interactions with environment. In this article, we will introduce the concept of well-known regression analyses such as linear and logistic regressions that has been widely used in clinical data analyses and modern statistical models such as Bayesian networks that has been introduced to analyze more complicated data. Also we will discuss how to represent the interaction among clinical, genomic, and environmental data in using modern statistical models. We conclude this article with a promising modern statistical method called Bayesian networks that is suitable in analyzing big data sets that consists with different type of large data from clinical, genomic, and environmental data. Such statistical model form big data will provide us with more comprehensive understanding of human physiology and disease.","In this article, we will introduce the concept of well-known regression analyses such as linear and logistic regressions that has been widely used in clinical data analyses and modern statistical models such as Bayesian networks that has been introduced to analyze more complicated data."
"Several approaches have been proposed to recognize human emotions based on facial expressions or physiological signals, relatively rare work has been done to fuse these two, and other, modalities to improve the accuracy and robustness of the emotion recognition system. In this paper, we propose two methods based on feature-level and decision-level to fuse facial and physiological modalities. At feature-level fusion, we have tested the mutual information approach for selecting the most relevant and principal component analysis to reduce their dimensionality. For decision-level fusion, we have implemented two methods; the first is based on voting process and the second is based on dynamic Bayesian Networks. The system is validated using data obtained through an emotion elicitation experiment based on the International Affective Picture System. Results show that feature-level fusion is better than decision-level fusion. (C) 2014 Elsevier Masson SAS. All rights reserved.",""
"Bayesian Networks (BN) have been a popular predictive modeling formalism in bioinformatics, but their application in modern genomics has been slowed by an inability to cleanly handle domains with mixed discrete and continuous variables. Existing free BN software packages either discretize continuous variables, which can lead to information loss, or do not include inference routines, which makes prediction with the BN impossible. We present CGBayesNets, a BN package focused around prediction of a clinical phenotype from mixed discrete and continuous variables, which fills these gaps. CGBayesNets implements Bayesian likelihood and inference algorithms for the conditional Gaussian Bayesian network (CGBNs) formalism, one appropriate for predicting an outcome of interest from, e. g., multimodal genomic data. We provide four different network learning algorithms, each making a different tradeoff between computational cost and network likelihood. CGBayesNets provides a full suite of functions for model exploration and verification, including cross validation, bootstrapping, and AUC manipulation. We highlight several results obtained previously with CGBayesNets, including predictive models of wood properties from tree genomics, leukemia subtype classification from mixed genomic data, and robust prediction of intensive care unit mortality outcomes from metabolomic profiles. We also provide detailed example analysis on public metabolomic and gene expression datasets. CGBayesNets is implemented in MATLAB and available as MATLAB source code, under an Open Source license and anonymous download at http://www.cgbayesnets.com.","Existing free BN software packages either discretize continuous variables, which can lead to information loss, or do not include inference routines, which makes prediction with the BN impossible."
"This contribution concerns variance analysis of linear multi-input single-output models when the inputs are temporally white but where different inputs may be correlated. An expression is provided for the variance of a linearly parametrized estimate of the frequency response function from one block, i.e. from one input to the output. In particular, this expression reveals that the variance increases in one block when the number of estimated parameters in another block is increased, but levels off when the number of parameters in the other block reaches the number of parameters in the block in question. It also quantifies exactly how correlation between inputs affects the resulting accuracy and a graphical representation is provided for this purpose. The results are applicable to parallel MISO Hammerstein models when the nonlinearities are known and generalize an existing variance expression for this type of model. (C) 2014 Elsevier Ltd. All rights reserved.",""
"In Australia the European carp is widespread, environmentally damaging and difficult to control. Genetic control options are being developed for this species but risk-assessment studies to support these options have been limited. The key science challenge in this context is our limited understanding of complex and highly variable ecosystems. Hierarchical models are one way to approach this complexity and heterogeneity. These models treat the factors that determine risk as a joint probability distribution that can be factored into a series of simpler conditional distributions to allow Bayesian inference following observed outcomes. Designing a risk assessment around this approach, however, requires that the assessment endpoints (such as impacts on native species) are measurable, and that monitoring strategies are carefully designed and implemented in order that risk predictions are compared to outcomes. We therefore suggest that an evidence-based framework, supported by careful hazard analysis and quantitative risk assessment, and implemented within a stage-released protocol, is the safest way to move beyond the current emphasis on contained laboratory studies and qualitative risk assessments. We highlight impediments to this approach, and use the nontarget impacts of daughterless carp in Australian billabongs as a case study to illustrate three methodological tools that not only provide solutions to some of these impediments but also encourage stakeholder participation in the risk assessment process.","These models treat the factors that determine risk as a joint probability distribution that can be factored into a series of simpler conditional distributions to allow Bayesian inference following observed outcomes."
"A variety of data-driven models focused on remaining lifetime prediction have been developed under condition-based monitoring framework. These models either assume an analytical formula for the underlying degradation path is known, or the number of degradation states could be determined subjectively. This paper proposes an adaptive discrete-state model to estimate system remaining lifetime based on Bayesian Belief Network (BBN) theory. The model consists of three steps: degradation state identification, degradation state characterization, and remaining life prediction. Our approach does not require an explicit distribution function to characterize the fault evolutionary process. Because the BBN model leverages the validity measures to determine the optimum state number, it avoids the state identification errors under limited feature data. The performance of the BBN model is validated and verified by actual and simulated bearing life data. Numerical examples show that the Bayesian degradation model outperforms a time-based maintenance policy both in cost and reliability.",""
"The gene regulation network (GRN) is a high-dimensional complex system, which can be represented by various mathematical or statistical models. The ordinary differential equation (ODE) model is one of the popular dynamic GRN models. High-dimensional linear ODE models have been proposed to identify GRNs, but with a limitation of the linear regulation effect assumption. In this article, we propose a sparse additive ODE (SA-ODE) model, coupled with ODE estimation methods and adaptive group least absolute shrinkage and selection operator (LASSO) techniques, to model dynamic GRNs that could flexibly deal with nonlinear regulation effects. The asymptotic properties of the proposed method are established and simulation studies are performed to validate the proposed approach. An application example for identifying the nonlinear dynamic GRN of T-cell activation is used to illustrate the usefulness of the proposed method.",""
"Computational models of interactive narrative offer significant potential for creating educational game experiences that are procedurally tailored to individual players and support learning. A key challenge posed by interactive narrative is devising effective director agent models that dynamically sequence story events according to players' actions and needs. In this paper, we describe a supervised machine-learning framework to model director agent strategies in an educational interactive narrative Crystal Island. Findings from two studies with human participants are reported. The first study utilized a Wizard-of-Oz paradigm where human \"wizards\" directed participants through Crystal Island's mystery storyline by dynamically controlling narrative events in the game environment. Interaction logs yielded training data for machine learning the conditional probabilities of a dynamic Bayesian network (DBN) model of the human wizards' directorial actions. Results indicate that the DBN model achieved significantly higher precision and recall than naive Bayes and bigram model techniques. In the second study, the DBN director agent model was incorporated into the runtime version of Crystal Island, and its impact on students' narrative-centered learning experiences was investigated. Results indicate that machine-learning director agent strategies from human demonstrations yield models that positively shape players' narrative-centered learning and problem-solving experiences.",""
"Recommendation systems in software engineering (SE) should be designed to integrate evidence into practitioners experience. Bayesian networks (BNs) provide a natural statistical framework for evidence-based decision-making by incorporating an integrated summary of the available evidence and associated uncertainty (of consequences). In this study, we follow the lead of computational biology and healthcare decision-making, and investigate the applications of BNs in SE in terms of 1) main software engineering challenges addressed, 2) techniques used to learn causal relationships among variables, 3) techniques used to infer the parameters, and 4) variable types used as BN nodes. We conduct a systematic mapping study to investigate each of these four facets and compare the current usage of BNs in SE with these two domains. Subsequently, we highlight the main limitations of the usage of BNs in SE and propose a Hybrid BN to improve evidence-based decision-making in SE. In two industrial cases, we build sample hybrid BNs and evaluate their performance. The results of our empirical analyses show that hybrid BNs are powerful frameworks that combine expert knowledge with quantitative data. As researchers in SE become more aware of the underlying dynamics of BNs, the proposed models will also advance and naturally contribute to evidence based-decision-making.",""
"Bayesian networks (BNs) are graphical modeling tools that are generally recommended for exploring what-if scenarios, visualizing systems and problems, and for communication between stakeholders during decision making. In this article, we investigate their potential for exploring different perspectives in trade disputes. To do so, we draw on a specific case study that was arbitrated by the World Trade Organization (WTO): the Australia-New Zealand apples dispute. The dispute centered on disagreement about judgments contained within Australia's 2006 import risk analysis (IRA). We built a range of BNs of increasing complexity that modeled various approaches to undertaking IRAs, from the basic qualitative and semi-quantitative risk analyses routinely performed in government agencies, to the more complex quantitative simulation undertaken by Australia in the apples dispute. We found the BNs useful for exploring disagreements under uncertainty because they are probabilistic and transparently represent steps in the analysis. Different scenarios and evidence can easily be entered. Specifically, we explore the sensitivity of the risk output to different judgments (particularly volume of trade). Thus, we explore how BNs could usefully aid WTO dispute settlement. We conclude that BNs are preferable to basic qualitative and semi-quantitative risk analyses because they offer an accessible interface and are mathematically sound. However, most current BN modeling tools are limited compared with complex simulations, as was used in the 2006 apples IRA. Although complex simulations may be more accurate, they are a black box for stakeholders. BNs have the potential to be a transparent aid to complex decision making, but they are currently computationally limited. Recent technological software developments are promising.",""
"Domino effects are low-probability high-consequence accidents causing severe damage to humans, process plants, and the environment. Because domino effects affect large areas and are difficult to control, preventive safety measures have been given priority over mitigative measures. As a result, safety distances and safety inventories have been used as preventive safety measures to reduce the escalation probability of domino effects. However, these safety measures are usually designed considering static accident scenarios. In this study, we show that compared to a static worst-case accident analysis, a dynamic consequence analysis provides a more rational approach for risk assessment and management of domino effects. This study also presents the application of Bayesian networks and conflict analysis to risk-based allocation of chemical inventories to minimize the consequences and thus to reduce the escalation probability. It emphasizes the risk management of chemical inventories as an inherent safety measure, particularly in existing process plants where the applicability of other safety measures such as safety distances is limited.",""
"We investigate the use of structure learning in Bayesian networks for a complex multimodal task of action detection in soccer videos. We illustrate that classical score-oriented structure learning algorithms, such as the K2 one whose usefulness has been demonstrated on simple tasks, fail in providing a good network structure for classification tasks where many correlated observed variables are necessary to make a decision. We then compare several structure learning objective functions, which aim at finding out the structure that yields the best classification results, extending existing solutions in the literature. Experimental results on a comprehensive data set of 7 videos show that a discriminative objective function based on conditional likelihood yields the best results, while augmented approaches offer a good compromise between learning speed and classification accuracy.","We illustrate that classical score-oriented structure learning algorithms, such as the K2 one whose usefulness has been demonstrated on simple tasks, fail in providing a good network structure for classification tasks where many correlated observed variables are necessary to make a decision."
"We assess the accuracy of various state-of-the-art statistics and machine learning methods for reconstructing gene and protein regulatory networks in the context of circadian regulation. Our study draws on the increasing availability of gene expression and protein concentration time series for key circadian clock components in Arabidopsis thaliana. In addition, gene expression and protein concentration time series are simulated from a recently published regulatory network of the circadian clock in A. thaliana, in which protein and gene interactions are described by a Markov jump process based on Michaelis-Menten kinetics. We closely follow recent experimental protocols, including the entrainment of seedlings to different light-dark cycles and the knock-out of various key regulatory genes. Our study provides relative network reconstruction accuracy scores for a critical comparative performance evaluation, and sheds light on a series of highly relevant questions: it quantifies the influence of systematically missing values related to unknown protein concentrations and mRNA transcription rates, it investigates the dependence of the performance on the network topology and the degree of recurrency, it provides deeper insight into when and why non-linear methods fail to outperform linear ones, it offers improved guidelines on parameter settings in different inference procedures, and it suggests new hypotheses about the structure of the central circadian gene regulatory network in A. thaliana.","Our study provides relative network reconstruction accuracy scores for a critical comparative performance evaluation, and sheds light on a series of highly relevant questions: it quantifies the influence of systematically missing values related to unknown protein concentrations and mRNA transcription rates, it investigates the dependence of the performance on the network topology and the degree of recurrency, it provides deeper insight into when and why non-linear methods fail to outperform linear ones, it offers improved guidelines on parameter settings in different inference procedures, and it suggests new hypotheses about the structure of the central circadian gene regulatory network in A."
"This paper presents a Bayesian Network (BN)-based modeling method for cascading crisis events. Crisis events have occurred more frequently in recent years, such as typhoons, rainstorms, and floods, posing a great threat to humans. Addressing these crises requires a more effective method for crisis early-warning and disaster mitigation in crisis management However, few modeling methods can combine the crisis chain reaction (macro-view) and the elements within the crisis event (micro-view) in a cascading crisis events. Existing classical methods fail to consider the causal relations linking the micro to macro level in crisis events, which affects the forecasting accuracy and effectiveness. Based on systems theory, this paper first abstracts the crisis event as a three-layer structure model consisting of input elements, state elements and output elements from a micro-view. Next, a cascading crisis events Bayesian Network (CCEBN) model is developed by merging the single crisis events Bayesian Networks (SCEBNs). This method efficiently combines the crisis event's micro-view and the macro-view. The proposed BN-based model makes it possible to forecast and analyze the chain reaction path and the potential losses due to a crisis event. Finally, sample application is provided to illustrate the utility of the model. The experimental results indicate that the method can effectively improve the forecasting accuracy. (C) 2014 Elsevier B.V. All rights reserved.",""
"The analysis of whole-genome or exome sequencing data from trios and pedigrees has been successfully applied to the identification of disease-causing mutations. However, most methods used to identify and genotype genetic variants from next-generation sequencing data ignore the relationships between samples, resulting in significant Mendelian errors, false positives and negatives. Here we present a Bayesian network framework that jointly analyzes data from all members of a pedigree simultaneously using Mendelian segregation priors, yet providing the ability to detect de novo mutations in offspring, and is scalable to large pedigrees. We evaluated our method by simulations and analysis of whole-genome sequencing (WGS) data from a 17-individual, 3-generation CEPH pedigree sequenced to 50x average depth. Compared with singleton calling, our family caller produced more high-quality variants and eliminated spurious calls as judged by common quality metrics such as Ti/Tv, Het/Hom ratios, and dbSNP/SNP array data concordance, and by comparing to ground truth variant sets available for this sample. We identify all previously validated de novo mutations in NA12878, concurrent with a 7x precision improvement. Our results show that our method is scalable to large genomics and human disease studies.",""
"With ever smaller processors and ubiquitous Internet connectivity, the pervasive computing environments from Mark Weiser's vision are coming closer. For their context-awareness, they will have to incorporate data from the abundance of sensors integrated in everyday life and to benefit from continuous machine-to-machine communications. Along with huge opportunities, this also poses problems: sensor measurements may conflict, processing times of logical and statistical reasoning algorithms increase non-deterministically polynomially or even exponentially, and wireless networks might become congested by the transmissions of all measurements. Bayesian networks are a good starting point for inference algorithms in pervasive computing, but still suffer from information overload in terms of network load and computation time. Thus, this work proposes to distribute processing with a modular Bayesian approach, thereby segmenting complex Bayesian networks. The introduced ''Bayeslets'' can be used to transmit and process only information which is valuable for its receiver. Two methods to measure the worth of information for the purpose of segmentation are presented and evaluated. As an example for a context-aware service, they are applied to a scenario from cooperative vehicular services, namely adaptive cruise control. (C) 2013 Elsevier B.V. All rights reserved.","Bayesian networks are a good starting point for inference algorithms in pervasive computing, but still suffer from information overload in terms of network load and computation time."
"As the information available to na < ve users through autonomous data sources continues to increase, mediators become important to ensure that the wealth of information available is tapped effectively. A key challenge that these information mediators need to handle is the varying levels of incompleteness in the underlying databases in terms of missing attribute values. Existing approaches such as QPIAD aim to mine and use Approximate Functional Dependencies (AFDs) to predict and retrieve relevant incomplete tuples. These approaches make independence assumptions about missing values-which critically hobbles their performance when there are tuples containing missing values for multiple correlated attributes. In this paper, we present a principled probabilistic alternative that views an incomplete tuple as defining a distribution over the complete tuples that it stands for. We learn this distribution in terms of Bayesian networks. Our approach involves mining/\"learning\" Bayesian networks from a sample of the database, and using it to do both imputation (predict a missing value) and query rewriting (retrieve relevant results with incompleteness on the query-constrained attributes, when the data sources are autonomous). We present empirical studies to demonstrate that (i) at higher levels of incompleteness, when multiple attribute values are missing, Bayesian networks do provide a significantly higher classification accuracy and (ii) the relevant possible answers retrieved by the queries reformulated using Bayesian networks provide higher precision and recall than AFDs while keeping query processing costs manageable.","We present empirical studies to demonstrate that (i) at higher levels of incompleteness, when multiple attribute values are missing, Bayesian networks do provide a significantly higher classification accuracy and (ii) the relevant possible answers retrieved by the queries reformulated using Bayesian networks provide higher precision and recall than AFDs while keeping query processing costs manageable."
"This paper addresses an important issue for intelligent transportation system, namely the ability of vehicles to safely and reliably localize themselves within an a priori known road map network. For this purpose, we propose an approach based on hybrid dynamic bayesian networks enabling to implement in a unified framework two of the most successful families of probabilistic model commonly used for localization: linear Kalman filters and Hidden Markov Models. The combination of these two models enables to manage and manipulate multi-hypotheses and multi-modality of observations characterizing Map Matching problems and it improves integrity approach. Another contribution of the paper is a chained-form state space representation of vehicle evolution which permits to deal with non-linearity of the used odometry model. Experimental results, using data from encoders' sensors, a DGPS receiver and an accurate digital roadmap, illustrate the performance of this approach, especially in ambiguous situations.",""
"It is important for clinicians to understand which are the clinical signs, the patient characteristics and the procedures that are related with the occurrence of hypertrophic burn scars in order to carry out a possible prognostic assessment. Providing clinicians with an easy-to- use tool for predicting the risk of pathological scars. A total of 703 patients with 2440 anatomical burn sites who were admitted to the Department of Plastic and Reconstructive Surgery, Burn Center of the Traumatological Hospital in Torino between January 1994 and May 2006 were included in the analysis. A Bayesian network (BN) model was implemented. The probability of developing a hypertrophic scar was evaluated on a number of scenarios. The error rate of the BN model was assessed internally and it was equal to 24.83%. While classical statistical method as logistic models can infer only which variables are related to the final outcome, the BN approach displays a set of relationships between the final outcome (scar type) and the explanatory covariates (patient's age and gender, burn surface area, full-thickness burn surface area, burn anatomical area and wound-healing time; burn treatment options such as advanced dressings, type of surgical approach, number of surgical procedures, type of skin graft, excision and coverage timing). A web-based interface to handle the BN model was developed on the website (burns header). Clinicians who registered at the website could submit their data in order to get from the BN model the predicted probability of observing a pathological scar type.",""
"Climate change and land-use change are having substantial impacts on biodiversity world-wide, but few studies have considered the impact of these factors together. If the combined effects of climate and land-use change are greater than the effects of each threat individually, current conservation management strategies may be inefficient and/or ineffective. This is particularly important with respect to freshwater ecosystems because freshwater biodiversity has declined faster than either terrestrial or marine biodiversity over the last three decades. This is the first study to model the independent and combined effects of climate change and land-use change on freshwater macroinvertebrates and fish. Using a case study in south-east Queensland, Australia, we built a Bayesian belief network populated with a combination of field data, simulations, existing models and expert judgment. Different land-use and climate scenarios were used to make predictions on how the richness of freshwater macroinvertebrates and fish is likely to respond in future. We discovered little change in richness averaged across the region, but identified important impacts and effects at finer scales. High nutrients and high runoff as a result of urbanization combined with high nutrients and high water temperature as a result of climate change and were the leading drivers of potential declines in macroinvertebrates and fish at fine scales. Synthesis and applications. This is the first study to separate out the constituent drivers of impacts on biodiversity that result from climate change and land-use change. Mitigation requires management actions that reduce in-stream nutrients, slows terrestrial runoff and provides shade, to improve the resilience of biodiversity in streams. Encouragingly, the restoration of riparian habitats is identified as an important buffering tool that can mitigate the negative effects of climate change and land-use change.",""
"Dynamic Bayesian networks (DBNs) are probabilistic graphical models that have become a ubiquitous tool for compactly describing statistical relationships among a group of stochastic processes. A suite of elaborately designed inference algorithms makes it possible for intelligent systems to use a DBN to make inferences in uncertain conditions. Unfortunately, exact inference or even approximation in a DBN has been proved to be NP-hard and is generally computationally prohibitive. In this paper, we investigate a sliding window framework for approximate inference in DBNs to reduce the computational burden. By introducing a sliding window that moves forward as time progresses, inference at any time is restricted to a quite narrow region of the network. The main contributions to the sliding window framework include an exploration of its foundations, explication of how it operates, and the proposal of two strategies for adaptive window size selection. To make this framework available as an inference engine, the interface algorithm widely used in exact inference is then integrated with the framework for approximate inference in DBNs. After analyzing its computational complexity, further empirical work is presented to demonstrate the validity of the proposed algorithms.","A suite of elaborately designed inference algorithms makes it possible for intelligent systems to use a DBN to make inferences in uncertain conditions."
"In the last decade Dynamic Bayesian Networks (DBNs) have become one type of the most attractive probabilistic modelling framework extensions of Bayesian Networks (BNs) for working under uncertainties from a temporal perspective. Despite this popularity not many researchers have attempted to study the use of these networks in anomaly detection or the implications of data anomalies on the outcome of such models. An abnormal change in the modelled environment's data at a given time, will cause a trailing chain effect on data of all related environment variables in current and consecutive time slices. Albeit this effect fades with time, it still can have an ill effect on the outcome of such models. In this paper we propose an algorithm for pilot error detection, using DBNs as the modelling framework for learning and detecting anomalous data. We base our experiments on the actions of an aircraft pilot, and a flight simulator is created for running the experiments. The proposed anomaly detection algorithm has achieved good results in detecting pilot errors and effects on the whole system.",""
"This study explored the participation of children in walking and bicycling for transportation, school, and various leisure purposes, and the relation with social and physical environmental characteristics and sociodemographics. Detailed individual travel data, including all walking and bicycling trips from a random sample of 4,293 children in the primary-school-age category in the Netherlands were investigated. Specifically, a Bayesian belief network was proposed that derives and represents all direct and indirect relations between the variables. The participation in active travel behavior has a direct relationship with all trip characteristics such as travel time and distance, and trip purpose, and is related to the car possession of the household. The degree of urbanization also is an important explanatory variable for participation in walking and bicycling by children. All the other social and physical environmental characteristics have an indirect influence on travel mode choice.",""
"This study presents a predictive model to be used in scheduling patients in an urban outpatient clinic. The model is based upon actual patient characteristics from a physical therapy clinic within an urban health and wellness center situated in a public university. A number of reported patients' characteristics such as age, education level, distance from the clinic, historical attendance records, etc. were examined to determine if they significantly impacted the patients' missing scheduled appointments (no-shows.) Decision tree analysis was used to develop a model that assessed the likelihood of a patient's no-show, using key patient characteristics and attendance records. Such a model can be used to assist with scheduling patients in an outpatient clinic, while attempting to increase the clinic's overall utilization. Four tree growing criteria were examined to develop the model with the strongest predictive power. Predictive power of each method was assessed by using the entire dataset as well as using split sampling. The results were then compared with those of a Bayesian networks model and a neural networks model. In addition, the trade-off between the selected decision tree model's predictive power versus simplicity of the associated classification rules was examined. We also assessed the impact of various levels of overbooking on the clinic's utilization when using patients' schedules based on the predictive model. (C) 2014 Elsevier Ltd. All rights reserved.","In addition, the trade-off between the selected decision tree model's predictive power versus simplicity of the associated classification rules was examined."
"Plant-wide oscillations are common in many industrial processes. They may impact the overall process performance and reduce profitability. It is important to detect and diagnose such oscillations. This paper reviews advances in diagnosis of plant-wide oscillations. The main focus of this study is on identifying possible root causes of oscillations using two techniques, one based on data analysis in the temporal and spectral domains and the other based on process connectivity analysis. The process data-based analysis provides an effective way to capture the difference between the root cause variable and the secondary propagated oscillating variables. It is shown that process topology-based methods are capable of finding oscillation propagation pathways and, thus, help in determining the root cause. This paper discusses and compares five such methods-spectral envelope, adjacency matrix, Granger causality, transfer entropy, and Bayesian network inference methods- by application to an industrial benchmark dataset. (c) 2014 American Institute of Chemical Engineers AIChE J, 60: 2019-2034, 2014","This paper discusses and compares five such methods-spectral envelope, adjacency matrix, Granger causality, transfer entropy, and Bayesian network inference methods- by application to an industrial benchmark dataset."
"To be effective, management of protected areas should be based on the best available evidence, including the scientific literature and expert knowledge. However, lack of such evidence in a suitable form to support decision-making may hinder effective management. Here we examine the use of Bayesian networks to support the management of protected areas, through the development of habitat suitability models for eight species of conservation concern. Bayesian networks were constructed on the basis of the scientific literature and expert knowledge, and were then tested using results from a field survey. Models of all species demonstrated very high discrimination between presence and absence sites, as indicated by AUC values >0.8, with values >0.9 obtained for four species, and Kappa values in the range of 0.4-0.9. The Bayesian networks were then used to examine the impact of different management interventions on habitat suitability of each species, including tree cutting, grazing and burning. Species differed in terms of their sensitivity to different management interventions, and model output provided evidence of both negative and positive interactions between types of intervention. These results highlight the trade-offs that must often be made when undertaking conservation management, and demonstrate the value of Bayesian networks in helping to make such trade-offs explicit. The identification of management impacts through analysis of available evidence also demonstrates the value of Bayesian networks for supporting evidence-based approaches to protected area management. (C) 2014 Published by Elsevier GmbH.",""
"We explored the development of sensitivity to causal relations in children's inductive reasoning. Children (5-, 8-, and 12-year-olds) and adults were given trials in which they decided whether a property known to be possessed by members of one category was also possessed by members of (a) a taxonomically related category or (b) a causally related category. The direction of the causal link was either predictive (prey -> predator) or diagnostic (predator -> prey), and the property that participants reasoned about established either a taxonomic or causal context. There was a causal asymmetry effect across all age groups, with more causal choices when the causal link was predictive than when it was diagnostic. Furthermore, context-sensitive causal reasoning showed a curvilinear development, with causal choices being most frequent for 8-year-olds regardless of context. Causal inductions decreased thereafter because 12-year-olds and adults made more taxonomic choices when reasoning in the taxonomic context. These findings suggest that simple causal relations may often be the default knowledge structure in young children's inductive reasoning, that sensitivity to causal direction is present early on, and that children over-generalize their causal knowledge when reasoning. (C) 2013 Elsevier Inc. All rights reserved.",""
"In this paper we investigate methods for learning hybrid Bayesian networks from data. First we utilize a kernel density estimate of the data in order to translate the data into a mixture of truncated basis functions (MoTBF) representation using a convex optimization technique. When utilizing a kernel density representation of the data, the estimation method relies on the specification of a kernel bandwidth. We show that in most cases the method is robust wrt. the choice of bandwidth, but for certain data sets the bandwidth has a strong impact on the result. Based on this observation, we propose an alternative learning method that relies on the cumulative distribution function of the data. Empirical results demonstrate the usefulness of the approaches: Even though the methods produce estimators that are slightly poorer than the state of the art (in terms of loglikelihood), they are significantly faster, and therefore indicate that the MoTBF framework can be used for inference and learning in reasonably sized domains. Furthermore, we show how a particular sub-class. of MoTBF potentials (learnable by the proposed methods) can be exploited to significantly reduce complexity during inference. (C) 2013 Elsevier Inc. All rights reserved.","Empirical results demonstrate the usefulness of the approaches: Even though the methods produce estimators that are slightly poorer than the state of the art (in terms of loglikelihood), they are significantly faster, and therefore indicate that the MoTBF framework can be used for inference and learning in reasonably sized domains."
"For many problem domains, such as medicine, chain graphs are more attractive than Bayesian networks as they support representing interactions between variables that have no natural direction. In particular, interactions between variables that result from certain feedback mechanisms can be represented by chain graphs. Using qualitative abstractions of probabilistic interactions is also of interest, as these allow focusing on patterns in the interactions rather than on the numerical detail. Such patterns are often known by experts and sufficient for making decisions. So far, qualitative abstractions of probabilistic interactions have only been developed for Bayesian networks in the form of qualitative probabilistic networks. In this paper, such qualitative abstractions are developed for chain graphs with the practical aim of using qualitative knowledge as constraints on the hyperspace of probability distributions. The usefulness of qualitative chain graphs is explored for modelling and reasoning about the interactions between diseases. (C) 2013 Elsevier Inc. All rights reserved.",""
"Decision-theoretic troubleshooting is one of the areas to which Bayesian networks can be applied. Given a probabilistic model of a malfunctioning man-made device, the task is to construct a repair strategy with minimal expected cost. The problem has received considerable attention over the past two decades. Efficient solution algorithms have been found for simple cases, whereas other variants have been proven NP-complete. We study several variants of the problem found in literature, and prove that computing approximate troubleshooting strategies is NP-hard. In the proofs, we exploit a close connection to setcovering problems. (C) 2013 Elsevier Inc. All rights reserved.",""
"Non-parametric density estimation is an important technique in probabilistic modeling and reasoning with uncertainty. We present a method for learning mixtures of polynomials (Mops) approximations of one-dimensional and multidimensional probability densities from data. The method is based on basis spline interpolation, where a density is approximated as a linear combination of basis splines. We compute maximum likelihood estimators of the mixing coefficients of the linear combination. The Bayesian information criterion is used as the score function to select the order of the polynomials and the number of pieces of the Mop. The method is evaluated in two ways. First, we test the approximation fitting. We sample artificial datasets from known one-dimensional and multidimensional densities and learn MoP approximations from the datasets. The quality of the approximations is analyzed according to different criteria, and the new proposal is compared with MoPs learned with Lagrange interpolation and mixtures of truncated basis functions. Second, the proposed method is used as a non-parametric density estimation technique in Bayesian classifiers. Two of the most widely studied Bayesian classifiers, the naive Bayes and tree-augmented naive Bayes classifiers, are implemented and compared. Results on real datasets show that the non-parametric Bayesian classifiers using MOPS are comparable to the kernel density-based Bayesian classifiers. We provide a free R package implementing the proposed methods. (C) 2013 Elsevier Inc. All rights reserved.","Second, the proposed method is used as a non-parametric density estimation technique in Bayesian classifiers."
"Upon varying parameters in a sensitivity analysis of a Bayesian network, the standard approach is to co-vary the parameters from the same conditional distribution such that their proportions remain the same. Alternative co-variation schemes are, however, possible. In this paper we investigate the properties of the standard proportional co-variation and introduce two alternative schemes: uniform and order-preserving co-variation. We theoretically investigate the effects of using alternative co-variation schemes on the so-called sensitivity function, and conclude that its general form remains the same under any linear co-variation scheme. In addition, we generalise the CD-distance for bounding global belief change to explicitly include the co-variation scheme under consideration. We prove a tight lower bound on this distance for parameter changes in single conditional probability tables. (C) 2013 Elsevier Inc. All rights reserved.",""
"The specification of conditional probability tables (CPTs) is a difficult task in the construction of probabilistic graphical models. Several types of canonical models have been proposed to ease that difficulty. Noisy-threshold models generalize the two most popular canonical models: the noisy-or and the noisy-and. When using the standard inference techniques the inference complexity is exponential with respect to the number of parents of a variable. More efficient inference techniques can be employed for CPTs that take a special form. CPTs can be viewed as tensors. Tensors can be decomposed into linear combinations of rank-one tensors, where a rank-one tensor is an outer product of vectors. Such decomposition is referred to as Canonical Polyadic (CP) or CANDECOMP-PARAFAC (CP) decomposition. The tensor decomposition offers a compact representation of CPTs which can be efficiently utilized in probabilistic inference. In this paper we propose a CP decomposition of tensors corresponding to CPTs of threshold functions, exactly l-out-of-k functions, and their noisy counterparts. We prove results about the symmetric rank of these tensors in the real and complex domains. The proofs are constructive and provide methods for CP decomposition of these tensors. An analytical and experimental comparison with the parent-divorcing method (which also has a polynomial complexity) shows superiority of the CP decomposition-based method. The experiments were performed on subnetworks of the well-known QMRT-DT network generalized by replacing noisy-or by noisy-threshold models. (C) 2013 Elsevier Inc. All rights reserved.","When using the standard inference techniques the inference complexity is exponential with respect to the number of parents of a variable."
"Case-based reasoning (CBR) is one of the matured paradigms of artificial intelligence for problem solving. CBR has been applied in many areas in the commercial sector to assist daily operations. However, CBR is relatively new in the field of forensic science. Even though forensic personnel have consciously used past experiences in solving new cases, the idea of applying machine intelligence to support decision-making in forensics is still in its infancy and poses a great challenge. This paper highlights the limitation of the methods used in forensics compared with a CBR method in the analysis of forensic evidences. The design and development of an Intelligent Forensic Autopsy Report System (I-AuReSys) basing on a CBR method along with the experimental results are presented. Our system is able to extract features by using an information extraction (IE) technique from the existing autopsy reports; then the system analyzes the case similarities by coupling the CBR technique with a Naive Bayes learner for feature-weights learning; and finally it produces an outcome recommendation. Our experimental results reveal that the CBR method with the implementation of a learner is indeed a viable alternative method to the forensic methods with practical advantages. (C) 2013 Elsevier Ltd. All rights reserved.",""
"This paper presents a method of estimating discrete multivariate probability distributions from scarce historical data. Of particular interest is the estimation of the probabilities of rare events. The method is based on maximizing the information entropy subject to equality constraints on the moments of the estimated probability distributions. Two criteria are proposed for optimal selections of the moment functions. The method models nonlinear and nonmonotonic relations with an optimal level of model complexity. Not only does it allow for the estimation of the probabilities of rare events, but, together with Bayesian networks, it also provides a framework to model fault propagation in complex highly interactive systems. An application of this work is in risk assessment and fault detection using Bayesian networks, especially when an accurate first-principles model is not available. The performance of the method is shown through an example.",""
"Spatial variations of future droughts across the Gunnison River Basin in CO, USA, are evaluated in this study, using a recently developed probabilistic forecast model. The Standardized Runoff Index (SRI) is employed to analyze drought status across the spatial extent of the basin. The runoff generated at each spatial unit of the basin is estimated by a distributed-parameter and physically-based hydrologic model. Using the historical runoff at each spatial unit, a statistical forecast model is developed within Bayesian networks. The forecast model applies a family of multivariate distribution functions to forecast future drought conditions given the drought status in the past. Given the runoff in the past (January-June), the forecast model is applied in estimating the runoff across the basin in the forecast period (July-December). The main advantage of the forecast model is its probabilistic features in analyzing future droughts. It develops conditional probabilities of a given forecast variable, and returns the highest probable forecast along with an assessment of the uncertainty around that value. Bayesian networks can also estimate the probability of future droughts with different seventies, given the drought status of the predictor period. Moreover, the model can be used to generate maps showing the runoff variation over the basin with the particular chance of occurrence in the future. Our results indicate that the statistical method applied in this study is a useful procedure in probabilistic forecast of future droughts given the spatio-temporal characteristics of droughts in the past. The techniques presented in this manuscript are suitable for probabilistic drought forecasting and have potential to improve drought characterization in different applications. (c) 2014 Published by Elsevier B.V.",""
"Dams' safety is highly important for authorities around the world. The impacts of a dam failure can be enormous. Models for investigating dam safety are required for helping decision-makers to mitigate the possible adverse consequences of flooding. A model for earth dam safety must specify clearly possible contributing factors, failure modes and potential consequences of dam failure. Probabilistic relations between variables should also be specified. Bayesian networks (BNs) have been identified as tools that would assist dam engineers on assessing risks. BNs are graphical models that facilitate the construction of a joint probability distribution. Most of the time, the variables included in a model for earth dam risk assessment involve continuous quantities. The presence of continuous random variables makes the implementation of discrete BNs difficult. An alternative to discrete BNs is the use of non-parametric continuous BNs, which will be briefly described in this article. As an example, a model for earth dams' safety in the State of Mexico will be discussed. Results regarding the quantification of conditional rank correlations through ratios of unconditional rank correlations have not been presented before and are introduced herein. While the complete application of the model for the State of Mexico is presented in an accompanying paper, here some results regarding model use are shown for demonstration purposes. The methods presented in this article can be applied for investigating risks of failure of civil infrastructures other than earth dams.",""
"Existing Bayesian models, especially nonparametric Bayesian methods, rely on specially conceived priors to incorporate domain knowledge for discovering improved latent representations. While priors affect posterior distributions through Bayes' rule, imposing posterior regularization is arguably more direct and in some cases more natural and general. In this paper, we present regularized Bayesian inference (RegBayes), a novel computational framework that performs posterior inference with a regularization term on the desired post-data posterior distribution under an information theoretical formulation. RegBayes is more flexible than the procedure that elicits expert knowledge via priors, and it covers both directed Bayesian networks and undirected Markov networks. When the regularization is induced from a linear operator on the posterior distributions, such as the expectation operator, we present a general convex-analysis theorem to characterize the solution of RegBayes. Furthermore, we present two concrete examples of RegBayes, infinite latent support vector machines (iLSVM) and multi-task infinite latent support vector machines (MT-iLSVM), which explore the large-margin idea in combination with a nonparametric Bayesian model for discovering predictive latent features for classification and multi-task learning, respectively. We present efficient inference methods and report empirical studies on several benchmark data sets, which appear to demonstrate the merits inherited from both large-margin learning and Bayesian nonparametrics. Such results contribute to push forward the interface between these two important subfields, which have been largely treated as isolated in the community.","In this paper, we present regularized Bayesian inference (RegBayes), a novel computational framework that performs posterior inference with a regularization term on the desired post-data posterior distribution under an information theoretical formulation."
"Unanswered questions remain in determining which high-risk node-negative colon cancer (CC) cohorts benefit from adjuvant therapy and how it may differ in an equal access population. Machine-learned Bayesian Belief Networks (ml-BBNs) accurately estimate outcomes in CC, providing clinicians with Clinical Decision Support System (CDSS) tools to facilitate treatment planning. We evaluated ml-BBNs ability to estimate survival and recurrence in CC. We performed a retrospective analysis of registry data of patients with CC to train-test-crossvalidate ml-BBNs using the Department of Defense Automated Central Tumor Registry (January 1993 to December 2004). Cases with events or follow-up that passed quality control were stratified into 1-, 2-, 3-, and 5-year survival cohorts. ml-BBNs were trained using machine-learning algorithms and k-fold crossvalidation and receiver operating characteristic curve analysis used for validation. BBNs were comprised of 5301 patients and areas under the curve ranged from 0.85 to 0.90. Positive predictive values for recurrence and mortality ranged from 78 to 84 per cent and negative predictive values from 74 to 90 per cent by survival cohort. In the 12-month model alone, 1,132,462,080 unique rule sets allow physicians to predict individual recurrence/mortality estimates. Patients with Stage II (N0M0) CC benefit from chemotherapy at different rates. At one year, all patients older than 73 years of age with T2-4 tumors and abnormal carcinoembryonic antigen levels benefited, whereas at five years, all had relative reduction in mortality with the largest benefit amongst elderly, highest T-stage patients. ml-BBN can readily predict which high-risk patients benefit from adjuvant therapy. CDSS tools yield individualized, clinically relevant estimates of outcomes to assist clinicians in treatment planning.",""
"Building Bayesian belief networks in the absence of data involves the challenging task of eliciting conditional probabilities from experts to parameterize the model. In this paper, we develop an analytical method for determining the optimal order for eliciting these probabilities. Our method uses prior distributions on network parameters and a novel expected proximity criteria, to propose an order that maximizes information gain per unit elicitation time. We present analytical results when priors are uniform Dirichlet; for other priors, we find through experiments that the optimal order is strongly affected by which variables are of primary interest to the analyst. Our results should prove useful to researchers and practitioners involved in belief network model building and elicitation.",""
"The significant increase in the complexity and autonomy of the hardware systems renders the verification of the functional safety of each individual component as well as of the entire system a complex task and underlines the need for integrated, model based tools that would assist this process. In this paper the authors present such a tool, coupled with an approach to functional safety analysis, based on the integration of functional tests into the model itself. The analysis of the resulting model is done through a stochastic Bayesian model. This approach strives to both bypass the necessity for costly hardware testing and integrate the functional safety analysis into an intuitive component development process. (C) 2013 Elsevier B.V. All rights reserved.",""
"Fall incidents among the elderly often occur in the home and can cause serious injuries affecting their independent living. This paper presents an approach where data from wearable sensors integrated in a smart home environment is combined using a dynamic Bayesian network. The smart home environment provides contextual data, obtained from environmental sensors, and contributes to assessing a fall risk probability. The evaluation of the developed system is performed through simulation. Each time step is represented by a single user activity and interacts with a fall sensors located on a mobile device. A posterior probability is calculated for each recognized activity or contextual information. The output of the system provides a total risk assessment of falling given a response from the fall sensor.",""
"The purpose of this paper is the study of relationships between negative psychological features in young competitive team players via a Bayesian network (BN). The BN let us describe in graphical terms the dependencies and conditional independencies between variables in a multivariate context, and therefore it let us analyze the relationships between the type of motivational climate and orientation of team players (from two different frameworks: goal orientation theory, and self-determination motivation theory), and we took into account their willingness to accept the use of gamesmanship and cheating at the games, related with somatic and cognitive pre-competitive anxiety. The BN analysis includes three reasoning patterns: evidential reasoning, causal reasoning and intercausal reasoning. We built a BN from a data set composed of twelve psychological variables, which were identified as relevant to this study. The structure and parameters were learnt with TETRAD IV and the BN was implemented in Netica software.",""
"Many biological research areas such as drug design require gene regulatory networks to provide clear insight and understanding of the cellular process in living cells. This is because interactions among the genes and their products play an important role in many molecular processes. A gene regulatory network can act as a blueprint for the researchers to observe the relationships among genes. Due to its importance, several computational approaches have been proposed to infer gene regulatory networks from gene expression data. In this review, six inference approaches are discussed: Boolean network, probabilistic Boolean network, ordinary differential equation, neural network, Bayesian network, and dynamic Bayesian network. These approaches are discussed in terms of introduction, methodology and recent applications of these approaches in gene regulatory network construction. These approaches are also compared in the discussion section. Furthermore, the strengths and weaknesses of these computational approaches are described. (C) 2014 Elsevier Ltd. All rights reserved.","In this review, six inference approaches are discussed: Boolean network, probabilistic Boolean network, ordinary differential equation, neural network, Bayesian network, and dynamic Bayesian network."
"Influence maximization problem has gained much attention, which is to find the most influential people. Efficient algorithms have been proposed to solve influence maximization problem according to the proposed diffusion models. Existing diffusion models assume that a node influences its neighbors once, and there is no time constraint in activation process. However, in real-world marketing situations, people influence his/her acquaintances repeatedly, and there are often time restrictions for a marketing. This paper proposes a new realistic influence diffusion model Continuously activated and Time-restricted IC (CT-IC) model which generalizes the IC model. In CT-IC model, every active node activate its neighbors repeatedly, and activation continues until a given time. We first prove CT-IC model satisfies monotonicity and submodularity for influence spread. We then provide an efficient method for calculating exact influence spread for a directed tree. Finally, we propose a scalable influence evaluation algorithm under CT-IC model CT-IPA. Our experiments show CT-IC model finds seeds of higher influence spread than IC model, and CT-IPA is four orders of magnitude faster than the greedy algorithm while providing similar influence spread. (C) 2014 Elsevier B.V. All rights reserved.",""
"The graphical structure of a Bayesian network (BN) makes it a technology well-suited for developing decision support models from a combination of domain knowledge and data. The domain knowledge of experts is used to determine the graphical structure of the BN, corresponding to the relationships and between variables, and data is used for learning the strength of these relationships. However, the available data seldom match the variables in the structure that is elicited from experts, whose models may be quite detailed; consequently, the structure needs to be abstracted to match the data. Up to now, this abstraction has been informal, loosening the link between the final model and the experts' knowledge. In this paper, we propose a method for abstracting the BN structure by using four 'abstraction' operations: node removal, node merging, state-space collapsing and edge removal. Some of these steps introduce approximations, which can be identified from changes in the set of conditional independence (CI) assertions of a network. (C) 2014 Elsevier B.V. All rights reserved.",""
"Problem of waste treatment is very important for environment and the share of tires in this scope is high. The aim of this paper is to analyse retreading as one very interesting way of tires' treatment. In practice, firms involved with commercial vehicles exploitation have to decide whether to retread used tires or not, depending on the number of retreadings of used tires and travelled distances after each retreading. An approach based on Bayesian networks is proposed as a tool for decision making support in tire retreading process. Analysis is performed on database of tires' exploitation from a company of public passenger transportation and the statistical results are used as inputs to the proposed model. The results obtained according to the proposed model provide a good basis when it comes to making a decision whether to retread or not a used tire. (C) 2014 Elsevier B.V. All rights reserved.",""
"Bayesian networks (BNs) are probabilistic models used for classification and clustering in several fields. Their ability to deal with unobserved variables and to integrate data and expert knowledge make them an appropriate technique for modeling eye functionality measurements in glaucoma. In this study, a set of BNs is used to simultaneously perform classification of early glaucoma and cluster data into different stages of disease. A novel learning algorithm that combines clustering and quasi-greedy search is also proposed. The classification performances of the models are evaluated on an independent dataset, while the clusters are compared to K-means, previous publications, and direct knowledge. The use of clustering and structure learning enabled the exploration of the visual field patterns of the diseasewhile obtaining good results both on pre-(50% sensitivity at 90% specificity) and post-(85% sensitivity at 90% specificity) diagnosis data. Clusters obtained were insightful and in conformity with consolidated knowledge in the field.","Bayesian networks (BNs) are probabilistic models used for classification and clustering in several fields."
"A risk-based inspection system might improve the efficiency of the organic farming certification system and ultimately provide a basis for increased competitiveness of this sector. This requires the definition of an effective inspection procedure that allows statistical evaluation of critical risk factors for noncompliance. In this article, we present a study based on data from selected control bodies in five European countries that is aimed at determining the feasibility of risk-based inspections in the organic sector according to the data that are currently routinely recorded. Bayesian networks are used for identification of the factors that can affect the risk of noncompliance. The results show that previous/concurrent noncompliant behavior explains most of the risk, and that the risk increases with farm size and the complexity of their operations. The data currently recorded by control bodies appear to be insufficient to establish an effective risk-based approach to these inspections.",""
"In this paper, we propose an intelligent situation recognition model by collecting and analyzing multiple sensor signals. Multiple sensor signals are collected for fixed time window. A training set of collected sensor data for each situation is provided to K2-learning algorithm to generate Bayesian networks representing causal relationship between sensors for the situation. Statistical characteristics of sensor values and topological characteristics of generated graphs are learned for each situation. A neural network is designed to classify the current situation based on the extracted features from collected multiple sensor values. The proposed method is implemented and tested with UCI machine learning repository data.",""
"Mechanistic philosophy of science views a large part of scientific activity as engaged in modelling mechanisms. While science textbooks tend to offer qualitative models of mechanisms, there is increasing demand for models from which one can draw quantitative predictions and explanations. Casini et al. (Theoria 26(1):5-33, 2011) put forward the Recursive Bayesian Networks (RBN) formalism as well suited to this end. The RBN formalism is an extension of the standard Bayesian net formalism, an extension that allows for modelling the hierarchical nature of mechanisms. Like the standard Bayesian net formalism, it models causal relationships using directed acyclic graphs. Given this appeal to acyclicity, causal cycles pose a prima facie problem for the RBN approach. This paper argues that the problem is a significant one given the ubiquity of causal cycles in mechanisms, but that the problem can be solved by combining two sorts of solution strategy in a judicious way.",""
"A fully automated data-driven methodology for the detection of pipe bursts and other events that induce similar abnormal pressure/flow variations (e.g.,unauthorized consumptions) at the district metered area (DMA) level has been recently developed by the authors. This methodology works by simultaneously analyzing the data coming on-line from all the pressure and/or flow sensors deployed in a DMA. It makes synergistic use of several self-learning artificial intelligence (AI) and statistical techniques. These include (1)wavelets for the de-noising of the recorded pressure/flow signals; (2)artificial neural networks (ANNs) for the short-term forecasting of pressure/flow signal values; (3)statistical process control (SPC) techniques for the short-term and long-term analysis of the burst/other event-induced pressure/flow variations; and (4)Bayesian inference systems (BISs) for inferring the probability that a pipe burst/other event has occurred in the DMA being studied, raising the corresponding detection alarms, and provide information useful for performing event diagnosis. This paper focuses on the (re)calibration of the above detection methodology with the aim of improving the forecasting performances of the ANN models and the classification performances of the BIS used to raise the detection alarms (i.e.,DMA-level BIS). This is achieved by using (1)an Evolutionary Algorithm optimization strategy for selecting the best ANN input structures and related parameter values to be used for training the ANN models, and (2)an Expectation Maximization strategy for (re)calibrating the values in the conditional probability tables (CPTs) of the DMA-level BIS. The (re)calibration procedure is tested on a case study involving several DMAs in the U.K. with real-life pipe bursts/other events, engineered pipe burst events (i.e.,simulated by opening fire hydrants), and synthetic pipe burst events (i.e.,simulated by arbitrarily adding \"burst flows\" to an actual flow signal). The results obtained illustrate that the new (re)calibration procedure improves the performance of the event detection methodology in terms of increased detection speed and reliability.","These include (1)wavelets for the de-noising of the recorded pressure/flow signals; (2)artificial neural networks (ANNs) for the short-term forecasting of pressure/flow signal values; (3)statistical process control (SPC) techniques for the short-term and long-term analysis of the burst/other event-induced pressure/flow variations; and (4)Bayesian inference systems (BISs) for inferring the probability that a pipe burst/other event has occurred in the DMA being studied, raising the corresponding detection alarms, and provide information useful for performing event diagnosis."
"The following article outlines of an assessment of the adaptive capacity of stakeholder groups in the Trondheimsfjord region to the impacts related to local changes in Periphylla periphylla (jellyfish) concentrations. This paper addresses the interaction between the socio-ecological system and the marine ecosystem and the management challenges inherent therein by focusing on a serious management problem that is occurring in several Norwegian fjords. This is the recent superabundance of the lower trophic level jellyfish species P. periphylla, which competes with commercial Norwegian fish species for a wide variety of pelagic organisms including redfeed (Calanus finmarchicus), a key species in the coastal ecosystem and a particularly important food item for all codfishes in coastal waters. P. periphylla has, however, also some properties that might make it a valuable new resource in Norwegian waters, namely its potential for being a new and abundant source of collagen. The question addressed here is how to manage this jellyfish species in a manner that is rational from both sodo-political and ecological perspectives, exploring stakeholder perceptions concerning their adaptation options and capacity to implement these options to this new resource and management mitigation options based on a set of stakeholder driven future scenarios. (C) 2014 Elsevier Ltd. All rights reserved.",""
"Weigh-in-Motion (WIM) systems are used, among other applications, in pavement and bridge reliability. The system measures quantities such as individual axle load, vehicular loads, vehicle speed, vehicle length and number of axles. Because of the nature of traffic configuration, the quantities measured are evidently regarded as random variables. The dependence structure of the data of such complex systems as the traffic systems is also very complex. It is desirable to be able to represent the complex multidimensional-distribution with models where the dependence may be explained in a clear way and different locations where the system operates may be treated simultaneously. Bayesian Networks (BNs) are models that comply with the characteristics listed above. In this paper we discuss BN models and results concerning their ability to adequately represent the data. The paper places attention on the construction and use of the models. We discuss applications of the proposed BNs in reliability analysis. In particular we show how the proposed BNs may be used for computing design values for individual axles, vehicle weight and maximum bending moments of bridges in certain time intervals. These estimates have been used to advise authorities with respect to bridge reliability. Directions as to how the model may be extended to include locations where the WIM system does not operate are given whenever possible. These ideas benefit from structured expert judgment techniques previously used to quantify Hybrid Bayesian Networks (HBNs) with success. (C) 2014 Elsevier Ltd. All rights reserved.",""
"The widespread use and applicability of Evolutionary Algorithms is due in part to the ability to adapt them to a particular problem-solving context by tuning their parameters. This is one of the problems that a user faces when applying an Evolutionary Algorithm to solve a given problem. Before running the algorithm, the user typically has to specify values for a number of parameters, such as population size, selection rate, and probability operators. This paper empirically assesses the performance of an automatic parameter tuning system in order to avoid the problems of time requirements and the interaction of parameters. The system, based on Bayesian Networks and Case-Based Reasoning methodology, estimates the best parameter setting for maximizing the performance of Evolutionary Algorithms. The algorithms are applied to solve a basic problem in constraint-based, geometric parametric modeling, as an instance of general constraint-satisfaction problems. The experimental results demonstrate the validity of the proposed system and its potential effectiveness for configuring algorithms. (C) 2014 Elsevier B.V. All rights reserved.",""
"In multi-label classification the goal is to assign an instance to a set of different classes. This task is normally addressed either by defining a compound class variable with all the possible combinations of labels (label power-set methods) or by building independent classifiers for each class (binary relevance methods). The first approach suffers from high computationally complexity, while the second approach ignores possible dependencies among classes. Chain classifiers have been recently proposed to address these problems, where each classifier in the chain learns and predicts the label of one class given the attributes and all the predictions of the previous classifiers in the chain. In this paper we introduce a method for chaining Bayesian classifiers that combines the strengths of classifier chains and Bayesian networks for multi-label classification. A Bayesian network is induced from data to: (i) represent the probabilistic dependency relationships between classes, (ii) constrain the number of class variables used in the chain classifier by considering conditional independence conditions, and (iii) reduce the number of possible chain orders. The effects in the Bayesian chain classifier performance of considering different chain orders, training strategies, number of class variables added in the base classifiers, and different base classifiers, are experimentally assessed. In particular, it is shown that a random chain order considering the constraints imposed by a Bayesian network with a simple tree-based structure can have very competitive results in terms of predictive performance and time complexity against related state-of-the-art approaches. (C) 2013 Elsevier B. V. All rights reserved.","In multi-label classification the goal is to assign an instance to a set of different classes."
"Effective knowledge integration plays a very important role in knowledge engineering and knowledge-based machine learning. The combination of Bayesian networks (BNs) has shown a promising technique in knowledge fusion and the way of combining BNs remains a challenging research topic. An effective method of BNs combination should not impose any particular constraints on the underlying BNs such that the method is applicable to a variety of knowledge engineering scenarios. In general, a sound method of BNs combination should satisfy three fundamental criteria, that is, avoiding cycles, preserving the conditional independencies of BN structures, and preserving the characteristics of individual BN parameters, respectively. However, none of the existing BNs combination method satisfies all the aforementioned criteria. Accordingly, there are only marginal theoretical contributions and limited practical values of previous research on BNs combination. In this paper, following the approach adopted by existing BNs combination methods, we assume that there is an ancestral ordering shared by individual BNs that helps avoid cycles. We first design and develop a novel BNs combination method that focuses on the following two aspects: (1) a generic method for combining BNs that does not impose any particular constraints on the underlying BNs, and (2) an effective approach ensuring that the last two criteria of BNs combination are satisfied. Further through a formal analysis, we compare the properties of the proposed method and that of three classical BNs combination methods, and hence to demonstrate the distinctive advantages of the proposed BNs combination method. Finally, we apply the proposed method in recommender systems for estimating users' ratings based on their implicit preferences, bank direct marketing for predicting clients' willingness of deposit subscription, and disease diagnosis for assessing patients' breast cancer risk. (C) 2013 Elsevier Ltd. All rights reserved.",""
"Three levels of competitiveness affect the success of business enterprises in a globally competitive environment: the competitiveness of the company, the competitiveness of the industry in which the company operates and the competitiveness of the country where the business is located. This study analyses the competitiveness of the automotive industry in association with the national competitiveness perspective using a methodology based on Bayesian Causal Networks. First, we structure the competitiveness problem of the automotive industry through a synthesis of expert knowledge in the light of the World Economic Forum's competitiveness indicators. Second, we model the relationships among the variables identified in the problem structuring stage and analyse these relationships using a Bayesian Causal Network. Third, we develop policy suggestions under various scenarios to enhance the national competitive advantages of the automotive industry. We present an analysis of the Turkish automotive industry as a case study. It is possible to generalise the policy suggestions developed for the case of Turkish automotive industry to the automotive industries in other developing countries where country and industry competitiveness levels are similar to those of Turkey. (C) 2013 Elsevier B.V. All rights reserved.",""
"Background: It is of great importance to identify molecular processes and pathways that are involved in disease etiology. Although there has been an extensive use of various high-throughput methods for this task, pathogenic pathways are still not completely understood. Often the set of genes or proteins identified as altered in genome-wide screens show a poor overlap with canonical disease pathways. These findings are difficult to interpret, yet crucial in order to improve the understanding of the molecular processes underlying the disease progression. We present a novel method for identifying groups of connected molecules from a set of differentially expressed genes. These groups represent functional modules sharing common cellular function and involve signaling and regulatory events. Specifically, our method makes use of Bayesian statistics to identify groups of co-regulated genes based on the microarray data, where external information about molecular interactions and connections are used as priors in the group assignments. Markov chain Monte Carlo sampling is used to search for the most reliable grouping. Results: Simulation results showed that the method improved the ability of identifying correct groups compared to traditional clustering, especially for small sample sizes. Applied to a microarray heart failure dataset the method found one large cluster with several genes important for the structure of the extracellular matrix and a smaller group with many genes involved in carbohydrate metabolism. The method was also applied to a microarray dataset on melanoma cancer patients with or without metastasis, where the main cluster was dominated by genes related to keratinocyte differentiation. Conclusion: Our method found clusters overlapping with known pathogenic processes, but also pointed to new connections extending beyond the classical pathways.",""
"In Bayesian networks, the K2 algorithm is one of the most effective structure-learning methods. However, because the performance of the K2 algorithm depends on node ordering, more effective node ordering inference methods are needed. In this paper, we therefore introduce a new node ordering algorithm based on a novel scoring function. Because a child has a better conditional frequency or probability under a correct parent than an incorrect one, we have designed a novel scoring function to evaluate this conditional frequency. Given two variables, our scoring function infers which is the better parent variable. Consequently, the proposed method infers candidate parents by considering all pairs of variables; it then uses these parents as input for the K2 algorithm. Experimental results indicate that our proposed method outperforms previous methods. (C) 2014 Elsevier B. V. All rights reserved.","However, because the performance of the K2 algorithm depends on node ordering, more effective node ordering inference methods are needed."
"Inferring gene regulatory networks (GRNs) is a major issue in systems biology, which explicitly characterizes regulatory processes in the cell. The Path Consistency Algorithm based on Conditional Mutual Information (PCA-CMI) is a well-known method in this field. In this study, we introduce a new algorithm (IPCA-CMI) and apply it to a number of gene expression data sets in order to evaluate the accuracy of the algorithm to infer GRNs. The IPCA-CMI can be categorized as a hybrid method, using the PCA-CMI and Hill-Climbing algorithm (based on MIT score). The conditional dependence between variables is determined by the conditional mutual information test which can take into account both linear and nonlinear genes relations. IPCA-CMI uses a score and search method and defines a selected set of variables which is adjacent to one of X or Y. This set is used to determine the dependency between X and Y. This method is compared with the method of evaluating dependency by PCA-CMI in which the set of variables adjacent to both X and Y, is selected. The merits of the IPCA-CMI are evaluated by applying this algorithm to the DREAM3 Challenge data sets with n variables and n samples (n = 10,50,100) and to experimental data from Escherichia coli containing 9 variables and 9 samples. Results indicate that applying the IPCA-CMI improves the precision of learning the structure of the GRNs in comparison with that of the PCA-CMI.",""
"In this paper, it is argued that single function dual process theory is a more credible psychological account of non-monotonicity in human conditional reasoning than recent attempts to apply logic programming (LP) approaches in artificial intelligence to these data. LP is introduced and among other critiques, it is argued that it is psychologically unrealistic in a similar way to hash coding in the classicism vs. connectionism debate. Second, it is argued that causal Bayes nets provide a framework for modelling probabilistic conditional inference in System 2 that can deal with patterns of inference LP cannot. Third, we offer some speculations on how the cognitive system may avoid problems for System 1 identified by Fodor in 1983. We conclude that while many problems remain, the probabilistic single function dual processing theory is to be preferred over LP as an account of the non-monotonicity of human reasoning.","Second, it is argued that causal Bayes nets provide a framework for modelling probabilistic conditional inference in System 2 that can deal with patterns of inference LP cannot."
"Analysis of people's movements represented by continuous sequences of spatio-temporal data tuples have received lots of attention in recent years. The focus of those studies was mostly GPS data recorded on a constant sample rate. However, the creation of intelligent location-aware models and environments also requires reliable localization in indoor environments as well as in mixed indoor/outdoor scenarios. In these cases, signal loss makes usage of GPS infeasible; therefore other recording technologies evolved. Our approach is analysis of episodic movement data. This data contains some uncertainties among time (continuity), space (accuracy), and the number of recorded objects (coverage). Prominent examples of episodic movement data are spatio-temporal activity logs, cell-based tracking data, and billing records. To give one detailed example, Bluetooth tracking monitors the presence of mobile phones and intercoms within a sensor's footprints. Usage of multiple sensors provides flows among the sensors. Most existing data mining algorithms use interpolation and therefore are infeasible for this kind of data. For example, speed and movement direction cannot be derived directly from episodic data; trajectories may not be depicted as a continuous line; and densities cannot be computed. Still, the data hold much information on group movement. Our approach is to aggregate movement in order to overcome the uncertainties. Deriving a number of objects for the spatio-temporal compartments and transitions among them gives interesting insights on the spatio-temporal behavior of moving objects. As a next step to support analysts, we propose clustering the spatio-temporal presence and flow situations. This work focuses as well on creation of a descriptive probability model for the movement based on Spatial Bayesian Networks. We present our methods on a real world data set collected during a football game in Nimes, France in June 2011.",""
"A Bayesian network (BN) based fault diagnosis framework for semiconductor etching equipment is presented. Suggested framework contains data preprocessing, data synchronization, time series modeling, and BN inference, and the established BNs show the cause and effect relationship in the equipment module level. Statistically significant state variable identification (SVID) data of etch equipment are preselected using principal component analysis (PCA) and derivative dynamic time warping (DDTW) is employed for data synchronization. Elman's recurrent neural networks (ERNNs) for individual SVID parameters are constructed, and the predicted errors of ERNNs are then used for assigning prior conditional probability in BN inference of the fault diagnosis. For the demonstration of the proposed methodology, 300 mm etch equipment model is reconstructed in subsystem levels, and several fault diagnosis scenarios are considered. BNs for the equipment fault diagnosis consists of three layers of nodes, such as root cause (RC), module (M), and data parameter (DP), and the constructed BN illustrates how the observed fault is related with possible root causes. Four out of five different types of fault scenarios are successfully diagnosed with the proposed inference methodology.","Suggested framework contains data preprocessing, data synchronization, time series modeling, and BN inference, and the established BNs show the cause and effect relationship in the equipment module level."
"This paper deals with the estimation of state changes in system descriptions for dynamic Bayesian networks (DBNs) by using a genetic procedure and particle filters (PFs). We extend the DBN scheme to more general cases with unknown Directed Acyclic Graph (DAG) and state changes. First, we summarize the basic model of DBN where the DAG can be changed and the state transition occurs. In the genetic procedure to estimate DAG changes, we utilize the mutation operation (called Evolutionary Programming: EP) to the DAG to maintain consistency. By defining the possible DAG structure and state changes as particles, we formalize the optimization as the PF procedure. The weight of a particle representing the DAG and state transition is defined as the capability to approximate the probability distribution function obtained from a table of cases. We apply the estimation scheme of the paper to an artificially generated DBN, in which the state of the variables and the changed structure of the DAG are already known, to prove the applicability of the method, and discuss its applicability to debt rating. (C) 2014 Elsevier B.V. All rights reserved.",""
"The ever evolving telecommunication networks in terms of their technology, infrastructure, and supported services have always posed challenges to the network managers to come up with an efficient Network Management System (NMS) for effective network management. The need for automated and efficient management of the current networks, more specifically the Next Generation Network (NGN), is the subject addressed in this research. A detailed description of the management challenges in the context of current networks is presented and then this work enlists the desired features and characteristics of an efficient NMS. It then proposes that there is a need to apply Artificial Intelligence (AI) and Machine Learning (ML) approaches for enhancing and automating the functions of NMS. The first contribution of this work is a comprehensive survey of the AI and ML approaches applied to the domain of NM. The second contribution of this work is that it presents the reasoning and evidence to support the choice of Bayesian Networks (BN) as a viable solution for ML-based NMS. The final contribution of this work is that it proposes and implements three novel NM solutions based on the BN approach, namely BN-based Admission Control (BNAC), BN-based Distributed Admission Control (BNDAC) and BN-based Intelligent Traffic Engineering (BNITE), along with the description of algorithms underpinning the proposed framework.",""
"We investigate the usefulness of complex flood damage models for predicting relative damage to residential buildings in a spatial and temporal transfer context. We apply eight different flood damage models to predict relative building damage for five historic flood events in two different regions of Germany. Model complexity is measured in terms of the number of explanatory variables which varies from 1 variable up to 10 variables which are singled out from 28 candidate variables. Model validation is based on empirical damage data, whereas observation uncertainty is taken into consideration. The comparison of model predictive performance shows that additional explanatory variables besides the water depth improve the predictive capability in a spatial and temporal transfer context, i.e., when the models are transferred to different regions and different flood events. Concerning the trade-off between predictive capability and reliability the model structure seem more important than the number of explanatory variables. Among the models considered, the reliability of Bayesian network-based predictions in space-time transfer is larger than for the remaining models, and the uncertainties associated with damage predictions are reflected more completely.",""
"The advances in computer systems and resources make it possible to develop models to simulate the behavior of the fluids in greenhouses. However, the prediction of the gradients of mass and energy in the greenhouses with the crop and natural ventilation is difficult due to the stochastic nature of the wind and the relationships of dependence among temperature, CO, and relative humidity. There are heuristic techniques, such as the Bayesian Networks, which help to know the relationships among the variables that cannot be determined with statistical tools. The objective of the present study was to determine temperature, CO, concentration and relative humidity with respect to crop height, in a greenhouse with natural ventilation, by means of Bayesian Networks applied to a model of Computational Fluid Dynamic. The Bayesian Network made it possible to determine the spaces of the greenhouse with adverse environmental conditions for the crop development and the most probable climatic states, from the relationships among the variables studied.",""
"Many medical conditions are only indirectly observed through symptoms and tests. Developing predictive models for such conditions is challenging since they can be thought of as 'latent' variables. They are not present in the data and often get confused with measurements. As a result, building a model that fits data well is not the same as making a prediction that is useful for decision makers. In this paper, we present a methodology for developing Bayesian network (BN) models that predict and reason with latent variables, using a combination of expert knowledge and available data. The method is illustrated by a case study into the prediction of acute traumatic coagulopathy (ATC), a disorder of blood clotting that significantly increases the risk of death following traumatic injuries. There are several measurements for ATC and previous models have predicted one of these measurements instead of the state of ATC itself. Our case study illustrates the advantages of models that distinguish between an underlying latent condition and its measurements, and of a continuing dialogue between the modeller and the domain experts as the model is developed using knowledge as well as data. (C) 2013 Elsevier Inc. All rights reserved.",""
"Introduction: Autonomous chronic disease management requires models that are able to interpret time series data from patients. However, construction of such models by means of machine learning requires the availability of costly health-care data, often resulting in small samples. We analysed data from chronic obstructive pulmonary disease (COPD) patients with the goal of constructing a model to predict the occurrence of exacerbation events, i.e., episodes of decreased pulmonary health status. Methods: Data from 10 COPD patients, gathered with our home monitoring system, were used for temporal Bayesian network learning, combined with bootstrapping methods for data analysis of small data samples. For comparison a temporal variant of augmented naive Bayes models and a temporal nodes Bayesian network (TNBN) were constructed. The performances of the methods were first tested with synthetic data. Subsequently, different COPD models were compared to each other using an external validation data set. Results: The model learning methods are capable of finding good predictive models for our COPD data. Model averaging over models based on bootstrap replications is able to find a good balance between true and false positive rates on predicting COPD exacerbation events. Temporal naive Bayes offers an alternative that trades some performance for a reduction in computation time and easier interpretation. (C) 2013 Elsevier Inc. All rights reserved.",""
"Multi Organ Dysfunction Syndrome (MODS) represents a continuum of physiologic derangements and is the major cause of death in the Intensive Care Unit (ICU). Scoring systems for organ failure have become an integral part of critical care practice and play an important role in ICU-based research by tracking disease progression and facilitating patient stratification based on evaluation of illness severity during ICU stay. In this study a Dynamic Bayesian Network (DBN) was applied to model SOFA severity score changes in 79 adult critically ill patients consecutively admitted to the general ICU of the Sant'Andrea University hospital (Rome, Italy) from September 2010 to March 2011, with the aim to identify the most probable sequences of organs failures in the first week after the ICU admission. Approximately 56% of patients were admitted into the ICU with lung failure and about 27% of patients with heart failure. Results suggest that, given the first organ failure at the ICU admission, a sequence of organ failures can be predicted with a certain degree of probability. Sequences involving heart, lung, hematologic system and liver turned out to be the more likely to occur, with slightly different probabilities depending on the day of the week they occur. DBNs could be successfully applied for modeling temporal systems in critical care domain. Capability to predict sequences of likely organ failures makes DBNs a promising prognostic tool, intended to help physicians in undertaking therapeutic decisions in a patient-tailored approach. (C) 2014 Published by Elsevier Inc.",""
"Belief polarization occurs when 2 people with opposing prior beliefs both strengthen their beliefs after observing the same data. Many authors have cited belief polarization as evidence of irrational behavior. We show, however, that some instances of polarization are consistent with a normative account of belief revision. Our analysis uses Bayesian networks to characterize different kinds of relationships between hypotheses and data, and distinguishes between cases in which normative reasoners with opposing beliefs should both strengthen their beliefs, cases in which both should weaken their beliefs, and cases in which one should strengthen and the other should weaken his or her belief. We apply our analysis to several previous studies of belief polarization and present a new experiment that suggests that people tend to update their beliefs in the directions predicted by our normative account.",""
"Most present research into facial expression recognition focuses on the visible spectrum, which is sensitive to illumination change. In this paper, we focus on integrating thermal infrared data with visible spectrum images for spontaneous facial expression recognition. First, the active appearance model AAM parameters and three defined head motion features are extracted from visible spectrum images, and several thermal statistical features are extracted from infrared (IR) images. Second, feature selection is performed using the F-test statistic. Third, Bayesian networks BNs and support vector machines SVMs are proposed for both decision-level and feature-level fusion. Experiments on the natural visible and infrared facial expression (NVIE) spontaneous database show the effectiveness of the proposed methods, and demonstrate thermal IR images' supplementary role for visible facial expression recognition.",""
"The main objective of this study is the development of a correlation model in dynamic Bayesian belief networks (DBBNs) followed by an inverse economic analysis. This is based on a quadratic hierarchical Bayesian inference prediction method using Markov chain Monte Carlo simulations. The developed model is implemented to predict the future degradation and maintenance budget for a suspension bridge system. Bayesian inference is applied to find the posterior probability density function of the source parameters (damage indices and serviceability), given 10 years maintenance data. The simulated risk prediction under decreased serviceability conditions gives posterior distributions based on a prior distribution and likelihood data updated from annual maintenance tasks. Compared with a conventional linear prediction model, the proposed quadratic model provides highly improved convergence and closeness to the measured data. Finally, the developed inverse DBBN analysis method allows forecasts of future performance and the financial management of complex infrastructures by providing the sensitivity of serviceability and risky factors to the maintenance budgets of structural components and the overall system.","This is based on a quadratic hierarchical Bayesian inference prediction method using Markov chain Monte Carlo simulations."
"Integrated importance measures have been developed to study the effect of component state probabilities and state transition rates on the multi-state system performance, identifying the weakest component to facilitate the system maintenance and optimization activities. This article proposes an analytical method based on multi-state multi-valued decision diagram for computing the integrated importance measure values. Following a discussion of decomposition and physical meaning of integrated importance measures, the modeling method of multi-state multi-valued decision diagram based on multi-state fault tree analysis is introduced. A five-step integrated importance measure analysis approach based on multi-state multi-valued decision diagram is then proposed. Two case studies are implemented to demonstrate the presented methods. Complexity analysis shows that the multi-state multi-valued decision diagram-based method is more computationally efficient than the existing method using Markov-Bayesian networks.",""
"Graphical models for probabilistic reasoning are now in widespread use. Many approaches have been developed such as Bayesian network. A newly developed approach named as dynamic uncertain causality graph (DUCG) is initially presented in a previous paper, in which only the inference algorithm in terms of individual events and probabilities is addressed. In this paper, we first explain the statistic basis of DUCG. Then, we extend the algorithm to the form of matrices of events and probabilities. It is revealed that the representation of DUCG can be incomplete and the exact probabilistic inference may still be made. A real application of DUCG for fault diagnoses of a generator system of a nuclear power plant is demonstrated, which involves >600 variables. Most inferences take <1 s with a laptop computer. The causal logic between inference result and observations is graphically displayed to users so that they know not only the result, but also why the result obtained.","A newly developed approach named as dynamic uncertain causality graph (DUCG) is initially presented in a previous paper, in which only the inference algorithm in terms of individual events and probabilities is addressed."
"The effectiveness of various adaptation options is dependent on the capacity to plan, design and implement them. Understanding the determinants of adaptive capacity is, therefore, crucial for effective responses to climate change. This paper offers an assessment of adaptive capacity across a range of sectors in South East Queensland, Australia. The paper has four parts, including (1) an overview of adaptive capacity, in particular as a learning process; (2) a description of the various methods used to determine adaptive capacity; (3) a synthesis of the determinants of adaptive capacity; and (4) the identification of mechanisms to build adaptive capacity in the region. We conclude that the major issue impacting adaptive capacity is not the availability of physical resources but the dominant social, political and institutional culture of the region.",""
"Maritime accidents involving ships carrying passengers may pose a high risk with respect to human casualties. For effective risk mitigation, an insight into the process of risk escalation is needed. This requires a proactive approach when it comes to risk modelling for maritime transportation systems. Most of the existing models are based on historical data on maritime accidents, and thus they can be considered reactive instead of proactive. This paper introduces a systematic, transferable and proactive framework estimating the risk for maritime transportation systems, meeting the requirements stemming from the adopted formal definition of risk. The framework focuses on ship-ship collisions in the open sea, with a RoRo/Passenger ship (RoPax) being considered as the struck ship. First, it covers an identification of the events that follow a collision between two ships in the open sea, and, second, it evaluates the probabilities of these events, concluding by determining the severity of a collision. The risk framework is developed with the use of Bayesian Belief Networks and utilizes a set of analytical methods for the estimation of the risk model parameters. Finally, a case study is presented, in which the risk framework developed here is applied to a maritime transportation system operating in the Gulf of Finland (GoF). The results obtained are compared to the historical data and available models, in which a RoPax was involved in a collision, and good agreement with the available records is found. (C) 2013 The Authors. Published by Elsevier Ltd. All rights reserved.",""
"Reliability modeling of multi-state hierarchical systems is challenging because of the complex system structures and imbalanced reliability information available at different system levels. This paper proposes a Bayesian multi-level information aggregation approach to model the reliability of multi-level hierarchical systems by utilizing all available reliability information throughout the system. Cascading failure dependency among components and/or sub-systems at the same level is explicitly considered. The proposed methodology can significantly improve the accuracy of system-level reliability modeling. A case study demonstrates the effectiveness of the proposed methodology. (C) 2013 Elsevier Ltd. All rights reserved.",""
"Promoting situation awareness is an important design objective for a wide variety of domains, especially for process systems where the information flow is quite high and poor decisions may lead to serious consequences. In today's process systems, operators are often moved to a control room far away from the physical environment, and increasing amounts of information are passed to them via automated systems, they therefore need a greater level of support to control and maintain the facilities in safe conditions. This paper proposes a situation risk awareness approach for process systems safety where the effect of ever-increasing situational complexity on human decision-makers is a concern. To develop the approach, two important aspects - addressing hazards that arise from hardware failure and reducing human error through decision-making - have been considered. The proposed situation risk awareness approach includes two major elements: an evidence preparation component and a situation assessment component. The evidence preparation component provides the soft evidence, using a fuzzy partitioning method, that is used in the subsequent situation assessment component. The situation assessment component includes a situational network based on dynamic Bayesian networks to model the abnormal situations, and a fuzzy risk estimation method to generate the assessment result. A case from US Chemical Safety Board investigation reports has been used to illustrate the application of the proposed approach. (C) 2013 Elsevier Ltd. All rights reserved.",""
"Neurodegenerative diseases in general and specifically late-onset Alzheimer's disease (LOAD) involve a genetically complex and largely obscure ensemble of causative and risk factors accompanied by complex feedback responses. The advent of \"high-throughput\" transcriptome investigation technologies such as microarray and deep sequencing is increasingly being combined with sophisticated statistical and bioinformatics analysis methods complemented by knowledge-based approaches such as Bayesian Networks or network and graph analyses. Together, such \"integrative\" studies are beginning to identify co-regulated gene networks linked with biological pathways and potentially modulating disease predisposition, outcome, and progression. Specifically, bioinformatics analyses of integrated microarray and genotyping data in cases and controls reveal changes in gene expression of both protein-coding and small and long regulatory RNAs; highlight relevant quantitative transcriptional differences between LOAD and non-demented control brains and demonstrate reconfiguration of functionally meaningful molecular interaction structures in LOAD. These may be measured as changes in connectivity in \"hub nodes\" of relevant gene networks (Zhang etal., 2013).We illustrate here the open analytical questions in the transcriptome investigation of neurodegenerative disease studies, proposing \"ad hoc\" strategies for the evaluation of differential gene expression and hints for a simple analysis of the non-coding RNA (ncRNA) part of such datasets. We then survey the emerging role of long ncRNAs (IncRNAs) in the healthy and diseased brain transcriptome and describe the main current methods for computational modeling of gene networks. We propose accessible modular and pathway-oriented methods and guidelines for bioinformatics investigations of whole transcriptome next generation sequencing datasets. We finally present methods and databases for functional interpretations of IncRNAs and propose a simple heuristic approach to visualize and represent physical and functional interactions of the coding and non-coding components of the transcriptome. Integrating in a functional and integrated vision coding and ncRNA analyses is of utmost importance for current and future analyses of neurodegenerative transcriptomes.",""
"Background: The inference of gene regulatory networks (GRNs) from experimental observations is at the heart of systems biology. This includes the inference of both the network topology and its dynamics. While there are many algorithms available to infer the network topology from experimental data, less emphasis has been placed on methods that infer network dynamics. Furthermore, since the network inference problem is typically underdetermined, it is essential to have the option of incorporating into the inference process, prior knowledge about the network, along with an effective description of the search space of dynamic models. Finally, it is also important to have an understanding of how a given inference method is affected by experimental and other noise in the data used. Results: This paper contains a novel inference algorithm using the algebraic framework of Boolean polynomial dynamical systems (BPDS), meeting all these requirements. The algorithm takes as input time series data, including those from network perturbations, such as knock-out mutant strains and RNAi experiments. It allows for the incorporation of prior biological knowledge while being robust to significant levels of noise in the data used for inference. It uses an evolutionary algorithm for local optimization with an encoding of the mathematical models as BPDS. The BPDS framework allows an effective representation of the search space for algebraic dynamic models that improves computational performance. The algorithm is validated with both simulated and experimental microarray expression profile data. Robustness to noise is tested using a published mathematical model of the segment polarity gene network in Drosophila melanogaster. Benchmarking of the algorithm is done by comparison with a spectrum of state-of-the-art network inference methods on data from the synthetic IRMA network to demonstrate that our method has good precision and recall for the network reconstruction task, while also predicting several of the dynamic patterns present in the network. Conclusions: Boolean polynomial dynamical systems provide a powerful modeling framework for the reverse engineering of gene regulatory networks, that enables a rich mathematical structure on the model search space. A C++ implementation of the method, distributed under LPGL license, is available, together with the source code, at http://www.paola-vera-licona.net/Software/EARevEng/REACT.html.","Background: The inference of gene regulatory networks (GRNs) from experimental observations is at the heart of systems biology."
"The bias-variance dilemma is a well-known and important problem in Machine Learning. It basically relates the generalization capability (goodness of fit) of a learning method to its corresponding complexity. When we have enough data at hand, it is possible to use these data in such a way so as to minimize overfitting (the risk of selecting a complex model that generalizes poorly). Unfortunately, there are many situations where we simply do not have this required amount of data. Thus, we need to find methods capable of efficiently exploiting the available data while avoiding overfitting. Different metrics have been proposed to achieve this goal: the Minimum Description Length principle (MDL), Akaike's Information Criterion (AIC) and Bayesian Information Criterion (BIC), among others. In this paper, we focus on crude MDL and empirically evaluate its performance in selecting models with a good balance between goodness of fit and complexity: the so-called bias-variance dilemma, decomposition or tradeoff. Although the graphical interaction between these dimensions (bias and variance) is ubiquitous in the Machine Learning literature, few works present experimental evidence to recover such interaction. In our experiments, we argue that the resulting graphs allow us to gain insights that are difficult to unveil otherwise: that crude MDL naturally selects balanced models in terms of bias-variance, which not necessarily need be the gold-standard ones. We carry out these experiments using a specific model: a Bayesian network. In spite of these motivating results, we also should not overlook three other components that may significantly affect the final model selection: the search procedure, the noise rate and the sample size.",""
"Motivation: Knowing the location of a protein within the cell is important for understanding its function, role in biological processes, and potential use as a drug target. Much progress has been made in developing computational methods that predict single locations for proteins. Most such methods are based on the over-simplifying assumption that proteins localize to a single location. However, it has been shown that proteins localize to multiple locations. While a few recent systems attempt to predict multiple locations of proteins, their performance leaves much room for improvement. Moreover, they typically treat locations as independent and do not attempt to utilize possible inter- dependencies among locations. Our hypothesis is that directly incorporating inter- dependencies among locations into both the classifier-learning and the prediction process can improve location prediction performance. Results: We present a new method and a preliminary system we have developed that directly incorporates inter- dependencies among locations into the location-prediction process of multiply-localized proteins. Our method is based on a collection of Bayesian network classifiers, where each classifier is used to predict a single location. Learning the structure of each Bayesian network classifier takes into account inter- dependencies among locations, and the prediction process uses estimates involving multiple locations. We evaluate our system on a dataset of single-and multi-localized proteins ( the most comprehensive protein multi-localization dataset currently available, derived from the DBMLoc dataset). Our results, obtained by incorporating inter- dependencies, are significantly higher than those obtained by classifiers that do not use inter- dependencies. The performance of our system on multi-localized proteins is comparable to a top performing system ( YLoc(+)), without being restricted only to location-combinations present in the training set.","Our hypothesis is that directly incorporating inter- dependencies among locations into both the classifier-learning and the prediction process can improve location prediction performance."
"Motivation: Reverse engineering GI networks from experimental data is a challenging task due to the complex nature of the networks and the noise inherent in the data. One way to overcome these hurdles would be incorporating the vast amounts of external biological knowledge when building interaction networks. We propose a framework where GI networks are learned from experimental data using Bayesian networks (BNs) and the incorporation of external knowledge is also done via a BN that we call Bayesian Network Prior (BNP). BNP depicts the relation between various evidence types that contribute to the event 'gene interaction' and is used to calculate the probability of a candidate graph (G) in the structure learning process. Results: Our simulation results on synthetic, simulated and real biological data show that the proposed approach can identify the underlying interaction network with high accuracy even when the prior information is distorted and outperforms existing methods.",""
"We introduce a novel algorithm for inference of causal gene interactions, termed CaSPIAN (Causal Subspace Pursuit for Inference and Analysis of Networks), which is based on coupling compressive sensing and Granger causality techniques. The core of the approach is to discover sparse linear dependencies between shifted time series of gene expressions using a sequential list-version of the subspace pursuit reconstruction algorithm and to estimate the direction of gene interactions via Granger-type elimination. The method is conceptually simple and computationally efficient, and it allows for dealing with noisy measurements. Its performance as a stand-alone platform without biological side-information was tested on simulated networks, on the synthetic IRMA network in Saccharomyces cerevisiae, and on data pertaining to the human HeLa cell network and the SOS network in E. coli. The results produced by CaSPIAN are compared to the results of several related algorithms, demonstrating significant improvements in inference accuracy of documented interactions. These findings highlight the importance of Granger causality techniques for reducing the number of false-positives, as well as the influence of noise and sampling period on the accuracy of the estimates. In addition, the performance of the method was tested in conjunction with biological side information of the form of sparse \"scaffold networks'', to which new edges were added using available RNA-seq or microarray data. These biological priors aid in increasing the sensitivity and precision of the algorithm in the small sample regime.","We introduce a novel algorithm for inference of causal gene interactions, termed CaSPIAN (Causal Subspace Pursuit for Inference and Analysis of Networks), which is based on coupling compressive sensing and Granger causality techniques."
"This paper addresses modeling causal relationships over event streams where data are unbounded and hence incremental modeling is required. There is no existing work for incremental causal modeling over event streams. Our approach is based on Popper's three conditions which are generally accepted for inferring causality - temporal precedence of cause over effect, dependency between cause and effect, and elimination of plausible alternatives. We meet these conditions by proposing a novel incremental causal network construction algorithm. This algorithm infers causality by learning the temporal precedence relationships using our own new incremental temporal network construction algorithm and the dependency by adopting a state of the art incremental Bayesian network construction algorithm called the Incremental Hill-Climbing Monte Carlo. Moreover, we provide a mechanism to infer only strong causality, which provides a way to eliminate weak alternatives. This research benefits causal analysis over event streams by providing a novel two layered causal network without the need for prior knowledge. Experiments using synthetic and real datasets demonstrate the efficacy of the proposed algorithm. (C) 2013 Elsevier Inc. All rights reserved.",""
"This study examines the challenging problem of modelling the interaction between individual attentional limitations and decision-making performance in networked human-automation system tasks. Analysis of real experimental data from a task involving networked supervision of multiple unmanned aerial vehicles by human participants shows that both task load and network message quality affect performance, but that these effects are modulated by individual differences in working memory (WM) capacity. These insights were used to assess three statistical approaches for modelling and making predictions with real experimental networked supervisory performance data: classical linear regression, non-parametric Gaussian processes and probabilistic Bayesian networks. It is shown that each of these approaches can help designers of networked human-automated systems cope with various uncertainties in order to accommodate future users by linking expected operating conditions and performance from real experimental data to observable cognitive traits like WM capacity.Practitioner Summary: Working memory (WM) capacity helps account for inter-individual variability in operator performance in networked unmanned aerial vehicle supervisory tasks. This is useful for reliable performance prediction near experimental conditions via linear models; robust statistical prediction beyond experimental conditions via Gaussian process models and probabilistic inference about unknown task conditions/WM capacities via Bayesian network models.","These insights were used to assess three statistical approaches for modelling and making predictions with real experimental networked supervisory performance data: classical linear regression, non-parametric Gaussian processes and probabilistic Bayesian networks."
"We address the problem of feature selection for classifying a diverse set of chemicals using an array of metal oxide sensors. Our aim is to evaluate a filter approach to feature selection with reference to previous work, which used a wrapper approach on the same data set, and established best features and upper bounds on classification performance. We selected feature sets that exhibit the maximal mutual information with the identity of the chemicals. The selected features closely match those found to perform well in the previous study using a wrapper approach to conduct an exhaustive search of all permitted feature combinations. By comparing the classification performance of support vector machines (using features selected by mutual information) with the performance observed in the previous study, we found that while our approach does not always give the maximum possible classification performance, it always selects features that achieve classification performance approaching the optimum obtained by exhaustive search. We performed further classification using the selected feature set with some common classifiers and found that, for the selected features, Bayesian Networks gave the best performance. Finally, we compared the observed classification performances with the performance of classifiers using randomly selected features. We found that the selected features consistently outperformed randomly selected features for all tested classifiers. The mutual information filter approach is therefore a computationally efficient method for selecting near optimal features for chemical sensor arrays.","Our aim is to evaluate a filter approach to feature selection with reference to previous work, which used a wrapper approach on the same data set, and established best features and upper bounds on classification performance."
"Dealing with incomplete data is a pervasive problem in statistical surveys. Bayesian networks have been recently used in missing data imputation. In this research, we propose a new methodology for the multivariate imputation of missing data using discrete Bayesian networks and conditional Gaussian Bayesian networks. Results from imputing missing values in coronary artery disease data set and milk composition data set as well as a simulation study from cancer-neapolitan network are presented to demonstrate and compare the performance of three Bayesian network-based imputation methods with those of multivariate imputation by chained equations (MICE) and the classical hot-deck imputation method. To assess the effect of the structure learning algorithm on the performance of the Bayesian network-based methods, two methods called Peter-Clark algorithm and greedy search-and-score have been applied. Bayesian network-based methods are: first, the method introduced by Di Zio et al. [Bayesian networks for imputation, J. R. Stat. Soc. Ser. A 167 (2004), 309-322] in which, each missing item of a variable is imputed using the information given in the parents of that variable; second, the method of Di Zio et al. [Multivariate techniques for imputation based on Bayesian networks, Neural Netw. World 15 (2005), 303-310] which uses the information in the Markov blanket set of the variable to be imputed and finally, our new proposed method which applies the whole available knowledge of all variables of interest, consisting the Markov blanket and so the parent set, to impute a missing item. Results indicate the high quality of our new proposed method especially in the presence of high missingness percentages and more connected networks. Also the new method have shown to be more efficient than the MICE method for small sample sizes with high missing rates.",""
"Adverse weather has considerable effects on reliability of electric power systems. On the other hand, using the Bayesian networks (BN) in reliability studies provides the possibility of analyzing different factors, especially the importance evaluation of system components. In this paper, a novel approach is presented to consider the weather conditions and load level variation in construction and implementation of the BN associated with the composite power systems. In this approach, the geographical division model is used to model weather conditions in various regions of the given power system and also different sections of overhead transmission lines. The obtained BN provides an effective framework for comprehensive reliability study of composite power systems. Copyright (C) 2012 John Wiley & Sons, Ltd.",""
"Inferring time-varying networks is important to understand the development and evolution of interactions over time. However, the vast majority of currently used models assume direct measurements of node states, which are often difficult to obtain, especially in fields like cell biology, where perturbation experiments often only provide indirect information of network structure. Here we propose hidden Markov nested effects models (HM-NEMs) to model the evolving network by a Markov chain on a state space of signalling networks, which are derived from nested effects models (NEMs) of indirect perturbation data. To infer the hidden network evolution and unknown parameter, a Gibbs sampler is developed, in which sampling network structure is facilitated by a novel structural Metropolis-Hastings algorithm. We demonstrate the potential of HM-NEMs by simulations on synthetic time-series perturbation data. We also show the applicability of HM-NEMs in two real biological case studies, in one capturing dynamic crosstalk during the progression of neutrophil polarisation, and in the other inferring an evolving network underlying early differentiation of mouse embryonic stem cells.",""
"An important step in systems biology is to improve our knowledge of how genes causally interact with one another. A few approaches have been proposed for the estimation of causal effects among genes, either based on observational data alone or requiring a very precise intervention design with one knock-out experiment for each gene. We recently suggested a more flexible algorithm, using a Markov chain Monte Carlo algorithm and the Mallows ranking model, that can analyze any intervention design, including partial or multiple knock-outs, using the framework of Gaussian Bayesian networks. We previously demonstrated the superior performance of this algorithm in comparison to alternative methods, although it can be computationally expensive to implement. The aim of this paper is to propose an alternative approach taking advantage of node pair ordering preferences to sample the posterior distribution according to the Babington-Smith ranking distribution. This novel algorithm proved, both in a simulation study and on the DREAM4 challenge data, to attain estimation of the causal effects as accurate as the MCMC-Mallows approach with a highly improved computational efficiency, being at least 100 times faster. We also tested our algorithm on the Rosetta Compendium dataset with more contrasted results. We nevertheless anticipate that our new approach might be very useful for practical biological applications.",""
"Bayesian networks provide an attractive representation of structured probabilistic information. There is thus much interest in 'learning' BNs from data. In this paper the problem of learning a Bayesian network using integer programming is presented. The SCLP (Solving Constraint Integer Programming) framework is used to do this. Although cutting planes are a key ingredient in our approach, primal heuristics and efficient propagation are also important.",""
"The learning of a Bayesian network structure, especially in the case of wide domains, can be a complex, time-consuming and imprecise process. Therefore, the interest of the scientific community in learning Bayesian network structure from data is increasing: many techniques or disciplines such as data mining, text categorization, and ontology building, can take advantage from this process. In the literature, there are many structural learning algorithms but none of them provides good results for each dataset. This paper introduces a method for structural learning of Bayesian networks based on a MultiExpert approach. The proposed method combines five structural learning algorithms according to a majority vote combining rule for maximizing their effectiveness and, more generally, the results obtained by using of a single algorithm. This paper shows an experimental validation of the proposed algorithm on standard datasets.",""
"We present two novel automated image analysis methods to differentiate centroblast (CB) cells from noncentroblast (non-CB) cells in digital images of H&E-stained tissues of follicular lymphoma. CB cells are often confused by similar looking cells within the tissue, therefore a system to help their classification is necessary. Our methods extract the discriminatory features of cells by approximating the intrinsic dimensionality from the subspace spanned by CB and non-CB cells. In the first method, discriminatory features are approximated with the help of singular value decomposition (SVD), whereas in the second method they are extracted using Laplacian Eigenmaps. Five hundred high-power field images were extracted from 17 slides, which are then used to compose a database of 213 CB and 234 non-CB region of interest images. The recall, precision, and overall accuracy rates of the developed methods were measured and compared with existing classification methods. Moreover, the reproducibility of both classification methods was also examined. The average values of the overall accuracy were 99.22%+/- 0.75% and 99.07%+/- 1.53% for COB and CLEM, respectively. The experimental results demonstrate that both proposed methods provide better classification accuracy of CB/non-CB in comparison with the state of the art methods. (c) 2013 International Society for Advancement of Cytometry","CB cells are often confused by similar looking cells within the tissue, therefore a system to help their classification is necessary."
"In a power transformer as one of the major component in electric power networks, partial discharge (PD) is a major source of insulation failure. Therefore the accurate and high speed techniques for locating of PD sources are required regarding to repair and maintenance. In this paper an attempt has been made to introduce the novel methods based on two different artificial neural networks (ANN) for identifying PD location in the power transformers. In present report Fuzzy ARTmap and Bayesian neural networks are employed for PD locating while using detailed model (DM) for a power transformer for simulation purposes. In present paper PD phenomenon is implemented in different points of transformer winding using three-capacitor model. Then impulse test is applied to transformer terminals in order to use produced current in neutral point for training and test of employed ANNs. In practice obtained current signals include noise components. Thus the performance of Fuzzy ARTmap and Bayesian networks for correct identification of PD location in a noisy condition for detected currents is also investigated. In this paper RBF learning procedure is used for Bayesian network, while Markov chain Monte Carlo (MCMC) method is employed for training of Fuzzy ARTmap network for locating PD in a power transformer winding and results are compared.",""
"Objectives: Temporal abstraction (TA) of clinical data aims to abstract and interpret clinical data into meaningful higher-level interval concepts. Abstracted concepts are used for diagnostic, prediction and therapy planning purposes. On the other hand, temporal Bayesian networks (TBNs) are temporal extensions of the known probabilistic graphical models, Bayesian networks. TBNs can represent temporal relationships between events and their state changes, or the evolution of a process, through time. This paper offers a survey on techniques/methods from these two areas that were used independently in many clinical domains (e.g. diabetes, hepatitis, cancer) for various clinical tasks (e.g. diagnosis, prognosis). A main objective of this survey, in addition to presenting the key aspects of TA and TBNs, is to point out important benefits from a potential integration of TA and TBNs in medical domains and tasks. The motivation for integrating these two areas is their complementary function: TA provides clinicians with high level views of data while TBNs serve as a knowledge representation and reasoning tool under uncertainty, which is inherent in all clinical tasks. Methods: Key publications from these two areas of relevance to clinical systems, mainly circumscribed to the latest two decades, are reviewed and classified. TA techniques are compared on the basis of: (a) knowledge acquisition and representation for deriving TA concepts and (b) methodology for deriving basic and complex temporal abstractions. TBNs are compared on the basis of: (a) representation of time, (b) knowledge representation and acquisition, (c) inference methods and the computatidnal demands of the network, and (d) their applications in medicine. Results: The survey performs an extensive comparative analysis to illustrate the separate merits and limitations of various TA and TBN techniques used in clinical systems with the purpose of anticipating potential gains through an integration of the two techniques, thus leading to a unified methodology for clinical systems. The surveyed contributions are evaluated using frameworks of respective key features. In addition, for the evaluation of TBN methods, a unifying clinical domain (diabetes) is used. Conclusion: The main conclusion transpiring from this review is that techniques/methods from these two areas, that so far are being largely used independently of each other in clinical domains, could be effectively integrated in the context of medical decision-support systems. The anticipated key benefits of the perceived integration are: (a) during problem solving, the reasoning can be directed at different levels of temporal and/or conceptual abstractions since the nodes of the TBNs can be complex entities, temporally and structurally and (b) during model building, knowledge generated in the form of basic and/or complex abstractions, can be deployed in a TBN. (C) 2014 Elsevier B.V. All rights reserved.","Methods: Key publications from these two areas of relevance to clinical systems, mainly circumscribed to the latest two decades, are reviewed and classified."
"The paper considers the problem of optimal sequential design for graphical models. Oil and gas exploration is the main application. Here, the outcomes at prospects or reservoir units are highly dependent on each other. The joint probability model for all node variables is considered known. As data is collected, this probability model is updated. The sequential design problem entails a dynamic selection of nodes for data collection, where the goal is to maximize utility, here defined via entropy or total expected profit. With a large number of nodes, the optimal solution to this selection problem is not tractable. An approximation based on a subdivision of the graph is considered. Within the small clusters the design problem can be solved exactly. The results on clusters are combined in a dynamic manner, to create sequential designs for the entire graph. The merging of clusters also gives upper bounds for the actual utility. Several synthetic models are studied, along with two real cases from the oil and gas industry. In these examples Bayesian networks or Markov random fields are used. The sequential model updating and data collection strategies provide useful guidelines to policy makers. (C) 2013 Elsevier B.V. All rights reserved.",""
"Operators handling abnormal situations in safety-critical environments need to be supported from a cognitive perspective to reduce their workload, stress, and consequent error rate. Of the various cognitive activities, a correct understanding of the situation, i.e. situation awareness (SA), is a crucial factor in improving performance and reducing error. However, existing system safety researches focus mainly on technical issues and often neglect SA. This study presents an innovative cognition-driven decision support system called the situation awareness support system (SASS) to manage abnormal situations in safety-critical environments in which the effect of situational complexity on human decision-makers is a concern. To achieve this objective, a situational network modeling process and a situation assessment model that exploits the specific capabilities of dynamic Bayesian networks and risk indicators are first proposed. The SASS is then developed and consists of four major elements: 1) a situation data collection component that provides the current state of the observable variables based on online conditions and monitoring systems, 2) a situation assessment component based on dynamic Bayesian networks (DBN) to model the hazardous situations in a situational network and a fuzzy risk estimation method to generate the assessment result, 3) a situation recovery component that provides a basis for decision-making to reduce the risk level of situations to an acceptable level, and 4) a human-computer interface. The SASS is partially evaluated by a sensitivity analysis, which is carried out to validate DBN-based situational networks, and SA measurements are suggested for a full evaluation of the proposed system. The performance of the SASS is demonstrated by a case taken from US Chemical Safety Board reports, and the results demonstrate that the SASS provides a useful graphical, mathematically consistent system for dealing with incomplete and uncertain information to help operators maintain the risk of dynamic situations at an acceptable level. (C) 2014 Elsevier B.V. All rights reserved.",""
"An information fusion system with local sensors sometimes requires the capability to represent the temporal changes of uncertain sensory information in dynamic and uncertain situation to access to a hypothesis node which cannot be observed directly. One of the central issue and challenging problem is the decision of what combination and order of sensors allocation should be selected between sensors, in order to maximize the global gain in the flow of information, when the data association is limited. In this area, Bayesian Networks (BNs) can constitute a coherent fusion structure and introduce different options (the combination of sensors allocation) for achieving to the hypothesis node through a number of intermediate nodes that are interrelated by cause and effect. BNs can rank the options in terms of their probabilities from Bayes' theorem calculation. But, decision making based on probabilities and numerical representations might not be appropriate. Thus, re-ranking the set of options based on multiple criteria such as those of multi-criteria decision aid (MCDA) should be ideally considered. Re-ranking and selecting the appropriate options are considered as a multi-attribute decision making (MADM) problem by user interaction as semi-automatically decision support. In this paper, Multi Attribute Decision Making (MADM) techniques as TOPSIS, SAW, and Mixed (Rank Average) for decision-making as well as AHP and Entropy for obtaining the weights of attributes have been used. Since MADM techniques give most probably different results according to different approaches and assumptions in the same problem, statistical analysis done on them. According to the results, the correlation between compared techniques for re-ranking BN options is strong and positive because of the close proximity of weights suggested by AHP and Entropy. Mixed method as compared to TOPSIS and SAW is the preferred technique when there is no historical (real) decision-making case; moreover, AHP is more acceptable than Entropy for weighting.",""
"Probabilistic mapping of the health status instrument SF-12 onto the health utility instrument EuroQol-5 dimensions (EQ-5D)-3L using the UK-population-based scoring model showed encouraging results as compared to other mapping methods, although its predictive performance using the US-population-based EQ-5D scoring models has not been investigated. In addition, a new and improved US-population-based EQ-5D scoring method has recently been developed and suggested for use in applications that required US societal health state values. In this study, we assessed predictive performance of the probabilistic mapping approach using the US-population-based scoring models on EQ-5D utility scores based on SF-12 responses and compared the results with those of other mapping methods. Using a sample of 19,678 adults from the 2003 Medical Expenditure Panel Survey, we evaluated the predictive performance of probabilistic mapping using Bayesian networks, response mapping using multinomial logistic regression, ordinary least squares, and censored least absolute deviations models by implementing a fivefold cross-validation method. The EQ-5D utility scores were generated using two US-population-based models: D1 and MM-OC. Overall, the probabilistic mapping approach using Bayesian networks consistently outperformed other mapping methods with mean squared errors (MSE) of 0.007 and 0.007, mean absolute errors (MAE) of 0.057 and 0.039, and overall R (2) of 0.773 and 0.770 for the US-population-based EQ-5D scoring D1 and MM-OC models, respectively. The probabilistic mapping approach can be useful to estimate EQ-5D utility scores from SF-12 responses with better predictive measures in terms of MSE, MAE, and R (2) than other common mapping methods.","Using a sample of 19,678 adults from the 2003 Medical Expenditure Panel Survey, we evaluated the predictive performance of probabilistic mapping using Bayesian networks, response mapping using multinomial logistic regression, ordinary least squares, and censored least absolute deviations models by implementing a fivefold cross-validation method."
"Effective wayfinding is the successful interplay of human and environmental factors resulting in a person successfully moving from their current position to a desired location in a timely manner. To date this process has not been modelled to reflect this interplay. This paper proposes a complex modelling system approach of wayfinding by using Bayesian Networks to model this process, and applies the model to airports. The model suggests that human factors have a greater impact on effective wayfinding in airports than environmental factors. The greatest influences on human factors are found to be the level of spatial anxiety experienced by travellers and their cognitive and spatial skills. The model also predicted that the navigation pathway that a traveller must traverse has a larger impact on the effectiveness of an airport's environment in promoting effective wayfinding than the terminal design.",""
"We performed an extensive evaluation of inference methods on simulated and experimental expression data. The results reveal low prediction accuracies for unsupervised techniques with the notable exception of the Z-SCORE method on knockout data. In all other cases, the supervised approach achieved the highest accuracies and even in a semi-supervised setting with small numbers of only positive samples, outperformed the unsupervised techniques.","We performed an extensive evaluation of inference methods on simulated and experimental expression data."
"Cells execute their functions through dynamic operations of biological networks. Dynamic networks delineate the operation of biological networks in terms of temporal changes of abundances or activities of nodes (proteins and RNAs), as well as formation of new edges and disappearance of existing edges over time. Global genomic and proteomic technologies can be used to decode dynamic networks. However, using these experimental methods, it is still challenging to identify temporal transition of nodes and edges. Thus, several computational methods for estimating dynamic topological and functional characteristics of networks have been introduced. In this review, we summarize concepts and applications of these computational methods for inferring dynamic networks and further summarize methods for estimating spatial transition of biological networks.",""
NA,""
"Fault diagnosis includes the main task of classification. Bayesian networks (BNs) present several advantages in the classification task, and previous works have suggested their use as classifiers. Because a classifier is often only one part of a larger decision process, this article proposes, for industrial process diagnosis, the use of a Bayesian method called dynamic Markov blanket classifier that has as its main goal the induction of accurate Bayesian classifiers having dependable probability estimates and revealing actual relationships among the most relevant variables. In addition, a new method, named variable ordering multiple offspring sampling capable of inducing a BN to be used as a classifier, is presented. The performance of these methods is assessed on the data of a benchmark problem known as the Tennessee Eastman process. The obtained results are compared with naive Bayes and tree augmented network classifiers, and confirm that both proposed algorithms can provide good classification accuracies as well as knowledge about relevant variables.","Fault diagnosis includes the main task of classification."
"We apply and evaluate a recent machine learning method for the automatic classification of seismic waveforms. The method relies on Dynamic Bayesian Networks (DBN) and supervised learning to improve the detection capabilities at 3C seismic stations. A time-frequency decomposition provides the basis for the required signal characteristics we need in order to derive the features defining typical \"signal\" and \"noise\" patterns. Each pattern class is modeled by a DBN, specifying the interrelationships of the derived features in the time-frequency plane. Subsequently, the models are trained using previously labeled segments of seismic data. The DBN models can now be compared against in order to determine the likelihood of new incoming seismic waveform segments to be either signal or noise. As the noise characteristics of seismic stations varies smoothly in time (seasonal variation as well as anthropogenic influence), we accommodate in our approach for a continuous adaptation of the DBN model that is associated with the noise class. Given the difficulty for obtaining a golden standard for real data (ground truth) the proof of concept and evaluation is shown by conducting experiments based on 3C seismic data from the International Monitoring Stations, BOSA and LPAZ.","We apply and evaluate a recent machine learning method for the automatic classification of seismic waveforms."
"The adaptive nature of the Forecasting the Impacts of Nanomaterials in the Environment (FINE) Bayesian network is explored. We create an updated FINE model (FINEAgNp-2) for predicting aquatic exposure concentrations of silver nanoparticles (AgNP) by combining the expert-based parameters from the baseline model established in previous work with literature data related to particle behavior, exposure, and nano-ecotoxicology via parameter learning. We validate the AgNP forecast from the updated model using mesocosm-scale field data and determine the sensitivity of several key variables to changes in environmental conditions, particle characteristics, and particle fate. Results show that the prediction accuracy of the FINEAgNp-2 model increased approximately 70% over the baseline model, with an error rate of only 20%, suggesting that FINE is a reliable tool to predict aquatic concentrations of nano-silver. Sensitivity analysis suggests that fractal dimension, particle diameter, conductivity, time, and particle fate have the most influence on aquatic exposure given the current knowledge; however, numerous knowledge gaps can be identified to suggest further research efforts that will reduce the uncertainty in subsequent exposure and risk forecasts. (C) 2013 Elsevier B.V. All rights reserved.",""
"Objective To develop a decision support system to identify patients at high risk for hyperlactatemia based upon routinely measured vital signs and laboratory studies. Materials and methods Electronic health records of 741 adult patients at the University of California Davis Health System who met at least two systemic inflammatory response syndrome criteria were used to associate patients' vital signs, white blood cell count (WBC), with sepsis occurrence and mortality. Generative and discriminative classification (naive Bayes, support vector machines, Gaussian mixture models, hidden Markov models) were used to integrate heterogeneous patient data and form a predictive tool for the inference of lactate level and mortality risk. Results An accuracy of 0.99 and discriminability of 1.00 area under the receiver operating characteristic curve (AUC) for lactate level prediction was obtained when the vital signs and WBC measurements were analysed in a 24h time bin. An accuracy of 0.73 and discriminability of 0.73 AUC for mortality prediction in patients with sepsis was achieved with only three features: median of lactate levels, mean arterial pressure, and median absolute deviation of the respiratory rate. Discussion This study introduces a new scheme for the prediction of lactate levels and mortality risk from patient vital signs and WBC. Accurate prediction of both these variables can drive the appropriate response by clinical staff and thus may have important implications for patient health and treatment outcome. Conclusions Effective predictions of lactate levels and mortality risk can be provided with a few clinical variables when the temporal aspect and variability of patient data are considered.","Generative and discriminative classification (naive Bayes, support vector machines, Gaussian mixture models, hidden Markov models) were used to integrate heterogeneous patient data and form a predictive tool for the inference of lactate level and mortality risk."
"This paper presents a new methodology for developing operating rules for conjunctive use of surface and groundwater. Bayesian Networks-based operating rules are trained and verified using the results of a multi-objective optimization model. Reduction of pumping costs, improving the groundwater quality, water supply with acceptable quality and controlling the groundwater table fluctuations are considered as objective functions of the optimization model. In order to provide Pareto fonts among these conflicting objectives, the combination of MODFLOW and MT3D groundwater quantity and quality simulation models and Non-dominated Sorting Genetic Algorithm II (NSGA-II) are used. The best solutions on the Pareto fronts, which are selected using the Young and Nash bargaining theories, are used to train and verify Bayesian Networks (BNs). In real-time water allocation from surface and groundwater resources, the BNs-based rules can be used without any need to run the time consuming optimization and conflict resolution models. The proposed methodology is applied to the conjunctive use of water resources in the Tehran region, Iran. The results show that using the operating rules can improve the groundwater quality and control the groundwater table fluctuations in the study area.",""
"The development of an analytical framework relating agricultural conditions and ecosystem services (ES) provision could be very useful for developing land-use systems which sustain natural resources for future use. According to this, a conceptual network was developed, based on literature review and expert knowledge, about the functional relationships between agricultural management and ES provision in the Pampa region (Argentina). We selected eight ES to develop this conceptual network: (1) carbon (C) balance, (2) nitrogen (N) balance, (3) groundwater contamination control, (4) soil water balance, (5) soil structural maintenance, (6) N2O emission control, (7) regulation of biotic adversities, and (8) biodiversity maintenance. This conceptual network revealed a high degree of interdependence among ES provided by Pampean agroecosystems, finding two trade-offs, and two synergies among them. Then, we analyzed the conceptual network structure, and found that both environmental and management variables influenced ES provision. Finally, we selected four ES to parameterize and quantify along 10 growing seasons (2000/2001-2009/2010) through a probabilistic methodology called Bayesian Networks. Only N balance was negatively impacted by agricultural management; while C balance, groundwater contamination control, and N2O emission control were not. Outcomes of our work emphasize the idea that qualitative and quantitative methodologies should be implemented together to assess ES provision in Pampean agroecosystems, as well as in other agricultural systems.",""
"Undirected latent variable models represent an important class of graphical models that have been successfully developed to deal with various tasks. One common challenge in learning such models is to determine the number of hidden units that are unknown a priori. Although Bayesian nonparametrics have provided promising results in bypassing the model selection problem in learning directed Bayesian Networks, very little effort has been made toward applying Bayesian nonparametrics to learn undirected latent variable models. In this paper, we present the infinite exponential family Harmonium (iEFH), a bipartite undirected latent variable model that automatically determines the number of latent units from an unbounded pool. We also present two important extensions of iEFH to 1) multiview iEFH for dealing with heterogeneous data, and 2) infinite maximum-margin Harmonium (iMMH) for incorporating supervising side information to learn predictive latent features. We develop variational inference algorithms to learn model parameters. Our methods are computationally competitive because of the avoidance of selecting the number of latent units. Our extensive experiments on real image datasets and text datasets appear to demonstrate the benefits of iEFH and iMMH inherited from Bayesian nonparametrics and max-margin learning. Such results were not available until now and contribute to expanding the scope of Bayesian nonparametrics to learn the structures of undirected latent variable models.","We develop variational inference algorithms to learn model parameters."
"This paper presents a novel system integration technical risk assessment model (SITRAM), which is based on Bayesian belief networks (BBN) coupled with parametric models (PM). This model provides statistical information for decision makers, improving risk management of complex projects. System integration technical risks (SITR) represent a significant part of project risks associated with the development of large software intensive systems for defense and commercial applications. We propose a conceptual modeling framework to address the problem of SITR assessment in the early stages of a system life cycle. Initial risks' taxonomy and risks' interrelations have been identified using a hierarchical holographic modeling (HHM) approach. The framework includes a set of BBN models, representing relations between risk contributing factors, and complementing PMs, used to provide input data to the BBN models. In this paper, we present the rationale and the modeling objectives, and describe the concepts and details of BBN experimental model design and implementation. To address practical limitations, heuristic techniques have been proposed for easing the generation of conditional probability tables. PM design principles are described and examples are presented. In conclusion, we summarize the benefits and constraints of SITR assessment based on BBN models. Further research directions and model improvements are also presented.",""
"Internet-scale software becomes more and more important as a mode to construct software systems when Internet is developing rapidly. Internet-scale software comprises a set of widely distributed software entities which are running in open, dynamic and uncontrollable Internet environment. There are several aspects impacting dependability of Internet-scale software, such as technical, organizational, decisional and human aspects. It is very important to evaluate dependability of Internet-scale software by integrating all the aspects and analyzing system architecture from the most foundational elements. However, it is lack of such an evaluation model. An evaluation model of dependability for Internet-scale software on the basis of Bayesian Networks is proposed in this paper. The structure of Internet-scale software is analyzed. An evaluating system of dependability for Internet-scale software is established. It includes static metrics, dynamic metrics, prior metrics and correction metrics. A process of trust attenuation based on assessment is proposed to integrate subjective trust factors and objective dependability factors which impact on system quality. In this paper, a Bayesian Network is build according to the structure analysis. A bottom-up method that use Bayesian reasoning to analyses and calculate entity dependability and integration dependability layer by layer is described. A unified dependability of the whole system is worked out and is corrected by objective data. The analysis of experiment in a real system proves that the model in this paper is capable of evaluating the dependability of Internet-scale software clearly and objectively. Moreover, it offers effective help to the design, development, deployment and assessment of Internet-scale software. (C) 2013 Elsevier Inc. All rights reserved.",""
"Probabilistic models are desirable for diagnosis and prognosis to account for many sources of uncertainty. However, the probability distributions of system variables and the conditional probability relationships between components or subsystems may be unknown or incomplete. Useful information for such systems may be available from multiple sources and in varying formats such as operational and laboratory data, mathematical models, reliability data, or expert opinion. Conventional modeling techniques do not generally include all such information. This paper presents a methodology for constructing a system model for use in diagnosis and prognosis in the presence of such heterogeneous information. An attractive modeling paradigm for this methodology is the dynamic Bayesian network (DBN). A systematic approach is developed to incorporate various types of data in the DBN and to learn the probability distributions of the system variables as well as the conditional probability relationships between them. The structure and distribution parameters of the DBN are obtained via established learning methods. The proposed methodology is demonstrated for a hydraulic actuator system. Copyright (c) 2013 John Wiley & Sons, Ltd.",""
"Fact finders in legal trials often need to evaluate amass of weak, contradictory and ambiguous evidence. There are two general ways to accomplish this task: by holistically forming a coherent mental representation of the case, or by atomistically assessing the probative value of each item of evidence and integrating the values according to an algorithm. Parallel constraint satisfaction models of cognitive coherence posit that a coherent mental representation is created by discounting contradicting evidence, inflating supporting evidence and interpreting ambivalent evidence in away coherent with the emerging decision. This leads to inflated support for whichever hypothesis the fact finder accepts as true. Using a Bayesian network to model the direct dependencies between the evidence, the intermediate hypotheses and the main hypothesis, parameterised with (conditional) subjective probabilities elicited from the subjects, I demonstrate experimentally how an atomistic evaluation of evidence leads to a convergence of the computed posterior degrees of belief in the guilt of the defendant of those who convict and those who acquit. The atomistic evaluation preserves the inherent uncertainty that largely disappears in a holistic evaluation. Since the fact finders' posterior degree of belief in the guilt of the defendant is the relevant standard of proof in many legal systems, this result implies that using an atomistic evaluation of evidence, the threshold level of posterior belief in guilt required for a conviction may often not be reached.",""
"Bayesian networks (BNs) are tools for representing expert knowledge or evidence. They are especially useful for synthesising evidence or belief concerning a complex intervention, assessing the sensitivity of outcomes to different situations or contextual frameworks and framing decision problems that involve alternative types of intervention. Bayesian networks are useful extensions to logic maps when initiating a review or to facilitate synthesis and bridge the gap between evidence acquisition and decision-making. Formal elicitation techniques allow development of BNs on the basis of expert opinion. Such applications are useful alternatives to 'empty' reviews, which identify knowledge gaps but fail to support decision-making. Where review evidence exists, it can inform the development of a BN. We illustrate the construction of a BN using a motivating example that demonstrates how BNs can ensure coherence, transparently structure the problem addressed by a complex intervention and assess sensitivity to context, all of which are critical components of robust reviews of complex interventions. We suggest that BNs should be utilised to routinely synthesise reviews of complex interventions or empty reviews where decisions must be made despite poor evidence. Copyright (C) 2013 John Wiley & Sons, Ltd.",""
"There is an intrinsic risk associated with tunnel construction, particularly in urban areas where a number of third party persons and properties are involved. Due to the limited availability of data for accidents and the complexity associated with their causation, it is therefore necessary to combine available historical data and expert judgment to consider all relevant factors to undertake a realistic risk analysis. Thus, this paper presents a hybrid approach that can be used to undertake a probabilistic risk assessment of the risks associated with tunneling and its likelihood to damage to existing properties using the techniques of Bayesian Networks (BN) and a Relevance Vector Machine (RVM). A causal framework that integrates the techniques is also proposed to facilitate the development of the proposed model. The developed risk model is applied to a real tunnel construction project in Wuhan, China. The results derived from the project demonstrated the model's ability to accurately assess risks during tunneling, specifically the identification of accident scenarios and the quantification of the probability and severity of possible accidents. The potential of this risk model to be used as a decision-making support tool was also explored. (C) 2013 Elsevier Ltd. All rights reserved.",""
"The paper utilizes Port State Control inspection data for discovering interactions between the numbers of various types of deficiencies found on ships and between the deficiencies and ship's involvement in maritime traffic accidents and incidents. Bayesian network models for describing the dependencies of the inspection results, ship age, type, flag, accident involvement, and incidents reported by the Vessel Traffic Service are learned from the Finnish Port State Control data from 2009-2011, 2004-2010 Baltic Sea accident statistics and the reported Gulf of Finland Vessel Traffic Service incidents within 2004-2008. Two alternative Bayesian network algorithms are applied to the model construction. Further, additional models including a hidden variable which represents the complete system and its safety features and which links the accident and incident involvement and Port State Control findings are presented. Based on model-data fit comparisons and 10-fold cross-validation, a constraint-based learning algorithm NPC mainly outperforms the score-based algorithm repeated hill-climbing with random restarts. For the highest scoring models, mutual information and influence of evidence analyses are conducted in order to analyze which network variables and variable states are the most influential ones for determining the accident involvement. The analyses suggest that knowledge on ship type, the Port State Control inspection type and the number of structural conditions related deficiencies are among the ones providing the most information regarding accident involvement and the true state of the hidden system safety variable. (C) 2013 Elsevier Ltd. All rights reserved.",""
"A novel Bayesian network methodology has been developed to enable the prediction of fatigue fracture of cardiac lead medical devices. The methodology integrates in-vivo device loading measurements, patient demographics, patient activity level, in-vitro fatigue strength measurements, and cumulative damage modeling techniques. Many plausible combinations of these variables can be simulated within a Bayesian network framework to generate a family of fatigue fracture survival curves, enabling sensitivity analyses and the construction of confidence bounds on reliability predictions. The method was applied to the prediction of conductor fatigue fracture near the shoulder for two market-released cardiac defibrillation leads which had different product performance histories. The case study used recently published data describing the in-vivo curvature conditions and the in-vitro fatigue strength. The prediction results from the methodology aligned well with the observed qualitative ranking of field performance, as well as the quantitative field survival from fracture. This initial success suggests that study of further extension of this method to other medical device applications is warranted. (C) 2013 Elsevier Ltd. All rights reserved.",""
"It is studied how to aggregate the probabilistic predictions generated by different SPODE (Super-Parent-One-Dependence Estimators) classifiers. It is shown that aggregating such predictions via compression-based weights achieves a slight but consistent improvement of performance over previously existing aggregation methods, including Bayesian Model Averaging and simple average (the approach adopted by the AODE algorithm). Then, attention is given to the problem of choosing the prior probability distribution over the models; this is an important issue in any Bayesian ensemble of models. To robustly deal with the choice of the prior, the single prior over the models is substituted by a set of priors over the models (credal set), thus obtaining a credal ensemble of Bayesian classifiers. The credal ensemble recognizes the prior-dependent instances, namely the instances whose most probable class varies when different prior over the models are considered. When faced with prior-dependent instances, the credal ensemble remains reliable by returning a set of classes rather than a single class. Two credal ensembles of SPODEs are developed; the first generalizes the Bayesian Model Averaging and the second the compression-based aggregation. Extensive experiments show that the novel ensembles compare favorably to traditional methods for aggregating SPODEs and also to previous credal classifiers. (C) 2012 Elsevier B.V. All rights reserved.","It is studied how to aggregate the probabilistic predictions generated by different SPODE (Super-Parent-One-Dependence Estimators) classifiers."
"In clinical research an early and prompt detection of the risk class of a new patient may really play a crucial role in determining the effectiveness of the treatment and, consequently, achieving a satisfying prognosis of the patient's chances. There exists a number of popular rule-based algorithms for classification, whose performances are very attractive whenever data of large number of patients are available. However, when datasets only include data of a few hundred patients, the most common approaches give unstable results and developing effective decision-support systems become scientifically challenging. Since rules can be derived from different models as well as expert knowledge resources, each of them having its advantages and weaknesses, this article suggests a hybrid approach to address the classification problem when the number of patients is too small to effectively use a single technique only. The hybrid strategy was applied to a case study and its predictive performance was compared with performances of each single approach: due to the seriousness of a misclassification of high-risk patients, special attention was paid on the specificity. The results show that the hybrid strategy outperforms each single strategy involved.","There exists a number of popular rule-based algorithms for classification, whose performances are very attractive whenever data of large number of patients are available."
"In risk assessment of maritime transportation, estimation of accidental oil outflow from tankers is important for assessing environmental impacts. However, there typically is limited data concerning the specific structural design and tank arrangement of ships operating in a given area. Moreover, there is uncertainty about the accident scenarios potentially emerging from ship encounters. This paper proposes a Bayesian network (BN) model for reasoning under uncertainty for the assessment of accidental cargo oil outflow in a ship-ship collision where a product tanker is struck. The BN combines a model linking impact scenarios to damage extent with a model for estimating the tank layouts based on limited information regarding the ship. The methodology for constructing the model is presented and output for two accident scenarios is shown. The discussion elaborates on the issue of model validation, both in terms of the BN and in light of the adopted uncertainty/bias-based risk perspective. (C) 2013 The Authors. Published by Elsevier Ltd. All rights reserved.",""
"Ensemble techniques have been widely used to improve classification performance also in the case of GP-based systems. These techniques should improve classification accuracy by using voting strategies to combine the responses of different classifiers. However, even reducing the number of classifiers composing the ensemble, by selecting only those appropriately \"diverse\" according to a given measure, gives no guarantee of obtaining significant improvements in both classification accuracy and generalization capacity. This paper presents a novel approach for combining GP-based ensembles by means of a Bayesian Network. The proposed system is able to learn and combine decision tree ensembles effectively by using two different strategies: in the first, decision tree ensembles are learned by means of a boosted GP algorithm; in the second, the responses of the ensemble are combined using a Bayesian network, which also implements a selection strategy to reduce the number of classifiers. Experiments on several data sets show that the approach obtains comparable or better accuracy with respect to other methods proposed in the literature, considerably reducing the number of classifiers used. In addition, a comparison with similar approaches, confirmed the goodness of our method and its superiority with respect to other selection techniques based on diversity. (C) 2013 Elsevier Inc. All rights reserved.","Ensemble techniques have been widely used to improve classification performance also in the case of GP-based systems."
"We present a new technique for interactively mining patterns and generating explanations by harnessing the expertise of domain experts. Key to the approach is the distinction between what is unexpected from the perspective of the computational data mining process and what is surprising to the domain experts and interesting relative to their needs. We demonstrate the potential of the approach for discovering patterns and generating rich explanations in a clinical domain. Discovering interesting facts in clinical data is a grand challenge, because medical practitioners and clinicians generally have exceptional knowledge in the problem domain in which they work, however, this knowledge is typically difficult to isolate computationally. To identify the desired surprising patterns, we formally record user knowledge and use that knowledge to filter and constrain the output from an objective data mining technique, with the user making the final judgement about whether a rule is surprising. Specifically, we introduce an unexpectedness algorithm based on association rule mining and Bayesian Networks and a E-explanations technique for explanation generation to identify unexpected patterns. An implemented prototype is successfully demonstrated using a large clinical database recording incidence, prevalance, and outcome of dialysis and kidney transplant patients.",""
"This article presents an innovative approach towards integrating logistic regression and Bayesian networks (BNs) into maritime risk assessment. The approach has been developed and applied to a case study in the maritime industry, but has the potential for being adapted to other industries. Various applications of BNs as a modelling tool in maritime risk analysis have been widely seen in relevant literature. However, a common criticism of the Bayesian approach is that it requires too much information in the form of prior probabilities, and that such information is often difficult, if not impossible, to obtain in risk assessment. The traditional and common way to estimate prior probability of an accident is to use expert estimation (inputs) as a measure of uncertainty in risk analysis. In order to address the inherited problems associated with subjective probability (expert estimation), this study develops a binary logistic regression method of providing input for a BN, making use of different maritime accident data resources. Relevant risk assessment results have been achieved by measuring the safety levels of different types of vessels in different situations.","This article presents an innovative approach towards integrating logistic regression and Bayesian networks (BNs) into maritime risk assessment."
"Wildfires can result in significant economic and social losses. Prescribed fire is commonly applied to reduce fuel loads and thereby decrease future fire risk to life and property. Fuel treatments can occur in the landscape or adjacent to houses. Location of the prescribed burns can significantly alter the risk of house loss. Furthermore the cost of treating fuels in the landscape is far cheaper than treating fuels adjacent to the houses. Here we develop a Bayesian Network to examine the relative reduction in risk that can be achieved by prescribed burning in the landscape compared with a 500 m interface zone adjacent to houses. We then compare costs of management treatments to determine the most cost-effective method of reducing risk to houses. Burning in the interface zone resulted in the greatest reduction in risk of fires reaching the houses and the intensity of these fires. Fuel treatment in the interface zone allows for a direct transfer of benefits from the fuel treatment. Costs of treating fuels in the interface were significantly higher on a per hectare basis, but the extent of area requiring treatment was considerably lower. Results of this study demonstrate that treatment of fuels at the interface is not only the best means of reducing risk, it is also the most cost-effective. Crown Copyright (C) 2013 Published by Elsevier Ltd. All rights reserved.",""
"Land use decisions result from complex deliberative processes and fundamentally influence the livelihoods of many. These decisions are made based on quantitatively measurable information like topography and on qualitative criteria such as personal preferences. Bayesian networks (BN) are able to integrate both quantitative and qualitative data and are thus suitable to approach such processes. We model land use decisions in a pre-Alpine area in Switzerland, integrating biophysical data and local actors' knowledge into a spatially explicit BN. A structured experts' process to elaborate three different BN including agriculture, forestry, and settlement provides the base for the modeling. A spatially explicit updating of the BN via questionnaires enables us to take local actors' characteristics into account. Results show which drivers are most important for land use decision-making in our case study region, and how an alteration of these drivers could change future land use. Furthermore, focusing on the probability of occurrence of various land uses in a spatially explicit manner gives insights into path-dependency of land use change. This knowledge can serve as information for planners and policy makers to design more effective policy instruments. (C) 2013 Elsevier Ltd. All rights reserved.",""
"Historical marine accident/incident data remain severely underused in regulatory development as well as during design and operation. It is widely recognized that this is mainly the result of underreporting in commercially available databases and in databases maintained by national authorities. A factor further signifying this underuse is the evident improper reporting because most data are maintained as textual information requiring significant amounts of time and effort to distill and use the essential characteristics of accidents. Compounded with improved accessibility to an ever increasing amount of historical records, there is a need to develop the means that all the available information from marine accident/incidents is fully used in decision-making during development of new regulations, design, and operation. This article elaborates on the underlying causes for the current unsatisfactory state of affairs and details the description of the structures adopted for the development of appropriate marine accident databases using Bayesian Belief Networks as the platform for translating the information contained in the databases to probabilistic risk-based knowledge-intensive models. The article further explains the use of these models within a risk-based ship design framework, concluding with an example case study of application for fire safety onboard passenger ships.",""
"Toxic blooms of Lyngbya majuscula occur in coastal areas worldwide and have major ecological, health and economic consequences. The exact causes and combinations of factors which lead to these blooms are not clearly understood. Lyngbya experts and stakeholders are a particularly diverse group, including ecologists, scientists, state and local government representatives, community organisations, catchment industry groups and local fishermen. An integrated Bayesian network approach was developed to better understand and model this complex environmental problem, identify knowledge gaps, prioritise future research and evaluate management options.",""
"Knowledge on failure events and their associated factors, gained from past construction projects, is regarded as potentially extremely useful in risk management. However, a number of circumstances are constraining its wider use. Such knowledge is usually scarce, seldom documented, and even unavailable when it is required. Further, there exists a lack of proven methods to integrate and analyze it in a cost-effective way. This article addresses possible options to overcome these difficulties. Focusing on limited but critical potential failure events, the article demonstrates how knowledge on a number of important potential failure events in tunnel works can be integrated. The problem of unavailable or incomplete information was addressed by gathering judgments from a group of experts. The elicited expert knowledge consisted of failure scenarios and associated probabilistic information. This information was integrated using Bayesian belief-networks-based models that were first customized in order to deal with the expected divergence in judgments caused by epistemic uncertainty of risks. The work described in the article shows that the developed models that integrate risk-related knowledge provide guidance as to the use of specific remedial measures.",""
"BackgroundIdentifying different patterns of allergens and understanding their predictive ability in relation to asthma and other allergic diseases is crucial for the design of personalized diagnostic tools. MethodsAllergen-IgE screening using ImmunoCAP ISAC((R)) assay was performed at age 11yrs in children participating a population-based birth cohort. Logistic regression (LR) and nonlinear statistical learning models, including random forests (RF) and Bayesian networks (BN), coupled with feature selection approaches, were used to identify patterns of allergen responses associated with asthma, rhino-conjunctivitis, wheeze, eczema and airway hyper-reactivity (AHR, positive methacholine challenge). Sensitivity/specificity and area under the receiver operating characteristic (AUROC) were used to assess model performance via repeated validation. ResultsSerum sample for IgE measurement was obtained from 461 of 822 (56.1%) participants. Two hundred and thirty-eight of 461 (51.6%) children had at least one of 112 allergen components IgE>0ISU. The binary threshold >0.3ISU performed less well than using continuous IgE values, discretizing data or using other data transformations, but not significantly (p=0.1). With the exception of eczema (AUROC0.5), LR, RF and BN achieved comparable AUROC, ranging from 0.76 to 0.82. Dust mite, pollens and pet allergens were highly associated with asthma, whilst pollens and dust mite with rhino-conjunctivitis. Egg/bovine allergens were associated with eczema. ConclusionsAfter validation, LR, RF and BN demonstrated reasonable discrimination ability for asthma, rhino-conjunctivitis, wheeze and AHR, but not for eczema. However, further improvements in threshold ascertainment and/or value transformation for different components, and better interpretation algorithms are needed to fully capitalize on the potential of the technology.","Logistic regression (LR) and nonlinear statistical learning models, including random forests (RF) and Bayesian networks (BN), coupled with feature selection approaches, were used to identify patterns of allergen responses associated with asthma, rhino-conjunctivitis, wheeze, eczema and airway hyper-reactivity (AHR, positive methacholine challenge)."
"Objective: In this paper we present an evaluation of the role of reliability indicators in glaucoma severity prediction. In particular, we investigate whether it is possible to extract useful information from tests that would be normally discarded because they are considered unreliable. Methods: We set up a predictive modelling framework to predict glaucoma severity from visual field (VF) tests sensitivities in different reliability scenarios. Three quality indicators were considered in this study: false positives rate, false negatives rate and fixation losses. Glaucoma severity was evaluated by considering a 3-levels version of the Advanced Glaucoma Intervention Study scoring metric. A bootstrapping and class balancing technique was designed to overcome problems related to small sample size and unbalanced classes. As a classification model we selected Naive Bayes. We also evaluated Bayesian networks to understand the relationships between the different anatomical sectors on the VF map. Results: The methods were tested on a data set of 28,778 VF tests collected at Moorfields Eye Hospital between 1986 and 2010. Applying Friedman test followed by the post hoc Tukey's honestly significant difference test, we observed that the classifiers trained on any kind of test, regardless of its reliability, showed comparable performance with respect to the classifier trained only considering totally reliable tests (p-value >0.01). Moreover, we showed that different quality indicators gave different effects on prediction results. Training classifiers using tests that exceeded the fixation losses threshold did not have a deteriorating impact on classification results (p-value >0.01). On the contrary, using only tests that fail to comply with the constraint on false negatives significantly decreased the accuracy of the results (p-value <0.01). Meaningful patterns related to glaucoma evolution were also extracted. Conclusions: Results showed that classification modelling is not negatively affected by the inclusion of less reliable tests in the training process. This means that less reliable tests do not subtract useful information from a model trained using only completely reliable data. Future work will be devoted to exploring new quantitative thresholds to ensure high quality testing and low re-test rates. This could assist doctors in tuning patient follow-up and therapeutic plans, possibly slowing down disease progression. (C) 2013 Elsevier B.V. All rights reserved.","As a classification model we selected Naive Bayes."
"Motivation: For biological pathways, it is common to measure a gene expression time series after various knockdowns of genes that are putatively involved in the process of interest. These interventional time-resolved data are most suitable for the elucidation of dynamic causal relationships in signaling networks. Even with this kind of data it is still a major and largely unsolved challenge to infer the topology and interaction logic of the underlying regulatory network. Results: In this work, we present a novel model-based approach involving Boolean networks to reconstruct small to medium-sized regulatory networks. In particular, we solve the problem of exact likelihood computation in Boolean networks with probabilistic exponential time delays. Simulations demonstrate the high accuracy of our approach. We apply our method to data of Ivanova et al. (2006), where RNA interference knockdown experiments were used to build a network of the key regulatory genes governing mouse stem cell maintenance and differentiation. In contrast to previous analyses of that data set, our method can identify feedback loops and provides new insights into the interplay of some master regulators in embryonic stem cell development.",""
"In order to increase the diagnostic accuracy of ground-source heat pump (GSHP) system, especially for multiple-simultaneous faults, the paper proposes a multi-source information fusion based fault diagnosis methodology by using Bayesian network, due to the fact that it is considered to be one of the most useful models in the filed of probabilistic knowledge representation and reasoning, and can deal with the uncertainty problem of fault diagnosis well. The Bayesian networks based on sensor data and observed information of human being are established, respectively. Each Bayesian network consists of two layers: fault layer and fault symptom layer. The Bayesian network structure is established according to the cause and effect sequence of faults and symptoms, and the parameters are studied by using Noisy-OR and Noisy-MAX model. The entire fault diagnosis model is established by combining the two proposed Bayesian networks. Six fault diagnosis cases of GSHP system are studied, and the results show that the fault diagnosis model using evidences from only sensor data is accurate for single fault, while it is not accurate enough for multiple-simultaneous faults. By adding the observed information as evidences, the probability of fault present for single fault of \"Refrigerant overcharge\" increases to 100% from 99.69%, and the probabilities of fault present for multiple-simultaneous faults of \"Non-condensable gas\" and \"Expansion valve port largen\" increases to almost 100% from 61.1% and 52.3%, respectively. In addition, the observed information can correct the wrong fault diagnostic results, such as \"Evaporator fouling\". Therefore, the multi-source information fusion based fault diagnosis model using Bayesian network can increase the fault diagnostic accuracy greatly. (C) 2013 Elsevier Ltd. All rights reserved.",""
"There are lots of different software metrics discovered and used for defect prediction in the literature. Instead of dealing with so many metrics, it would be practical and easy if we could determine the set of metrics that are most important and focus on them more to predict defectiveness. We use Bayesian networks to determine the probabilistic influential relationships among software metrics and defect proneness. In addition to the metrics used in Promise data repository, we define two more metrics, i.e. NOD for the number of developers and LOCQ for the source code quality. We extract these metrics by inspecting the source code repositories of the selected Promise data repository data sets. At the end of our modeling, we learn the marginal defect proneness probability of the whole software system, the set of most effective metrics, and the influential relationships among metrics and defectiveness. Our experiments on nine open source Promise data repository data sets show that response for class (RFC), lines of code (LOC), and lack of coding quality (LOCQ) are the most effective metrics whereas coupling between objects (CBO), weighted method per class (WMC), and lack of cohesion of methods (LCOM) are less effective metrics on defect proneness. Furthermore, number of children (NOC) and depth of inheritance tree (DIT) have very limited effect and are untrustworthy. On the other hand, based on the experiments on Poi, Tomcat, and Xalan data sets, we observe that there is a positive correlation between the number of developers (NOD) and the level of defectiveness. However, further investigation involving a greater number of projects is needed to confirm our findings.",""
"Business management involves collecting information, goods, and funds as they move from supplier to manufacturer to wholesaler to retailer to consumer. Such business comprises interconnected parts that can be fundamentally complex and dynamic. A disturbance in one subnet of the system may thus have an opposed impact on another subnets, thus disturbing the business. Disruptions can have expensive and extensive results. This research aims to improve an increased Bayesian network method to consider business disruptions. The goal is to develop strategies that can diminish the opposed impacts of the disruptions and improve overall system reliability. Two influence agents are specified: the Bayesian and junction lack influence agents. An industrial model is used to demonstrate the proposed application, making the business more reliable. Moreover, two network learning methodologies are reviewed to update the probabilities in a model. The neural network seems to be a more favorable updating tool.",""
"ObjectivesResponse to the oncology drug gemcitabine may be variable in part due to genetic differences in the enzymes and transporters responsible for its metabolism and disposition. The aim of our in-silico study was to identify gene variants significantly associated with gemcitabine response that may help to personalize treatment in the clinic.MethodsWe analyzed two independent data sets: (a) genotype data from NCI-60 cell lines using the Affymetrix DMET 1.0 platform combined with gemcitabine cytotoxicity data in those cell lines, and (b) genome-wide association studies (GWAS) data from 351 pancreatic cancer patients treated on an NCI-sponsored phase III clinical trial. We also performed a subset analysis on the GWAS data set for 135 patients who were given gemcitabine+placebo. Statistical and systems biology analyses were performed on each individual data set to identify biomarkers significantly associated with gemcitabine response.ResultsGenetic variants in the ABC transporters (ABCC1, ABCC4) and the CYP4 family members CYP4F8 and CYP4F12, CHST3, and PPARD were found to be significant in both the NCI-60 and GWAS data sets. We report significant association between drug response and variants within members of the chondroitin sulfotransferase family (CHST) whose role in gemcitabine response is yet to be delineated.ConclusionBiomarkers identified in this integrative analysis may contribute insights into gemcitabine response variability. As genotype data become more readily available, similar studies can be conducted to gain insights into drug response mechanisms and to facilitate clinical trial design and regulatory reviews. (C) 2014 Wolters Kluwer Health | Lippincott Williams & Wilkins.",""
"Bayesian networks (BNs) have become an essential tool for reasoning under uncertainty in complex models. In particular, the subclass of Gaussian Bayesian networks (GBNs) can be used to model continuous variables with Gaussian distributions. Here we focus on the task of learning GBNs from data. Factorization of the multivariate Gaussian joint density according to a directed acyclic graph (DAG) provides an alternative and interchangeable representation of a GBN by using the Gaussian conditional univariate densities of each variable given its parents in the DAG. With this latter conditional specification of a GBN, the learning process involves determination of the mean vector, regression coefficients and conditional variances parameters. Some approaches have been proposed to learn these parameters from a Bayesian perspective using different priors, and therefore some hyperparameter values are tuned. Our goal is to deal with the usual prior distributions given by the normal/inverse gamma form and to evaluate the effect of prior hyperparameter choice on the posterior distribution. As usual in Bayesian robustness, a large class of priors expressed by many hyperparameter values should lead to a small collection of posteriors. From this perspective and using Kullback-Leibler divergence to measure prior and posterior deviations, a local sensitivity measure is proposed to make comparisons. If a robust Bayesian analysis is developed by studying the sensitivity of Bayesian answers to uncertain inputs, this method will also be useful for selecting robust hyperparameter values. (C) 2013 Elsevier Inc. All rights reserved.","With this latter conditional specification of a GBN, the learning process involves determination of the mean vector, regression coefficients and conditional variances parameters."
"Bayesian network analysis is an attractive approach for studying the functional integration of brain networks, as it includes both the locations of connections between regions of the brain (functional connectivity) and more importantly the direction of the causal relationship between the regions (directed functional connectivity). Further, these approaches are more attractive than other functional connectivity analyses in that they can often operate on larger sets of nodes and run searches over a wide range of candidate networks. An important study by Smith et al. (2011) illustrated that many Bayesian network approaches did not perform well in identifying the directionality of connections in simulated single-subject data. Since then, new Bayesian network approaches have been developed that have overcome the failures in the Smith work. Additionally, an important discovery was made that shows a preprocessing step used in the Smith data puts some of the Bayesian network methods at a disadvantage. This work provides a review of Bayesian network analyses, focusing on the methods used in the Smith work as well as methods developed since 2011 that have improved estimation performance. Importantly, only approaches that have been specifically designed for fMRI data perform well, as they have been tailored to meet the challenges of fMRI data. Although this work does not suggest a single best model, it describes the class of models that perform best and highlights the features of these models that allow them to perform well on fMRI data. Specifically, methods that rely on non-Gaussianity to direct causal relationships in the network perform well. (C) 2013 Elsevier Inc. All rights reserved.",""
"Understanding and managing ecosystems affected by several anthropogenic stressors require methods that enable analyzing the joint effects of different factors in one framework. Further, as scientific knowledge about natural systems is loaded with uncertainty, it is essential that analyses are based on a probabilistic approach. We describe in this article about building a Bayesian decision model, which includes three stressors present in the Gulf of Finland. The outcome of the integrative model is a set of probability distributions for future nutrient concentrations, herring stock biomass, and achieving the water quality targets set by HELCOM Baltic Sea Action Plan. These distributions can then be used to derive the probability of reaching the management targets for each alternative combination of management actions.",""
"Integrating the ecosystem service concept into land use planning requires tools that allow rapid and transparent assessment of ecosystem services. The demand for simple indicators has stimulated the emergence of land use based proxy methods. Although these have been very powerful to create policy awareness on different levels, they are insufficient when it comes to land use and policy planning for ecosystem service delivery. Discarding the complex ecological reality or scientific uncertainty poses serious risks for adverse effects of policies. This explorative study constitutes the basis for the further development of a tool to link land use planning for ecosystem service bundle optimization, capturing inherent ecological complexity and uncertainty. Particular emphasis was placed on the biophysical potential of an ecosystem to deliver services and the link with the actual land use. The EBI - Ecosystem Service Bundle Index - builds on a Bayesian network model that allows integration of biophysical and socio-economic processes as well as land use planning policies driving the delivery of bundles of ecosystem services. The EBI prototype was tested in a pilot study area using three interacting ecosystem services. Incorporation of judicial land use claims, more intense involvement of stakeholders and other improvements are being developed. (C) 2013 Elsevier Ltd. All rights reserved.",""
"Dams are civil engineering structures to hinder water flows. Criteria such as purpose, size and construction material are useful to categorise them. The latter is used to classify earth dams', which tend to have higher risk levels than other types. The failure of a dam leads to significant economic loss and usually to catastrophic impacts. In an effort to comprehensively examine the variables that influence earth dam breaks and describe their interactions, a model has been developed in such a way that it allows to assess risks and to prioritise the allocation of resources for maintenance activities. The research was carried out by systematically reviewing the literature, which led to the choice of Bayesian Networks (BNs) as a tool for assessing risks. Using data from seven case studies in Mexico, a model was built, which helped to rank the dams under study, leading to results comparable with those reported in the literature. While the particular type of BN used and its quantification is presented more extensively in an accompanying paper, the model may be of interest for dam owners, managers, practitioners and academics on their efforts to manage earth dams' risks.",""
"Probabilistic and possibilistic networks are important tools proposed for an efficient representation and analysis of uncertain information. The inference process has been studied in depth in these graphical models. We cite in particular compilation-based inference which has recently triggered the attention of several researchers. In this paper, we are interested in comparing this inference mechanism in the probabilistic and possibilistic frameworks in order to unveil common points and differences between these two settings. In fact, we will propose a generic framework supporting both Bayesian networks and possibilistic networks (product-based and min-based ones). The proposed comparative study points out that the inference process depends on the specificity of each framework, namely in the interpretation of the handled uncertainty degrees (probability\possibility) and appropriate operators (*\min and +\max). (C) 2013 Elsevier Inc. All rights reserved.","The inference process has been studied in depth in these graphical models."
"Knowing students' learning styles allows us to improve their experience in an educational environment. Particularly, the perception style is one of the most important dimensions of the learning styles since it describes the way students perceive the world as well as the kind of learning content they prefer. Several approaches to detect students' perception style according to Felder's model have been proposed. However, these approaches exhibit several limitations that make their implementation difficult. Thus, we propose a novel approach to detect the perception style of a student by analyzing his/her interaction with games, namely puzzle games. To carry out this detection, we track how students play a puzzle game and extract information about this interaction. Then, we train a Naive Bayes Classifier to infer the students' perception style by using the information extracted. We have evaluated our proposed approach with 47 Computer Engineering students. Experimental results showed that the perception style was successfully predicted through the use of games, with an accuracy of 85%. Finally, we conclude that games are a promising environment where the students' perception style can be detected. (C) 2013 Elsevier Ltd. All rights reserved.","Then, we train a Naive Bayes Classifier to infer the students' perception style by using the information extracted."
"Objective: To identify metabolomic biomarkers predictive of Intensive Care Unit (ICU) mortality in adults. Rationale: Comprehensive metabolomic profiling of plasma at ICU admission to identify biomarkers associated with mortality has recently become feasible. Methods: We performed metabolomic profiling of plasma from 90 ICU subjects enrolled in the BWH Registry of Critical Illness (RoCI). We tested individual metabolites and a Bayesian Network of metabolites for association with 28-day mortality, using logistic regression in R, and the CGBayesNets Package in MATLAB. Both individual metabolites and the network were tested for replication in an independent cohort of 149 adults enrolled in the Community Acquired Pneumonia and Sepsis Outcome Diagnostics (CAPSOD) study. Results: We tested variable metabolites for association with 28-day mortality. In RoCI, nearly one third of metabolites differed among ICU survivors versus those who died by day 28 (N = 57 metabolites, p<.05). Associations with 28-day mortality replicated for 31 of these metabolites (with p<.05) in the CAPSOD population. Replicating metabolites included lipids (N = 14), amino acids or amino acid breakdown products (N = 12), carbohydrates (N = 1), nucleotides (N = 3), and 1 peptide. Among 31 replicated metabolites, 25 were higher in subjects who progressed to die; all 6 metabolites that are lower in those who die are lipids. We used Bayesian modeling to form a metabolomic network of 7 metabolites associated with death (gamma-glutamylphenylalanine, gamma-glutamyltyrosine, 1-arachidonoylGPC(20: 4), taurochenodeoxycholate, 3-(4-hydroxyphenyl) lactate, sucrose, kynurenine). This network achieved a 91% AUC predicting 28-day mortality in RoCI, and 74% of the AUC in CAPSOD (p<.001 in both populations). Conclusion: Both individual metabolites and a metabolomic network were associated with 28-day mortality in two independent cohorts. Metabolomic profiling represents a valuable new approach for identifying novel biomarkers in critically ill patients.","We tested individual metabolites and a Bayesian Network of metabolites for association with 28-day mortality, using logistic regression in R, and the CGBayesNets Package in MATLAB."
"Motivation: Histone proteins are subject to various posttranslational modifications (PTMs). Elucidating their functional relationships is crucial toward understanding many biological processes. Bayesian network (BN)-based approaches have shown the advantage of revealing causal relationships, rather than simple cooccurrences, of PTMs. Previous works employing BNs to infer causal relationships of PTMs require that all confounders should be included. This assumption, however, is unavoidably violated given the fact that several modifications are often regulated by a common but unobserved factor. An existing non-parametric method can be applied to tackle the problem but the complexity and inflexibility make it impractical. Results: We propose a novel BN-based method to infer causal relationships of histone modifications. First, from the evidence that nucleosome organization in vivo significantly affects the activities of PTM regulators working on chromatin substrate, hidden confounders of PTMs are selectively introduced by an information-theoretic criterion. Causal relationships are then inferred from a network model of both PTMs and the derived confounders. Application on human epigenomic data shows the advantage of the proposed method, in terms of computational performance and support from literature. Requiring less strict data assumptions also makes it more practical. Interestingly, analysis of the most significant relationships suggests that the proposed method can recover biologically relevant causal effects between histone modifications, which should be important for future investigation of histone crosstalk.",""
"With the increasing organizational dependence on information systems, information systems security has become a very critical issue in enterprise risk management. In information systems, security risks are caused by various interrelated internal and external factors. A security vulnerability could also propagate and escalate through the causal chains of risk factors via multiple paths, leading to different system security risks. In order to identify the causal relationships among risk factors and analyze the complexity and uncertainty of vulnerability propagation, a security risk analysis model (SRAM) is proposed in this paper. In SRAM, a Bayesian network (BN) is developed to simultaneously define the risk factors and their causal relationships based on the knowledge from observed cases and domain experts. Then, the security vulnerability propagation analysis is performed to determine the propagation paths with the highest probability and the largest estimated risk value. SRAM enables organizations to establish proactive security risk management plans for information systems, which is validated via a case study. (C) 2013 Elsevier Inc. All rights reserved.",""
"Learning large Bayesian networks (BN) from data is a challenging problem due to the vastness of the structure space. An effective way to turn this problem affordable is the use of super-structures SS (undirected graphs that contain the BN skeleton). However, the literature has been lacking of specialized methods for estimating SS. We present here two algorithms intended for such purpose in the hybrid approach of BN structure learning. The first one, called Opt01SS, learns SS using only zero-and-first-order conditional independence (Cl) tests in a way that allows dealing with the presence of approximate-deterministic relationships and inconsistent CIs, commonly found in small samples. The second algorithm, called OptHPC, is a computational optimized version of the recent HPC algorithm (De Morais and Aussem 2010, [17]) that showed an attractive accuracy for SS recovery. Results on various benchmark networks showed that the proposed algorithms achieve a balance between sensitivity and specificity clearly more favorable for the task of SS estimation than several representative state-of-the-art methods. The computational cost was also found to be reasonable, being Opt01SS one of the most competitive among the analyzed algorithms. (C) 2013 Elsevier B.V. All rights reserved.",""
"Background: During embryogenesis, signaling molecules produced by one cell population direct gene regulatory changes in neighboring cells and influence their developmental fates and spatial organization. One of the earliest events in the development of the vertebrate embryo is the establishment of three germ layers, consisting of the ectoderm, mesoderm and endoderm. Attempts to measure gene expression in vivo in different germ layers and cell types are typically complicated by the heterogeneity of cell types within biological samples (i.e., embryos), as the responses of individual cell types are intermingled into an aggregate observation of heterogeneous cell types. Here, we propose a novel method to elucidate gene regulatory circuits from these aggregate measurements in embryos of the frog Xenopus tropicalis using gene network inference algorithms and then test the ability of the inferred networks to predict spatial gene expression patterns. Results: We use two inference models with different underlying assumptions that incorporate existing network information, an ODE model for steady-state data and a Markov model for time series data, and contrast the performance of the two models. We apply our method to both control and knockdown embryos at multiple time points to reconstruct the core mesoderm and endoderm regulatory circuits. Those inferred networks are then used in combination with known dorsal-ventral spatial expression patterns of a subset of genes to predict spatial expression patterns for other genes. Both models are able to predict spatial expression patterns for some of the core mesoderm and endoderm genes, but interestingly of different gene subsets, suggesting that neither model is sufficient to recapitulate all of the spatial patterns, yet they are complementary for the patterns that they do capture. Conclusion: The presented methodology of gene network inference combined with spatial pattern prediction provides an additional layer of validation to elucidate the regulatory circuits controlling the spatial-temporal dynamics in embryonic development.","Here, we propose a novel method to elucidate gene regulatory circuits from these aggregate measurements in embryos of the frog Xenopus tropicalis using gene network inference algorithms and then test the ability of the inferred networks to predict spatial gene expression patterns."
"Under the umbrella of the Semantic Web, Linking Open Data projects have made available a large number of semantically intra-and inter-connected links. As an example, in the biomedical domain, data about disorders, disease related genes and proteins, clinical trials, and drugs or interventions are accessible on the Linked Open Data cloud. In addition, domain ontologies have been used to annotate scientific data. For instance, publications in PubMed have been annotated using controlled vocabulary (CV) terms from ontologies such as the Medical Subject Header (MeSH) or the Unified Medical Language System (UMLS). These annotations have been successfully mined to discover associations between drugs and diseases using techniques that have been labeled as Literature-Based Discovery (LBD). Given the large scale of the linked datasets in the Linked Open Data cloud, there is a need to develop scalable techniques that can provide answers in close to real time, to explain a phenomena, to identify anomalies, or to explore a discovery. This paper describes an authority flow based ranking technique that is inspired by LBD methods. The ranking is tailored to a layered graph. The input terms are in the first layer and the ranking will efficiently identify and assign high scores to terms in a third (or subsequent) layer, corresponding to potential novel discoveries. The terms, links and scores are modeled as a Bayesian network. Two sampling techniques are proposed to only traverse the terms that may have high scores. The first technique implements a Direct Sampling reasoning algorithm to approximate the ranking scores of nodes in the Bayesian network; it visits only the nodes with the highest probability. The second technique samples paths in the Bayesian network with the highest conditional probability. An experimental study reveals that the proposed ranking techniques are able to reproduce state-of-the-art discoveries. In addition, the sampling-based approaches are able to reduce execution times and reach high levels of accuracy.",""
"The development of a practical model for incident management is investigated through Bayesian networks (BNs) in this study. BNs are capable of accurately predicting incident durations and can easily be incorporated into incident management activities of traffic management centers to improve the real-time decision-making process. Three structure learning algorithms were used to construct BN structures. They were estimated by using 2005 New Jersey incident data; the best-performing one was chosen for the incident duration prediction with the use of the 10-fold cross-validation method and the Bayesian information criterion statistic. To demonstrate the performance of Bayesian learning, the chosen model was fed by 2011 New Jersey incident data on a monthly and quarterly basis. Comparing the prediction results for 2011 data with and without adaptive learning showed that the developed BN had the capability to automatically adapt itself to future conditions by learning the patterns of new incidents and their respective durations.",""
"The field of traffic accident analysis has long been dominated by traditional statistical analysis. With the recent advances in data collection, storage, and archival methods, the size of accident data sets has grown significantly. This result in turn has motivated research on applying data mining and complex network analysis algorithms to traffic accident analysis; the data mining and complex network analysis algorithms are designed specifically to handle data sets with large dimensions. This paper explores the potential for using two such methods namely, a modularity-optimizing community detection algorithm and the association rule learning algorithm to identify important accident characteristics. As a case study, the algorithms were applied to an accident data set compiled for Interstate 190 in the Buffalo Niagara, New York, metropolitan area. Specifically, the community detection algorithm was used to cluster the data to reduce the inherent heterogeneity, and then the association rule learning algorithm was applied to each cluster to discern meaningful patterns within each, related particularly to high accident frequency locations (hot spots) and incident clearance time. To demonstrate the benefits of clustering, the association rule algorithm was also applied to the whole data set (before clustering) and the results were compared with those discovered from the clusters. The study results indicated that (a) the community detection algorithm was quite effective in identifying clusters with discernible characteristics, (b) clustering helped unveil relationships and accident causative factors that remained hidden when the analysis was performed on the whole data set, and (c) the association rule learning algorithm yielded useful insight into accident hot spots and incident clearance time along I-190.",""
"A continuous-time Markov process (CTMP) is a collection of variables indexed by a continuous quantity, time. It obeys the Markov property that the distribution over a future variable is independent of past variables given the state at the present time. We introduce continuous-time Markov process representations and algorithms for filtering, smoothing, expected sufficient statistics calculations, and model estimation, assuming no prior knowledge of continuous-time processes but some basic knowledge of probability and statistics. We begin by describing \"flat\" or unstructured Markov processes and then move to structured Markov processes (those arising from state spaces consisting of assignments to variables) including Kronecker, decision-diagram, and continuous-time Bayesian network representations. We provide the first connection between decision-diagrams and continuous-time Bayesian networks.",""
"This paper describes the use of (supervised) data mining to predict casing corrosion in carbon geological storage projects. This study discusses: 1) data pre-processing such as missing value handling and discretisation; 2) feature selection methods such as correlation coefficient, signal-to-noise ratio, information gain, Gini index, and the k-nearest neighbour (KNN) approach; 3) classification techniques including decision trees (C4.5 and CART) and Bayesian networks; 4) evaluation methods like cross-validation as four successive steps of supervised learning. The experimental analysis of the casing corrosion problem based on the given supervised learning framework shows the effectiveness of data mining techniques in finding features relevant to the problem under study and in building models to predict and identify casing corrosion.","This study discusses: 1) data pre-processing such as missing value handling and discretisation; 2) feature selection methods such as correlation coefficient, signal-to-noise ratio, information gain, Gini index, and the k-nearest neighbour (KNN) approach; 3) classification techniques including decision trees (C4."
"Prediction at ungauged sites is essential for water resources planning and management. Ungauged sites have no observations about the magnitude of floods, but some site and basin characteristics are known. Regression models relate physiographic and climatic basin characteristics to flood quantiles, which can be estimated from observed data at gauged sites. However, some of these models assume linear relationships between variables and prediction intervals are estimated by the variance of the residuals in the estimated model. Furthermore, the effect of the uncertainties in the explanatory variables on the dependent variable cannot be assessed. This paper presents a methodology to propagate the uncertainties that arise in the process of predicting flood quantiles at ungauged basins by a regression model. In addition, Bayesian networks (BNs) were explored as a feasible tool for predicting flood quantiles at ungauged sites. Bayesian networks benefit from taking into account uncertainties thanks to their probabilistic nature. They are able to capture non-linear relationships between variables and they give a probability distribution of discharge as a result. The proposed BN model can be applied to supply the estimation uncertainty in national flood discharge mappings. The methodology was applied to a case study in the Tagus basin in Spain.","Regression models relate physiographic and climatic basin characteristics to flood quantiles, which can be estimated from observed data at gauged sites."
"The complexity of IBD is well recognized as are the putative four major components of its pathogenesis, i.e. environment, genetic makeup, gut microbiota and mucosal immune response. Each of these components is extremely complex on its own, and at present should be more appropriately defined by the terms 'exposome', 'genome', 'microbiome' and 'immunome', respectively, based on the 'ome' suffix that refers to a totality of some sort. None of these 'omes' is apparently capable of causing IBD by itself; it is instead the intricate and reciprocal interaction among them, through the so-called 'IBD interactome', that results in the emergence of IBD, or more appropriately the 'IBD integrome'. To deal with and understand such overwhelming biological complexity, new approaches and tools are needed, and these are represented by 'omics', defined as the study of related sets of biological molecules in a comprehensive fashion, such as genomics, transcriptomics, proteomics, metabolomics, and so on. Numerous bioinformatics-based tools are available to explore and take advantage of the massive amount of information that can be generated by the analysis of the various omes and their interactions, aiming at identifying the molecular interactome underlying any particular status of health and disease. These novel approaches are fully applicable to IBD and allow us to achieve the ultimate goal of developing and applying personalized medicine and far more effective therapies to individual patients with Crohn's disease and ulcerative colitis. For the practicing gastroenterologist, an omics-based delivery of healthcare may be intimidating, but it must be accepted and implemented if he or she is to provide the best possible care to IBD patients. (C) 2014 S. Karger AG, Basel",""
"This study adopted a novel methodology-a support vector machine (SVM) with two penalty parameters-for the evaluation of real-time crash risk on urban expressway segments by using dual-loop detector data. The purpose of this study was to develop a model that can effectively identify traffic conditions prone to crashes and support implementation of proactive traffic safety management. On the basis of crash data and the corresponding detector data collected on expressways of Shanghai, China, different combinations of dual-loop detector data and time segments before crashes were used to develop the optimal crash risk estimation model by SVM. The transferability of the SVM model was assessed by examining whether the model developed on one expressway was applicable to other similar ones. In addition, the prediction results and transferability of the SVM model were compared with those given by other frequently used classification algorithms, including logistic regression, Bayesian networks, native Bayes classifier, k-nearest neighbor, and back propagation neural network. The results showed that the SVM model was more suitable to the prediction of real-time crash risk with small-scale data than other algorithms, with its accuracy in classifying crashes reaching a best of 80%. A comparison to similar studies by other researchers implied that the proposed model achieved better prediction accuracy.","In addition, the prediction results and transferability of the SVM model were compared with those given by other frequently used classification algorithms, including logistic regression, Bayesian networks, native Bayes classifier, k-nearest neighbor, and back propagation neural network."
"Considering the increasing importance of adaptive approaches in CALL systems, this study implemented a machine learning based student modeling middleware with Bayesian networks. The profiling approach of the student modeling system is based on Felder and Silverman's Learning Styles Model and Felder and Soloman's Index of Learning Styles Questionnaire. The questionnaire was adapted to Turkish for this experimental study conducted with respect to the visual/verbal and active/reflective dimensions of the model. A topic in EFL was chosen for the learning content design, which was also carried into the digital domain and remastered as separate learning scenes for different learning styles. Computer software was also implemented to carry out the experimental learning processes. A quasi-experimental pre-test, post-test design was conducted with 46 volunteers, with 23 students assigned each to a control and an experimental group to compare academic achievement between student-based learning and conventional computer-based learning. No significant difference was found in academic achievement between the control and experimental groups after the experimental treatment. The diagnostic performance of the proposed student modeling system was also compared with performances from similar studies. This student modeling system had a successful prediction rate of 41% on the visual/verbal dimension and 54% on the active/reflective dimension, respectively.",""
"Urban flooding introduces significant risk to society. Non-stationarity leads to increased uncertainty and this is challenging to include in actual decision-making. The primary objective of this study was to develop a risk assessment and decision support framework for pluvial urban flood risk under nonstationary conditions using an influence diagram (ID) which is a Bayesian network (BN) extended with decision and utility nodes. Non-stationarity is considered to be the influence of climate change where extreme precipitation patterns change over time. The overall risk is quantified in monetary terms expressed as expected annual damage. The network is dynamic in as much as it assesses risk at different points in time. The framework provides means for decision-makers to assess how different decisions on flood adaptation affect the risk now and in the future. The result from the ID was extended with a cost-benefit analysis defining the net benefits for the investment plans. We tested our framework in a case study where the risk for flooding was assessed on a railway track in Risskov, Aarhus. Drainage system improvements are planned for the area. Our study illustrates with the use of an ID how risk for flooding increases over time, and the benefits of implementing flood adaptation measures.",""
"Modern natural hazards research requires dealing with several uncertainties that arise from limited process knowledge, measurement errors, censored and incomplete observations, and the intrinsic randomness of the governing processes. Nevertheless, deterministic analyses are still widely used in quantitative hazard assessments despite the pitfall of misestimating the hazard and any ensuing risks. In this paper we show that Bayesian networks offer a flexible framework for capturing and expressing a broad range of uncertainties encountered in natural hazard assessments. Although Bayesian networks are well studied in theory, their application to real-world data is far from straightforward, and requires specific tailoring and adaptation of existing algorithms. We offer suggestions as how to tackle frequently arising problems in this context and mainly concentrate on the handling of continuous variables, incomplete data sets, and the interaction of both. By way of three case studies from earthquake, flood, and landslide research, we demonstrate the method of data-driven Bayesian network learning, and showcase the flexibility, applicability, and benefits of this approach. Our results offer fresh and partly counterintuitive insights into well-studied multivariate problems of earthquake-induced ground motion prediction, accurate flood damage quantification, and spatially explicit landslide prediction at the regional scale. In particular, we highlight how Bayesian networks help to express information flow and independence assumptions between candidate predictors. Such knowledge is pivotal in providing scientists and decision makers with well-informed strategies for selecting adequate predictor variables for quantitative natural hazard assessments.",""
"The aim of this paper is to develop an automated system for epileptic seizure prediction from intracranial EEG signals based on Hilbert-Huang transform (HHT) and Bayesian classifiers. Proposed system includes decomposition of the signals into intrinsic mode functions for obtaining features and use of Bayesian networks with correlation based feature selection for binary classification of preictal and interictal recordings. The system was trained and tested on Freiburg EEG database. 58 hours of preictal data, 40-minute data blocks prior to each of 87 seizures collected from 21 patients, and 503.1 hours of interictal data were examined resulting in 96.55% sensitivity with 0.21 false alarms per hour, 13.896% average proportion of time spent in warning, and 33.21 minutes of average detection latency using 30-second EEG segments with 50% overlap and a simple postprocessing technique resulting in a decision (a seizure is expected/not expected) every 5 minutes. High sensitivity and low false positive rate with reasonable detection latency show that HHT based features are acceptable for patient specific seizure prediction from intracranial EEG data. Time spent for testing an EEG segment was 4.1451 seconds on average, which makes the system viable for use in real-time seizure control systems.","The aim of this paper is to develop an automated system for epileptic seizure prediction from intracranial EEG signals based on Hilbert-Huang transform (HHT) and Bayesian classifiers."
"We explore Bayesian network models that generate four-part harmonies according to the melody of a soprano voice. The probabilistic models that most studies have proposed already contain nodes or states that represent chord symbols. Chord symbols, however, may not be suitable as a node of probabilistic models because they are ambiguous; chords with different voicings are represented as the same symbol. In this paper, we develop Bayesian networks that do and do not include chord nodes and compare the harmonies generated by them. Experimental results show that the non-chord model generates harmonies in which each voice has smoother note motions than the chord model.",""
"In this paper, two Bayesian methods for the development of accident prediction models are compared: the well-acknowledged Empirical Bayes (EB) method and a recently developed method based on Bayesian Probabilistic Networks (BPNs). Brief descriptions of the two methods are provided and their commonalities, differences, advantages and disadvantages are discussed. Both methods can be used to develop models for the multivariate prediction of accident events and can be included in road infrastructure safety management systems such as road safety impact assessments or road safety audits. Using a comprehensive data-set taken from the Austrian rural motorway network, it is shown that the predictions of both models are in good accordance with the data. It is observed that the BPN models show a higher degree of correlation with the data than the models developed using the EB method, as measured through the higher values of the correlation coefficients (similar to 5-10%).",""
"We introduce a subclass of chain event graphs that we call stratified chain event graphs, and present a dynamic programming algorithm for the optimal selection of such chain event graphs that maximizes a decomposable score derived from a complete independent sample. We apply the algorithm to such a dataset, with a view to deducing the causal structure of the variables under the hypothesis that there are no unobserved confounders. We show that the algorithm is suitable for small problems. Similarities with and differences to a dynamic programming algorithm for MAP learning of Bayesian networks are highlighted, as are the relations to causal discovery using Bayesian networks,",""
"Purpose - As organizations increase their dependence on supply chain networks, they become more susceptible to their suppliers' disaster risk profiles, as well as other categories of risk associated with supply chains. Therefore, it is imperative that supply chain network participants are capable of assessing the disaster risks associated with their supplier base. The purpose of this paper is to assess the supplier disaster risks, which are a key element of external risk in supply chains. Design/methodology/approach - The study participants are 15 automotive casting suppliers who display a significant degree of disaster risks to a major US automotive company. Bayesian networks are used as a methodology for examining the supplier disaster risk profiles for these participants. Findings - The results of this study show that Bayesian networks can be effectively used to assist managers in making decisions regarding current and prospective suppliers vis-a-vis their potential revenue impact as illustrated through their corresponding disaster risk profiles. Research limitations/implications - A limitation to the use of Bayesian networks for modeling disaster risk profiles is the proper identification of risk events and risk categories that can impact a supply chain. Practical implications - The methodology used in this study can be adopted by managers to assist them in making decisions regarding current or prospective suppliers vis-a-vis their corresponding disaster risk profiles. Originality/value - As part of a comprehensive supplier risk management program, organizations along with their suppliers can develop specific strategies and tactics to minimize the effects of supply chain disaster risk events.",""
"Credal networks are graph-based statistical models whose parameters take values in a set, instead of being sharply specified as in traditional statistical models (e.g., Bayesian networks). The computational complexity of inferences on such models depends on the irrelevance/independence concept adopted. In this paper, we study inferential complexity under the concepts of epistemic irrelevance and strong independence. We show that inferences under strong independence are NP-hard even in trees with binary variables except for a single ternary one. We prove that under epistemic irrelevance the polynomial-time complexity of inferences in credal trees is not likely to extend to more general models (e.g., singly connected topologies). These results clearly distinguish networks that admit efficient inferences and those where inferences are most likely hard, and settle several open questions regarding their computational complexity. We show that these results remain valid even if we disallow the use of zero probabilities. We also show that the computation of bounds on the probability of the future state in a hidden Markov model is the same whether we assume epistemic irrelevance or strong independence, and we prove a similar result for inference in naive Bayes structures. These inferential equivalences are important for practitioners, as hidden Markov models and naive Bayes structures are used in real applications of imprecise probability.","The computational complexity of inferences on such models depends on the irrelevance/independence concept adopted."
"In this paper, a new diagnosis strategy for micro-computer controlled straight electro pneumatic braking system is developed to improve the diagnostic efficiency, which makes full use of some reliability theories and fuzzy set techniques. Specifically, it adopts expert elicitation and fuzzy set theory to evaluate the failure rate of the basic events for the braking system, and uses a dynamic fault tree model to capture the dynamic failure mechanisms and calculates some reliability results by mapping a dynamic fault tree into an equivalent Bayesian network (BN). Furthermore, the schemes are proposed to update the diagnostic importance factor (DIF) and the cut sets according to the sensors data. Finally, an efficient diagnostic algorithm is developed based on these reliability results to guide the maintenance crew to diagnose the braking system. The experimental results demonstrate that the proposed method can locate the fault of the braking system with less diagnosis cost.",""
"Computer-aided ECG analysis is very important for early diagnosis of heart diseases. Automated ECG analysis integrated with experts' opinions may provide more accurate and reliable results for detection of arrhythmia. In this study, a novel genetic algorithm-neural network (GA-NN) approach is proposed as a classifier, and compared with other classification methods. The GA-NN approach was shown to perform better than alternative approaches (e.g. k-nn, SVM, naive Bayes, Bayesian networks) on the UCI Arrythmia and the novel TEPAS ECG datasets, where the GA resulted in a feature reduction of 95%. Based on the selected features, several rule extraction algorithms are applied to allow the interpretation of the classification results by the experts. In this application, the accuracy and interpretability of results are more important than processing speed. The results show that neural network based approaches benefit greatly from dimensionality reduction, and by employing GA, we can train the NN reliably.","In this study, a novel genetic algorithm-neural network (GA-NN) approach is proposed as a classifier, and compared with other classification methods."
"Mobile devices can now handle a great deal of information thanks to the convergence of diverse functionalities. Mobile environments have already shown great potential in terms of providing customized service to users because they can record meaningful and private information continually for long periods of time. The research for understanding, searching and summarizing the everyday-life of human has received increasing attention in recent years due to the digital convergence. In this paper, we propose a mobile life browser, which visualizes and searches human's mobile life based on the contents and context of lifelog data. The mobile life browser is for searching the personal information effectively collected on his/her mobile device and for supporting the concept-based searching method by using concept networks and Bayesian networks. In the experiments, we collected the real mobile log data from three users for a month and visualized the mobile lives of the users with the mobile life browser developed. Some tests on searching tasks confirmed that the result using the proposed concept-based searching method is promising.",""
"We proposed an approach for environmental flow decision-making based on Bayesian networks considering seasonal water use conflicts between agriculture and ecosystems. Three steps were included in the approach: water shortage assessment after environmental flow allocation using a production-loss model considering temporal variations of river flows; trade-off analysis of water use outcomes by Bayesian networks; and environmental flow decision-making based on a risk assessment under different management strategies. An agricultural water shortage model and a production-loss model were integrated after satisfying environmental flows with temporal variability. The case study in the Yellow River estuary indicated that the average difference of acceptable economic loss for winter wheat irrigation stakeholders was 10% between water saving measures and water diversion projects. The combination of water diversion projects and water-saving measures would allow 4.1% more river inflow to be allocated to ecological needs in normal years without further economic losses in agriculture.",""
"Several counterparts of Bayesian networks based on different paradigms have been proposed in evidence theory. Nevertheless, none of them is completely satisfactory. In this paper we will present a new one, based on a recently introduced concept of conditional independence. We define a conditioning rule for variables, and the relationship between conditional independence and irrelevance is studied with the aim of constructing a Bayesian-network-like model. Then, through a simple example, we will show a problem appearing in this model caused by the use of a conditioning rule. We will also show that this problem can be avoided if undirected or compositional models are used instead.",""
"Given a fixed dependency graph G that describes a Bayesian network of binary variables X-1, ... X-n, our main result is a tight bound on the mutual information I-c(Y-1, ... ,Y-k) = Sigma(k)(j=1) H(Y-j)/c - H(Y-1, . . . ,Y-k) of an observed subset Y-1, ... ,Y-k of the variables X-1, ... ,X-n. Our bound depends on certain quantities that can be computed from the connective structure of the nodes in G. Thus it allows to discriminate between different dependency graphs for a probability distribution, as we show from numerical experiments.",""
"Automatic traffic accident detection, especially not recorded by traffic police, is crucial to accident black spots identification and traffic safety. A new method of detecting traffic accidents is proposed based on temporal data mining, which can identify the unknown and unrecorded accidents by traffic police. Time series model was constructed using ternary numbers to reflect the state of traffic flow based on cell transmission model. In order to deal with the aftereffects of linear drift between time series and to reduce the computational cost, discrete Fourier transform was implemented to turn time series from time domain to frequency domain. The pattern of the time series when an accident happened could be recognized using the historical crash data. Then taking Euclidean distance as the similarity evaluation function, similarity data mining of the transformed time series was carried out. If the result was less than the given threshold, the two time series were similar and an accident happened probably. A numerical example was carried out and the results verified the effectiveness of the proposed method.",""
"A systematic method is presented for evaluating the slope safety utilizing multi-source monitoring information. First, a Bayesian network with continuously distributed variables for a slope involving the factor of safety, multiple monitoring indexes and their influencing parameters (e.g. friction angle and cohesion) is constructed. Then the prior probabilities for the Bayesian network are quantified considering model and parameter uncertainties. After that, multi-source monitoring information is used to update the probability distributions of the soil or rock model parameters and the factor of safety using Markov chain Monte Carlo simulation. An example of a slope with multiple monitoring parameters is presented to illustrate the proposed methodology. The method is able to integrate multi-source information based on slope stability mechanisms, and update the soil or rock parameters, the slope factor of safety, and the failure probability with the integrated monitoring information. Hence the evaluation becomes more reliable with the support of multiple sources of site-specific information. (C) 2013 Elsevier Ltd. All rights reserved.",""
"Fault tree analysis is a well-structured, precise, and powerful tool for system evaluation. However, the conventional approach has been found to be inadequate to deal with the absence of fault data, failure dependency, and uncertainty problems. This paper presents a comprehensive study on the evaluation of data communication system (DCS) using dynamic fault tree approach based on fuzzy set. It makes use of the advantages of the dynamic fault tree for modelling, fuzzy set theory for handling uncertainty, and Bayesian network (BN) for inference ability. Specifically, it adopts expert elicitation and fuzzy set theory to evaluate the failure rates of the basic events for DCS and uses a dynamic fault tree model to capture the dynamic failure mechanisms. Furthermore, some reliability parameters can be calculated by mapping a dynamic fault tree into an equivalent BN. The results show that the proposed method is more flexible and adaptive than conventional fault tree analysis for fault diagnosis and reliability estimation of DCS.","It makes use of the advantages of the dynamic fault tree for modelling, fuzzy set theory for handling uncertainty, and Bayesian network (BN) for inference ability."
"This paper proposes an end-to-end algorithm for multiple small objects tracking in noisy video using a combination of Gaussian mixture based background segmentation along with a Dynamic Bayesian Networks (DBNs) based tracking. Background segmentation is based on an adaptive backgrounding method that models each pixel as a mixture of Gaussians with spatial prior and uses an online approximation to update the model, the spatial prior is constructed for small objects. Furthermore, we create observation model with hidden variable based on multi-cue statistical object model and employ Kalman filter as inference algorithm. Finally, we use linear assignment problem (LAP) algorithm to perform the models matching. The experimental results show the proposed method outperforms competing method, and demonstrate the effectiveness of the proposed method. (C) 2013 Elsevier GmbH. All rights reserved.","Furthermore, we create observation model with hidden variable based on multi-cue statistical object model and employ Kalman filter as inference algorithm."
"We develop a novel approach for incorporating expert rules into Bayesian networks for classification of Mycobacterium tuberculosis complex (MTBC) clades. The proposed knowledge-based Bayesian network (KBBN) treats sets of expert rules as prior distributions on the classes. Unlike prior knowledge-based support vectormachine approaches which require rules expressed as polyhedral sets, KBBN directly incorporates the rules without any modification. KBBN uses data to refine rule-based classifiers when the rule set is incomplete or ambiguous. We develop a predictive KBBN model for 69 MTBC clades found in the SITVIT international collection. We validate the approach using two testbeds that model knowledge of the MTBC obtained from two different experts and large DNA fingerprint databases to predict MTBC genetic clades and sublineages. These models represent strains of MTBC using high throughput biomarkers called spacer oligonucleotide types (spoligotypes), since these are routinely gathered from MTBC isolates of tuberculosis (TB) patients. Results show that incorporating rules into problems can drastically increase classification accuracy if data alone are insufficient. The SITVIT KBBN is publicly available for use on the World Wide Web.","We develop a novel approach for incorporating expert rules into Bayesian networks for classification of Mycobacterium tuberculosis complex (MTBC) clades."
"When making decisions under uncertainty, the optimal choices are often difficult to discern, especially if not enough information has been gathered. Two key questions in this regard relate to whether one should stop the information gathering process and commit to a decision (stopping criterion), and if not, what information to gather next (selection criterion). In this paper, we show that the recently introduced notion, Same-Decision Probability (SDP), can be useful as both a stopping and a selection criterion, as it can provide additional insight and allow for robust decision making in a variety of scenarios. This query has been shown to be highly intractable, being PPPP-complete, and is exemplary of a class of queries which correspond to the computation of certain expectations. We propose the first exact algorithm for computing the SDP, and demonstrate its effectiveness on several real and synthetic networks. Finally, we present new complexity results, such as the complexity of computing the SDP on models with a Naive Bayes structure. Additionally, we prove that computing the non-myopic value of information is complete for the same complexity class as computing the SDP.",""
"Graphical models, such as Bayesian Networks and Markov networks play an important role in artificial intelligence and machine learning. Inference is a central problem to be solved on these networks. This, and other problems on these graph models are often known to be hard to solve in general, but tractable on graphs with bounded Treewidth. Therefore, finding or approximating the Treewidth of a graph is a fundamental problem related to inference in graphical models. In this paper, we study the approximability of a number of graph problems: Treewidth and Pathwidth of graphs, Minimum Fill-In, OneShot Black ( and Black-White) pebbling costs of directed acyclic graphs, and a variety of different graph layout problems such as Minimum Cut Linear Arrangement and Interval Graph Completion. We show that, assuming the recently introduced Small Set Expansion Conjecture, all of these","Inference is a central problem to be solved on these networks."
"Accurate evaluation and characterization of defects in multilayered structures from eddy current nondestructive testing (NDT) signals are a difficult inverse problem. There is scope for improving the current methods used for solving the inverse problem by incorporating information of uncertainty in the inspection process. Here, we propose to evaluate defects quantitatively from eddy current NDT signals using Bayesian networks (BNs). BNs are a useful method in handling uncertainty in the inspection process, eventually leading to the more accurate results. The domain knowledge and the experimental data are used to generate the BN models. The models are applied to predict the signals corresponding to different defect characteristic parameters or to estimate defect characteristic parameters from eddy current signals in real time. Finally, the estimation results are analyzed. Compared to the least squares regression method, BNs are more robust with higher accuracy and have the advantage of being a bidirectional inferential mechanism. This approach allows results to be obtained in the form of full marginal conditional probability distributions, providing more information on the defect. The feasibility of BNs presented and discussed in this paper has been validated.","Compared to the least squares regression method, BNs are more robust with higher accuracy and have the advantage of being a bidirectional inferential mechanism."
"Decisions affecting the management of natural resources in agricultural landscapes are influenced by both social and ecological factors. Models that integrate these factors are likely to better predict the outcomes of natural resource management decisions compared to those that do not take these factors into account. We demonstrate how Bayesian Networks can be used to integrate ecological and social data and expert opinion to model the cost-effectiveness of revegetation activities for restoring biodiversity in agricultural landscapes. We demonstrate our approach with a case-study in grassy woodlands of south-eastern Australia. In our case-study, cost-effectiveness is defined as the improvement in native reptile and beetle species richness achieved per dollar spent on a restoration action. Socio-ecological models predict that weed control, the planting of trees and shrubs, the addition of litter and timber, and the addition of rocks are likely to be the most cost-effective actions for improving reptile and beetle species richness. The cost-effectiveness of restoration actions is lower in remnant and revegetated areas than in cleared areas because of the higher marginal benefits arising from acting in degraded habitats. This result is contingent on having favourable landowner attitudes. Under the best-case landowner demographic scenarios the greatest biodiversity benefits are seen when cleared areas are restored. We find that current restoration investment practices may not be increasing faunal species richness in agricultural landscapes in the most cost-effective way, and that new restoration actions may be necessary. Integrated socio-ecological models support transparent and cost-effective conservation investment decisions. Application of these models highlights the importance of collecting both social and ecological data when attempting to understand and manage socio-ecological systems. (C) 2013 Elsevier Ltd. All rights reserved.",""
"Hepatocellular carcinoma (HCC) is one of the most common malignant tumors. Clinical symptoms attributable to HCC are usually absent, thus often miss the best therapeutic opportunities. Traditional Chinese Medicine (TCM) plays an active role in diagnosis and treatment of HCC. In this paper, we proposed a particle swarm optimization-based hierarchical feature selection (PSOHFS) model to infer potential syndromes for diagnosis of HCC. Firstly, the hierarchical feature representation is developed by a three-layer tree. The clinical symptoms and positive score of patient are leaf nodes and root in the tree, respectively, while each syndrome feature on the middle layer is extracted from a group of symptoms. Secondly, an improved PSO-based algorithm is applied in a new reduced feature space to search an optimal syndrome subset. Based on the result of feature selection, the causal relationships of symptoms and syndromes are inferred via Bayesian networks. In our experiment, 147 symptoms were aggregated into 27 groups and 27 syndrome features were extracted. The proposed approach discovered 24 syndromes which obviously improved the diagnosis accuracy. Finally, the Bayesian approach was applied to represent the causal relationships both at symptom and syndrome levels. The results show that our computational model can facilitate the clinical diagnosis of HCC.",""
"Obtaining operation rules (OR) for multi-reservoir water systems through optimization and simulation processes has been an intensely studied topic. However, an innovative approach for the integration of two approaches - network flow simulation models and evolutionary multi-objective optimization (EMO) - is proposed for obtaining the operation rules for integrated water resource management (IWRM). This paper demonstrates a methodology based on the coupling of an EMO algorithm (NSGA-II or Non-dominated Sorting Genetic Algorithm) with an existing water resources allocation simulation network flow model (SIMGES). The implementation is made for a real case study, the Mijares River basin (Spain) which is characterized by severe drought events, a very traditional water rights system and its historical implementation of the conjunctive use of surface and ground water. The established operation rules aim to minimize the maximum deficit in the short term without compromising the maximum deficits in the long term. This research proves the utility of the proposed methodology by coupling NSGA-II and SIMGES to find the optimal reservoir operation rules in multi-reservoir water systems.",""
"Robustness in biological networks can be regarded as an important feature of living systems. A system maintains its functions against internal and external perturbations, leading to topological changes in the network with varying delays. To understand the flexibility of biological networks, we propose a novel approach to analyze time-dependent networks, based on the framework of network completion, which aims to make the minimum amount of modifications to a given network so that the resulting network is most consistent with the observed data. We have developed a novel network completion method for time-varying networks by extending our previous method for the completion of stationary networks. In particular, we introduce a double dynamic programming technique to identify change time points and required modifications. Although this extended method allows us to guarantee the optimality of the solution, this method has relatively low computational efficiency. In order to resolve this difficulty, we developed a heuristic method for speeding up the calculation of minimum least squares errors. We demonstrate the effectiveness of our proposed methods through computational experiments using synthetic data and real microarray gene expression data. The results indicate that our methods exhibit good performance in terms of completing and inferring gene association networks with time-varying structures.",""
"A multi-state system with multi-state components is a model of systems, where performance, capacity, or reliability levels of the systems are represented as states. It usually has more than two states, and thus can be considered as a multi-valued function, called a structure function. Since many structure functions are monotone increasing, their multi-state systems can be represented compactly by edge-valued multivalued decision diagrams (EVMDDs). This paper presents an analysis method of multi-state systems with multi-state components using EVMDDs. Experimental results show that, by using EVMDDs, structure functions can be represented more compactly than existing methods using ordinary MDDs. Further, EVMDDs yield comparable computation time for system analysis. This paper also proposes a new diagnosis method using EVMDDs, and shows that the proposed method can infer the most probable causes for system failures more efficiently than conventional methods based on Bayesian networks.",""
"The use of sensor networks has been proposed for military surveillance and environmental monitoring applications. Those systems are composed of a heterogeneous set of sensors to observe the environment. In centralised systems the observed data will be conveyed to the control room to process the data. Human operators are supposed to give a semantic interpretation of the observed data. They are searching for suspicious or unwanted behaviour. The increase of surveillance sensors in the military domain requires a huge amount of human operators which is far beyond available resources. Automated systems are needed to give a context sensitive semantic interpretation of the observed kinematic data. As a proof of concept two automatic surveillance projects will be discussed in this paper. The first project is about a centralised system based on the AIS-Automated Identification System which will be used to monitor ship movements automatically. The second project is about a decentralised system composed of a network of cameras installed at a military area. There is a need for a surveillance system along the coast of Europe. There is an increase of illegal drugs transport from the open sea, intrusion of boat refuges, illegal fishing, pollution of the sea by illegal chemical and oil pollution by ships. An automated sensor system is needed to detect illegal intruders and suspicious ship movements. Vessels fitted with AIS transceivers and transponders can be tracked by AIS base stations located along coast lines or, when out of range of terrestrial networks, through a growing number of satellites that are fitted with special AIS receivers. AIS data include a unique identifier of a vessel and kinematic data such as its position, course and speed. The proposed system enables identification, and tracking of vessels and to detect unwanted or illegal behaviour of ship movements. If ships violate traffic rules, enter forbidden areas or approach a critical infrastructure an alert will be generated automatically in the control room. Human operators start an emergency procedure. The second project is about a network of cameras installed at a military area. The area is monitored by multiple cameras with non-overlapping field of views monitored by human operators. We developed an automated surveillance system. At the entrance gate the identity of visitors will be checked by a face recognition system. In case of intruders, unwanted behaviour, trouble makers the emotional state of the visitor will be assessed by an analysis of facial expressions using the Active Appearance model. If unwanted behaviour is detected an alert is send the control room. Also license place of cars will be recognized using a system based on Neocognitron Neural Networks. Moving objects as persons and vehicles will be detected, localized and tracked. Kinematic parameters are extracted and a semantic interpretation of their behaviour is automatically generated using a rule based system and Bayesian networks. Cars violating the traffic rules or passing speed limits or entering forbidden areas or stopping/parking at forbidden places will be detected. A prototype of a system has been developed which is able to monitor the area 24 hours a day, 7 days a week.",""
"In this paper, we describe an application of Bayesian networks in the prognoses of the properly utilization of thermostatic valves, by using the SamIam program. The application was validated based on the data obtained by measures in the District Heating System of Bucharest. Thermostatic valves transformed heating systems into efficient technical facilities, able to properly react to any internal and external disturbing factors. The diagnoses concerning the properly use of thermostatic valves represent the original contribution of the research team. At the end of the paper it will be illustrated practical applications of the theoretical knowledge discussed throughout the article in a number of case studies which present several problems related to the thermostatic valves.",""
"This paper proposes a semi-non parametric density estimation framework for high-dimensional data. Dimensionality reduction is achieved by reorganizing the domain variables set into a junction tree of cliques each containing a small number of variables where factorization of the joint density into a tree is carried out by learning the Bayesian Network (BN) structure graph and by searching the maximum spanning tree over the moralized-triangulated graph of the obtained BN. To estimate the density of the junction tree elements, we propose a novel technique using local Independent Component Analysis (ICA) method based on fuzzy clustering. The main contribution relates to the development of a generic framework through a combination of three complimentary modules: (1) BN structure learning, (2) fuzzy clustering, and (3) linear ICA method. This allows us to exploit the separation power of recently developed ICA tools. Hence, depending on the data characteristics, the user can choose among a wide range of ICA and BN tools the most suitable one. We experimentally evaluated our approach in a supervised classification problem and the obtained results indicate an improvement in accuracy.","We experimentally evaluated our approach in a supervised classification problem and the obtained results indicate an improvement in accuracy."
"The rationale of XML design is to transfer and store data at different levels. A key feature of these levels in an XML document is to identify its components for additional processing. XML components can expose sensitive information after application of data mining techniques over a shared database. Therefore, privacy preservation of sensitive information must be ensured prior to signify the outcome especially in sensitive XML Association Rules. Privacy issues in XML domain are not exceptionally addressed to determine a solution by the academia in a reliable and precise manner. In this paper, we have proposed a model for identifying sensitive items (nodes) to declare sensitive XML association rules and then to hide them. Bayesian networks-based central tendency measures are applied in declaration of sensitive XML association rules. K2 algorithm is used to generate Bayesian networks to ensure reliability and accuracy in preserving privacy of XML Association Rules. The proposed model is tested and compared using several case studies and large UCI machine learning datasets. The experimental results show improved accuracy and reliability of proposed model without any side effects such as new rules and lost rules. The proposed model uses the same minimum support threshold to find XML Association Rules from the original and transformed data sources. The significance of the proposed model is to minimize an incredible disclosure risk involved in XML association rule mining from external parties in a competitive business environment.",""
"Large data has been accumulating in all aspects of our lives for quite some time. Advances in sensor technology, the Internet, wireless communication, and inexpensive memory have all contributed to an explosion of \"Big Data\". System of Systems (SOS) integrate independently operating, non-homogeneous systems to achieve a higher goal than the sum of the parts. Today's SoS are also contributing to the existence of unmanageable \"Big Data\". Recent efforts have developed a promising approach, called \"Data Analytics\", which uses statistical and computational intelligence (Cl) tools such as principal component analysis (PCA), clustering, fuzzy logic, neuro-computing, evolutionary computation (such as genetic algorithms), Bayesian networks, etc. to reduce the size of \"Big Data\" to a manageable size and apply these tools to (a) extract information, (b) build a knowledge base using the derived data, and (c) eventually develop a non-parametric model for the \"Big Data\". This paper demonstrates how to construct a bridge between SoS and Data Analytics to develop reliable models for such systems. The subject material for this demonstration is using data analytics to generate a model to forecast produced photovoltaic energy to assist in the optimization of a micro grid SoS. Tools like fuzzy interference, neural networks, PCA, and genetic algorithms are used. (C) 2013 Elsevier Ltd. All rights reserved.",""
"As the combination of parameter learning and structure learning, learning Bayesian networks can also be examined, Parameter learning is estimation of the dependencies in the network. Structural learning is the estimation of the links of the network. In terms of whether the structure of the network is known and whether the variables are all observable, there are four types of learning Bayesian networks cases. In this paper, first introduce two cases of learning Bayesian networks from complete data: known structure and unobservable variables and unknown structure and unobservable variables. Next, we study two cases of learning Bayesian networks from incomplete data: known network structure and unobservable variables, unknown network structure and unobservable variables.",""
"Bayesian networks are possibly the most successful graphical models to build decision support systems. Building the structure of large networks is still a challenging task, but Bayesian methods are particularly suited to exploit experts' degree of belief in a quantitative way while learning the network structure from data. In this paper details are provided about how to build a prior distribution on the space of network structures by eliciting a chain graph model on structural reference features. Several structural features expected to be often useful during the elicitation are described. The statistical background needed to effectively use this approach is summarized, and some potential pitfalls are illustrated. Finally, a few seminal contributions from the literature are reformulated in terms of structural features.",""
"Given the increasing cooperation between organizations, the flexible exchange of security information across the allied organizations is critical to effectively manage information systems (IS) security in a distributed environment. In this paper, we develop a cooperative model for IS security risk management in a distributed environment. In the proposed model, the exchange of security information among the interconnected IS under distributed environment is supported by Bayesian networks (BNs). In addition, for an organization's IS, a BN is utilized to represent its security environment and dynamically predict its security risk level, by which the security manager can select an optimal action to safeguard the firm's information resources. The actual case studied illustrates the cooperative model presented in this paper and how it can be exploited to manage the distributed IS security risk effectively.",""
"The complexity of continuous care settings has increased due to an ageing population, a dwindling number of caregivers and increasing costs. Electronic healthcare (eHealth) solutions are often introduced to deal with these issues. This technological equipment further increases the complexity of healthcare as the caregivers are responsible for integrating and configuring these solutions to their needs. Small differences in user requirements often occur between various environments where the services are deployed. It is difficult to capture these nuances at development time. Consequently, the services are not tuned towards the users' needs. This paper describes our experiences with extending an eHealth application with self-learning components such that it can automatically adjust its parameters at run-time to the users' needs and preferences. These components gather information about the usage of the application. This collected information is processed by data mining techniques to learn the parameter values for the application. Each discovered parameter is associated with a probability, which expresses its reliability. Unreliable values are filtered. The remaining parameters and their reliability are integrated into the application. The eHealth application is the ontology-based Nurse Call System (oNCS), which assesses the priority of a call based on the current context and assigns the most appropriate caregiver to a call. Decision trees and Bayesian networks are used to learn and adjust the parameters of the oNCS. For a realistic dataset of 1050 instances, correct parameter values are discovered very efficiently as the components require at most 100 ms execution time and 20 MB memory. (C) 2013 Elsevier Ltd. All rights reserved.",""
"The effects of both abiotic factors and biotic interactions among guilds (i.e., inter-guild effects) have been suggested to be important for understanding spatial variation in species diversity; however, compared to the abiotic effects, the processes by which the inter-guild effects are mediated have been little described. Hence, we investigated stream invertebrate assemblages on Hokkaido Island, Japan, and assessed how the processes of determining regional patterns in species diversity differed among guilds (collector-filterers, collector-gatherers/shredders, scrapers, and predators) by taking both inter-guild and abiotic effects into consideration using Bayesian networks. Collector-gatherers/shredders, collector-filterers, and predators exhibited significant regional gradients in taxonomic richness. Gradients in the former two guilds can be generated by variation in flood disturbance regardless of interactions with other guilds. The gradient in predator taxonomic richness was indirectly related to the disturbance and was directly generated by bottom-up effects through their prey (collector-gatherers/shredders and collector-filterers). We found that not only environmental factors, but also inter-guild effects may be essential for forming the regional gradient in predators, unlike those for collector-gatherers/shredders and collector-filterers. The processes underlying the regional variation in taxonomic richness of the three guilds are interpreted in terms of the more individuals hypothesis, facilitation, and predator-prey relationships.",""
"Large investments are made annually to develop and maintain IT systems. Successful outcome of IT projects is therefore crucial for the economy. Yet, many IT projects fail completely or are delayed or over budget, or they end up with less functionality than planned. This article describes a Bayesian decision-support model. The model is based on expert elicited data from 51 experts. Using this model, the effect management decisions have upon projects can be estimated beforehand, thus providing decision support for the improvement of IT project performance.",""
"Strict enforcement of forest protection and massive afforestation campaigns have contributed to a significant increase in China's forest cover during the last 20 years. At the same time, demographic changes in rural areas due to changes in reproduction patterns and the emigration of younger population segments have affected land-use strategies. We identified proximate causes and underlying drivers that influence the decisions of farm households to plant trees on former cropland with Bayesian networks (BNs). BNs allow the incorporation of causal relationships in data analysis and can combine qualitative stakeholder knowledge with quantitative data. We defined the structure of the network with expert knowledge and in-depth discussions with land users. The network was calibrated and validated with data from a survey of 509 rural households in two upland areas of Yunnan Province in Southwest China. The results substantiate the influence of land endowments, labor availability and forest policies for switching from cropland to tree planting. State forest policies have constituted the main underlying driver to the forest transition in the past, but private afforestation activities increasingly dominate the expansion of tree cover. Farmers plant trees on private incentives mainly to cash in on the improved economic opportunities provided by tree crops, but tree planting also constitutes an important strategy to adjust to growing labor scarcities. (C) 2013 Elsevier Ltd. All rights reserved.",""
"Customer churn prediction is becoming an increasingly important business analytics problem for telecom operators. In order to increase the efficiency of customer retention campaigns, churn prediction models need to be accurate as well as compact and interpretable. Although a myriad of techniques for churn prediction has been examined, there has been little attention for the use of Bayesian Network classifiers. This paper investigates the predictive power of a number of Bayesian Network algorithms, ranging from the Naive Bayes classifier to General Bayesian Network classifiers. Furthermore, a feature selection method based on the concept of the Markov Blanket, which is genuinely related to Bayesian Networks, is tested. The performance of the classifiers is evaluated with both the Area under the Receiver Operating Characteristic Curve and the recently introduced Maximum Profit criterion. The Maximum Profit criterion performs an intelligent optimization by targeting this fraction of the customer base which would maximize the profit generated by a retention campaign. The results of the experiments are rigorously tested and indicate that most of the analyzed techniques have a comparable performance. Some methods, however, are more preferred since they lead to compact networks, which enhances the interpretability and comprehensibility of the churn prediction models.","Although a myriad of techniques for churn prediction has been examined, there has been little attention for the use of Bayesian Network classifiers."
"Research in the field of social representations and of norm conditionality has enabled the construction of specific tools such as the conditional script questionnaire (CSQ) and the use of various methods of analysis. The first aim of this study is to show the differences of conditionality between male and female drivers toward pedestrians. To test this hypothesis, a version of the CSQ has been fitted to the pedestrian. We show that conditionality toward the pedestrian is more significant for men in certain specific situations and we highlight that women are more aware of the vulnerability of pedestrians. The second stage is aimed at improving knowledge between conditionality and risk taking, by using Bayes' theorem for the first time. It is demonstrated that a Bayesian network based on the CSQ can be built in order to model the perception of hazardous situations.",""
"Utility functions in the form of tables or matrices have often been used to combine discretely rated decision-making criteria. Matrix elements are usually specified individually, so no one rule or principle can be easily stated for the utility function as a whole. A series of five matrices are presented that aggregate criteria two at a time using simple rules that express a varying degree of constraint of the lower rating over the higher. A further nine possible matrices were obtained by using a different rule either side of the main axis of the matrix to describe situations where the criteria have a differential influence on the outcome. Uncertainties in the criteria are represented by three alternative frequency distributions from which the assessors select the most appropriate. The output of the utility function is a distribution of rating frequencies that is dependent on the distributions of the input criteria. In pest risk analysis (PRA), seven of these utility functions were required to mimic the logic by which assessors for the European and Mediterranean Plant Protection Organization arrive at an overall rating of pest risk. The framework enables the development of PRAs that are consistent and easy to understand, criticize, compare, and change. When tested in workshops, PRA practitioners thought that the approach accorded with both the logic and the level of resolution that they used in the risk assessments.",""
"Fungi can serve as highly tractable models for understanding genetic basis of sexual development in multicellular organisms. Applying a reverse-genetic approach to advance such a model, we used random and multitargeted primers to assay gene expression across perithecial development in Neurospora crassa. We found that functionally unclassified proteins accounted for most upregulated genes, whereas downregulated genes were enriched for diverse functions. Moreover, genes associated with developmental traits exhibited stage-specific peaks of expression. Expression increased significantly across sexual development for mating type gene mat a-1 and for mat A-1 specific pheromone precursor ccg-4. In addition, expression of a gene encoding a protein similar to zinc finger, stc1, was highly upregulated early in perithecial development, and a strain with a knockout of this gene exhibited arrest at the same developmental stage. A similar expression pattern was observed for genes in RNA silencing and signaling pathways, and strains with knockouts of these genes were also arrested at stages of perithecial development that paralleled their peak in expression. The observed stage specificity allowed us to correlate expression upregulation and developmental progression and to identify regulators of sexual development. Bayesian networks inferred from our expression data revealed previously known and new putative interactions between RNA silencing genes and pathways. Overall, our analysis provides a fine-scale transcriptomic landscape and novel inferences regarding the control of the multistage development process of sexual crossing and fruiting body development in N. crassa.","We found that functionally unclassified proteins accounted for most upregulated genes, whereas downregulated genes were enriched for diverse functions."
"Neuronal morphology is hugely variable across brain regions and species, and their classification strategies are a matter of intense debate in neuroscience. GABAergic cortical interneurons have been a challenge because it is difficult to find a set of morphological properties which clearly define neuronal types. A group of 48 neuroscience experts around the world were asked to classify a set of 320 cortical GABAergic interneurons according to the main features of their three-dimensional morphological reconstructions. A methodology for building a model which captures the opinions of all the experts was proposed. First, one Bayesian network was learned for each expert, and we proposed an algorithm for clustering Bayesian networks corresponding to experts with similar behaviors. Then, a Bayesian network which represents the opinions of each group of experts was induced. Finally, a consensus Bayesian multinet which models the opinions of the whole group of experts was built. A thorough analysis of the consensus model identified different behaviors between the experts when classifying the interneurons in the experiment. A set of characterizing morphological traits for the neuronal types was defined by performing inference in the Bayesian multinet. These findings were used to validate the model and to gain some insights into neuron morphology. (C) 2013 Elsevier Inc. All rights reserved.","Neuronal morphology is hugely variable across brain regions and species, and their classification strategies are a matter of intense debate in neuroscience."
"Much effort has been made to better understand the complex integration of distinct parts of the human brain using functional magnetic resonance imaging (fMRI). Altered functional connectivity between brain regions is associated with many neurological and mental illnesses, such as Alzheimer and Parkinson diseases, addiction, and depression. In computational science, Bayesian networks (BN) have been used in a broad range of studies to model complex data set in the presence of uncertainty and when expert prior knowledge is needed. However, little is done to explore the use of BN in connectivity analysis of fMRI data. In this paper, we present an up-to-date literature review and methodological details of connectivity analyses using BN, while highlighting caveats in a real-world application. We present a BN model of fMRI dataset obtained from sixty healthy subjects performing the stop-signal task (SST), a paradigm widely used to investigate response inhibition. Connectivity results are validated with the extant literature including our previous studies. By exploring the link strength of the learned BNs and correlating them to behavioral performance measures, this novel use of BN in connectivity analysis provides new insights to the functional neural pathways underlying response inhibition. (C) 2013 Elsevier Inc. All rights reserved.",""
"Assistive systems for persons with cognitive disabilities (e.g., dementia) are difficult to build due to the wide range of different approaches people can take to accomplishing the same task, and the significant uncertainties that arise from both the unpredictability of client's behaviours and from noise in sensor readings. Partially observable Markov decision process (POMDP) models have been used successfully as the reasoning engine behind such assistive systems for small multi-step tasks such as hand washing. POMDP models are a powerful, yet flexible framework for modelling assistance that can deal with uncertainty and utility. Unfortunately, POMDPs usually require a very labour intensive, manual procedure for their definition and construction. Our previous work has described a knowledge driven method for automatically generating POMDP activity recognition and context sensitive prompting systems for complex tasks. We call the resulting POMDP a SNAP (SyNdetic Assistance Process). The spreadsheet-like result of the analysis does not correspond to the POMDP model directly and the translation to a formal POMDP representation is required. To date, this translation had to be performed manually by a trained POMDP expert. In this paper, we formalise and automate this translation process using a probabilistic relational model (PRM) encoded in a relational database. The database encodes the relational skeleton of the PRM, and includes the goals, action preconditions, environment states, cognitive model, client and system actions (i.e., the outcome of the SNAP analysis), as well as relevant sensor models. The database is easy to approach for someone who is not an expert in POMDPs, allowing them to fill in the necessary details of a task using a simple and intuitive procedure. The database, when filled, implicitly defines a ground instance of the relational skeleton, which we extract using an automated procedure, thus generating a POMDP model of the assistance task. A strength of the database is that it allows constraints to be specified, such that we can verify the POMDP model is, indeed, valid for the task given the analysis. We demonstrate the method by eliciting three assistance tasks from non-experts: handwashing, and toothbrushing for elderly persons with dementia, and on a factory assembly task for persons with a cognitive disability. We validate the resulting POMDP models using case-based simulations to show that they are reasonable for the domains. We also show a complete case study of a designer specifying one database, including an evaluation in a real-life experiment with a human actor. Crown Copyright (C) 2013 Published by Elsevier Inc. All rights reserved.",""
"Bridging the gap between the theory of Bayesian networks and solving an actual problem is still a big challenge and this is in particular true for medical problems, where such a gap is clearly evident. We argue that Bayesian networks offer appropriate technology for the successful modelling of medical problems, including the personalisation of healthcare. Personalisation is an important aspect of remote disease management systems. It involves the forecasting of progression of a disease based on the interpretation of patient data by a disease model. A natural foundation for disease models is physiological knowledge, as such knowledge facilitates building clinically understandable models. This paper proposes ways to represent such knowledge as part of engineering principles employed in building clinically practical probabilistic models. The methodology has been used to construct a temporal Bayesian network model for preeclampsia - a pregnancy-related disorder. The model is the first of its kind and an integral part of a mobile home-monitoring system intended for use in daily pregnancy care. We conducted an evaluation study with actual patient data to obtain insight into the model's performance and suitability. The results obtained are encouraging and show the potential of exploiting physiological knowledge for personalised decision-support systems. (C) 2013 Elsevier Inc. All rights reserved.",""
"In this paper we describe a system designed for assisting geneticists in vegetal genetic improvement tasks. The system is based on the use of Bayesian networks. It has been developed under the industrial demands emerging from the area of Campo de Dallas in Almeria (Spain), and is therefore oriented to producing new tomato varieties, which constitute the main product in the area. The paper concentrates on the main aspects of the design of the system. (C) 2013 Elsevier Inc. All rights reserved.",""
"In recent years electronic tracking has provided voluminous data on vessel movements, leading researchers to try various data mining techniques to find patterns and, especially, deviations from patterns, i.e., for anomaly detection. Here we describe anomaly detection with data mined Bayesian Networks, learning them from real world Automated Identification System (AIS) data, and from supplementary data, producing both dynamic and static Bayesian network models. We find that the learned networks are quite easy to examine and verify despite incorporating a large number of variables. We also demonstrate that combining dynamic and static modelling approaches improves the coverage of the overall model and thereby anomaly detection performance. (C) 2013 Elsevier Inc. All rights reserved.",""
"Over the last decade, a normative framework for making causal inferences, Bayesian Probabilistic Causal Networks, has come to dominate psychological studies of inference based on causal relationships. The following causal networks-[X -> Y -> Z, X <- Y -> Z, X -> Y <- Z]-supply answers for questions like, \"Suppose both X and Y occur, what is the probability Z occurs?\" or \"Suppose you intervene and make Y occur, what is the probability Z occurs?\" In this review, we provide a tutorial for how normatively to calculate these inferences. Then, we systematically detail the results of behavioral studies comparing human qualitative and quantitative judgments to the normative calculations for many network structures and for several types of inferences on those networks. Overall, when the normative calculations imply that an inference should increase, judgments usually go up; when calculations imply a decrease, judgments usually go down. However, 2 systematic deviations appear. First, people's inferences violate the Markov assumption. For example, when inferring Z from the structure X -> Y -> Z, people think that X is relevant even when Y completely mediates the relationship between X and Z. Second, even when people's inferences are directionally consistent with the normative calculations, they are often not as sensitive to the parameters and the structure of the network as they should be. We conclude with a discussion of productive directions for future research.","Over the last decade, a normative framework for making causal inferences, Bayesian Probabilistic Causal Networks, has come to dominate psychological studies of inference based on causal relationships."
"DNA mixtures are challenging not only at low template DNA level but also at highly balanced quantitative ratio. In this latter case, interpretation may be complicated by the joint action of combinatorial uncertainty and stochastic effects of the PCR. We explore this particular and so far little noticed aspect of mixture interpretation by first providing a complete quantitative combinatorial analysis of the two-person mixture model (2PM) at highly balanced ratio of contributors, and then by carrying out a calibration study of the 2PM model on good quality experimental mixtures. The calibration tests provided the evidence for the existence of irregular distribution of peak heights, that can misguide the correct genotype assignment at high template ratios too. Repeating the experiment, performing Bayesian analysis to the whole evidence and developing a careful joint prediction of all plausible genotype datasets is highly mandatory in these cases, prior to set evidentiary LRs and use them in court. (C) 2013 Elsevier Ireland Ltd. All rights reserved.",""
"The genetic characterization of unbalanced mixed stains remains an important area where improvement is imperative. In fact, with current methods for DNA analysis (Polymerase Chain Reaction with the SGM Plus (TM) multiplex kit), it is generally not possible to obtain a conventional autosomal DNA profile of the minor contributor if the ratio between the two contributors in a mixture is smaller than 1:10. This is a consequence of the fact that the major contributor's profile 'masks' that of the minor contributor. Besides known remedies to this problem, such as Y-STR analysis, a new compound genetic marker that consists of a Deletion/Insertion Polymorphism (DIP), linked to a Short Tandem Repeat (STR) polymorphism, has recently been developed and proposed elsewhere in literature [1]. The present paper reports on the derivation of an approach for the probabilistic evaluation of DIP-STR profiling results obtained from unbalanced DNA mixtures. The procedure is based on object-oriented Bayesian networks (OOBNs) and uses the likelihood ratio as an expression of the probative value. OOBNs are retained in this paper because they allow one to provide a clear description of the genotypic configuration observed for the mixed stain as well as for the various potential contributors (e.g., victim and suspect). These models also allow one to depict the assumed relevance relationships and perform the necessary probabilistic computations. (C) 2013 Elsevier Ireland Ltd. All rights reserved.",""
"Class-level models capture relational statistics over object attributes and their connecting links, answering questions such as \"what is the percentage of friendship pairs where both friends are women?\" Class-level relationships are important in themselves, and they support applications like policy making, strategic planning, and query optimization. We represent class statistics using Parametrized Bayes Nets (PBNs), a first-order logic extension of Bayes nets. Queries about classes require a new semantics for PBNs, as the standard grounding semantics is only appropriate for answering queries about specific ground facts. We propose a novel random selection semantics for PBNs, which does not make reference to a ground model, and supports class-level queries. The parameters for this semantics can be learned using the recent pseudo-likelihood measure (Schulte in SIAM SDM, pp. 462-473, 2011) as the objective function. This objective function is maximized by taking the empirical frequencies in the relational data as the parameter settings. We render the computation of these empirical frequencies tractable in the presence of negated relations by the inverse Mobius transform. Evaluation of our method on four benchmark datasets shows that maximum pseudo-likelihood provides fast and accurate estimates at different sample sizes.",""
"The management of freshwater ecosystems is usually targeted through the regulation of water quantity (limiting diversions and providing environmental flows) and regulation of water quality (setting limits or targets for constituent concentrations). Climate change is likely to affect water quantity and quality in multiple ways and the future management of freshwater ecosystems requires predictions of plausible future conditions. We use a suite of ecologically-relevant hydrological indicators to determine the significance of projected climate-driven hydrological changes in the Upper Murrumbidgee River Catchment in south eastern Australia in relation to river regulation. We also determine the possible water quality changes (in relation to guidelines for aquatic ecosystem protection) associated with the climate change projections to identify the combined effects of hydrological and water quality changes. The results of this study suggest that river regulation has resulted in greater changes to ecologically-relevant streamflow characteristics than climate change scenarios that involve a 1 and 2 degrees C temperature rise in the Upper Murrumbidgee River Catchment. In contrast to the projected hydrological changes, Bayesian Network modelling suggests very small changes to violations of water quality thresholds designed to protect aquatic ecosystems as a result of climate change. By identifying key components of the flow and water quality regimes that may be affected by climate change, we are able to provide managers with information that assists in developing adaptation initiatives.",""
"Artificial Intelligence is a brain child of Alan Turing and his universal programmable computer. During the 1960s and 1970s, AI researchers used computers for exploring intuitions about intelligence and for writing programs displaying intelligent behavior. A significant change occurred however in the 1980s, as many AI researchers moved from the early AI paradigm of writing programs for ill-defined problems to writing solvers for well-defined mathematical models like Constraint Satisfaction Problems, Strips Planning, SAT, Bayesian Networks, Partially Observable Markov Decision Processes and General Game Playing. Solvers are programs that take a compact description of a particular model instance and automatically compute its solution. Unlike the early AI programs, solvers are general as they must deal with any instance that fits the model. Many ideas have been advanced to address this crisp computational challenge from which a number of lessons can be drawn. In this paper, I revisit the problem of generality in AI, look at the way in which this `Models and Solvers' agenda addresses the problem, and discuss the relevance of this agenda to the grand AI goal of a computational account of intelligence and human cognition.",""
"Several studies have described the prevalence and severity of diagnostic errors. Diagnostic errors can arise from cognitive, training, educational and other issues. Examples of cognitive issues include flawed reasoning, incomplete knowledge, faulty information gathering or interpretation, and inappropriate use of decision-making heuristics. We describe a new approach, case-based fuzzy cognitive maps, for medical diagnosis and evaluate it by comparison with Bayesian belief networks. We created a semantic web framework that supports the two reasoning methods. We used database of 174 anonymous patients from several European hospitals: 80 of the patients were female and 94 male with an average age 45 16 (average +/- stdev). Thirty of the 80 female patients were pregnant. For each patient, signs/symptoms/observables/age/sex were taken into account by the system. We used a statistical approach to compare the two methods. (C) 2013 Elsevier Ireland Ltd. All rights reserved.",""
"Monitoring a complex process often involves keeping an eye on hundreds or thousands of sensors to determine whether or not the process is stable. We have been working with dynamic data from an oil production facility in the North sea, where unstable situations should be identified as soon as possible. Motivated by this problem setting, we propose a general model for classification in dynamic domains, and exemplify its use by showing how it can be employed for activity detection. We construct our model by using well known statistical techniques as building-blocks, and evaluate each step in the model-building process empirically. Exact inference in the proposed model is intractable, so in this paper we experiment with an approximate inference scheme. (C) 2013 Elsevier Ltd. All rights reserved.","Motivated by this problem setting, we propose a general model for classification in dynamic domains, and exemplify its use by showing how it can be employed for activity detection."
"We study maintenance of a complex dynamic system consisting of ageing and unobservable components under a predetermined threshold reliability level. Our aim is to construct an optimum replacement policy for the components of the system by minimizing total number of replacements or total replacement cost. We represent the problem with dynamic Bayesian networks (DBNs). We prove that under the existence of a predetermined threshold reliability, performing replacements at periods when the system reliability just falls below the threshold assures optimum replacement times. Four component selection approaches and their cost focused versions are proposed to choose the component to replace and are tested on a complex dynamic problem. Their performances are analyzed under various threshold and cost levels.",""
"A model based on a Bayesian Belief Network (BBN) has been constructed for the Baltic Sea with the aim of investigating future scenarios of human activities in the region and informing environmental management strategies, such as those developed under a Science and Policy Integration for Coastal Zone Assessment Systems Approach Framework application. This paper describes necessary refinements to take into account historical influences on this relatively enclosed system. BBNs are static models and therefore do not incorporate feedback loops, whereas natural systems clearly display feedback mechanisms. This paper describes the implementation of one step feedback loops into a BBN model in an attempt to partly remove this constraint. Feedback loops within this stochastic model were shown to improve its accuracy. The drivers, both natural and anthropogenic, having greatest impact on the environment are identified. These refinements were made to improve its accuracy in modelling the system and gives insights into the functioning of that system. (C) 2013 Elsevier Ltd. All rights reserved.",""
"The Bayesian estimation of the conditional Gaussian parameter needs to define several a priori parameters. The proposed approach is free from this definition of priors. We use the Implicit estimation method for learning from observations without a prior knowledge. We illustrate the interest of such an estimation method by giving first the Bayesian Expectation A Posteriori estimator for conditional Gaussian parameters. Then, we describe the Implicit estimators for the same parameters. Moreover, an experimental study is proposed in order to compare both approaches.",""
"A medical diagnosis system (DRCAD), which consists of two sub-modules Bayesian and rule-based inference models, is presented in this study. Three types of tests are conducted to assess the performances of the models producing synthetic data based on the ALARM network. The results indicate that the linear combination of the aforementioned models leads to a 5% and a 30% improvement in medical diagnosis when compared to the Rule Based Method and the Bayesian Network Based Method, respectively.","A medical diagnosis system (DRCAD), which consists of two sub-modules Bayesian and rule-based inference models, is presented in this study."
"It is an effective strategy to use both genetic perturbation data and gene expression data to infer regulatory networks that aims to improve the detection accuracy of the regulatory relationships among genes. Based on both types of data, the genetic regulatory networks can be accurately modeled by Structural Equation Modeling (SEM). In this paper, a linear regression (LR) model is formulated based on the SEM, and a novel iterative scheme using Bayesian inference is proposed to estimate the parameters of the LR model (LRBI). Comparative evaluations of LRBI with other two algorithms, the Adaptive Lasso (AL-Based) and the Sparsity-aware Maximum Likelihood (SML), are also presented. Simulations show that LRBI has significantly better performance than AL-Based, and overperforms SML in terms of power of detection. Applying the LRBI algorithm to experimental data, we inferred the interactions in a network of 35 yeast genes. An open-source program of the LRBI algorithm is freely available upon request.","In this paper, a linear regression (LR) model is formulated based on the SEM, and a novel iterative scheme using Bayesian inference is proposed to estimate the parameters of the LR model (LRBI)."
"Sustainability Appraisal (SA) is a complex task that involves integration of social, environmental and economic considerations and often requires trade-offs between multiple stakeholders that may not easily be brought to consensus. Classical SA, often compartmentalised in the rigid boundary of disciplines, can facilitate discussion, but can only partially inform decision makers as many important aspects of sustainability remain abstract and not interlinked. A fully integrated model can overcome compartmentality in the assessment process and provides opportunity for a better integrative exploratory planning process. The objective of this paper is to explore the benefit of an integrated modelling approach to SA and how a structured integrated model can be used to provide a coherent, consistent and deliberative platform to assess policy or planning proposals. The paper discusses a participative and integrative modelling approach to urban river corridor development, incorporating the principal of sustainability. The paper uses a case study site in Sheffield, UK, with three alternative development scenarios, incorporating a number of possible riverside design features. An integrated SA model is used to develop better design by optimising different design elements and delivering a more sustainable (re)-development plan. We conclude that participatory integrated modelling has strong potential for supporting the SA processes. A high degree of integration provides the opportunity for more inclusive and informed decision-making regarding issues of urban development. It also provides the opportunity to reflect on their long-term dynamics, and to gain insights on the interrelationships underlying persistent sustainability problems. Thus the ability to address economic, social and environmental interdependencies within policies, plans, and legislations is enhanced. (C) 2013 Elsevier Ltd. All rights reserved.",""
"Giving useful recommendations to students to improve collaboration in a learning experience requires tracking and analyzing student team interactions, identifying the problems and the target student. Previously, we proposed an approach to track students and assess their collaboration, but it did not perform any decision analysis to choose a recommendation for the student. In this paper, we propose an influence diagram, which includes the observable variables relevant for assessing collaboration, and the variable representing whether the student collaborates or not. We have analyzed the influence diagram with two machine learning techniques: an attribute selector, indicating the most important attributes that the model uses to recommend, and a decision tree algorithm revealing four different scenarios of recommendation. These analyses provide two useful outputs: (a) an automatic recommender, which can warn of problematic circumstances, and (b) a pedagogical support system (decision tree) that provides a visual explanation of the recommendation suggested. (C) 2013 Elsevier Ltd. All rights reserved.",""
"This paper presents a quantitative reliability and availability evaluation method for subsea blowout preventer (BOP) system by translating fault tree (FT) into dynamic Bayesian networks (DBN) directly, taking account of imperfect repair. The FTs of series system and parallel system are translated into Bayesian networks, and extended to DBN subsequently. The multi-state degraded system is used to model the imperfect repair in the DBN. Using the proposed method, the DBN of subsea BOP system is established. The reliability and availability with respect to perfect repair and imperfect repair are evaluated. The mutual information is researched in order to assess the important degree of basic events. The effects of degradation probability on the performances are studied. The results show that the perfect and imperfect repairs can improve the performances of series, parallel and subsea BOP systems significantly, whereas the imperfect repair cannot degrade the performances significantly in comparison with the perfect repair. To improve the performances of subsea BOP system, eight basic events, involving LWHCO, LLPR, LCC, LLICV, SLPSV, LRPIL, PIHF and SVLPLE should given more attention, and the degradation probability of basic events, especially the ones with high sensitive to system failure, should be reduced as much as possible. (C) 2013 Elsevier Ltd. All rights reserved.",""
"Background: Identification of true to breed type animal for conservation purpose is imperative. Breed dilution is one of the major problems in sustainability except cases of commercial crossbreeding under controlled condition. Breed descriptor has been developed to identify breed but such descriptors cover only \"pure breed\" or true to the breed type animals excluding undefined or admixture population. Moreover, in case of semen, ova, embryo and breed product, the breed cannot be identified due to lack of visible phenotypic descriptors. Advent of molecular markers like microsatellite and SNP have revolutionized breed identification from even small biological tissue or germplasm. Microsatellite DNA marker based breed assignments has been reported in various domestic animals. Such methods have limitations viz. non availability of allele data in public domain, thus each time all reference breed has to be genotyped which is neither logical nor economical. Even if such data is available but computational methods needs expertise of data analysis and interpretation. Results: We found Bayesian Networks as best classifier with highest accuracy of 98.7% using 51850 reference allele data generated by 25 microsatellite loci on 22 goat breed population of India. The F-ST values in the study were seen to be low ranging from 0.051 to 0.297 and overall genetic differentiation of 13.8%, suggesting more number of loci needed for higher accuracy. We report here world's first model webserver for breed identification using microsatellite DNA markers freely accessible at http://cabin.iasri.res.in/gomi/. Conclusion: Higher number of loci is required due to less differentiable population and large number of breeds taken in this study. This server will reduce the cost with computational ease. This methodology can be a model for various other domestic animal species as a valuable tool for conservation and breed improvement programmes.","Results: We found Bayesian Networks as best classifier with highest accuracy of 98."
"Gaze behaviour is known to indicate information gathering. It is therefore suggested that it could be used to derive information about the driver's next planned objective in order to identify intended manoeuvres without relying solely on car data. Ultimately this would be practically realised by an Advanced Driver Assistance System (ADAS) using gaze data to correctly infer the intentions of the driver from what is implied by the incoming gaze data available to it. Neural Networks' ability to approximate arbitrary functions from observed data therefore makes them a candidate for modelling driver intent. Previous work has shown that significantly distinct gaze patterns precede each of the driving manoeuvres analysed indicating that eye movement data might be used as input to ADAS supplementing sensors, such as CAN-Bus (Controller Area Network), laser, radar or LIDAR (Light Detection and Ranging) in order to recognise intended driving manoeuvres. In this study, drivers' gaze behaviour was measured prior to and during the execution of different driving manoeuvres performed in a dynamic driving simulator. Artificial Neural Networks (ANNs), Bayesian Networks (BNs), and Naive Bayes Classifiers (NBCs) were then trained using gaze data to act as classifiers that predict the occurrence of certain driving manoeuvres. This has previously been successfully demonstrated with real traffic data [1]. Issues considered here included the amount of data that is used for predictive purposes prior to the manoeuvre, the accuracy of the predictive models at different times prior to the manoeuvre taking place and the relative difficulty of predicting a lane change left manoeuvre against predicting a lane change right manoeuvre. (C) 2013 Elsevier B.V. All rights reserved.","Artificial Neural Networks (ANNs), Bayesian Networks (BNs), and Naive Bayes Classifiers (NBCs) were then trained using gaze data to act as classifiers that predict the occurrence of certain driving manoeuvres."
"Survival prediction and treatment selection in lung cancer care are characterised by high levels of uncertainty. Bayesian Networks (BNs), which naturally reason with uncertain domain knowledge, can be applied to aid lung cancer experts by providing personalised survival estimates and treatment selection recommendations. Based on the English Lung Cancer Database (LUCADA), we evaluate the feasibility of BNs for these two tasks, while comparing the performances of various causal discovery approaches to uncover the most feasible network structure from expert knowledge and data. We show first that the BN structure elicited from clinicians achieves a disappointing area under the ROC curve of 0.75 (+/- 0.03), whereas a structure learned by the CAMML hybrid causal discovery algorithm, which adheres with the temporal restrictions, achieves 0.81 (+/- 0.03). Second, our causal intervention results reveal that BN treatment recommendations, based on prescribing the treatment plan that maximises survival, can only predict the recorded treatment plan 29% of the time. However, this percentage rises to 76% when partial matches are included.",""
"Molecular entities work in concert as a system and mediate phenotypic outcomes and disease states. There has been recent interest in modelling the associations between molecular entities from their observed expression profiles as networks using a battery of algorithms. These networks have proven to be useful abstractions of the underlying pathways and signalling mechanisms. Noise is ubiquitous in molecular data and can have a pronounced effect on the inferred network. Noise can be an outcome of several factors including: inherent stochastic mechanisms at the molecular level, variation in the abundance of molecules, heterogeneity, sensitivity of the biological assay or measurement artefacts prevalent especially in high-throughput settings. The present study investigates the impact of discrepancies in noise variance on pair-wise dependencies, conditional dependencies and constraint-based Bayesian network structure learning algorithms that incorporate conditional independence tests as a part of the learning process. Popular network motifs and fundamental connections, namely: (a) common-effect, (b) three-chain, and (c) coherent type-I feed-forward loop (FFL) are investigated. The choice of these elementary networks can be attributed to their prevalence across more complex networks. Analytical expressions elucidating the impact of discrepancies in noise variance on pairwise dependencies and conditional dependencies for special cases of these motifs are presented. Subsequently, the impact of noise on two popular constraint-based Bayesian network structure learning algorithms such as Grow-Shrink (GS) and Incremental Association Markov Blanket (IAMB) that implicitly incorporate tests for conditional independence is investigated. Finally, the impact of noise on networks inferred from publicly available single cell molecular expression profiles is investigated. While discrepancies in noise variance are overlooked in routine molecular network inference, the results presented clearly elucidate their non-trivial impact on the conclusions that in turn can challenge the biological significance of the findings. The analytical treatment and arguments presented are generic and not restricted to molecular data sets.","While discrepancies in noise variance are overlooked in routine molecular network inference, the results presented clearly elucidate their non-trivial impact on the conclusions that in turn can challenge the biological significance of the findings."
"Although the Monte-Carlo Simulation (MCS) technique can evaluate a reliability of most structural systems, its processing time equals, approximately, the reciprocal of the probability of failure. While the Stochastic Finite Element (SFE) method could help to solve such a drawback, it is limited to specific computer programs, in which the mean and the coefficient of random variables are estimated by a perturbation, or by a weighted integral method. Therefore, SFE may not be easily applicable when using commercial software or systems that are not prepared with the prerequisite programming. To overcome these limitations, the RSM can be applied, because its accuracy depends on both the distance of axial points, and the linearity of the Limit State Functions (LSFs). The correlation among random variables and the response of a system is evaluated by composing a Bayesian belief nets (BBN). Consequently, the proposed Linear Adaptive Weighted Response Surface Method (LAW-RSM) with BBN modeling produces improved converged reliability indices than conventional RSMs and detail observation for the uncertainties in structural components.",""
"This paper presents an extension to a partially observable Markov decision process so that its solution can take into account, at the beginning of the planning, the possible availability of free information in future time periods. It is assumed that such information has a Bayesian network structure. The proposed approach requires a smaller computational effort than the classical approaches used to solve dynamic Bayesian networks. Furthermore, it allows the user to (1)take advantage of prior probability distributions of relevant random variables that do not necessarily have a direct causal relationship with the state of the system; and (2)rationally take into account the effects of accidental or rare events (such as seismic activities) that may occur during future time periods of the planning horizon. The methodology is illustrated through an example problem that concerns the optimization of inspection, maintenance, and rehabilitation strategies of road pavement over a 14-year planning horizon. (C) 2013 American Society of Civil Engineers.",""
"Naive Bayes is among the simplest probabilistic classifiers. It often performs surprisingly well in many real world applications, despite the strong assumption that all features are conditionally independent given the class. In the learning process of this classifier with the known structure, class probabilities and conditional probabilities are calculated using training data, and then values of these probabilities are used to classify new observations. In this paper, we introduce three novel optimization models for the naive Bayes classifier where both class probabilities and conditional probabilities are considered as variables. The values of these variables are found by solving the corresponding optimization problems. Numerical experiments are conducted on several real world binary classification data sets, where continuous features are discretized by applying three different methods. The performances of these models are compared with the naive Bayes classifier, tree augmented naive Bayes, the SVM, C4.5 and the nearest neighbor classifier. The obtained results demonstrate that the proposed models can significantly improve the performance of the naive Bayes classifier, yet at the same time maintain its simple structure.","Naive Bayes is among the simplest probabilistic classifiers."
"In this article, we explore the use of Bayesian networks for identifying the timbre of musical instruments. Peak spectral amplitude in ten frequency windows is extracted for each of 20 time windows to be used as features. Over a large data set of 24,000 audio examples covering the full musical range of 24 different common orchestral instruments, four different Bayesian network structures, including naive Bayes, are examined and compared with two support vector machines and a k-nearest neighbor classifier. Classification accuracy is examined by instrument, instrument family, and data set size. Bayesian networks with conditional dependencies in the time and frequency dimensions achieved 98 percent accuracy in the instrument classification task and 97 percent accuracy in the instrument family identification task. These results demonstrate a significant improvement over the previous approaches in the literature on this data set. Additionally, we tested our Bayesian approach on the widely used Iowa musical instrument data set, with similar results.","Over a large data set of 24,000 audio examples covering the full musical range of 24 different common orchestral instruments, four different Bayesian network structures, including naive Bayes, are examined and compared with two support vector machines and a k-nearest neighbor classifier."
"Many risks are involved in software development and risk management has become one of the key activities in software development. Bayesian networks (BNs) have been explored as a tool for various risk management practices, including the risk management of software development projects. However, much of the present research on software risk analysis focuses on finding the correlation between risk factors and project outcome. Software project failures are often a result of insufficient and ineffective risk management. To obtain proper and effective risk control, risk planning should be performed based on risk causality which can provide more risk information for decision making. In this study, we propose a model using BNs with causality constraints (BNCC) for risk analysis of software development projects. Through unrestricted automatic causality learning from 302 collected software project data, we demonstrated that the proposed model can not only discover causalities in accordance with the expert knowledge but also perform better in prediction than other algorithms, such as logistic regression, C4.5, Naive Bayes, and general BNs. This research presents the first causal discovery framework for risk causality analysis of software projects and develops a model using BNCC for application in software project risk management. (C) 2012 Elsevier B.V. All rights reserved.","Through unrestricted automatic causality learning from 302 collected software project data, we demonstrated that the proposed model can not only discover causalities in accordance with the expert knowledge but also perform better in prediction than other algorithms, such as logistic regression, C4."
"Several multimedia applications need to reason with concepts and their media properties in specific domain contexts. Media properties of concepts exhibit some unique characteristics that cannot be dealt with conceptual modeling schemes followed in the existing ontology representation and reasoning schemes. We have proposed a new perceptual modeling technique for reasoning with media properties observed in multimedia instances and the latent concepts. Our knowledge representation scheme uses a causal model of the world where concepts manifest in media properties with uncertainties. We introduce a probabilistic reasoning scheme for belief propagation across domain concepts through observation of media properties. In order to support the perceptual modeling and reasoning paradigm, we propose a new ontology language, Multimedia Web Ontology Language (MOWL). Our primary contribution in this article is to establish the need for the new ontology language and to introduce the semantics of its novel language constructs. We establish the generality of our approach with two disperate knowledge-intensive applications involving reasoning with media properties of concepts. Categories and Subject Descriptors: F.4.1 [Mathematical Logic and Formal Languages]: Mathematical Logic; H.2.4 [Database Management]: Systems-Multimedia databases; H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems; I.2.4 [Artificial Intelligence]: Knowledge Representation Formalisms and Methods-Representation languages",""
"Use of parametric classification of beach morphodynamic state has been steadily increasing in coastal research, despite identification of several shortcomings of their representativeness as elementary beach descriptors. In this paper, we analyse the parametric classification of beach morphodynamic state in a set of six embayed beaches in southwestern Portugal, exposed to diverse settings, from high-energy (dissipative to intermediate) to low-energy (intermediate to reflective) conditions. Applicability of parametric approaches considered within the dimensionless space defined by Omega (dimensionless fall velocity) and RTR (relative tidal range) according to averaged wave, tide and sediment characteristics of beaches, was assessed in association with a probabilistic analysis, implemented through a Bayesian network model, that considered the full range of wave, tide and sediment conditions at each site. Both parametric approaches were compared to field-based beach state classification implemented using a novel hierarchical framework for beach state analysis. The classification obtained within a probabilistic framework provided an innovative approach for analysis of beach state and mobility, extending the insights on beach morphodynamic behaviour obtained from averaged environmental parameters. Reflective to lower-intermediate beach conditions showed better agreement with parametric approaches, while poor beach state differentiation was achieved for intermediate beach types. Limitations in the application of beach classification models result from (i) an inadequacy of existing beach state types in differentiating beaches, particularly within the intermediate domain and for geologically controlled embayed beaches, and (ii) shortcomings involved in the parametric approaches. The beach state models disregard the major role of geological control in embayed beach morphodynamic behaviour (in modulating beach shape and segmentation, influencing sediment size and availability, causing alongshore variations in the wave field and creating topographically induced nearshore circulations). Existing beach state models are unable to adequately represent the widely variable conditions observed in embayed beaches. (C) 2013 Elsevier B.V. All rights reserved.","Use of parametric classification of beach morphodynamic state has been steadily increasing in coastal research, despite identification of several shortcomings of their representativeness as elementary beach descriptors."
"Reconstructing gene regulatory networks from high-throughput measurements represents a key problem in functional genomics. It also represents a canonical learning problem and thus has attracted a lot of attention in both the informatics and the statistical learning literature. Numerous approaches have been proposed, ranging from simple clustering to rather involved dynamic Bayesian network modeling, as well as hybrid ones that combine a number of modeling steps, such as employing ordinary differential equations coupled with genome annotation. These approaches are tailored to the type of data being employed. Available data sources include static steady state data and time course data obtained either for wild type phenotypes or from perturbation experiments. This review focuses on the class of autoregressive models using time course data for inferring gene regulatory networks. The central themes of sparsity, stability and causality are discussed as well as the ability to integrate prior knowledge for successful use of these models for the learning task at. hand. (C) 2013 Elsevier Inc. All rights reserved.",""
"Understanding the control of cellular networks consisting of gene and protein interactions and their emergent properties is a central activity of Systems Biology research. For this, continuous, discrete, hybrid, and stochastic methods have been proposed. Currently, the most common approach to modelling accurate temporal dynamics of networks is ordinary differential equations (ODE). However, critical limitations of ODE models are difficulty in kinetic parameter estimation and numerical solution of a large number of equations, making them more suited to smaller systems. In this article, we introduce a novel recurrent artificial neural network (RNN) that addresses above limitations and produces a continuous model that easily estimates parameters from data, can handle a large number of molecular interactions and quantifies temporal dynamics and emergent systems properties. This RNN is based on a system of ODEs representing molecular interactions in a signalling network. Each neuron represents concentration change of one molecule represented by an ODE. Weights of the RNN correspond to kinetic parameters in the system and can be adjusted incrementally during network training. The method is applied to the p53-Mdm2 oscillation system a crucial component of the DNA damage response pathways activated by a damage signal. Simulation results indicate that the proposed RNN can successfully represent the behaviour of the p53-Mdm2 oscillation system and solve the parameter estimation problem with high accuracy. Furthermore, we presented a modified form of the RNN that estimates parameters and captures systems dynamics from sparse data collected over relatively large time steps. We also investigate the robustness of the p53-Mdm2 system using the trained RNN under various levels of parameter perturbation to gain a greater understanding of the control of the p53-Mdm2 system. Its outcomes on robustness are consistent with the current biological knowledge of this system. As more quantitative data become available on individual proteins, the RNN would be able to refine parameter estimation and mapping of temporal dynamics of individual signalling molecules as well as signalling networks as a system. Moreover, RNN can be used to modularise large signalling networks. (C) 2013 Elsevier Ireland Ltd. All rights reserved.",""
"What kind of information do people use to make predictions? Causal Bayes nets theory implies that people should follow structural constraints like the Markov property in the form of the screening-off rule, but previous work shows little evidence that people do. We tested six hypotheses that attempt to explain violations of screening off, some by asserting that people use mechanistic knowledge to infer additional latent structure. In three experiments, we manipulated whether the causal relations among variables within a causal structure were supported by the same or different mechanisms. The experiments differed in the type of causal structures (common cause vs. chain), the way that causal structures were presented (verbal description vs. observational learning), how the mechanisms were presented (explicit description vs. implicit description vs. visual hint), and the number of predictions requested (2 vs. 24). The results revealed that the screening-off rule was violated more often when the mechanisms were the same than when they were different. The findings suggest that people use knowledge about underlying mechanisms to infer latent structure for prediction. (C) 2013 Elsevier Inc. All rights reserved.",""
"Reservoir simulation models are used both in the development of new fields and in developed fields where production forecasts are needed for investment decisions. When simulating a reservoir, one must account for the physical and chemical processes taking place in the subsurface. Rock and fluid properties are crucial when describing the flow in porous media. In this paper, the authors are concerned with estimating the permeability field of a reservoir. The problem of estimating model parameters such as permeability is often referred to as a history-matching problem in reservoir engineering. Currently, one of the most widely used methodologies which address the history-matching problem is the ensemble Kalman filter (EnKF). EnKF is a Monte Carlo implementation of the Bayesian update problem. Nevertheless, the EnKF methodology has certain limitations that encourage the search for an alternative method.For this reason, a new approach based on graphical models is proposed and studied. In particular, the graphical model chosen for this purpose is a dynamic non-parametric Bayesian network (NPBN). This is the first attempt to approach a history-matching problem in reservoir simulation using a NPBN-based method. A two-phase, two-dimensional flow model was implemented for a synthetic reservoir simulation exercise, and initial results are shown. The methods' performances are evaluated and compared. This paper features a completely novel approach to history matching and constitutes only the first part (part I) of a more detailed investigation. For these reasons (novelty and incompleteness), many questions are left open and a number of recommendations are formulated, to be investigated in part II of the same paper.",""
"Rationale: Unsupervised statistical learning techniques, such as exploratory factor analysis (EFA) and hierarchical clustering (HC), have been used to identify asthma phenotypes, with partly consistent results. Some of the inconsistency is caused by the variable selection and demographic and clinical differences among study populations. Objectives: To investigate the effects of the choice of statistical method and different preparations of data on the clustering results; and to relate these to disease severity. Methods: Several variants of EFA and HC were applied and compared using various sets of variables and different encodings and transformations within a dataset of 383 children with asthma. Variables included lung function, inflammatory and allergy markers, family history, environmental exposures, and medications. Clusters and original variables were related to asthma severity (logistic regression and Bayesian network analysis). Measurements and Main Results: EFA identified five components (eigenvalues >= 1) explaining 35% of the overall variance. Variations of the HC (as linkage-distance functions) did not affect the cluster inference; however, using different variable encodings and transformations did. The derived clusters predicted asthma severity less than the original variables. Prognostic factors of severity were medication usage, current symptoms, lung function, paternal asthma, body mass index, and age of asthma onset. Bayesian networks indicated conditional dependence among variables. Conclusions: The use of different unsupervised statistical learning methods and different variable sets and encodings can lead to multiple and inconsistent subgroupings of asthma, not necessarily correlated with severity. The search for asthma phenotypes needs more careful selection of markers, consistent across different study populations, and more cautious interpretation of results from unsupervised learning.","Clusters and original variables were related to asthma severity (logistic regression and Bayesian network analysis)."
"Following the Integrated Water Resources Management approach, the European Water Framework Directive demands Member States to develop water management plans at the catchment level. Those plans have to integrate the different interests and must be developed with stakeholder participation. To face these requirements, managers need tools to assess the impacts of possible management alternatives on natural and socio-economic systems. These tools should ideally be able to address the complexity and uncertainties of the water system, while serving as a platform for stakeholder participation. The objective of our research was to develop a participatory integrated assessment model, based on the combination of a crop model, an economic model and a participatory Bayesian network, with an application in the middle Guadiana sub-basin, in Spain. The methodology is intended to capture the complexity of water management problems, incorporating the relevant sectors, as well as the relevant scales involved in water management decision making. The integrated model has allowed us testing different management, market and climate change scenarios and assessing the impacts of such scenarios on the natural system (crops), on the socio-economic system (farms) and on the environment (water resources). Finally, this integrated assessment modelling process has allowed stakeholder participation, complying with the main requirements of current European water laws. (C) 2013 Elsevier Ltd. All rights reserved.",""
"Despite growing interest over the decades, the question of estimating cognitive workload of operators involved in complex multitask operations, such as helicopter pilots, remains a key issue. One of the main difficulties facing workload inference models is that no single specific indicator of workload exists, so that multiple sources of information have to be inputted to the model. The question then arises as to the nature and the quantity of features to be used for increasing model performance. In this research, done in cooperation with Eurocopter, the effectiveness of physiological, psychological, and cognitive features for estimating helicopter pilots' workload was systematically investigated, using Bayesian networks (BNs). The study took place in two different contexts: a constrained laboratory situation with low ecological validity and a more realistic and challenging situation relying on virtual reality. The constrained conditions of the laboratory study allowed us for testing various combinations of entropy-based physiological, cognitive, and affect features as inputs of BN models. These three different kinds of features are shown to carry complementary information that can be used with advantage by the model. The results also suggest that increasing the number of physiological inputs improves the model performance. The second study aimed at challenging some of these conclusions in a more ecological context, by using the NH90 full-flight simulator of the Helisim company. The results emphasize the problem of accessing the ground truth, as well as the need for an efficient feature selection or extraction step prior to the classification step.","One of the main difficulties facing workload inference models is that no single specific indicator of workload exists, so that multiple sources of information have to be inputted to the model."
"Probability trees are a powerful data structure for representing probabilistic potentials. However, their complexity can become intractable if they represent a probability distribution over a large set of variables. In this paper, we study the problem of decomposing a probability tree as a product of smaller trees, with the aim of being able to handle bigger probabilistic potentials. We propose exact and approximate approaches and evaluate their behaviour through an extensive set of experiments. (C) 2013 Elsevier Inc. All rights reserved.",""
"Bayesian Networks are increasingly being used to model complex socio-economic systems by expert knowledge elicitation even when data is scarce or does not exist. In this paper, a Multi-Objective Evolutionary Algorithm (MOEA) is presented for assessing the parameters (input relevance/weights) of fuzzy dependence relationships in a Bayesian Network (BN). The MOEA was designed to include a hybrid model that combines Monte-Carlo simulation and fuzzy inference. The MOEA-based prototype assesses the input weights of fuzzy dependence relationships by learning from available output data. In socio-economic systems, the determination of how a specific input variable affects the expected results can be critical and it is still one of the most important challenges in Bayesian modeling. The MOEA was checked by estimating the migrant stock as a relevant variable in a BN model for forecasting remittances. For a specific year, results showed similar input weights than those given by economists but it is very computationally demanding. The proposed hybrid-approach is an efficient procedure to estimate output values in BN. (C) 2013 Elsevier Ltd. All rights reserved.","The MOEA was designed to include a hybrid model that combines Monte-Carlo simulation and fuzzy inference."
"Objectives: Although the course of single diseases can be studied using traditional epidemiologic techniques, these methods cannot capture the complex joint evolutionary course of multiple disorders. In this study, multilevel temporal Bayesian networks were adopted to study the course of multimorbidity in the expectation that this would yield new clinical insight. Study Design and Setting: Clinical data of patients were extracted from 90 general practice registries in the Netherlands. One and half million patient-years were used for analysis. The simultaneous progression of six chronic cardiovascular conditions was investigated, correcting for both patient and practice-related variables. Results: Cumulative incidence rates of one or more new morbidities rapidly increase with the number of morbidities present at baseline, ranging up to 47% and 76% for 3- and 5-year follow-ups, respectively. Hypertension and lipid disorders, as health risk factors, increase the cumulative incidence rates of both individual and multiple disorders. Moreover, in their presence, the observed cumulative incidence rates of combinations of cardiovascular disorders, that is, multimorbidity differs significantly from the expected rates. Conclusion: There are clear synergies between health risks and chronic diseases when multimorbidity within a patient progresses over time. The method used here supports a more comprehensive analysis of such synergies compared with what can be obtained by traditional statistics. (C) 2013 Elsevier Inc. All rights reserved.",""
"Bayesian networks are graphical models that represent the joint distribution of a set of variables using directed acyclic graphs. The graph can be manually built by domain experts according to their knowledge. However, when the dependence structure is unknown (or partially known) the network has to be estimated from data by using suitable learning algorithms. In this paper, we deal with a constraint-based method to perform Bayesian networks structural learning in the presence of ordinal variables. We propose an alternative version of the PC algorithm, which is one of the most known procedures, with the aim to infer the network by accounting for additional information inherent to ordinal data. The proposal is based on a nonparametric test, appropriate for ordinal variables. A comparative study shows that, in some situations, the proposal discussed here is a slightly more efficient solution than the PC algorithm.",""
"For developing countries, such as Chile, we study the influential factors for adoption and usage of broadband services. In particular, subsidies on the broadband price are analyzed to see if this initiative has a significant effect in the broadband penetration. To carry out this study, machine learning techniques are used to identify different household profiles using the data obtained from a survey on access, use, and users of broadband Internet from Chile. Different policies are proposed for each group found, which were then evaluated empirically through Bayesian networks. Results show that an unconditional subsidy for the Internet price does not seem to be very appropriate for everyone since it is only significant for some households groups. The evaluation using Bayesian networks showed that other polices should be considered as well such as the incorporation of computers, Internet applications development, and digital literacy training. (C) 2013 Elsevier Ltd. All rights reserved.",""
"Emergency Department (ED) triage is a process of determining illness severity and accordingly assigning patient priority. The Emergency Severity Index (ESI) is a 5-level acuity categorization system that aides in triage. This paper compared the capabilities of predicting ESI level using ordinal logistic regression (OLR), artificial neural networks (NNs), and naive Bayesian networks (NBNs). Data were obtained from Susquehanna Williamsport Hospital for 947 patients over a one month period in 2008. It contained the assigned ESI level, chief complaint, systolic blood pressure, pulse, respiration rate, temperature, oxygen saturation level (SaO(2)), age, gender, and pain level. An OLR model was fit using a subset of these covariates. NBNs and NNs were modeled to relax the inherent assumptions of linearity and covariate independence in logistic regression. These three techniques were compared using incremental training dataset sizes between 50% and 100% of given data. All models were >60% accurate using the entire dataset for training. It was found that NBNs and NNs were robust to data size changes and all models had evaluation speeds of less than 0.5 s. At this time the use of NBNs is recommended considering speed, accuracy, data utilization, model flexibility, and interpretability of the model. (C) 2013 Elsevier Ltd. All rights reserved.","This paper compared the capabilities of predicting ESI level using ordinal logistic regression (OLR), artificial neural networks (NNs), and naive Bayesian networks (NBNs)."
"In this paper, we investigate the impact of product, company context and regulatory environment factors for their potential impact on medical device development (MDD). The presented work investigates the impact of these factors on the Food and Drug Administration's (FDA) decision time for submissions that request clearance, or approval to launch a medical device in the market. Our overall goal is to identify critical factors using historical data and rigorous techniques so that an expert system can be built to guide product developers to improve the efficiency of the MDD process, and thereby reduce associated costs. We employ a Bayesian network (BN) approach, a well-known machine learning method, to examine what the critical factors in the MDD context are. This analysis is performed using the data from 2400 FDA approved orthopedic devices that represent products from 474 different companies. Presented inferences are to be used as the backbone of an expert system specific to MDD. (C) 2013 Elsevier Ltd. All rights reserved.","Presented inferences are to be used as the backbone of an expert system specific to MDD."
"It is a nontrivial task to build an accurate emerging pattern (EP) classifier from high-dimensional data because we inevitably face two challenges 1) how to efficiently extract a minimal set of strongly predictive EPs from an explosive number of candidate patterns, and 2) how to handle the highly sensitive choice of the minimal support threshold. To address these two challenges, we bridge causal relevance and EP discriminability (the predictive ability of emerging patterns) to facilitate EP mining and propose a new framework of mining EPs from high-dimensional data. In this framework, we study the relationships between causal relevance in a causal Bayesian network and EP discriminability in EP mining, and then reduce the pattern space of EP mining to direct causes and direct effects, or the Markov blanket (MB) of the class attribute in a causal Bayesian network. The proposed framework is instantiated by two EPs-based classifiers, CE-EP and MB-EP, where CE stands for direct Causes and direct Effects, and MB for Markov Blanket. Extensive experiments on a broad range of data sets validate the effectiveness of the CE-EP and MB-EP classifiers against other well-established methods, in terms of predictive accuracy, pattern numbers, running time, and sensitivity analysis.","It is a nontrivial task to build an accurate emerging pattern (EP) classifier from high-dimensional data because we inevitably face two challenges 1) how to efficiently extract a minimal set of strongly predictive EPs from an explosive number of candidate patterns, and 2) how to handle the highly sensitive choice of the minimal support threshold."
"Peer-to-peer networks (P2Ps) use reputation systems to provide incentives for nodes to offer high quality of service (QoS) and thwart the intentions of dishonest or selfish nodes. Existing reputation systems have two problems. First, they directly regard node reputation as trust. Rather, reputation represents the opinions formed by others about a node's QoS behavior, while trust represents a node's honesty and willingness to cooperate. In addition to trust, factors such as node capacity and lifetime also influence reputation. Due to these factors' heterogeneity and variance over time, reputation cannot directly reflect a node's trust or current QoS. Second, existing reputation systems guide a node to select the server with the highest reputation, which may not actually select the highest QoS server and would overload the highest reputed nodes. This work aims to accurately reflect node trust and provide guidance for high-QoS server selection. Through experimental study, we find that node trust, available capacity, and lifetime positively affect node reputation. Based on this observation, we first propose a manual trust model and an automatic trust model that remove the influence of additional factors on reputation to truly reflect node trust. We then propose a high-QoS server selection algorithm that separately considers node trust, current available capacity, and lifetime. Extensive simulation results demonstrate the effectiveness of the trust models in accurate node trust reflection compared with an existing reputation system. Moreover, the server selection algorithm dramatically increases the success rate of service requests and avoids overloading nodes.",""
"This study aimed to focus on medical knowledge representation and reasoning using the probabilistic and fuzzy influence processes, implemented in the semantic web, for decision support tasks. Bayesian belief networks (BBNs) and fuzzy cognitive maps (FCMs), as dynamic influence graphs, were applied to handle the task of medical knowledge formalization for decision support. In order to perform reasoning on these knowledge models, a general purpose reasoning engine, EYE, with the necessary plug-ins was developed in the semantic web. The two formal approaches constitute the proposed decision support system (DSS) aiming to recognize the appropriate guidelines of a medical problem, and to propose easily understandable course of actions to guide the practitioners. The urinary tract infection (UTI) problem was selected as the proof-of-concept example to examine the proposed formalization techniques implemented in the semantic web. The medical guidelines for UTI treatment were formalized into BBN and FCM knowledge models. To assess the formal models' performance, SS patient cases were extracted from a database and analyzed. The results showed that the suggested approaches formalized medical knowledge efficiently in the semantic web, and gave a front-end decision on antibiotics' suggestion for UTI. (C) 2013 Elsevier Ireland Ltd. All rights reserved.",""
"Feature fatigue (FF) is used to represent the phenomenon of customer's inconsistent satisfaction with products: customers prefer to choose products with more features and capabilities initially, but after having worked with a product, they become frustrated or dissatisfied with the usability problems caused by too many features. To \"defeat\" FF, it is essential for designers to decide what features should be added when developing a product to make the product attractive enough and not too hard to use at the same time. In this paper, a feature fatigue multi-objective genetic algorithm (FFMOGA) method is reported for solving the feature addition problem. In the proposed method, fitness functions are established based on Bayesian networks, which can represent the uncertain customer preferences and reflect the relationships among features. The computational experiments on a smart phone case show that the FFMOGA approach can find multiple solutions along the Pareto-optimal frontier for designers to select from, and these obtained solutions have good performance in convergence.",""
"Autonomous science augments the capabilities of planetary rovers by shifting the identification and selection of science targets from remote operators to the rover itself. This shift frees the rover from wasteful idle time and allows for more selective data collection. This paper presents an approach to autonomous science that is comprised of three components: a Bayesian network that uses image data to identify features; an evaluation algorithm that selects the best features; and, a path-planning algorithm that guides the rover to the most scientifically valuable features. Within this framework, the effectiveness of pairing a larger prime rover with a smaller scout rover to improve autonomous science is investigated. Laboratory-based experiments were used to validate the effectiveness of the Bayesian network for feature identification and the scoring algorithm that has been developed for feature evaluation. Simulations were used to compare the traditional use of a solo prime rover to that of also employing a scout. The results presented here indicate that the inclusion of a scout rover can allow the prime rover to avoid pitfalls or routes with low scientific value.",""
"The need for integration of model-based verification into industrial processes has produced several attempts to define Model-Driven solutions implementing a unifying approach to system development. A recent trend is to implement tool chains supporting the developer both in the design phase and V&V activities. In this Model-Driven context, specific domains require proper modelling approaches, especially for what concerns RAM (Reliability, Availability, Maintainability) analysis and fulfillment of international standards. This paper specifically addresses the definition of a Model-Driven approach for the evaluation of RAM attributes in railway applications to automatically generate formal models. For this aim we extend the MARTE-DAM UML profile with concepts related to maintenance aspects and service degradation, and show that the MARTE-DAM framework can be successfully specialized for the railway domain. Model transformations are then defined to generate Repairable Fault Tree and Bayesian Network models from MARTE-DAM specifications. The whole process is applied to the railway domain in two different availability studies. (C) 2013 Elsevier Ltd. All rights reserved.",""
"Existing models estimating oil spill costs at sea are based on data from the past, and they usually lack a systematic approach. This make them passive, and limits their ability to forecast the effect of the changes in the oil combating fleet or location of a spill on the oil spill costs. In this paper we make an attempt towards the development of a probabilistic and systematic model estimating the costs of clean-up operations for the Gulf of Finland. For this purpose we utilize expert knowledge along with the available data and information from literature. Then, the obtained information is combined into a framework with the use of a Bayesian Belief Networks. Due to lack of data, we validate the model by comparing its results with existing models, with which we found good agreement. We anticipate that the presented model can contribute to the cost-effective oil-combating fleet optimization for the Gulf of Finland. It can also facilitate the accident consequences estimation in the framework of formal safety assessment (FSA). (C) 2013 Elsevier Ltd. All rights reserved.",""
"Background: The problem of efficient utilization of genome-wide expression profiles for identification and prediction of complex disease conditions is both important and challenging. Polygenic pathologies such as most types of cancer involve disregulation of many interacting genes which has prompted search for suitable statistical models for their representation. By accounting for changes in gene regulations between comparable conditions, graphical statistical models are expected to improve prediction precision. Methods: In comparison problems with two or more experimental conditions, we represent the classes by categorical Bayesian networks that share one and the same graph structure but have class-specific probability parameters. The graph structure is learned by a score-based procedure that maximizes the difference between class probabilities using a suitable measure of divergence. The proposed framework includes an indirect model selection by adhering to a principle of optimal class separation and identifies interactions presenting significant difference between the compared conditions. Results: We evaluate the performance of the new model against some benchmark algorithms such as support vector machine, penalized linear regression and linear Gaussian networks. The classifiers are compared by prediction accuracy across 15 different data sets from breast, lung, gastric and renal cancer studies. In addition to the demonstrated strong performance against the competitors, the proposed method is able to identify disease specific changes in gene regulations which are inaccessible by other approaches. The latter is illustrated by analyzing some gene interactions differentiating adenocarcinoma and squamous cell lung cancers.","Results: We evaluate the performance of the new model against some benchmark algorithms such as support vector machine, penalized linear regression and linear Gaussian networks."
"We present an automatic classification method for astronomical catalogs with missing data. We use Bayesian networks and a probabilistic graphical model that allows us to perform inference to predict missing values given observed data and dependency relationships between variables. To learn a Bayesian network from incomplete data, we use an iterative algorithm that utilizes sampling methods and expectation maximization to estimate the distributions and probabilistic dependencies of variables from data with missing values. To test our model, we use three catalogs with missing data (SAGE, Two Micron All Sky Survey, and UBVI) and one complete catalog (MACHO). We examine how classification accuracy changes when information from missing data catalogs is included, how our method compares to traditional missing data approaches, and at what computational cost. Integrating these catalogs with missing data, we find that classification of variable objects improves by a few percent and by 15% for quasar detection while keeping the computational cost the same.","We present an automatic classification method for astronomical catalogs with missing data."
"Fuzzy Cognitive Mapping (FCM) is a widely used participatory modelling methodology in which stakeholders collaboratively develop a 'cognitive map' (a weighted, directed graph), representing the perceived causal structure of their system. This can be directly transformed by a workshop facilitator into simple mathematical models to be interrogated by participants by the end of the session. Such simple models provide thinking tools which can be used for discussion and exploration of complex issues, as well as sense checking the implications of suggested causal links. They increase stakeholder motivation and understanding of whole systems approaches, but cannot be separated from an intersubjective participatory context. Standard FCM methodologies make simplifying assumptions, which may strongly influence results, presenting particular challenges and opportunities. We report on a participatory process, involving local companies and organisations, focussing on the development of a bio-based economy in the Humber region. The initial cognitive map generated consisted of factors considered key for the development of the regional bio-based economy and their directional, weighted, causal interconnections. A verification and scenario generation procedure, to check the structure of the map and suggest modifications, was carried out with a second session. Participants agreed on updates to the original map and described two alternate potential causal structures. In a novel analysis all map structures were tested using two standard methodologies usually used independently: linear and sigmoidal FCMs, demonstrating some significantly different results alongside some broad similarities. We suggest a development of FCM methodology involving a sensitivity analysis with different mappings and discuss the use of this technique in the context of our case study. Using the results and analysis of our process, we discuss the limitations and benefits of the FCM methodology in this case and in general. We conclude by proposing an extended FCM methodology, including multiple functional mappings within one participant-constructed graph.",""
"Background: Complex diseases are often difficult to diagnose, treat and study due to the multi-factorial nature of the underlying etiology. Large data sets are now widely available that can be used to define novel, mechanistically distinct disease subtypes (endotypes) in a completely data-driven manner. However, significant challenges exist with regard to how to segregate individuals into suitable subtypes of the disease and understand the distinct biological mechanisms of each when the goal is to maximize the discovery potential of these data sets. Results: A multi-step decision tree-based method is described for defining endotypes based on gene expression, clinical covariates, and disease indicators using childhood asthma as a case study. We attempted to use alternative approaches such as the Student's t-test, single data domain clustering and the Modk-prototypes algorithm, which incorporates multiple data domains into a single analysis and none performed as well as the novel multi-step decision tree method. This new method gave the best segregation of asthmatics and non-asthmatics, and it provides easy access to all genes and clinical covariates that distinguish the groups. Conclusions: The multi-step decision tree method described here will lead to better understanding of complex disease in general by allowing purely data-driven disease endotypes to facilitate the discovery of new mechanisms underlying these diseases. This application should be considered a complement to ongoing efforts to better define and diagnose known endotypes. When coupled with existing methods developed to determine the genetics of gene expression, these methods provide a mechanism for linking genetics and exposomics data and thereby accounting for both major determinants of disease.",""
"Markov jump processes (or continuous-time Markov chains) are a simple and important class of continuous-time dynamical systems. In this paper, we tackle the problem of simulating from the posterior distribution over paths in these models, given partial and noisy observations. Our approach is an auxiliary variable Gibbs sampler, and is based on the idea of uniformization. This sets up a Markov chain over paths by alternately sampling a finite set of virtual jump times given the current path, and then sampling a new path given the set of extant and virtual jump times. The first step involves simulating a piecewise-constant inhomogeneous Poisson process, while for the second, we use a standard hidden Markov model forward filtering-backward sampling algorithm. Our method is exact and does not involve approximations like time-discretization. We demonstrate how our sampler extends naturally to MJP-based models like Markov-modulated Poisson processes and continuous-time Bayesian networks, and show significant computational benefits over state-of-the-art MCMC samplers for these models.",""
"Models of the human driving behavior are essential for the rapid prototyping of error-compensating assistance systems. Various authors proposed control-theoretic and production-system models. Here we present machine-learning alternatives to train assistance systems and estimate probabilistic driver models from human behavior traces. We present a partially autonomous driver assistance system based on Markov Decision Processes. Its assistance strategies are trained from human behavior traces using the Least Square Policy Iteration algorithm. The resulting system is able to reduce the number of collisions encountered when following a lead-vehicle. Furthermore, we present a Bayesian Autonomous Driver Mixture-of-Behaviors model for the longitudinal control of human drivers based on the modular and hierarchical composition of Dynamic Bayesian Networks. Their parameters and structures are estimated from human behavior traces using a discriminative scoring criterion based on the Bayesian Information Criterion. This allows the selection of pertinent percepts from the variety of percepts proposed for driver models according to their statistical relevance. The resulting driver model is able to reproduce the longitudinal control behavior of human drivers while driving unassisted or assisted by the presented assistance system. (C) 2013 Elsevier Ltd. All rights reserved.",""
"This paper describes the basis functioning and implementation of a computer-aided Bayesian Network (BN) method that is able to incorporate experts' knowledge for the benefit of remote sensing applications and other raster data analyses: Bayesian Network for Raster Data (BayNeRD). Using a case study of soybean mapping in Mato Grosso State, Brazil, BayNeRD was tested to evaluate its capability to support the understanding of a complex phenomenon through plausible reasoning based on data observation. Observations made upon Crop Enhanced Index (CEI) values for the current and previous crop years, soil type, terrain slope, and distance to the nearest road and water body were used to calculate the probability of soybean presence for the entire Mato Grosso State, showing strong adherence to the official data. CEI values were the most influencial variables in the calculated probability of soybean presence, stating the potential of remote sensing as a source of data. Moreover, the overall accuracy of over 91% confirmed the high accuracy of the thematic map derived from the calculated probability values. BayNeRD allows the expert to model the relationship among several observed variables, outputs variable importance information, handles incomplete and disparate forms of data, and offers a basis for plausible reasoning from observations. The BayNeRD algorithm has been implemented in R software and can be found on the internet.",""
"Background: Clinical knowledge about progress of diseases is characterised by temporal information as well as uncertainty. However, precise timing information is often unavailable in medicine. In previous research this problem has been tackled using Allen's qualitative algebra of time, which, despite successful medical application, does not deal with the associated uncertainty. Objectives: It is investigated whether and how Allen's temporal algebra can be extended to handle uncertainty to better fit available knowledge and data of disease processes. Methods: To bridge the gap between probability theory and qualitative time reasoning, methods from probabilistic logic are explored. The relation between the probabilistic logic representation and dynamic Bayesian networks is analysed. By studying a typical, and clinically relevant problem, the detection of exacerbations of chronic obstructive pulmonary disease (COPD), it is determined whether the developed probabilistic logic of qualitative time is medically useful. Results: The probabilistic logic extension of Allen's temporal algebra, called Qualitative Time CP-logic provides a tool to model disease processes at a natural level of abstraction and is sufficiently powerful to reason with imprecise, uncertain knowledge. The representation of the COPD disease process gives evidence that the framework can be applied functionally to a clinical problem. Conclusion: The combination of qualitative time and probabilistic logic offers a useful framework for modelling knowledge and data to describe disease processes in clinical medicine. (C) 2013 Elsevier B.V. All rights reserved.",""
"Due to the features of low energy consumption and flexible networking, nowadays the pyroelectric sensor has been applied widely in areas such as network instruction detection or human body target tracking recognition. Moreover, accurate estimation and judgment about the number of human targets moving in the networks is the foundation of tracking and recognition. This paper, under the condition of being lack of relevant prior knowledge, presents a novel method which selects the maximum likelihood function of the Bayesian network models as the independent criterion. In addition, the objective function is optimally solved by the Laplace estimation. The results of numerous experiments on both simulation and hardware experimental platforms are shown that this method has capability to blindly estimate the number of motion multiple human targets in wireless pyroelectric infrared sensor networks. (C) 2013 Elsevier B.V. All rights reserved.",""
"We introduce here the concept of Bayesian networks, in compound Poisson model, which provides a graphical modeling framework that encodes the joint probability distribution for a set of random variables within a directed acyclic graph. We suggest an approach proposal which offers a new mixed implicit estimator. We show that the implicit approach applied in compound Poisson model is very attractive for its ability to understand data and does not require any prior information. A comparative study between learned estimates given by implicit and by standard Bayesian approaches is established. Under some conditions and based on minimal squared error calculations, we show that the mixed implicit estimator is better than the standard Bayesian and the maximum likelihood estimators. We illustrate our approach by considering a simulation study in the context of mobile communication networks.",""
"We present a Bayesian network model for predicting the outcome of in vitro fertilization (IVF). The problem is characterized by a particular missingness process; we propose a simple but effective averaging approach which improves parameter estimates compared to the traditional MAP estimation. We present results with generated data and the analysis of a real data set. Moreover, we assess by means of a simulation study the effectiveness of the model in supporting the selection of the embryos to be transferred. (C) 2013 Elsevier Ltd. All rights reserved.",""
"We introduce in this paper the concept of vehicle indices in a cycle at a signalized intersection which are the positions of vehicles in the departure process of the cycle. We show that vehicle indices are closely related to the vehicle arrival and the departure processes at the intersection. Based on vehicle indices and sample travel times collected from mobile sensors, a three-layer Bayesian Network model is constructed to describe the stochastic intersection traffic flow by capturing the relationship of vehicle indices, and the arrival and departure processes. The non-homogeneous Poisson process and log-normal distributions are used respectively to model the stochastic arrival and departure processes. The methods of parameter learning and vehicle index inference are presented based on the observed intersection travel times. Simplification to the methods is discussed to reduce the computational effort of parameter learning and vehicle index estimation. The model is tested using data from NGSIM, a field test, and simulation with reasonable results. (C) 2013 Elsevier Ltd. All rights reserved.","The methods of parameter learning and vehicle index inference are presented based on the observed intersection travel times."
"Background: Reverse-engineering gene regulatory networks from expression data is difficult, especially without temporal measurements or interventional experiments. In particular, the causal direction of an edge is generally not statistically identifiable, i.e., cannot be inferred as a statistical parameter, even from an unlimited amount of non-time series observational mRNA expression data. Some additional evidence is required and high-throughput methylation data can viewed as a natural multifactorial gene perturbation experiment. Results: We introduce IDEM (Identifying Direction from Expression and Methylation), a method for identifying the causal direction of edges by combining DNA methylation and mRNA transcription data. We describe the circumstances under which edge directions become identifiable and experiments with both real and synthetic data demonstrate that the accuracy of IDEM for inferring both edge placement and edge direction in gene regulatory networks is significantly improved relative to other methods. Conclusion: Reverse-engineering directed gene regulatory networks from static observational data becomes feasible by exploiting the context provided by high-throughput DNA methylation data. An implementation of the algorithm described is available at http://code.google.com/p/idem/.",""
"Vehicular ad hoc networks (VANETs) have emerged as an application of mobile ad hoc networks (MANETs), which use dedicated short-range communication (DSRC) to allow vehicles in close proximity to communicate with each other or to communicate with roadside equipment. Applying wireless access technology in vehicular environments has led to the improvement of road safety and a reduction in the number of fatalities caused by road accidents through development of road safety applications and facilitation of information sharing between moving vehicles regarding the road. This paper focuses on developing a novel and nonintrusive driver behavior detection system using a context-aware system in VANETs to detect abnormal behaviors exhibited by drivers and to warn other vehicles on the road to prevent accidents from happening. A five-layer context-aware architecture is proposed, which is able to collect contextual information about the driving environment, to perform reasoning about certain and uncertain contextual information, and to react upon that information. A probabilistic model based on dynamic Bayesian networks (DBNs) in real time, inferring four types of driving behavior (normal, drunk, reckless, and fatigue) by combining contextual information about the driver, the vehicle, and the environment, is presented. The dynamic behavior model can capture the static and the temporal aspects related to the behavior of the driver, thus leading to robust and accurate behavior detection. The evaluation of behavior detection using synthetic data proves the validity of our model and the importance of including contextual information about the driver, the vehicle, and the environment.",""
"How do human infants learn the causal dependencies between events? Evidence suggests that this remarkable feat can be achieved by observation of only a handful of examples. Many computational models have been produced to explain how infants perform causal inference without explicit teaching about statistics or the scientific method. Here, we propose a spiking neuronal network implementation that can be entrained to form a dynamical model of the temporal and causal relationships between events that it observes. The network uses spike-time dependent plasticity, long-term depression, and heterosynaptic competition rules to implement Rescorla-Wagner-like learning. Transmission delays between neurons allow the network to learn a forward model of the temporal relationships between events. Within this framework, biologically realistic synaptic plasticity rules account for well-known behavioral data regarding cognitive causal assumptions such as backwards blocking and screening-off. These models can then be run as emulators for state inference. Furthermore, this mechanism is capable of copying synaptic connectivity patterns between neuronal networks by observing the spontaneous spike activity from the neuronal circuit that is to be copied, and it thereby provides a powerful method for transmission of circuit functionality between brain regions.","Many computational models have been produced to explain how infants perform causal inference without explicit teaching about statistics or the scientific method."
"The work presents a dynamic Bayesian networks (DBN) modeling of series, parallel and 2-out-of-3 (2003) voting systems, taking account of common-cause failure, imperfect coverage, imperfect repair and preventive maintenance. Seven basic events of one, two or three component failure are proposed to model the common-cause failure of the three-components-systems. The imperfect coverage is modeled in the conditional probability table by defining a coverage factor. A multi-state degraded component is used to model the imperfect repair and preventive maintenance. Using the proposed method, a DBN modeling of a subsea blowout preventer (BOP) control system is built, and the reliability and availability are evaluated. The mutual information is researched in order to assess the important degree of basic events. The effects of degradation probability, failure rate and mean time to repair (MTTR) on the performances are studied. The results show that the repairs and maintenance can improve the system performance significantly, whereas the imperfect repair cannot degrade the system performance significantly in comparison with the perfect repair, and the preventive maintenance can improve the system performance slightly in comparison with the imperfect repair. In order to improve the performance of subsea BOP control system, the single surface components and the components with all-common-cause failure should given more attention. The influence of degradation probability on the performance is in the order of PLC, PC and ES. The influence of failure rate and MTTR on the performance is in the order of PLC, ES, PC, DO, DI and Al. (C) 2013 Elsevier Ltd. All rights reserved.",""
"This paper describes ELaC, a fully implemented and evaluated novel integrated environment for personalized e-training in programming and the language C. Software development relies on many different programming languages and tools, ranging from procedural to object-oriented and query languages; an individual learning a new language may already know a range of other languages, or may know no other languages at all. Given the variety of backgrounds of prospective learners of programming, developing learning environments for all of them is not easy. In the light of these problems, this work has focused on the development of an original integrated e-training environment for programming and the language C, incorporating a student model responsible for identifying and updating the student's knowledge level, which takes into account each individual user's pace of learning. The system can adapt dynamically to each individual learner's needs by scheduling the sequence of learning lessons on the fly. This personalization allows each learner to complete the e-training course on at their own pace and according to their ability.",""
"This paper examines concepts of independence for full conditional probabilities; that is, for set-functions that encode conditional probabilities as primary objects, and that allow conditioning on events of probability zero. Full conditional probabilities have been used in economics, in philosophy, in statistics, in artificial intelligence. This paper characterizes the structure of full conditional probabilities under various concepts of independence; limitations of existing concepts are examined with respect to the theory of Bayesian networks. The concept of layer independence (factorization across layers) is introduced; this seems to be the first concept of independence for full conditional probabilities that satisfies the graphoid properties of Symmetry, Redundancy, Decomposition, Weak Union, and Contraction. A theory of Bayesian networks is proposed where full conditional probabilities are encoded using infinitesimals, with a brief discussion of hyperreal full conditional probabilities. (C) 2013 Elsevier Inc. All rights reserved.",""
"The search for a useful explanatory model based on a Bayesian Network (BN) now has a long and successful history. However, when the dependence structure between the variables of the problem is asymmetric then this cannot be captured by the BN. The Chain Event Graph (CEG) provides a richer class of models which incorporates these types of dependence structures as well as retaining the property that conclusions can be easily read back to the client. We demonstrate on a real health study how the CEG leads us to promising higher scoring models and further enables us to make more refined conclusions than can be made from the BN. Further we show how these graphs can express causal hypotheses about possible interventions that could be enforced. (C) 2013 The Authors. Published by Elsevier Inc. All rights reserved.",""
"This paper introduces the use of Bayesian networks (BNs) as an exploratory metamodelling tool for supporting simulation studies conducted with stochastic simulation models containing multiple inputs and outputs. BN metamodels combine simulation data with available expert knowledge into a non-parametric description of the joint probability distribution of discrete random variables representing simulation inputs and outputs. The distributions of the inputs are determined based on expert knowledge and/or a real-world data source while the conditional distributions of the outputs are estimated from' the simulation data. The exploratory use of the BN metamodels is an iterative process including the construction and validation of the BNs and allowing various analyses dealing with the dependencies among the inputs and the outputs, input uncertainty, and inverse reasoning. The results of these analyses are applied to guide and aid the utilization and interpretation of the simulation model under consideration. In addition, the analyses are used for studying the behaviour of the simulated system. The exploratory use is illustrated with an example involving a simulated queue.",""
"Recent spate of cyber attacks against critical infrastructure systems, which are vital to society, have shown that in addition to be infeasible to stop every possible attack it is imperative to keep such systems running. Survivability models and tools are good to evaluate system's capacity to handling undesired events. Current survivability measurement techniques are limited, since they only use performance to model system behavior, and do not take into account service interdependencies. This paper introduces a probabilistic model that offers a new direction in measuring survivability. The proposed model solves the issues with current models by combining the formalism of Bayesian networks with information diversity. Service interdependencies are properly taken into account and the information diversity metric is used to represent service behavior. In addition, the model is evaluated through a simulation of a SCADA system, where the entire process to construct and to use the model is detailed.",""
"The regulatory interactions in a cell form a complicated system, and an important goal of systems biology is to model and infer these interactions. The modeling and inference of genetic regulatory models requires understanding of the true biological interactions while incorporating the technological limitations on observation of the biological entities in estimation of a robust model. This review is structured as follows: (a) a brief description of the biological interactions is provided, (b) currently available technologies for measuring genomic characterizations are outlined, (c) followed by the description of the commonly used approaches for genetic regulatory network modeling, and (d) finally, some of the pertinent issues in modeling and inference of genetic interactions are discussed. (C) 2013 John Wiley & Sons, Ltd.","The modeling and inference of genetic regulatory models requires understanding of the true biological interactions while incorporating the technological limitations on observation of the biological entities in estimation of a robust model."
"Software projects have inherent uncertainties and risks. Social software projects suffer even more requirement changes and require more attention to risk management. Risk analysis and planning are complex, making it difficult to manage risks effectively through subjective judgment. At present, ample empirical research on intelligent decision-support models for risk analysis in software projects exists. However, to the best of our knowledge, empirical models for software project risk planning, or those related to integrative software risk analysis and planning are not available. Thus, the current study proposes an integrative framework for intelligent software project risk planning (IF-ISPRP) to help in minimizing the impacts of project risks and achieving a better foreseeable project outcome. IF-ISPRP includes two core components, namely, risk analysis module and risk planning module. The risk analysis module is to predict whether a project will be successful or not. The risk planning module is to produce a cost-minimal action set for risk control based on the risk analysis module. For integrative risk analysis and planning, we propose a novel many-to-many actionable knowledge discovery (MMAKD) method for complex risk planning. We also apply the framework on a social media platform project, Guangzhou Wireless City, and demonstrate how the model can generate a cost-minimal action set to mitigate the project risk. The risk-control actions found may help develop strategies on mitigating the risks of other social software projects. We hope that the proposed framework will provide an intelligent decision-support tool for project stakeholders to effectively control project risks by integrating risk analysis and planning. (C) 2013 Elsevier B.V. All rights reserved.",""
"Multimodal fusion is a complex topic. For surveillance applications audio-visual fusion is very promising given the complementary nature of the two streams. However, drawing the correct conclusion from multi-sensor data is not straightforward. In previous work we have analysed a database with audiovisual recordings of unwanted behavior in trains (Lefter et al., 2012) and focused on a limited subset of the recorded data. We have collected multi- and unimodal assessments by humans, who have given aggression scores on a 3 point scale. We showed that there are no trivial fusion algorithms to predict the multimodal labels from the unimodal labels since part of the information is lost when using the unimodal streams. We proposed an intermediate step to discover the structure in the fusion process. This step is based upon meta-features and we find a set of five which have an impact on the fusion process. In this paper we extend the findings in (Lefter et al., 2012) for the general case using the entire database. We prove that the meta-features have a positive effect on the fusion process in terms of labels. We then compare three fusion methods that encapsulate the meta-features. They are based on automatic prediction of the intermediate level variables and multimodal aggression from state of the art low level acoustic, linguistic and visual features. The first fusion method is based on applying multiple classifiers to predict intermediate level features from the low level features, and to predict the multimodal label from the intermediate variables. The other two approaches are based on probabilistic graphical models, one using (Dynamic) Bayesian Networks and the other one using Conditional Random Fields. We learn that each approach has its strengths and weaknesses in predicting specific aggression classes and using the meta-features yields significant improvements in all cases. (C) 2013 Elsevier B.V. All rights reserved.","The first fusion method is based on applying multiple classifiers to predict intermediate level features from the low level features, and to predict the multimodal label from the intermediate variables."
"In this paper, we investigate automatic classification of the socio-situational settings of transcripts of a spoken discourse. Knowledge of the socio-situational setting can be used to search for content recorded in a particular setting or to select context-dependent models for example in speech recognition. The subjective experiment we report on in this paper shows that people correctly classify 68% the socio-situational settings. Based on the cues that participants mentioned in the experiment, we developed two types of automatic socio-situational setting classification methods; a static socio-situational setting classification method using support vector machines (S3C-SVM), and a dynamic socio-situational classification method applying dynamic Bayesian networks (S3C-DBN). Using these two methods, we developed classifiers applying various features and combinations of features. The S3C-SVM method with sentence length, function word ratio, single occurrence word ratio, part of speech (POS) and words as features results in a classification accuracy of almost 90%. Using a bigram S3C-DBN with Pos tag and word features results in a dynamic classifier which can obtain nearly 89% classification accuracy. The dynamic classifiers not only can achieve similar results as the static classifiers, but also can track the socio-situational setting while processing a transcript or conversation. On discourses with a static social situational setting, the dynamic classifiers only need the initial 25% of data to achieve a classification accuracy close to the accuracy achieved when all data of a transcript is used. (c) 2013 Elsevier B.V. All rights reserved.","In this paper, we investigate automatic classification of the socio-situational settings of transcripts of a spoken discourse."
"We present a framework for sequential decision making in problems described by graphical models. The setting is given by dependent discrete random variables with associated costs or revenues. In our examples, the dependent variables are the potential outcomes (oil, gas or dry) when drilling a petroleum well. The goal is to develop an optimal selection strategy of wells that incorporates a chosen utility function within an approximated dynamic programming scheme. We propose and compare different approximations, from naive and myopic heuristics to more complex look-ahead schemes, and we discuss their computational properties. We apply these strategies to oil exploration over multiple prospects modeled by a directed acyclic graph, and to a reservoir drilling decision problem modeled by a Markov random field. The results show that the suggested strategies clearly improve the naive or myopic constructions used in petroleum industry today. This is useful for decision makers planning petroleum exploration policies. (C) 2013 Elsevier B.V. All rights reserved.",""
"A Decision Tree (DT) is a potential method for studying traffic accident severity. One of its main advantages is that Decision Rules (DRs) can be extracted from its structure. And these DRs can be used to identify safety problems and establish certain measures of performance. However, when only one DT is used, rule extraction is limited to the structure of that DT and some important relationships between variables cannot be extracted. This paper presents a more effective method for extracting rules from DTs. The method's effectiveness when applied to a particular traffic accident dataset is shown. Specifically, our study focuses on traffic accident data from rural roads in Granada (Spain) from 2003 to 2009 (both included). The results show that we can obtain more than 70 relevant rules from our data using the new method, whereas with only one DT we would have extracted only five relevant rules from the same dataset. (C) 2013 Elsevier Ltd. All rights reserved.",""
"Background: In recent years, there has been great interest in using transcriptomic data to infer gene regulatory networks. For the time being, methodological development in this area has primarily made use of graphical Gaussian models for observational wild-type data, resulting in undirected graphs that are not able to accurately highlight causal relationships among genes. In the present work, we seek to improve the estimation of causal effects among genes by jointly modeling observational transcriptomic data with arbitrarily complex intervention data obtained by performing partial, single, or multiple gene knock-outs or knock-downs. Results: Using the framework of causal Gaussian Bayesian networks, we propose a Markov chain Monte Carlo algorithm with a Mallows proposal model and analytical likelihood maximization to sample from the posterior distribution of causal node orderings, and in turn, to estimate causal effects. The main advantage of the proposed algorithm over previously proposed methods is its flexibility to accommodate any kind of intervention design, including partial or multiple knock-out experiments. Using simulated data as well as data from the Dialogue for Reverse Engineering Assessments and Methods (DREAM) 2007 challenge, the proposed method was compared to two alternative approaches: one requiring a complete, single knock-out design, and one able to model only observational data. Conclusions: The proposed algorithm was found to perform as well as, and in most cases better, than the alternative methods in terms of accuracy for the estimation of causal effects. In addition, multiple knock-outs proved to contribute valuable additional information compared to single knock-outs. Finally, the simulation study confirmed that it is not possible to estimate the causal ordering of genes from observational data alone. In all cases, we found that the inclusion of intervention experiments enabled more accurate estimation of causal regulatory relationships than the use of wild-type data alone.",""
"We study nonequilibrium thermodynamics of complex information flows induced by interactions between multiple fluctuating systems. Characterizing nonequilibrium dynamics by causal networks (i.e., Bayesian networks), we obtain novel generalizations of the second law of thermodynamics and the fluctuation theorem, which include an informational quantity characterized by the topology of the causal network. Our result implies that the entropy production in a single system in the presence of multiple other systems is bounded by the information flow between these systems. We demonstrate our general result by a simple model of biochemical adaptation.",""
"Catchment management is a process which increases the sustainable development and management of all catchment resources in order to maximize the balance among socioeconomic welfare and the sustainability of vital ecosystems. The increase of anthropogenic activities within river catchments causes degradation and serious problems for stakeholders and managers, particularly in arid and semi-arid regions. Although there are many techniques for solving these problems, it is not easy for catchment managers to apply them. An integrated Bayesian network model framework was applied to evaluate the sustainability of a semi-arid river catchment located in the Iranian Central Plateau river basin encompassing 32.6 km(2) area on the Hablehrood river catchment, located in the northern part of the Iranian Central Plateau river basin. The research illustrated the assessment of the relevant management problems, the model framework, and the techniques applied to extract input data. Results for the study area implementation and a suggestion for management are described and discussed. (c) 2013 Elsevier B.V. All rights reserved.",""
"A participatory modelling process has been conducted in two areas of the Guadiana river (the upper and the middle sub-basins), in Spain, with the aim of providing support for decision making in the water management field. The area has a semi-arid climate where irrigated agriculture plays a key role in the economic development of the region and accounts for around 90% of water use. Following the guidelines of the European Water Framework Directive, we promote stakeholder involvement in water management with the aim to achieve an improved understanding of the water system and to encourage the exchange of knowledge and views between stakeholders in order to help building a shared vision of the system. At the same time, the resulting models, which integrate the different sectors and views, provide some insight of the impacts that different management options and possible future scenarios could have. The methodology is based on a Bayesian network combined with an economic model and, in the middle Guadiana sub-basin, with a crop model. The resulting integrated modelling framework is used to simulate possible water policy, market and climate scenarios to find out the impacts of those scenarios on farm income and on the environment. At the end of the modelling process, an evaluation questionnaire was filled by participants in both sub-basins. Results show that this type of processes are found very helpful by stakeholders to improve the system understanding, to understand each other's views and to reduce conflict when it exists. In addition, they found the model an extremely useful tool to support management. The graphical interface, the quantitative output and the explicit representation of uncertainty helped stakeholders to better understand the implications of the scenario tested. Finally, the combination of different types of models was also found very useful, as it allowed exploring in detail specific aspects of the water management problems. (C) 2013 Elsevier Ltd. All rights reserved.",""
"Aquaculture activities are embedded in complex social-ecological systems. However, aquaculture development decisions have tended to be driven by revenue generation, failing to account for interactions with the environment and the full value of the benefits derived from services provided by local ecosystems. Trade-offs resulting from changes in ecosystem services provision and associated impacts on livelihoods are also often overlooked. This paper proposes an innovative application of Bayesian belief networks - influence diagrams - as a decision support system for mediating trade-offs arising from the development of shrimp aquaculture in Thailand. Senior experts were consulted (n = 12) and primary farm data on the economics of shrimp farming (n = 20) were collected alongside secondary information on ecosystem services, in order to construct and populate the network. Trade-offs were quantitatively assessed through the generation of a probabilistic impact matrix. This matrix captures nonlinearity and uncertainty and describes the relative performance and impacts of shrimp farming management scenarios on local livelihoods. It also incorporates export revenues and provision and value of ecosystem services such as coastal protection and biodiversity. This research shows that Bayesian belief modeling can support complex decision-making on pathways for sustainable coastal aquaculture development and thus contributes to the debate on the role of aquaculture in social-ecological resilience and economic development.",""
"Analyzing time-course expression data captured in microarray datasets is a complex undertaking as the vast and complex data space is represented by a relatively low number of samples as compared to thousands of available genes. Here, we developed the Interdependent Correlation Clustering (ICC) method to analyze relationships that exist among genes conditioned on the expression of a specific target gene in microarray data. Based on Correlation Clustering, the ICC method analyzes a large set of correlation values related to gene expression profiles extracted from given microarray datasets. ICC can be applied to any microarray dataset and any target gene. We applied this method to microarray data generated from wine fermentations and selected NSF1, which encodes a C2H2 zinc finger-type transcription factor, as the target gene. The validity of the method was verified by accurate identifications of the previously known functional roles of NSF1. In addition, we identified and verified potential new functions for this gene; specifically, NSF1 is a negative regulator for the expression of sulfur metabolism genes, the nuclear localization of Nsf1 protein (Nsf1p) is controlled in a sulfur-dependent manner, and the transcription of NSF1 is regulated by Met4p, an important transcriptional activator of sulfur metabolism genes. The inter-disciplinary approach adopted here highlighted the accuracy and relevancy of the ICC method in mining for novel gene functions using complex microarray datasets with a limited number of samples.",""
"In this article, Object-Oriented Bayesian Networks (OOBN) are proposed as a tool to model measurement errors in a categorical variable due to respondent. A mixed measurement error model is presented and an OOBN implementing such a model is introduced. The insertion of evidence represented by the observed value and its propagation throughout the network yields for each unit the probability distribution of the true value given the observed. Two methods are used to predict the individual true value and their performance is evaluated via simulation.",""
"Computer-based instructional simulations are becoming more and more ubiquitous, particularly in military and medical domains. As the technology that drives these simulations grows ever more sophisticated, the underlying pedagogical models for how instruction, assessment, and feedback are implemented within these systems must evolve accordingly. In this article, we review some of the existing educational approaches to medical simulations, and present pedagogical methodologies that have been used in the design and development of games and simulations at the University of California, Los Angeles, Center for Research on Evaluation, Standards, and Student Testing. In particular, we present a methodology for how automated assessments of computer-based simulations can be implemented using ontologies and Bayesian networks, and discuss their advantages and design considerations for pedagogical use.",""
"The development of causal modelling since the 1950s has been accompanied by a number of controversies, the most striking of which concerns the Markov condition. Reichenbach's conjunctive forks did satisfy the Markov condition, while Salmon's interactive forks did not. Subsequently some experts in the field have argued that adequate causal models should always satisfy the Markov condition, while others have claimed that non-Markovian causal models are needed in some cases. This paper argues for the second position by considering the multi-causal forks, which are widespread in contemporary medicine (Section 2). A non-Markovian causal model for such forks is introduced and shown to be mathematically tractable (Sections 6, 7, and 8). The paper also gives a general discussion of the controversy about the Markov condition (Section 1), and of the related controversy about probabilistic causality (Sections 3, 4, and 5).",""
"This article deals with the problem of the causes of the variation of sex ratio (proportion male) at birth. This problem is common to a number of areas in biology and medicine, for example, obstetrics, neurology/psychiatry, parasitology, virology, oncology, and teratology. It is established that there are significantly biased, but unexplained, sex ratios in each of these fields. Yet workers in them (with the possible exception of virology) have regarded the problem as a minor loose end, irrelevant to the field's major problems. However, as far as I know, no one has previously noted that unexplained biased sex ratios occur, and thus pose (perhaps similar) problems, in all these fields. Here it is suggested that similar sorts of solutions apply in each. Further research is proposed for testing each solution. If the argument here is substantially correct across this range of topics, it may lead to an improved understanding not only of sex ratio but also of some of the pathologies in these specialties.",""
"Gene expression is a highly regulated biological process that is fundamental to the existence of phenotypes of any living organism. The regulatory relations are usually modeled as a network; simply, every gene is modeled as a node and relations are shown as edges between two related genes. This paper presents a novel method for inferring correlation networks, networks constructed by connecting co-expressed genes, through predicting co-expression level from genes promoter's sequences. According to the results, this method works well on biological data and its outcome is comparable to the methods that use microarray as input. The method is written in C++ language and is available upon request from the corresponding author.",""
"Objectives. We examined depression within a multidimensional framework consisting of genetic, environmental, and sociobehavioral factors and, using machine learning algorithms, explored interactions among these factors that might better explain the etiology of depressive symptoms. Methods. We measured current depressive symptoms using the Center for Epidemiologic Studies Depression Scale (n = 6378 participants in the Wisconsin Longitudinal Study). Genetic factors were 78 single nucleotide polymorphisms (SNPs); environmental factors-13 stressful life events (SLEs), plus a composite proportion of SLEs index; and sociobehavioral factors-18 personality, intelligence, and other health or behavioral measures. We performed traditional SNP associations via logistic regression likelihood ratio testing and explored interactions with support vector machines and Bayesian networks. Results. After correction for multiple testing, we found no significant single genotypic associations with depressive symptoms. Machine learning algorithms showed no evidence of interactions. Naive Bayes produced the best models in both subsets and included only environmental and sociobehavioral factors. Conclusions. We found no single or interactive associations with genetic factors and depressive symptoms. Various environmental and sociobehavioral factors were more predictive of depressive symptoms, yet their impacts were independent of one another. A genome-wide analysis of genetic alterations using machine learning methodologies will provide a framework for identifying genetic-environmental-sociobehavioral interactions in depressive symptoms.","We performed traditional SNP associations via logistic regression likelihood ratio testing and explored interactions with support vector machines and Bayesian networks."
NA,""
"Many methods for causal inference generate directed acyclic graphs (DAGs) that formalize causal relations between n variables. Given the joint distribution on all these variables, the DAG contains all information about how intervening on one variable changes the distribution of the other n - 1 variables. However, quantifying the causal influence of one variable on another one remains a nontrivial question. Here we propose a set of natural, intuitive postulates that a measure of causal strength should satisfy. We then introduce a communication scenario, where edges in a DAG play the role of channels that can be locally corrupted by interventions. Causal strength is then the relative entropy distance between the old and the new distribution. Many other measures of causal strength have been proposed, including average causal effect, transfer entropy, directed information, and information flow. We explain how they fail to satisfy the postulates on simple DAGs of <= 3 nodes. Finally, we investigate the behavior of our measure on time-series, supporting our claims with experiments on simulated data.","Many methods for causal inference generate directed acyclic graphs (DAGs) that formalize causal relations between n variables."
"Relay sharing has been recently investigated to increase the performance of coexisting wireless multi-hop networks. In this paper, we analyze a scenario where two wireless ad hoc networks are willing to share some of their nodes, acting as relays, in order to gain benefits in terms of lower packet delivery delay and reduced loss probability. Bayesian network analysis is exploited to compute the probabilistic relationships between local parameters and overall performance, whereas the selection of the nodes to share is made by means of a game theoretic approach. Our results are then validated through the use of a system level simulator, which shows that an accurate selection of the shared nodes can significantly increase the performance gain with respect to a random selection scheme.",""
"Suppose that we rank-order the conditional probabilities for a group of subjects that are provided from a Bayesian network (BN) model of binary variables. The conditional probability is the probability that a subject has a certain attribute given an outcome of some other variables and the classification is based on the rank-order. Under the condition that the class sizes are equal across the class levels and that all the variables in the model are positively associated with each other, we compared the classification results between models of binary variables which share the same model structure. In the comparison, we used a BN model, called a similar BN model, which was constructed under some rule based on a set of BN models satisfying certain conditions. Simulation results indicate that the agreement level of the classification between a set of BN models and their corresponding similar BN model is considerably high with the exact agreement for about half of the subjects or more and the agreement up to one-class-level difference for about 90% or more.","The conditional probability is the probability that a subject has a certain attribute given an outcome of some other variables and the classification is based on the rank-order."
"In existing research on syntactic alternations such as the dative alternation, (give her the apple vs. give the apple to her), the linguistic data is often analysed with the help of logistic regression models. In this article, we evaluate the use of logistic regression for this type of research, and present two different approaches: Bayesian Networks and Memory-based learning. For the Bayesian Network, we use the higher-level semantic features suggested in the literature, while we limit ourselves to lexical items in the memory-based approach. We evaluate the suitability of the three approaches by applying them to a large data set (> 11,000 instances) extracted from the British National Corpus, and comparing their quality in terms of classification accuracy, their interpretability in the context of linguistic research, and their actual classification of individual cases. Our main finding is that the classifications are very similar across the three approaches, also when employing lexical items instead of the higher-level features, because most of the alternation is determined by the verb and the length of the two objects (here: her and the apple).","give the apple to her), the linguistic data is often analysed with the help of logistic regression models."
"Semiautomatic approaches are developed for wide area situation assessment in near-real-time. The two-step method consists of two granularity levels. The first entity assessment uses a new multi-target tracking (MTT) algorithm (hybridization of Gaussian mixture-Cardinalized probability hypothesis density (GM-CPHD) filter and multiple hypothesis tracker (MHT) with road constraints) on ground moving target indicator (GMTI) data. The situation is then assessed by detecting objects of interest such as convoys with other data types (synthetic aperture radar (SAR), video). These detections are based on Bayesian networks and their credibilistic counterpart.",""
"Land-use change models grounded in complexity theory such as agent-based models (ABMs) are increasingly being used to examine evolving urban systems. The objective of this study is to develop a spatial model that simulates land-use change under the influence of human land-use choice behavior. This is achieved by integrating the key physical and social drivers of land-use change using Bayesian networks (BNs) coupled with agent-based modeling. The BNAS model, integrated Bayesian network-based agent system, presented in this study uses geographic information systems, ABMs, BNs, and influence diagram principles to model population change on an irregular spatial structure. The model is parameterized with historical data and then used to simulate 20 years of future population and land-use change for the City of Surrey, British Columbia, Canada. The simulation results identify feasible new urban areas for development around the main transportation corridors. The obtained new development areas and the projected population trajectories with the\"what-if\" scenario capabilities can provide insights into urban planners for better and more informed land-use policy or decision-making processes.",""
"Condition-based maintenance (CBM) is increasingly applied to operational systems to reduce lifecycle costs. Predicting the performance of various CBM policies is a challenging task addressed in this work. We suggest a CBM framework that is based on system simulations and a targeted Bayesian network model. Simulations explore the robustness of various CBM policies under different scenarios. The Bayesian network, which is learned from the simulation data, is then used as an explanatory compact metamodel for failure prediction. The framework is demonstrated through a study of an operator of a freight rail fleet. This study demonstrates a significant profit improvement compared to other methods.",""
"According to the Hyogo Framework for Action, increasing resilience to drought requires the development of a people-centered monitoring and early warning system, or in other words, a system capable of providing useful and understandable information to the community at risk. To achieve this objective, it is crucial to negotiate a credible and legitimate knowledge system, which should include both expert and local knowledge. Although several benefits can be obtained, the integration of local and scientific knowledge to support drought monitoring is still far from being the standard in drought monitoring and early warning. This is due to many reasons, that is, the reciprocal skepticism of local communities and decision makers, and the limits in the capacity to understand and assess the complex web of drought impacts. This work describes a methodology based on the sequential implementation of Cognitive Mapping and Bayesian Belief Networks to collect, structure and analyze stakeholders' perceptions of drought impacts. The methodology was applied to analyze drought impacts at Lake Trasimeno (central Italy). A set of drought indicators was developed based on stakeholders' perceptions. A validation phase was carried out comparing the perceived indicators of drought and the physical indicators (i.e., Standard Precipitation Index and the level of the lake). Some preliminary conclusions were drawn concerning the reliability of local knowledge to support drought monitoring and early warning.",""
"Researchers and commissions contend that the risk of human extinction is high, but none of these estimates have been based upon a rigorous methodology suitable for estimating existential risks. This article evaluates several methods that could be used to estimate the probability of human extinction. Traditional methods evaluated include: simple elicitation; whole evidence Bayesian; evidential reasoning using imprecise probabilities; and Bayesian networks. Three innovative methods are also considered: influence modeling based on environmental scans; simple elicitation using extinction scenarios as anchors; and computationally intensive possible-worlds modeling. Evaluation criteria include: level of effort required by the probability assessors; level of effort needed to implement the method; ability of each method to model the human extinction event; ability to incorporate scientific estimates of contributory events; transparency of the inputs and outputs; acceptability to the academic community (e.g., with respect to intellectual soundness, familiarity, verisimilitude); credibility and utility of the outputs of the method to the policy community; difficulty of communicating the method's processes and outputs to nonexperts; and accuracy in other contexts. The article concludes by recommending that researchers assess the risks of human extinction by combining these methods.",""
"This study focuses on family farmer engagement in the Brazilian national programme for Production and use of Biodiesel (PNPB). The Brazilian government has been promoting the role of family farmers as producers of biomass for biodiesel since 2004; however, fewer than expected family farmers have decided to produce biomass for biodiesel. The North of Minas Gerais is one region where a biodiesel plant has been strategically located to source castor beans grown by family farmers. The target family farm type in this region specializes in beef and/or dairy production with low input pasture (approximately 30 ha per farm), maize intercropped with beans (approximately 1 ha per farm) and sugarcane (approximately 1 ha per farm). We selected this region for a case study to explore management decisions of farmers, industry and policy makers that influence family farmer engagement with biodiesel production through cultivation of castor beans. To evaluate outcomes for family farmers engaging with the PNPB, we focused on how cultivation of castor beans impacts family farmers in terms of income levels, income stability and levels of milk production. We used an application of systems thinking known as Bayesian network modelling (BNM). BNM was chosen for its suitability to integrate different types of knowledge and to include quantitative and qualitative variables. The study was built on a body of scientific literature explaining why family farmers have not been cultivating castor beans for biodiesel production and a body of experiential knowledge of local actors (farmers, extension officers, policy makers, biodiesel manufacturers and researchers in North of Minas Gerais). The complete BNM consisted of a 'cause and effect' diagram where the strengths of the causal relationships were quantified with elicited opinions from surveyed local actors. We used the complete BNM to explore scenarios that could improve outcomes for family farmers and consequently increase their level of engagement. For example, we addressed subsidy structures of the PNPB, crop management, farm-level trade-offs and value-chain innovations. We demonstrate that decisions to support family farmer engagement with biodiesel are not singular. Engagement by family farmers requires simultaneously: improvements in technical crop management, reductions in farm-level cash constraints and innovations in the production chain such that engagement of family farmers goes beyond cultivation of one more low-value crop. Finally we discuss some methodological issues from this application of BNM to farming systems research. (c) 2013 Elsevier Ltd. All rights reserved.",""
"Thermo-electrical power plants utilize fossil fuel oil to transform the calorific power of fuel into electric power. An optimal combustion in the boiler requires the fuel oil to be in its best conditions. One of fuel's most important properties to consider is viscosity. Viscosity has influence on the optimal combustion between fuel and air. Hardware viscosity meters for fuel oils are expensive and unreliable to operate in power plant conditions. Chemical laboratory measures viscosity accurately with special apparatus, but they cannot be used in a real time process. This paper describes the development of a virtual sensor that estimates fuel oil viscosity in the combustion process of a power plant. A virtual sensor or soft sensor is a computer program that estimates the value of a certain variable based on related measurements and a model of the process where the variable participates. In this project, a probabilistic model is constructed using automatic learning algorithms with historical data and experts' advice. The learning and validation experiments are described and discussed. The virtual sensor is installed in the Tuxpan Power Plant in Veracruz, Mexico. (C) 2013 Elsevier Ltd. All rights reserved.",""
"In this paper, we are interested in addressing risk analysis. We propose an influence diagram-based approach that focuses on a Benefit, Cost, Deficit (BCD) model. The BCD model is proposed for studying the intentional deviant behaviors of human operators in a system. In this model, the consequences of human actions are analyzed with respect to three parameters: benefit, cost and deficit. Our approach aims to expand the BCD model by integrating factors, such as those related to the organization of the system in question, that influence human operator actions. In addition, the approach considers multiple criteria that are related, for example, to safety and productivity. To build a model that evaluates the risk induced by human actions in a system and analyzes the impact of the different factors, we use influence diagrams. Influence diagrams are probabilistic graphical models that can deal with uncertainty and with incomplete and imprecise information. Influence diagrams also represent the interdependencies between the different variables of the studied problem. In addition, contrary to Bayesian networks, influence diagrams can rank a set of actions by providing information on which action carries the greatest risk or the most benefits. We applied this approach to a case study of an industrial rotary press, but it can also be used in other problems and sectors. (C) 2013 Elsevier Ltd. All rights reserved.",""
"Emerging targeted therapies have shown benefits such as less toxicity and higher effectiveness in specific types of cancer treatment; however, the accessibility of these advantages may rely on correct identification of suitable patients, which remains highly immature. We assume that copy number profiles, being accessible genomic data via microarray techniques, can provide useful information regarding drug response and shed light on personalized therapy. Based on the mechanism of action (MOA) of trastuzumab in the HER2 signaling pathway, a Bayesian network model in which copy number alterations (CNAs) serve as latent parents modifying signal transduction is applied. Two model parameters M-score and R-value which stand for the qualitative and quantitative effects of CNAs on drug effectiveness and are functions of conditional probabilities (CPs), are defined. An expectation-maximization (EM) algorithm is developed for estimating CPs, M-scores, and R-values from continuous measures, such as microarray data. We show through simulations that the EM algorithm can outperform classical threshold-based methods in the estimation of CPs and thereby provide improved performance for the detection of unfavorable CNAs. Several candidates of unfavorable CNAs to the trastuzumab therapy in breast cancer are provided in a real data example.",""
"This article covers the reliability assessment of the hull girder of a crude oil tanker, referring to a scenario in which the ship is exposed to sea loads after a damage to the bottom of the hull has occurred. A number of possible flooding configurations are examined, each one caused by a group of damage cases, characterized by different location and extent. Static loads, wave loads and residual structural resistance are determined for each damage case, with the objective of obtaining a prediction for the probability of the hull girder's failure. The various damage cases are compared to each other and unconditioned to derive the probability of failure extended to the ship's life due to a generic bottom damage. A probabilistic Bayesian Network model has been created to deal with these variables and with the dependency relationships existing between them. The results provided by the model are analyzed with the aim of identifying the parameters most influencing the problem. The work is intended to contribute to the development of a more rational treatment of accidental conditions in design structural requirements for ships. (C) 2013 Elsevier Ltd. All rights reserved.",""
"The causes of pedestrian accidents are a major concern to transportation engineers and other road safety professionals. Although studies have aimed at modelling and analysing the causes, they are either too stochastically oriented or excessively macroscopic, and so fail to analyse the interactions between pedestrians and their immediate environment. This study applies a Bayesian network modelling approach to investigate factors influencing pedestrian crossing behaviour in spring and summer using variables known from the literature. The model analyses the problem on three levels: graphical level, information level and quantitative level. The results show that pedestrians often exhibit rational crossing behaviour influenced predominantly by personal motives rather than external factors, even though the roadway environment is not always favourable. A sensitivity analysis revealed that signal timing phase length is the most influential parameter that affects pedestrian crossing behaviour.",""
"Based on review of patient data in case conferences over time, we hypothesized that clinically relevant data are omitted in routine soft tissue sarcoma staging. We examined subsets of a prospectively collected single institution soft tissue sarcoma database with respect to criteria of the AJCC versions 6 (2002) and 7 (2010) staging systems and examined their clinical outcomes. Relapse-free survival decreases with increasing primary tumor size in four categories, versus two categories used in AJCC 6 and 7 staging. Disease-specific survival decreases over three categories. Conversely, omission of tumor depth as a prognostic factor in version 7 appears supported, since tumor depth is not an independent risk factor for disease-specific survival by multivariate analysis. Patients with nodal disease and no other metastases fare better than patients with other metastases, but have inferior outcomes compared with patients with large high-grade tumors without nodal metastasis. Multivariate analysis identified size, site, grade, age, nodal metastatic disease, and other metastatic disease as independent risk factors for disease-specific survival. Versions 6 and 7 criteria are tacit regarding anatomic site and histology for tumors with identical FNCLCC grade. Improved patient risk assessment may be achieved by staging using a larger number of size categories. Staging system refinements come at the cost of a larger number of staging categories. Histology or site-specific staging systems, nomograms or Bayesian belief networks may provide more accurate means to assess clinical outcomes.",""
"The goal of the work is to improve the teaching-learning process through the inclusion of prediction features in a control system proposal namely Reactive Blended Learning. To achieve this goal, a model of the student has been proposed, whose considered outputs are the performance and a participation index that measures the activity level of the student in the class. The controller is based on fuzzy logic and uses the predictions of the model to anticipate the student's state. An important issue that has been taken into account is the limited time to identify the dynamics of the student learning before the course ends. This limitation has been treated through a three-stage process. It is important to remark that this work is not focused on obtaining a complete student model, but on getting useful information for the detection of trends in the teaching-learning process. Preliminary results on a real course are presented to attest the efficiency of the proposed control strategy. (C) 2013 Elsevier Ltd. All rights reserved.",""
"Wild sheep (Ovis orientalis) as the true ancestor of domestic sheep (Ovis aries) is currently listed as vulnerable (VU) by IUCN. Effective conservation of this species requires collecting all available ecological knowledge into a single framework that could be used for management decision making. Use of Bayesian Belief Networks for such purposes have been advocated over recent decades as this approach can integrate different sources of knowledge and perform comprehensive analyses. We built a decision support tool using BBN to assist in habitat management of wild sheep populations throughout the species' geographical range. The behaviour of the model was tested using scenario and sensitivity analysis. Habitat security, food and water suitability, and thermal cover were recognised as the main habitat variables determining wild sheep habitat suitability. Integrating the complex interactions between variables, the model can be applied for both diagnostic and predictive analyses answering \"what if\" and \"how\" questions. This approach can assist wildlife conservationists for building similar models for other poorly-studied threatened species. (C) 2013 Elsevier GmbH. All rights reserved.",""
"The arrival of new devices and techniques has brought tracking out of the investigational stage and into the wider world. Using Wi-Fi signals is an attractive and reasonably affordable option to deal with the currently unsolved problem of widespread tracking in an indoor environment. Here we present a system which aims at overcoming weaknesses in existing real time location systems (RTLS) by using the human approach of making educated guesses about future location. The hypothesis of this proposal is that knowledge of a person's historical movement habits allows for future location predictions to be made in the short, medium and long term. The primary research question that is foremost is whether the tracking capabilities of existing real time locating systems can be improved automatically by knowledge of previous movement especially in the short term in the case of emergency first responders by the application of a combination of artificial intelligence approaches. We conclude that HABITS improves on the standard Ekahau RTLS in term of accuracy (overcoming black spots), latency (giving position fixes when Ekahau cannot), cost (less APs are required than are recommended by Ekahau) and prediction (short term predictions are available from HABITS). These are features that no other indoor tracking system currently provides and could prove crucial in future emergency first responder incidents.",""
"Complex activities typically consist of multiple primitive events happening in parallel or sequentially over a period of time. Understanding such activities requires recognizing not only each individual event but, more importantly, capturing their spatiotemporal dependencies over different time intervals. Most of the current graphical model-based approaches have several limitations. First, time-sliced graphical models such as hidden Markov models (HMMs) and dynamic Bayesian networks are typically based on points of time and they hence can only capture three temporal relations: precedes, follows, and equals. Second, HMMs are probabilistic finite-state machines that grow exponentially as the number of parallel events increases. Third, other approaches such as syntactic and description-based methods, while rich in modeling temporal relationships, do not have the expressive power to capture uncertainties. To address these issues, we introduce the interval temporal Bayesian network (ITBN), a novel graphical model that combines the Bayesian Network with the interval algebra to explicitly model the temporal dependencies over time intervals. Advanced machine learning methods are introduced to learn the ITBN model structure and parameters. Experimental results show that by reasoning with spatiotemporal dependencies, the proposed model leads to a significantly improved performance when modeling and recognizing complex activities involving both parallel and sequential events.",""
"Objective Approximately 40% of patients receiving conditioning chemotherapy prior to autologous hematopoietic stem cell transplants (aHSCT) develop severe oral mucositis (SOM). Aside from disabling pain, ulcerative lesions associated with SOM predispose to poor health and economic outcomes. Our objective was to develop a probabilistic graphical model in which a cluster of single-nucleotide polymorphisms (SNPs) derived from salivary DNA could be used as a tool to predict SOM risk. Methods Salivary DNA was extracted from 153 HSCT patients and applied to Illumina BeadChips. Using sequential data analysis, we filtered extraneous SNPs, selected loci, and identified a predictive SNP network for OM risk. We then tested the predictive validity of the network using SNP array outputs from an independent HSCT cohort. Results We identified an 82-SNP Bayesian network (BN) that was related to SOM risk with a 10-fold cross-validation accuracy of 99.3% and an area under the ROC curve of 99.7%. Using samples from a small independent patient cohort (n=16), we demonstrated the network's predictive validity with an accuracy of 81.2% in the absence of any false positives. Conclusions Our results suggest that SNP-based BN developed from saliva-sourced DNA can predict SOM risk in patients prior to aHSCT.",""
"Temporal nodes Bayesian networks (TNBNs) are an alternative to dynamic Bayesian networks for temporal reasoning with much simpler and efficient models in some domains. TNBNs are composed of temporal nodes, temporal intervals, and probabilistic dependencies. However, methods for learning this type of models from data have not yet been developed. In this paper, we propose a learning algorithm to obtain the structure and temporal intervals for TNBNs from data. The method consists of three phases: (i) obtain an initial approximation of the intervals, (ii) obtain a structure using a standard algorithm and (iii) refine the intervals for each temporal node based on a clustering algorithm. We evaluated the method with synthetic data from three different TNBNs of different sizes. Our method obtains the best score using a combined measure of interval quality and prediction accuracy, and a competitive structural quality with lower running times, compared to other related algorithms. We also present a real world application of the algorithm with data obtained from a combined cycle power plant in order to diagnose temporal faults. (C) 2013 Elsevier Inc. All rights reserved.",""
"Using domain/expert knowledge when learning Bayesian networks from data has been considered a promising idea since the very beginning of the field. However, in most of the previously proposed approaches, human experts do not play an active role in the learning process. Once their knowledge is elicited, they do not participate any more. The interactive approach for integrating domain/expert knowledge we propose in this work aims to be more efficient and effective. In contrast to previous approaches, our method performs an active interaction with the expert in order to guide the search based learning process. This method relies on identifying the edges of the graph structure which are more unreliable considering the information present in the learning data. Another contribution of our approach is the integration of domain/expert knowledge at different stages of the learning process of a Bayesian network: while learning the skeleton and when directing the edges of the directed acyclic graph structure. (C) 2013 Elsevier Inc. All rights reserved.",""
"This paper considers the problem of ordering arc-reversal operations and breaking ties in cost measures when eliminating variables in Lazy AR Propagation (LPAR). In particular, the paper presents the BreakTies algorithm for breaking ties in cost measures when selecting the next arc to reverse in a variable elimination operation. BreakTies is based upon using a sequence of cost measures instead of randomly selecting an arc to reverse when multiple arcs share the same cost. The paper reports on an experimental evaluation of LPAR for belief update in Bayesian networks considering six sequences of five cost measures for breaking ties using BreakTies. The experimental results show that using BreakTies to select the next arc to reverse in a variable elimination operation can improve performance of LPAR. (C) 2013 Elsevier Inc. All rights reserved.",""
"In this article, I present the biological backgrounds of microarray, ChIP-chip and ChIP-Seq technologies and the application of computational methods in reverse engineering of gene regulatory networks (GRNs). The most commonly used GRNs models based on Boolean networks, Bayesian networks, relevance networks, differential and difference equations are described. A novel model for integration of prior biological knowledge in the GRNs inference is presented, too. The advantages and disadvantages of the described models are compared. The GRNs validation criteria are depicted. Current trends and further directions for GRNs inference using prior knowledge are given at the end of the paper.","A novel model for integration of prior biological knowledge in the GRNs inference is presented, too."
"The use of computer-based clinical decision support (CDS) tools is growing significantly in recent years. These tools help reduce waiting lists, minimise patient risks and, at the same time, optimise the cost health resources. In this paper, we present a CDS application that predicts the probability of having unstable angina based on clinical data. Due to the characteristics of the variables (mostly binary) a Bayesian network model was chosen to support the system. Bayesian-network model was constructed using a population of 1164 patients, and subsequently was validated with a population of 103 patients. The validation results, with a negative predictive value (NPV) of 91%, demonstrate its applicability to help clinicians. The final model was implemented as a web application that is currently been validated by clinician specialists. (C) 2013 Elsevier Ltd. All rights reserved.",""
"In the aftermath of a CBRN incident, there is an urgent need to reconstruct events in order to bring the perpetrators to court and to take preventive actions for the future. The challenge is to discriminate, based on available information, between alternative scenarios. Forensic interpretation is used to evaluate to what extent results from the forensic investigation favor the prosecutors' or the defendants' arguments, using the framework of Bayesian hypothesis testing. Recently, several new scientific disciplines have been used in a forensic context. In the AniBioThreat project, the framework was applied to veterinary forensic pathology, tracing of pathogenic microorganisms, and forensic entomology. Forensic entomology is an important tool for estimating the postmortem interval in, for example, homicide investigations as a complement to more traditional methods. In this article we demonstrate the applicability of the Bayesian framework for evaluating entomological evidence in a forensic investigation through the analysis of a hypothetical scenario involving suspect movement of carcasses from a clandestine laboratory. Probabilities of different findings under the alternative hypotheses were estimated using a combination of statistical analysis of data, expert knowledge, and simulation, and entomological findings are used to update the beliefs about the prosecutors' and defendants' hypotheses and to calculate the value of evidence. The Bayesian framework proved useful for evaluating complex hypotheses using findings from several insect species, accounting for uncertainty about development rate, temperature, and precolonization. The applicability of the forensic statistic approach to evaluating forensic results from a CBRN incident is discussed.",""
"Processes in the healthcare domain are characterized by coarsely predefined recurring procedures that are flexibly adapted by the personnel to suite-specific situations. In this setting, a workflow management system that gives guidance and documents the personnel's actions can lead to a higher quality of care, fewer mistakes, and higher efficiency. However, most existing workflow management systems enforce rigid inflexible workflows and rely on direct manual input. Both are inadequate for healthcare processes. In particular, direct manual input is not possible in most cases since (1) it would distract the personnel even in critical situations and (2) it would violate fundamental hygiene principles by requiring disinfected doctors and nurses to touch input devices. The solution could be activity recognition systems that use sensor data (e. g., audio and acceleration data) to infer the current activities by the personnel and provide input to a workflow (e. g., informing it that a certain activity is finished now). However, state-of-the-art activity recognition technologies have difficulties in providing reliable information. We describe a comprehensive framework tailored for flexible human-centric healthcare processes that improves the reliability of activity recognition data. We present a set of mechanisms that exploit the application knowledge encoded in workflows in order to reduce the uncertainty of this data, thus enabling unobtrusive robust healthcare workflows. We evaluate our work based on a real-world case study and show that the robustness of unobtrusive healthcare workflows can be increased to an absolute value of up to 91% (compared to only 12% with a classical workflow system). This is a major breakthrough that paves the way towards future IT-enabled healthcare systems.",""
"The issue of missing data may arise for researchers who deal with data gathering problems. Bayesian networks are one of the proposed methods that have been recently used in missing data imputation. The main objective of this research is to improve the efficiency of the Bayesian networks in nonignorable missing imputation, by adding missing indicator nodes for incomplete variables and constructing an augmented Bayesian network. Also, to consider the effect of different kinds of missingness mechanism (ignorable and nonignorable) on the performance of imputation methods. Four methods of imputation: random overall hot-deck imputation, within-class random hot-deck imputation, imputation using Bayesian networks and imputation using presented augmented Bayesian networks are compared using two indices: (1) a distance function and (2)Minimum Kullback-Leibler index. Results indicate the high-quality of the methods based on Bayesian networks relative to other imputation methods.",""
"The current study investigates the degree to which preschoolers can engage in causal inferences in a blocking paradigm, a paradigm in which a cue is consistently linked with a target, either alone (A-T) or paired with another cue (AB-T). Unlike previous blocking studies with preschoolers, we manipulated the causal structure of the events without changing the specific contingencies. In particular, cues were said to be either potential causes (prediction condition), or they were said to be potential effects (diagnosis condition). The causally appropriate inference is to block the redundant cue B when it is a potential cause of the target, but not when it is a potential effect. Findings show a stark difference in performance between preschoolers and adults: While adults blocked the redundant cue only in the prediction condition, children blocked the redundant cue indiscriminately across both conditions. Therefore, children, but not adults, ignored the causal structure of the events. These findings challenge a developmental account that attributes sophisticated machinery of causal reasoning to young children.","The current study investigates the degree to which preschoolers can engage in causal inferences in a blocking paradigm, a paradigm in which a cue is consistently linked with a target, either alone (A-T) or paired with another cue (AB-T)."
"One of the principal motivations for the new paradigm in reasoning was a reaction to the old (binary truth functional) paradigm's inability to deal with everyday non-monotonic inference. Within the new paradigm the problem of non-monotonicity is recast as the problem of belief revision or dynamic inference; that is, what happens when the probability distribution over which inferences are made changes from Pr-0 to Pr-1. Non-monotonicity arises when the new distribution, conditional on new information, I, changes the relevant probabilities, so that Pr-0(x) Pr-1(x), i.e., Pr-0(x) Pr-0(x|I). In this paper we first introduce the general problem of dynamic inference. We then consider the specific problems for dynamic conditional inference, in particular for modus tollens (MT). We then turn to possible reactions to these problems, looking at Oaksford and Chater's (2007) learning approach and causal Bayes nets. We conclude that most of the recent research on the non-monotonic effects observed in casual conditional inference and the suppression effect require a dynamic approach. However, the rational constraints on the transition from Pr-0 to Pr-1, when Pr-0(x) Pr-0(x|I), remain unclear.","One of the principal motivations for the new paradigm in reasoning was a reaction to the old (binary truth functional) paradigm's inability to deal with everyday non-monotonic inference."
"Multivariate time series (MTS) data such as time course gene expression data in genomics are often collected to study the dynamic nature of the systems. These data provide important information about the causal dependency among a set of random variables. In this paper, we introduce a computationally efficient algorithm to learn directed acyclic graphs (DAGs) based on MTS data, focusing on learning the local structure of a given target variable. Our algorithm is based on learning all parents (P), all children (C) and some descendants (D) (PCD) iteratively, utilizing the time order of the variables to orient the edges. This time series PCD-PCD algorithm (tsPCD-PCD) extends the previous PCD-PCD algorithm to dependent observations and utilizes composite likelihood ratio tests (CLRTs) for testing the conditional independence. We present the asymptotic distribution of the CLRT statistic and show that the tsPCD-PCD is guaranteed to recover the true DAG structure when the faithfulness condition holds and the tests correctly reject the null hypotheses. Simulation studies show that the CLRTs are valid and perform well even when the sample sizes are small. In addition, the tsPCD-PCD algorithm outperforms the PCD-PCD algorithm in recovering the local graph structures. We illustrate the algorithm by analyzing a time course gene expression data related to mouse T-cell activation.",""
"Cancer, a class of diseases, characterized by abnormal cell growth, has one of the highest overall death rates world-wide. Its development has been linked to aberrant genetic and epigenetic events, affecting the regulation of key genes that control cellular mechanisms. However, a major issue in cancer research is the lack of precise information on tumour pathways; therefore, the delineation of these and of the processes underlying disease proliferation is an important area of investigation. A computational approach to modelling malignant system events can help to improve understanding likely \"triggers\", i.e. initiating abnormal micro-molecular signals that occur during cancer development. Here, we introduce a network-based model for genetic and epigenetic events observed at different stages of colon cancer, with a focus on the gene relationships and tumour pathways. Additionally, we describe a case study on tumour progression recorded for two gene networks on colon cancer, carcinoma in situ. Our results to date showed that tumour progression rate is higher for a small, closely-associated network of genes than for a larger, less-connected set; thus, disease development depends on assessment of network properties. The current work aims to provide improved insight on the way in which aberrant modifications characterize cancer initiation and progression. The framework dynamics are described in terms of interdependencies between three main layers: genetic and epigenetic events, gene relationships and cancer stage levels.",""
" Here, we describe and rigorously prove the functional properties of a spike-based processor that uses ISI distributions to perform probabilistic inference. The abstract processor architecture serves as a building block for more concrete, neural implementations of the belief-propagation (BP) algorithm in arbitrary graphical models (e.g., Bayesian networks and factor graphs). The distributed nature of graphical models matches well with the architectural and functional constraints imposed by biology. In our model, ISI distributions represent the BP messages exchanged between factor nodes, leading to the interpretation of a single spike as a random sample that follows such a distribution. We verify the abstract processor model by numerical simulation in full graphs, and demonstrate that it can be applied even in the presence of analog variables. As a particular example, we also show results of a concrete, neural implementation of the processor, although in principle our approach is more flexible and allows different neurobiological interpretations. Furthermore, electrophysiological data from area LIP during behavioral experiments are assessed in light of ISI coding, leading to concrete testable, quantitative predictions and a more accurate description of these data compared to hitherto existing models."," Here, we describe and rigorously prove the functional properties of a spike-based processor that uses ISI distributions to perform probabilistic inference."
"Mutual dependence between features plays an important role in the formulation. of classifiers, clustering and other machine intelligent techniques. In this study a novel measure of mutual information known as integration to segregation (I2S), explaining the relationship between the two features is proposed. Some important characteristics of the proposed measure was investigated and its performance in terms of class imbalance measures was compared. It was shown that I2S possesses the characteristics, which are useful in controlling overfitting problems. In structure learning techniques such as Bayesian belief networks, conventional measures of dependency relationship cope with the overfitting problem by restricting the number of parents for a node; however it is still not impressive because complete overfitting is not eliminated. In contrast, I2S is capable of significantly maximizing the discriminant function with a better control of overfitting in the formulation of structure learning.","of classifiers, clustering and other machine intelligent techniques."
"Histone modifications are known to play an important role in the regulation of transcription. While individual modifications have received much attention in genome-wide analyses, little is known about their relationships. Some authors have built Bayesian networks of modifications, however most often they have used discretized data, and relied on unrealistic assumptions such as the absence of feedback mechanisms or hidden confounding factors. Here, we propose to infer undirected networks based on partial correlations between histone modifications. Within the partial correlation framework, correlations among two variables are controlled for associations induced by the other variables. Partial correlation networks thus focus on direct associations of histone modifications. We apply this methodology to data in CD4+ cells. The resulting network is well supported by common knowledge. When pairs of modifications show a large difference between their correlation and their partial correlation, a potential confounding factor is identified and provided as explanation. Data from different cell types (IMR90, H1) is also exploited in the analysis to assess the stability of the networks. The results are remarkably similar across cell types. Based on this observation, the networks from the three cell types are integrated into a consensus network to increase robustness. The data and the results discussed in the manuscript can be found, together with code, on http://spcn.molgen.mpg.de/index.html.",""
"Genome-wide gene expression profiles accumulate at an alarming rate, how to integrate these expression profiles generated by different laboratories to reverse engineer the cellular regulatory network has been a major challenge. To automatically infer gene regulatory pathways from genome-wide mRNA expression profiles before and after genetic perturbations, we introduced a new Bayesian network algorithm: Deletion Mutant Bayesian Network (DM_BN). We applied DM_BN to the expression profiles of 544 yeast single or double deletion mutants of transcription factors, chromatin remodeling machinery components, protein kinases and phosphatases in S. cerevisiae. The network inferred by this method identified causal regulatory and non-causal concurrent interactions among these regulators (genetically perturbed genes) that are strongly supported by the experimental evidence, and generated many new testable hypotheses. Compared to networks reconstructed by routine similarity measures or by alternative Bayesian network algorithms, the network inferred by DM_BN excels in both precision and recall. To facilitate its application in other systems, we packaged the algorithm into a user-friendly analysis tool that can be downloaded at http://www.picb.ac.cn/hanlab/DM_BN.html.",""
"The paper proposes a methodology based on Bayesian Networks for identifying the power two wheeler (PTW) driving patterns that arise at the emergence of a critical incident based on high resolution driving data (100 Hz) from a naturalistic PTW driving experiment. The proposed methodology aims at identifying the prevailing PTW drivers' actions at the beginning and during critical incidents and associating the critical incidents to specific PTW driving patterns. Results using data from one PTW driver reveal three prevailing driving actions for describing the onset of an incident and an equal number of actions that a PTW driver executes during the course of an incident to avoid a crash. Furthermore, the proposed methodology efficiently relates the observed sets of actions with different types of incidents occurring during overtaking or due to the interactions of the rider with moving or stationary obstacles and the opposing traffic. The observed interrelations define several driving patterns that are characterized by different initial actions, as well as by different likelihood of sequential actions during the incident. The proposed modeling may have significant implications to the efficient and less time consuming analysis of the naturalist data, as well as to the development of custom made PTW driver assistance systems. (C) 2013 Elsevier Ltd. All rights reserved.",""
"Service-Oriented Communication (SOC) is a key research issue to enable media communications using the Service-Oriented Architecture (SOA). Motivated by the necessity to guarantee the service quality of our web-based multimedia conferencing system, we present a Comprehensively Context-Aware (CoCA) approach in this paper. One major problem in the existing end-to-end Quality of Service (QoS) management solutions is that they analyse and exploit the relationships between the QoS metrics and corresponding contexts in an isolated manner. In this paper, we propose a novel approach to leveraging such relationships in a comprehensive manner based on Bayesian networks and the fuzzy set theory. This approach includes three phases: 1) information feedback and training, 2) QoS-to-context mapping, and 3) optimal context adaption. We implement the proposed CoCA in the real multimedia conferencing system and compare its performance with the existing bandwidth aware and playback buffer aware schemes. Experimental results show that the proposed CoCA outperforms the competing approaches in improving the average video Peak Signal-to-Noise Ratio (PSNR). It also exhibits good performance in preventing the playback buffer starvation.",""
"The concept of ecosystem services is increasingly used as a support for natural resource management decisions. While the science for assessing ecosystem services is improving, appropriate methods to address uncertainties in a quantitative manner are missing. Ignoring parameter uncertainties, modeling uncertainties and uncertainties related to human-environment interactions can modify decisions and lead to overlooking important management possibilities. In this contribution, we present a new approach for mapping the uncertainties in the assessment of multiple ecosystem services. The spatially explicit risk approach links Bayesian networks to a Geographic Information System for forecasting the value of a bundle of ecosystem services and quantifies the uncertainties related to the outcomes in a spatially explicit manner. We demonstrate that mapping uncertainties in ecosystem services assessments provides key information for decision-makers seeking critical areas in the delivery of ecosystem services in a case study in the Swiss Alps. The results suggest that not only the total value of the bundle of ecosystem services is highly dependent on uncertainties, but the spatial pattern of the ecosystem services values changes substantially when considering uncertainties. This is particularly important for the long-term management of mountain forest ecosystems, which have long rotation stands and are highly sensitive to pressing climate and socio-economic changes. (C) 2012 Elsevier Ltd. All rights reserved.",""
"The discharging of a gun results in the formation of extremely small particles known as gunshot residues (GSR). These may be deposited on the skin and clothing of the shooter, on other persons present, and on nearby items or surfaces. Several factors and their complex interactions affect the number of detectable GSR particles, which can deeply influence the conclusions drawn from likelihood ratios or posterior probabilities for prosecution hypotheses of interest. We present Bayesian network models for casework examples and demonstrate that probabilistic quantification of GSR evidence can be very sensitive to the assumptions concerning the model structure, prior probabilities, and the likelihood components. This finding has considerable implications for the use of statistical quantification of GSR evidence in the legal process.",""
"This paper presents a new evolutionary dynamic optimization algorithm, holographic memory-based Bayesian optimization algorithm (HM-BOA), whose objective is to address the weaknesses of sequential memory-based dynamic optimization approaches. To this end, holographic associative neural memory is applied to one of the recent successful memory-based evolutionary methods, DBN-MBOA (memory-based BOA with dynamic Bayesian networks). Holographic memory is appropriate for encoding environmental changes since its stimulus and response data are represented by a vector of complex numbers such that the phase and the magnitude denote the information and its confidence level, respectively. In the learning process in HM-BOA, holographic memory is trained by probabilistic models at every environmental change. Its weight matrix contains abstract information obtained from previous changes and is used for constructing a new probabilistic model when the environment changes. The unique features of HM-BOA are: 1) the stored information can be generalized, and 2) a small amount of memory is required for storing the probabilistic models. Experimental results adduce grounds for its effectiveness especially in random environments.",""
"The authors propose and test a causal model theory of reasoning about conditional arguments with causal content. According to the theory, the acceptability of modus ponens (MP) and affirming the consequent (AC) reflect the conditional likelihood of causes and effects based on a probabilistic causal model of the scenario being judged. Acceptability of MP is a judgment of causal power, the probability that the antecedent cause is efficacious in bringing about the consequent effect. Acceptability of AC is a judgment of diagnostic strength, the probability of the antecedent cause given the consequent effect. The model proposes that acceptability judgments are derived from a causal Bayesian network with a common effect structure in which the probability of the consequent effect is a function of the antecedent cause, alternative causes, and disabling conditions. In 2 experiments, the model was tested by collecting judgments of the causal parameters of conditionals and using them to derive predictions for MP and AC acceptability using 0 free parameters. To assess the validity of the model, its predictions were fit to the acceptability ratings and compared to the fits of 3 versions of Mental Models Theory. The fits of the causal model theory were superior. Experiment 3 provides direct evidence that people engage in a causal analysis and not a direct calculation of conditional probability when assessing causal conditionals. The causal model theory represents a synthesis across the disparate literatures on deductive, probabilistic, and causal reasoning.",""
"We present a Bayesian network (BN) model for forecasting Association Football match outcomes. Both objective and subjective information are considered for prediction, and we demonstrate how probabilities transform at each level of model component, whereby predictive distributions follow hierarchical levels of Bayesian inference. The model was used to generate forecasts for each match of the 2011/2012 English Premier League (EPL) season, and forecasts were published online prior to the start of each match. Profitability, risk and uncertainty are evaluated by considering various unit-based betting procedures against published market odds. Compared to a previously published successful BN model, the model presented in this paper is less complex and is able to generate even more profitable returns. (C) 2013 The Authors. Published by Elsevier B.V. All rights reserved.","Both objective and subjective information are considered for prediction, and we demonstrate how probabilities transform at each level of model component, whereby predictive distributions follow hierarchical levels of Bayesian inference."
"Knowledge about complex events is usually incomplete in practice. We distinguish between random variables that can be assigned a designated marker to model missing data values, and certain random variables to which the designated marker cannot be assigned. The ability to specify an arbitrary set of certain random variables provides an effective mechanism to control the uncertainty in form of missing data values. A finite axiomatization for the implication problem of saturated conditional independence statements is established under controlled uncertainty, relative to discrete probability measures. The completeness proof utilizes special probability models where two assignments have probability one half. The special probability models enable us to establish an equivalence between the implication problem and that of a propositional fragment in Cadoli and Schaerf's S-3 logic. Here, the propositional variables in S correspond to the random variables specified to be certain. The duality leads to an almost linear time algorithm to decide implication. It is shown that this duality cannot be extended to cover general conditional independence statements. All results subsume classical reasoning about saturated conditional independence statements as the idealized special case where every random variable is certain. Under controlled uncertainty, certain random variables allow us to soundly approximate classical reasoning about saturated conditional independence statements. (C) 2013 Elsevier B.V. All rights reserved.",""
"Substantial economic losses, building damage, and loss of life have been caused by secondary disasters that result from strong earthquakes. Earthquake disaster chains occur when secondary disasters take place in sequence. In this paper, we summarize 23 common earthquake disaster chains, whose structures include the serial, parallel, and parallel-serial (dendroid disaster chain) types. Evaluating the probability of powerful earthquake disaster chains is urgently needed for effective disaster prediction and emergency management. To this end, we introduce Bayesian networks (BNs) to assess powerful earthquake disaster chains. The structural graph of a powerful earthquake disaster chain is presented, and the proposed BN modeling method is provided and discussed. BN model of the earthquake-landslides-barrier lakes-floods disaster chain is established. The use of BN shows that such a model enables the effective analysis of earthquake disaster chains. Probability inference reveals that population density, loose debris volume, flooded areas, and landslide dam stability are the most critical links that lead to loss of life in earthquake disaster chains.","Probability inference reveals that population density, loose debris volume, flooded areas, and landslide dam stability are the most critical links that lead to loss of life in earthquake disaster chains."
"The design and implementation of effective environmental policies need to be informed by a holistic understanding of the system processes (biophysical, social and economic), their complex interactions, and how they respond to various changes. Models, integrating different system processes into a unified framework, are seen as useful tools to help analyse alternatives with stakeholders, assess their outcomes, and communicate results in a transparent way. This paper reviews five common approaches or model types that have the capacity to integrate knowledge by developing models that can accommodate multiple issues, values, scales and uncertainty considerations, as well as facilitate stakeholder engagement. The approaches considered are: systems dynamics, Bayesian networks, coupled component models, agent-based models and knowledge-based models (also referred to as expert systems). We start by discussing several considerations in model development, such as the purpose of model building, the availability of qualitative versus quantitative data for model specification, the level of spatio-temporal detail required, and treatment of uncertainty. These considerations and a review of applications are then used to develop a framework that aims to assist modellers and model users in the choice of an appropriate modelling approach for their integrated assessment applications and that enables more effective learning in interdisciplinary settings. (c) 2013 Elsevier Ltd. All rights reserved.",""
"Bayesian networks are knowledge representation tools that model the (in)dependency relationships among variables for probabilistic reasoning. Classification with Bayesian networks aims to compute the class with the highest probability given a case. This special kind is referred to as Bayesian network classifiers. Since learning the Bayesian network structure from a dataset can be viewed as an optimization problem, heuristic search algorithms may be applied to build high-quality networks in medium- or large-scale problems, as exhaustive search is often feasible only for small problems. In this paper, we present our new algorithm, ABC-Miner, and propose several extensions to it. ABC-Miner uses ant colony optimization for learning the structure of Bayesian network classifiers. We report extended computational results comparing the performance of our algorithm with eight other classification algorithms, namely six variations of well-known Bayesian network classifiers, cAnt-Miner for discovering classification rules and a support vector machine algorithm.","Classification with Bayesian networks aims to compute the class with the highest probability given a case."
"Symbolic pitch modeling is a way of incorporating knowledge about relations between pitches into the process of analyzing musical information or signals. In this paper, we propose a family of probabilistic symbolic polyphonic pitch models, which account for both the \"horizontal\" and the \"vertical\" pitch structure. These models are formulated as linear or log-linear interpolations of up to five sub-models, each of which is responsible for modeling a different type of relation. The ability of the models to predict symbolic pitch data is evaluated in terms of their cross-entropy, and of a newly proposed \"contextual cross-entropy\" measure. Their performance is then measured on synthesized polyphonic audio signals in terms of the accuracy of multiple pitch estimation in combination with a Nonnegative Matrix Factorization-based acoustic model. In both experiments, the log-linear combination of at least one \"vertical\" (e.g., harmony) and one \"horizontal\" (e.g., note duration) sub-model outperformed a pitch-dependent Bernoulli prior by more than 60% in relative cross-entropy and 3% in absolute multiple pitch estimation accuracy. This work provides a proof of concept of the usefulness of model interpolation, which may be used for improved symbolic modeling of other aspects of music in the future.",""
"In many domains where experts are the main source of knowledge, e.g., in reliability and risk management, a framework well suited for modeling, maintenance and exploitation of complex probabilistic systems is essential. In these domains, models usually define closed-world systems and result from the aggregation of multiple patterns repeated many times. Object Oriented-based Frameworks such as Probabilistic Relational Models (PRM) thus offer an effective way to represent such systems. They define patterns as classes and substitute large Bayesian networks (BN) by graphs of instances of these classes. In this framework, Structured Inference avoids many computation redundancies by exploiting class knowledge, hence reducing BN inference times by orders of magnitude. However, to keep modeling and maintenance costs low, object oriented-based framework's classes often encode only generic situations. More complex situations, even those repeated many times, are only represented by combinations of instances. In this paper, we propose to determine online such combination patterns and exploit them as classes to speed-up Structured Inference. We prove that determining an optimal set of patterns is NP-hard. We also provide an efficient algorithm to approximate this set and show numerical experiments that highlight its practical efficiency. (C) 2013 Elsevier Inc. All rights reserved.","In this framework, Structured Inference avoids many computation redundancies by exploiting class knowledge, hence reducing BN inference times by orders of magnitude."
"In this paper, we address the problem of complex object tracking using the particle filter framework, which essentially amounts to estimate high-dimensional distributions by a sequential Monte Carlo algorithm. For this purpose, we first exploit dynamic Bayesian networks to determine conditionally independent subspaces of the object's state space, which allows us to independently perform the particle filter's propagations and corrections over small spaces. Second, we propose a swapping process to transform the weighted particle set provided by the update step of the particle filter into a \"new particle set\" better focusing on high peaks of the posterior distribution. This new methodology, called Swapping-Based Partitioned Sampling, is proved to be mathematically sound and is successfully tested and validated on synthetic video sequences for single or multiple articulated object tracking. (C) 2013 Elsevier Inc. All rights reserved.",""
"As mobile game business becomes one of the most lucrative as well as fast-growing businesses, examining key success factors in this industry is of great interest. Utilizing a research method called Bayesian network, this paper models and tests interrelationship among product, marketing, consumer and competition variables. The current study surveys experts who launch many games in Korea. The three most crucial factors for successful games turn out to be targeting, awareness and consumers' willingness to pay (WTP). Many of the other factors influence the performance of games via these three factors. This paper not only investigates into the sensitivity of game performance to targeting and awareness levels but also examines the influences of product/marketing variables on consumers' first impression or willingness to pay. The findings on the roles of product or marketing factors that affect consumers' perceptions and responses, thereby competitiveness and success, will help game makers and distributors make reasonable decisions in allocating corporate resources more efficiently. (C) 2012 Elsevier Inc. All rights reserved.",""
"Software projects estimations are a crucial component of successful software development. There have been many approaches that deal with this problem by using different kinds of techniques. Most of the successful techniques rely on one shot prediction of some variables, as cost, quality or risk, taking into account some metrics. However, these techniques usually are not able to deal with uncertainty on the data, the relationships among metrics or the temporal aspect of projects. During the last decade, some researchers have proposed the use of Bayesian Belief Networks (BBNs) to perform better estimations, by explicitly taking into account the previous shortcomings. But, these approaches were based on manually defining those BBNs and handling only one of the estimation variables (cost, quality or risk). In this paper, we present an approach for semi-automatically building BBNs by using machine learning techniques. We describe two algorithms to generate such BBNs. The first one generates one-shot BBNs, while the second one generates BBNs that take into account the temporal aspect of project development. We performed experiments on real data coming from two software companies, obtaining a 63% of accuracy on multi-class classification. Our main interest was to find a semantically correct model that can be trained with future projects to increase its accuracy. In this sense, we introduce a well-balanced approach to make good predictions with strong explanatory power.","We performed experiments on real data coming from two software companies, obtaining a 63% of accuracy on multi-class classification."
"This paper presents a novel and systemic decision support model based on Bayesian Networks (BN) for safety control in dynamic complex project environments, which should go through the following three sections. At first, priori expert knowledge is integrated with training data in model design, aiming to improve the adaptability and practicability of model outcome. Then two indicators, Model Bias and Model Accuracy, are proposed to assess the effectiveness of BN in model validation, ensuring the model predictions are not significantly different from the actual observations. Finally we extend the safety control process to the entire life cycle of risk-prone events in model application, rather than restricted to pre-accident control, but during-construction continuous and post-accident control are included. Adapting its reasoning features, including forward reasoning, importance analysis and background reasoning, decision makers are provided with systematic and effective support for safety control in the overall work process. A frequent safety problem, ground settlement during Wuhan Changjiang Metro Shield Tunnel Construction (WCMSTC), is taken as a case study. Results demonstrate the feasibility of BN model, as well as its application potential. The proposed model can be used by practitioners in the industry as a decision support tool to increase the likelihood of a successful project in complex environments. (C) 2012 Elsevier Ltd. All rights reserved.",""
"Recently, mobile context inference becomes an important issue. Bayesian probabilistic model is one of the most popular probabilistic approaches for context inference. It efficiently represents and exploits the conditional independence of propositions. However, there are some limitations for probabilistic context inference in mobile devices. Mobile devices relatively lacks of sufficient memory. In this paper, we present a novel method for efficient Bayesian inference on a mobile phone. In order to overcome the constraints of the mobile environment, the method uses two-layered Bayesian networks with tree structure. In contrast to the conventional techniques, this method attempts to use probabilistic models with fixed tree structures and intermediate nodes. It can reduce the inference time by eliminating junction tree creation. To evaluate the performance of this method, an experiment is conducted with data collected over a month. The result shows the efficiency and effectiveness of the proposed method. (C) 2013 Elsevier Ltd. All rights reserved.","Recently, mobile context inference becomes an important issue."
"This paper constitutes a literature review on student modeling for the last decade. The review aims at answering three basic questions on student modeling: what to model, how and why. The prevailing student modeling approaches that have been used in the past 10 years are described, the aspects of students' characteristics that were taken into consideration are presented and how a student model can be used in order to provide adaptivity and personalisation in computer-based educational software is highlighted. This paper aims to provide important information to researchers, educators and software developers of computer-based educational software ranging from e-learning and mobile learning systems to educational games including stand alone educational applications and intelligent tutoring systems. In addition, this paper can be used as a guide for making decisions about the techniques that should be adopted when designing a student model for an adaptive tutoring system. One significant conclusion is that the most preferred technique for representing the student's mastery of knowledge is the overlay approach. Also, stereotyping seems to be ideal for modeling students' learning styles and preferences. Furthermore, affective student modeling has had a rapid growth over the past years, while it has been noticed an increase in the adoption of fuzzy techniques and Bayesian networks in order to deal the uncertainty of student modeling. (C) 2013 Elsevier Ltd. All rights reserved.",""
"Precipitation is one of the most important meteorological variables for defining the climate dynamics, but the spatial patterns of precipitation have not been fully investigated yet. The complex network theory, which provides a robust tool to investigate the statistical interdependence of many interacting elements, is used here to analyze the spatial dynamics of annual precipitation over seventy years (1941-2010). The precipitation network is built associating a node to a geographical region, which has a temporal distribution of precipitation, and identifying possible links among nodes through the correlation function. The precipitation network reveals significant spatial variability with barely connected regions, as Eastern China and Japan, and highly connected regions, such as the African Sahel, Eastern Australia and, to a lesser extent, Northern Europe. Sahel and Eastern Australia are remarkably dry regions, where low amounts of rainfall are uniformly distributed on continental scales and small-scale extreme events are rare. As a consequence, the precipitation gradient is low, making these regions well connected on a large spatial scale. On the contrary, the Asiatic South-East is often reached by extreme events such as monsoons, tropical cyclones and heat waves, which can all contribute to reduce the correlation to the short-range scale only. Some patterns emerging between mid-latitude and tropical regions suggest a possible impact of the propagation of planetary waves on precipitation at a global scale. Other links can be qualitatively associated to the atmospheric and oceanic circulation. To analyze the sensitivity of the network to the physical closeness of the nodes, short-term connections are broken. The African Sahel, Eastern Australia and Northern Europe regions again appear as the supernodes of the network, confirming furthermore their long-range connection structure. Almost all North-American and Asian nodes vanish, revealing that extreme events can enhance high precipitation gradients, leading to a systematic absence of long-range patterns.",""
"Bayesian Networks (BNs) are versatile probabilistic models applicable to many different biological phenomena. In biological applications the structure of the network is usually unknown and needs to be inferred from experimental data. BNFinder is a fast software implementation of an exact algorithm for finding the optimal structure of the network given a number of experimental observations. Its second version, presented in this article, represents a major improvement over the previous version. The improvements include (i) a parallelized learning algorithm leading to an order of magnitude speed-ups in BN structure learning time; (ii) inclusion of an additional scoring function based on mutual information criteria; (iii) possibility of choosing the resulting network specificity based on statistical criteria and (iv) a new module for classification by BNs, including cross-validation scheme and classifier quality measurements with receiver operator characteristic scores.","The improvements include (i) a parallelized learning algorithm leading to an order of magnitude speed-ups in BN structure learning time; (ii) inclusion of an additional scoring function based on mutual information criteria; (iii) possibility of choosing the resulting network specificity based on statistical criteria and (iv) a new module for classification by BNs, including cross-validation scheme and classifier quality measurements with receiver operator characteristic scores."
"We develop a new regression algorithm, cMIKANA, for inference of gene regulatory networks from combinations of steady-state and time-series gene expression data. Using simulated gene expression datasets to assess the accuracy of reconstructing gene regulatory networks, we show that steady-state and time-series data sets can successfully be combined to identify gene regulatory interactions using the new algorithm. Inferring gene networks from combined data sets was found to be advantageous when using noisy measurements collected with either lower sampling rates or a limited number of experimental replicates. We illustrate our method by applying it to a microarray gene expression dataset from human umbilical vein endothelial cells (HUVECs) which combines time series data from treatment with growth factor TNF and steady state data from siRNA knockdown treatments. Our results suggest that the combination of steady-state and time-series datasets may provide better prediction of RNA-to-RNA interactions, and may also reveal biological features that cannot be identified from dynamic or steady state information alone. Finally, we consider the experimental design of genomics experiments for gene regulatory network inference and show that network inference can be improved by incorporating steady-state measurements with time-series data.","We develop a new regression algorithm, cMIKANA, for inference of gene regulatory networks from combinations of steady-state and time-series gene expression data."
"Many interesting studies aimed at elucidating the connectivity structure of biomolecular pathways make use of abundance measurements, and employ statistical and information theoretic approaches to assess connectivities. These studies often do not address the effects of the dynamics of the underlying biological system, yet dynamics give rise to impactful issues such as timepoint selection and its effect on structure recovery. In this work, we study conditions for reliable retrieval of the connectivity structure of a dynamic system, and the impact of dynamics on structure-learning efforts. We encounter an unexpected problem not previously described in elucidating connectivity structure from dynamic systems, show how this confounds structure learning of the system and discuss possible approaches to overcome the confounding effect. Finally, we test our hypotheses on an accurate dynamic model of the IGF signalling pathway. We use two structure-learning methods at four time points to contrast the performance and robustness of those methods in terms of recovering correct connectivity.",""
"Genetic-based machine learning (GBML) systems, which employ evolutionary algorithms (EAs) as search mechanisms, evolve rule-based classification models to represent target concepts. Compared to Michigan-style GBML, Pittsburgh-style GBML is expected to achieve more compact solutions. It has been shown that standard recombination operators in EAs do not assure an effective evolutionary search to solve sophisticated problems that contain strong interactions between features. On the other hand, when dealing with real-world classification tasks, irrelevant features not only complicate the problem but also incur unnecessary matchings in GBML systems, which increase the computational cost a lot. To handle the two problems mentioned above in an integrated manner, a new Pittsburgh-style GBML system is proposed. In the proposed method, classifiers are generated and recombined at two levels. At the high level, classifiers are recombined by rule-wise uniform crossover operators since each classifier consists of a variable-size rule set. At the low level, single rules contained in classifiers are reproduced via sampling Bayesian networks that characterize the global statistical information extracted from promising rules found so far. Furthermore, according to the statistical information in the rule population, an embedded approach is presented to detect and remove redundant features incrementally following the evolution of rule population. Results of empirical evaluation show that the proposed method outperforms the original Pittsburgh-style GBML system in terms of classification accuracy while reducing the computational cost. Furthermore, the proposed method is also competitive to other non-evolutionary, highly used machine learning methods. With respect to the performance of feature reduction, the proposed embedded approach is able to deliver solutions with higher classification accuracy when removing the same number of features as other feature reduction techniques do. (C) 2013 Elsevier B.V. All rights reserved.","Genetic-based machine learning (GBML) systems, which employ evolutionary algorithms (EAs) as search mechanisms, evolve rule-based classification models to represent target concepts."
"Protein structure prediction methods typically use statistical potentials, which rely on statistics derived from a database of know protein structures. In the vast majority of cases, these potentials involve pairwise distances or contacts between amino acids or atoms. Although some potentials beyond pairwise interactions have been described, the formulation of a general multibody potential is seen as intractable due to the perceived limited amount of data. In this article, we show that it is possible to formulate a probabilistic model of higher order interactions in proteins, without arbitrarily limiting the number of contacts. The success of this approach is based on replacing a naive table-based approach with a simple hierarchical model involving suitable probability distributions and conditional independence assumptions. The model captures the joint probability distribution of an amino acid and its neighbors, local structure and solvent exposure. We show that this model can be used to approximate the conditional probability distribution of an amino acid sequence given a structure using a pseudo-likelihood approach. We verify the model by decoy recognition and site-specific amino acid predictions. Our coarse-grained model is compared to state-of-art methods that use full atomic detail. This article illustrates how the use of simple probabilistic models can lead to new opportunities in the treatment of nonlocal interactions in knowledge-based protein structure prediction and design. Proteins 2013; 81:1340-1350. (c) 2013 Wiley Periodicals, Inc.",""
"Graphical models are popular statistical tools which are used to represent dependent or causal complex systems. Statistically equivalent causal or directed graphical models are said to belong to a Markov equivalent class. It is of great interest to describe and understand the space of such classes. However, with currently known algorithms, sampling over such classes is only feasible for graphs with fewer than approximately 20 vertices. In this paper, we design reversible irreducible Markov chains on the space of Markov equivalent classes by proposing a perfect set of operators that determine the transitions of the Markov chain. The stationary distribution of a proposed Markov chain has a closed form and can be computed easily. Specifically, we construct a concrete perfect set of operators on sparse Markov equivalence classes by introducing appropriate conditions on each possible operator. Algorithms and their accelerated versions are provided to efficiently generate Markov chains and to explore properties of Markov equivalence classes of sparse directed acyclic graphs (DAGs) with thousands of vertices. We find experimentally that in most Markov equivalence classes of sparse DAGs, (1) most edges are directed, (2) most undirected subgraphs are small and (3) the number of these undirected subgraphs grows approximately linearly with the number of vertices.",""
"Given n random variables and a set of m observations of each of the n variables, the Bayesian network structure learning problem is to learn a directed acyclic graph (DAG) on the n variables such that the implied joint probability distribution best explains the set of observations. Bayesian networks are widely used in many fields including data mining and computational biology. Globally optimal (exact) structure learning of Bayesian networks takes O(n(2).2(n)) time plus the cost of O(n.2(n)) evaluations of an application-specific scoring function whose run-time is at least linear in m. In this paper, we present a parallel algorithm for exact structure learning of a Bayesian network that is communication-efficient and workoptimal up to O(1/n.2(n)) processors. We further extend this algorithm to the important restricted case of structure learning with bounded node in-degree and investigate the performance gains achievable because of limiting node in-degree. We demonstrate the applicability of our method by implementation on an IBM Blue Gene/P system and an AMD Opteron InfiniBand cluster and present experimental results that characterize run-time behavior with respect to the number of variables, number of observations, and the bound on in-degree. (C) 2013 Elsevier Inc. All rights reserved.",""
"A stereoscopic visual fatigue measurement model based on Bayesian networks (BNs) is presented. Our approach focuses on the inter-dependencies between factors, such as contextual and environmental, and the phenomena of visual fatigue in stereoscopy. Specifically, the implementation of BN with the use of multiple features provides a systematic way to project and evaluate visual fatigue. Compared with another measurement model, our present BN-based scheme is more comprehensive. The test validation also indicates that our proposed model can be used as a reliable method for the visual fatigue inferring in stereoscopy. (C) The Authors. Published by SPIE under a Creative Commons Attribution 3.0 Unported License. Distribution or reproduction of this work in whole or in part requires full attribution of the original publication, including its DOI.",""
"Currently, fault diagnosis of reservoir facilities relies mostly on check-list evaluation. The results and qualities of evaluation are limited by experience and ability of the evaluators, which may not achieve the goal of systematic assessment in a consistent manner. To overcome the limitation of the traditional approach, this research develops a fault diagnosis and evaluation system for reservoir facility by utilizing multi-state Fault-Tree Analysis (FTA) technique, in conjunction with Bayesian Networks (BN) which incorporate expert experiences through lateral linkages among BN nodes and weighting factors. The system has been used to analyze and verify against three hydro-power systems currently in operation. It was found that through BN analysis the fault trend is consistent to that from historical data analysis via Weibull distribution. This indicates that the transformation of a multi-state Fault-Tree (FT) and BN is reasonable and practical. Based upon the analysis of BN by inputting prior information of the hydro-power systems, the probabilities of fault occurrences are effectively computed based on which proper preventive maintenance strategies can be established.",""
"Estimation of distribution algorithms (EDAs) constitute a new branch of evolutionary optimization algorithms that were developed as a natural alternative to genetic algorithms (GAs). Several studies have demonstrated that the heuristic scheme of EDAs is effective and efficient for many optimization problems. Recently, it has been reported that the incorporation of mutation into EDAs increases the diversity of genetic information in the population, thereby avoiding premature convergence into a suboptimal solution. In this study, we propose a new mutation operator, a transpose mutation, designed for Bayesian structure learning. It enhances the diversity of the offspring and it increases the possibility of inferring the correct arc direction by considering the arc directions in candidate solutions as bi-directional, using the matrix transpose operator. As compared to the conventional EDAs, the transpose mutation-adopted EDAs are superior and effective algorithms for learning Bayesian networks.",""
"Structural redundancy elimination in case resource pools (CRP) is critical for avoiding performance bottlenecks and maintaining robust decision capabilities in cloud computing services. For these purposes, this paper proposes a novel approach to ensure redundancy elimination of a reasoning system in CRP. By using alpha entropy and mutual information, functional measures to eliminate redundancy of a system are developed with respect to a set of outputs. These measures help to distinguish both the optimal feature and the relations among the nodes in reasoning networks from the redundant ones with the elimination criterion. Based on the optimal feature and its harmonic weight, a model for knowledge reasoning in CRP (CRPKR) is built to complete the task of query matching, and the missing values are estimated with Bayesian networks. Moreover, the robustness of decisions is verified through parameter analyses. This approach is validated by the simulation with benchmark data sets using cloud SQL. Compared with several state-of-the-art techniques, the results show that the proposed approach has a good performance and boosts the robustness of decisions.",""
"The success of combination antiretroviral therapy is limited by the evolutionary escape dynamics of HIV-1. We used Isotonic Conjunctive Bayesian Networks (I-CBNs), a class of probabilistic graphical models, to describe this process. We employed partial order constraints among viral resistance mutations, which give rise to a limited set of mutational pathways, and we modeled phenotypic drug resistance as monotonically increasing along any escape pathway. Using this model, the individualized genetic barrier (IGB) to each drug is derived as the probability of the virus not acquiring additional mutations that confer resistance. Drug-specific IGBs were combined to obtain the IGB to an entire regimen, which quantifies the virus' genetic potential for developing drug resistance under combination therapy. The IGB was tested as a predictor of therapeutic outcome using between 2,185 and 2,631 treatment change episodes of subtype B infected patients from the Swiss HIV Cohort Study Database, a large observational cohort. Using logistic regression, significant univariate predictors included most of the 18 drugs and single-drug IGBs, the IGB to the entire regimen, the expert rules-based genotypic susceptibility score (GSS), several individual mutations, and the peak viral load before treatment change. In the multivariate analysis, the only genotype-derived variables that remained significantly associated with virological success were GSS and, with 10-fold stronger association, IGB to regimen. When predicting suppression of viral load below 400 cps/ml, IGB outperformed GSS and also improved GSS-containing predictors significantly, but the difference was not significant for suppression below 50 cps/ml. Thus, the IGB to regimen is a novel data-derived predictor of treatment outcome that has potential to improve the interpretation of genotypic drug resistance tests.","Using logistic regression, significant univariate predictors included most of the 18 drugs and single-drug IGBs, the IGB to the entire regimen, the expert rules-based genotypic susceptibility score (GSS), several individual mutations, and the peak viral load before treatment change."
"It is known that multiple histone modifications act in a combinatorial fashion to form a 'histone code'. In this study, the pathways within Bayesian networks of histone modifications constructed indicated the combinatorial and causative relationships among 12 histone modifications studied at the region TSS + 1 kb in S. cerevisiae. After Bayesian networks for the genes with high transcript levels and low transcript levels were constructed, all combinations of histone modifications that the two networks indicated were analyzed. The results showed that four combinations were necessary for gene transcription. Other combinations played different roles in the regulation of gene transcription. In addition, our analysis suggested that the combinations in the top layer within the networks might affect the downstream histone modifications and associate with Pol II.",""
"A method of calculating the top event probability of a fault tree, where dynamic gates and repeated events are included and the occurrences of basic events follow nonexponential distributions, is proposed. The method is on the basis of the Bayesian network formulation for a DFT proposed by Yuge and Yanagi [1]. The formulation had a difficulty in calculating a sequence probability if components have nonexponential failure distributions. We propose an alternative method to obtain the sequence probability in this paper. First, a method in the case of the Erlang distribution is discussed. Then, Tijms's fitting procedure is applied to deal with a general distribution. The procedure gives a mixture of two Erlang distributions as an approximate distribution for a general distribution given the mean and standard deviation. A numerical example shows that our method works well for complex systems.",""
"Reliability analysis has become an integral part of system design and operation. This is especially true for systems performing critical tasks, such as mass transportation systems. This explains the numerous advances in the field of reliability modeling. More recently, some studies involving the use of Bayesian networks have been proven relevant to represent complex systems and perform reliability studies. In previous works, a generic methodology was introduced for developing a decision support tool to evaluate complex systems maintenance strategies. This article deals with development of such a decision tool dedicated to the maintenance of Paris metro rails. Indeed, owing to fulfillment of high-performance levels of safety and availability (the latter being especially critical at peak hours), operators need to estimate, hour by hour their ability to prevent or to detect broken rails. To address this problem, a decision support tool was developed, the aim of this article is to evaluate, compare and optimize various operating and maintenance strategies.",""
"We evaluated the effects of biophysical conditions and hatchery production on the early marine survival of coho salmon Oncorhynchus kisutch in the Strait of Georgia, British Columbia, Canada. Due to a paucity of balanced multivariate ecosystem data, we developed a probabilistic network that integrated physical and ecological data and information from literature, expert opinion, oceanographic models, and in situ observations. This approach allowed us to evaluate alternate hypotheses about drivers of early marine survival while accounting for uncertainties in relationships among variables. Probabilistic networks allow users to explore multiple environmental settings and evaluate the consequences of management decisions under current and projected future states. We found that the zooplankton biomass anomaly, calanoid copepod biomass, and herring biomass were the best indicators of early marine survival. It also appears that concentrating hatchery supplementation during periods of negative PDO and ENSO (Pacific Decadal and El Nino Southern Oscillation respectively), indicative of generally favorable ocean conditions for salmon, tends to increase survival of hatchery coho salmon while minimizing negative impacts on the survival of wild juveniles. Scientists and managers can benefit from the approach presented here by exploring multiple scenarios, providing a basis for open and repeatable ecosystem-based risk assessments when data are limited. (c) 2013 Elsevier Ltd. All rights reserved.",""
"Judea Pearl won the 2010 Rumelhart Prize in computational cognitive science due to his seminal contributions to the development of Bayes nets and causal Bayes nets, frameworks that are central to multiple domains of the computational study of mind. At the heart of the causal Bayes nets formalism is the notion of a counterfactual, a representation of something false or nonexistent. Pearl refers to Bayes nets as oracles for intervention, and interventions can tell us what the effect of action will be or what the effect of counterfactual possibilities would be. Counterfactuals turn out to be necessary to understand thought, perception, and language. This selection of papers tells us why, sometimes in ways that support the Bayes net framework and sometimes in ways that challenge it.",""
"This article reports results from two studies of how people answer counterfactual questions about simple machines. Participants learned about devices that have a specific configuration of components, and they answered questions of the form If component X had not operated [failed], would component Y have operated? The data from these studies indicate that participants were sensitive to the way in which the antecedent state is describedwhether component X had not operated or had failed. Answers also depended on whether the device is deterministic or probabilisticwhether X's causal parents always or only usually cause X to operate. Participants' explanations of their answers often invoked non-operation of causally prior components or unreliability of prior connections. They less often mentioned independence from these causal elements.",""
"Proteins are composed by amino acids, which are created by genes. To understand how different genes interact to create different proteins, we need to model the gene regulatory networks (GRNs) of different organisms. There are different models that attempt to model GRNs. In this paper, we use the popular S-System to model small networks. This model has been solved with different evolutionary computation techniques, which have obtained good results; yet, there are no models that achieve a perfect reconstruction of the network. We implement a variation of particle swarm optimization (PSO), called dissipative PSO (DPSO), to optimize the model; we also research the use of an L1 regularizer and compare it with other evolutionary computing approaches. To the best of our knowledge, neither the DPSO nor L1 optimizer has been jointly used to solve the S-System. We find that the combination of S-System and DPSO offers advantages over previously used methods, and presents promising results for inferencing larger and more complex networks.",""
"1. Ecological communities are composed of populations connected in tangled networks of ecological interactions. Therefore, the extinction of a species can reverberate through the network and cause other (possibly distantly connected) species to go extinct as well. The study of these secondary extinctions is a fertile area of research in ecological network theory. 2. However, to facilitate practical applications, several improvements to the current analytical approaches are needed. In particular, we need to consider that (i) species have different a priori' probabilities of extinction, (ii) disturbances can simultaneously affect several species, and (iii) extinction risk of consumers likely grows with resource loss. All these points can be included in dynamical models, which are, however, difficult to parameterize. 3. Here we advance the study of secondary extinctions with Bayesian networks. We show how this approach can account for different extinction responses using binary - where each resource has the same importance - and quantitative data - where resources are weighted by their importance. We simulate ecological networks using a popular dynamical model (the Allometric Trophic Network model) and use it to test our method. 4. We find that the Bayesian network model captures the majority of the secondary extinctions produced by the dynamical model and that consumers' responses to species loss are best modelled using a nonlinear sigmoid function. We also show that an approach based exclusively on food web structure loses power when species at higher trophic levels are preferentially lost. Because the loss of apex predators is unfortunately widespread, the results highlight a serious limitation of studies on network robustness.",""
"Natural hazards such as earthquakes threaten millions of people all around the world. In a few decades, most of these people will live in fast-growing, inter-connected urban environments. Assessing risk will, therefore, be an increasingly difficult task that will require new, multidisciplinary approaches to be tackled properly. We propose a novel approach based on different imaging technologies and a Bayesian information integration scheme to characterize exposure and vulnerability models, which are among the key components of seismic risk assessment.",""
"SeTES is a self-teaching expert system that (a) can incorporate evolving databases involving any type and amount of relevant data (geological, geophysical, geomechanical, stimulation, petrophysical, reservoir, production, etc.) originating from unconventional gas reservoirs, i.e., tight sands, shale or coalbeds, (b) can continuously update its built-in public database and refine the its underlying decision-making metrics and process, (c) can make recommendations about well stimulation, well location, orientation, design, and operation, (d) offers predictions of the performance of proposed wells (and quantitative estimates of the corresponding uncertainty), and (e) permits the analysis of data from installed wells for parameter estimation and continuous expansion of its database. Thus, SeTES integrates and processes information from multiple and diverse sources to make recommendations and support decision making at multiple time-scales, while expanding its internal database and explicitly addressing uncertainty. It receives and manages data in three forms: public data, that have been made available by various contributors, semi-public data, which conceal some identifying aspects but are available to compute important statistics, and a user's private data, which can be protected and used for more targeted design and decision making. It is the first implementation of a novel architecture that allows previously independent analysis methods and tools to share data, integrate results, and intelligently and iteratively extract the most value from the dataset. SeTES also presents a new paradigm for communicating research and technology to the public and distributing scientific tools and methods. It is expected to result in a significant improvement in reserve estimates, and increases in production by increasing efficiency and reducing uncertainty. (C) 2013 Elsevier Ltd. All rights reserved.",""
"Basin models are used to gain insights about a petroleum system, and to simulate geological processes required to form oil and gas accumulations. The focus of such simulations is usually on charge and timing-related issues, although uncertainty analysis about a wider range of parameters is becoming more common. Bayesian networks (BNs) are useful for decision making in geological prospect analysis and exploration. In this paper we propose a framework for merging these two methodologies: by doing so, we explicitly account for dependencies between the geological elements. The probabilistic description of the BN is trained by using multiple scenarios of Basin and Petroleum Systems Modelling (BPSM). A range of different input parameters are used for total organic content, heat flow, porosity and faulting to span a full categorical design for the BPSM scenarios. Given the consistent BN for trap, reservoir and source attributes, we demonstrate important decision-making applications, such as evidence propagation and the value of information.",""
"Inhaled corticosteroids (ICS) are the most commonly used controller medications prescribed for asthma. Two single-nucleotide polymorphisms (SNPs), rs1876828 in corticotrophin releasing hormone receptor 1 and rs37973 in GLCCI1, have previously been associated with corticosteroid efficacy. We studied data from four existing clinical trials of asthmatics, who received ICS and had lung function measured by forced expiratory volume in 1 s (FEV1) before and after the period of such treatment. We combined the two SNPs rs37973 and rs1876828 into a predictive test of FEV1 change using a Bayesian model, which identified patients with good or poor steroid response (highest or lowest quartile, respectively) with predictive performance of 65.7% (P=0.039 vs random) area under the receiver-operator characteristic curve in the training population and 65.9% (P=0.025 vs random) in the test population. These findings show that two genetic variants can be combined into a predictive test that achieves similar accuracy and superior replicability compared with single SNP predictors.",""
"The assessment of areas at risk from various soil threats is a key task within the proposed EU Soil Framework Directive. Such assessment is, however, hampered by the complex nature of the soil threats, which result from the sometimes poorly understood interaction of various soil physical properties, climatic factors and land management practices. Methodologies for risk assessment of soil threats are needed to protect the soil quality for future generations and to target resources to the areas at greatest risk. We present here a generic risk framework for the development of Bayesian Belief Networks (BBNs) to estimate the risk from soil threats. The generic BBN structure follows a standard risk assessment approach, where the risk is quantified by combining assessments of vulnerability and exposure. The soil's vulnerability to a given threat is determined from inherent soil and site characteristics as well as from climatic factors influencing soil characteristics, while the exposure estimate is based on an evaluation of the stresses inflicted by land management and climate. The generic framework is demonstrated by taking soil compaction as an example. Soil compaction is a major threat to soil function particularly in highly managed agricultural systems and is known to have many adverse effects on farming systems including decreased crop yield and soil productivity, increased management costs, increased emissions of greenhouse gases, and decreased water infiltration into the soil leading to accelerated run-off and risk of soil erosion. Existing modelling approaches to predict soil compaction risk either require data on soil mechanical behaviour that are difficult and expensive to collect, or are expert-based systems that are highly subjective and sometimes cannot accommodate the myriad of processes underlying compaction risk. Using the generic framework, a detailed BBN for assessing the risk of soil compaction is developed. The BBN allows for combining available data from standard soil surveys and land use databases with qualitative expert knowledge and explicitly accounts for uncertainties in the assessment of the risk. The BBN is applied to identify the distribution of the compaction risk across Scotland using data from the National Soils Inventory of Scotland. (C) 2013 Elsevier B.V. All rights reserved.",""
"Bayesian Networks (BNs) have received significant attention in various academic and industrial applications, such as modeling knowledge in image processing, engineering, medicine and bio-informatics. Preserving the privacy of sensitive data, owned by different parties, is often a critical issue. However, in many practical applications, BNs must train from data that gradually becomes available at different period of times, on which the traditional batch learning algorithms are not suitable or applicable. In this paper, an algorithm based on a new and efficient version of Sufficient Statistics is proposed for incremental learning with BNs. The standard kappa 2 algorithm is also modified to be utilized inside the incremental learning algorithm. Next, some secure building blocks such as secure comparison, and factorial, which are resistant against colluding attacks and could be applied securely over public channels like internet, are presented to be used inside the main protocol. Then a privacy-preserving protocol is proposed for incremental learning of BNs, in which the structure and probabilities are estimated incrementally from homogeneously distributed and gradually available data among two or multi-parties. Finally, security and complexity analysis along with the experimental results are presented to compare with the batch algorithm and to show its performance and applicability in real world applications. (C) 2013 Elsevier B. V. All rights reserved.",""
"A wide range of quantitative and qualitative modelling research on ecosystem services (ESS) has recently been conducted. The available models range between elementary, indicator-based models and complex process-based systems. A semi-quantitative modelling approach that has recently gained importance in ecological modelling is Bayesian belief networks (BBNs). Due to their high transparency, the possibility to combine empirical data with expert knowledge and their explicit treatment of uncertainties, BBNs can make a considerable contribution to the ESS modelling research. However, the number of applications of BBNs in ESS modelling is still limited. This review discusses a number of BBN-based ESS models developed in the last decade. A SWOT analysis highlights the advantages and disadvantages of BBNs in ESS modelling and pinpoints remaining challenges for future research. The existing BBN models are suited to describe, analyse, predict and value ESS. Nevertheless, some weaknesses have to be considered, including poor flexibility of frequently applied software packages, difficulties in eliciting expert knowledge and the inability to model feedback loops. (c) 2013 Elsevier Ltd. All rights reserved.",""
"In many arid and semi-arid regions agriculture is the main user of GW, causing problems with the quantity and quality of water, but there are few institutional policies and regulations governing sustainable GW exploitation. The authors suggest an integrated methodology for enabling local GW management, capable of combining the need for GW protection with socio-economic and behavioural determinants of GW use. In the proposed tool, integration is reinforced by the inclusion of multiple stakeholders, and the use of Bayesian Belief Networks (BBN) to simulate and explore these stakeholders' attitude to GW exploitation and their responses to the introduction of new protection policies. BBNs and hydrological system properties are integrated in a GIS-based decision support system - GeSAP - which can elaborate and analyse scenarios concerning the pressure on GW due to exploitation for irrigation, and the effectiveness of protection policies, taking into account the level of consensus. In addition, the GIS interface makes it possible to spatialize the information and to investigate model results. The paper presents the results of an experimental application of the GeSAP tool to support GW planning and management in the Apulia Region (Southern Italy). To evaluate the actual usability of the GeSAP tool, case study applications were performed involving the main experts in GW protection and the regional decision-makers. Results showed that GeSAP can simulate farmers' behaviour concerning the selection of water sources for irrigation, allowing evaluation of the effectiveness of a wide range of strategies which impact water demand and consumption. (c) 2013 Elsevier Ltd. All rights reserved.",""
"Occupational stress is a major health hazard and a serious challenge to the effective operation of any company and represents a major problem for both individuals and organizations. Previous researches have shown that high demands (e.g. workload, emotional) combined with low resources (e.g. support, control, rewards) are associated with adverse health (e.g. psychological, physical) and organizational impacts (e.g. reduced job satisfaction, sickness absence). The objective of the present work is to create a model to analyze how social support reduces the occupational stress caused by work demands. This study used existing Spanish national data on working conditions collected by the Spanish Ministry of Labour and Immigration in 2007, where 11,054 workers were interviewed by questionnaire. A probabilistic model was built using Bayesian networks to explain the relationships between work demands and occupational stress. The model also explains how social support contributes positively to reducing stress levels. The variables studied were intellectually demanding work, overwork, workday, stress, and social support. The results show the importance of social support and of receiving help from supervisors and co-workers in preventing occupational stress. The study provides a new methodology that explains and quantifies the effects of intellectually demanding work, overwork, and workday in occupational stress. Also, the study quantifies the importance of social support to reduce occupational stress. (C) 2013 Elsevier Ltd. All rights reserved.",""
"To achieve rapid and precise diagnosis of crop diseases, an active and dynamic method of diagnosis of crop diseases is needed and such a method is proposed in this paper. This method adopts Bayesian networks to represent the relationships among the symptoms and crop diseases. This method has two main differences from the existing diagnosis methods. First, it does not use all the symptoms in the diagnosis, but purposively selects a subset of symptoms which are the most relevant to diagnosis; the active symptom selection is based on the concept of a Markov blanket in a Bayesian network. Second, a specific incremental learning algorithm for Bayesian networks is also proposed to make the diagnosis model update dynamically over time in order to adapt to temporal changes of environment. Furthermore, the diagnosis results can be calculated without inference in Bayesian networks, so the method has low time complexity. Theoretical analysis and experimental results demonstrate that the proposed method can significantly enhance the performance of crop disease diagnosis. (C) 2011 Elsevier Ltd. All rights reserved.","Furthermore, the diagnosis results can be calculated without inference in Bayesian networks, so the method has low time complexity."
"A probabilistic model for estimating tunnel construction time is learnt with data from past tunnel projects. The model is based on the Dynamic Bayesian Network technique. The model inputs are determined through an analysis of data from three tunnels built by means of the conventional tunneling method. The data motivate the development of a novel probability distribution to describe the excavation performance. In addition, the probability of construction failure events and the delay caused by such failures are estimated using databases available in the literature. The model is applied to a case study, in which it is demonstrated how observations from the tunnel construction process can be included to continuously update the prediction of construction time. (C) 2013 Elsevier Ltd. All rights reserved.",""
"The production system and its maintenance system must be now developed on \"system thinking\" paradigm in order to guarantee that Key Performance Indicators (KPI) will be optimized all along the production system (operation) life. In a recursive way, maintenance system engineering has to integrate also KPI considerations with regards to its own enabling systems. Thus this paper develops a system-based methodology wherein a set of KPIs is computed in order to verify if the objectives of the production and maintenance systems are satisfied. In order to help the decision-making process for maintenance managers, a \"unified\" generic model have been developed. This model integrates (a) the interactions of the maintenance system with its enabling systems, (b) the impact of the maintenance strategies through the computation of some key performance indicators, and (c) different kinds of knowledge regarding the maintenance system and the system of interest, including quantitative and qualitative knowledge. This methodology is based on an executable unified model built with Probabilistic Relational Model (PRM). PRM allows a modular representation and inferences computation of large size models. The methodology added-value is shown on a test-bench. (C) 2013 Elsevier Ltd. All rights reserved.","PRM allows a modular representation and inferences computation of large size models."
"Blowouts are among the most undesired and feared accidents during drilling operations. The dynamic nature of blowout accidents, resulting from both rapidly changing physical parameters and time-dependent failure of barriers, necessitates techniques capable of considering time dependencies and changes during the lifetime of a well. The present work is aimed at demonstrating the application of bow-tie and Bayesian network methods in conducting quantitative risk analysis of drilling operations. Considering the former method, fault trees and an event tree are developed for potential accident scenarios, and then combined to build a bow-tie model. In the latter method, first, individual Bayesian networks are developed for the accident scenarios and finally, an object-oriented Bayesian network is constructed by connecting these individual networks. The Bayesian network method provides greater value than the bow-tie model since it can consider common cause failures and conditional dependencies along with performing probability updating and sequential learning using accident precursors. (C) 2013 Elsevier Ltd. All rights reserved.",""
"Increased nitrogen (N) inputs to freshwater wetlands potentially affect the interaction between nitrous oxide (N2O) emissions and outflow water quality. The purpose of this study is to evaluate the influence of N inputs on N removal, as well as the interaction between N2O emissions and outflow water quality, using a Bayesian Belief Network (BBN). The BBN was developed by linking wetland classification, biogeochemical processes, and environmental factors. Empirical data for 34 freshwater wetlands were gathered from a comprehensive review of published peer-reviewed and gray literature. The BEN was implemented using 30 wetlands (88% of the case file) and evaluated using a single test file containing 4 wetlands (12% of the case file). The BBN implies it is not average annual total N load entering the wetland, but the N removal efficiency that influences the interactions between N2O emissions and outflow water quality. Even though the network has a very low error rate indicating a high predictive accuracy, additional testing and larger training and testing datasets would increase confidence in the model's ability to provide robust predictions and to reduce the uncertainty resulting from an incomplete dataset and knowledge gaps regarding the interactions between N2O emissions and outflow water quality. (C) 2013 Elsevier Ltd. All rights reserved.","The BBN was developed by linking wetland classification, biogeochemical processes, and environmental factors."
"A tiered approach to contamination exposure assessment is currently adopted in many countries. Increasing the site-specific information in exposure assessments is generally recommended when guideline values for contaminants in soil are exceeded. This work details a Bayesian Network (BN) approach to developing a site-specific environmental exposure assessment that focuses on the simple mapping and assessment of assumptions and the effect of new data on assessment outcomes. The BN approach was applied to a floodplain system in New South Wales, Australia, where site-specific information about elevated antimony (Sb) concentrations and distribution in soils was available. Guidelines for exposure assessment in Australia are used as a template for this site, although the approach is generic. The BN-based assessment used an iterative approach starting with limited soil Sb data (41 samples ranging from 0 to 18 mg kg-(1) Sb) and extending the model with more detailed Sb data (145 samples ranging from 0 to 40 mg kg(-1) Sb). The analyses identified dominant exposure pathways and assessed the sensitivity of these pathways to changes in assumptions and the level of site-specific information available. In particular, there was a 10.8% probability of exceeding the tolerable daily intake of Sb in the case study when the limited soil Sb data was used, which increased to 26.2% with the more detailed sampling regime. There was also a 47% decrease in the probability of overexposure to Sb when the dermal bioavailability of arsenic (a similar metalloid) was used as a surrogate measure instead of a default bioavailability of 100%. We conclude that the BN approach to soil exposure assessment has merit both in the context of Australian and international soil exposure assessments. (C) 2013 Elsevier Ltd. All rights reserved.",""
"This paper reports on the purpose, design, methodology and target audience of E-learning courses in forensic interpretation offered by the authors since 2010, including practical experiences made throughout the implementation period of this project. This initiative was motivated by the fact that reporting results of forensic examinations in a logically correct and scientifically rigorous way is a daily challenge for any forensic practitioner. Indeed, interpretation of raw data and communication of findings in both written and oral statements are topics where knowledge and applied skills are needed. Although most forensic scientists hold educational records in traditional sciences, only few actually followed full courses that focussed on interpretation issues. Such courses should include foundational principles and methodology - including elements of forensic statistics - for the evaluation of forensic data in a way that is tailored to meet the needs of the criminal justice system. In order to help bridge this gap, the authors' initiative seeks to offer educational opportunities that allow practitioners to acquire knowledge and competence in the current approaches to the evaluation and interpretation of forensic findings. These cover, among other aspects, probabilistic reasoning (including Bayesian networks and other methods of forensic statistics, tools and software), case pre-assessment, skills in the oral and written communication of uncertainty, and the development of independence and self-confidence to solve practical inference problems. E-learning was chosen as a general format because it helps to form a trans-institutional online-community of practitioners from varying forensic disciplines and workfield experience such as reporting officers, (chief) scientists, forensic coordinators, but also lawyers who all can interact directly from their personal workplaces without consideration of distances, travel expenses or time schedules. In the authors' experience, the proposed learning initiative supports participants in developing their expertise and skills in forensic interpretation, but also offers an opportunity for the associated institutions and the forensic community to reinforce the development of a harmonized view with regard to interpretation across forensic disciplines, laboratories and judicial systems. (C) 2012 Elsevier Ireland Ltd. All rights reserved.","These cover, among other aspects, probabilistic reasoning (including Bayesian networks and other methods of forensic statistics, tools and software), case pre-assessment, skills in the oral and written communication of uncertainty, and the development of independence and self-confidence to solve practical inference problems."
"Bayesian belief networks are graphical probabilistic analysis tools for representing and analyzing problems involving uncertainty. The problem of monitoring the propagation of a contaminant in a water distribution system can be represented by using Bayesian networks (BNs). The presented methodology proposes the use of BN statistics to estimate the likelihood of the injection location of a contaminant and its propagation in the system. A clustering method, previously developed by the authors, is first applied to formulate a simplified representation of the distribution system based on nodal connectivity properties. Given evidence from clusters, information is combined through probabilistic inference using BNs to find the most likely source of contamination and its propagation in the network. The conditional independence assumptions with the BNs allow efficient calculation of the joint probabilities and diagnostic and predictive queries (e. g., the most likely event given evidence or the probability of an outcome given starting conditions). In addition, a theoretic information measure is suggested to evaluate the significance of the clusters relying on the BN model of the system and possible optimal sensor locations. The proposed methodology is developed and tested on two water supply systems. (C) 2013 American Society of Civil Engineers.","Given evidence from clusters, information is combined through probabilistic inference using BNs to find the most likely source of contamination and its propagation in the network."
"Conservation of free-ranging cheetah (Acinonyx jubatus) populations is multi faceted and needs to be addressed from an ecological, biological and management perspective. There is a wealth of published research, each focusing on a particular aspect of cheetah conservation. Identifying the most important factors, making sense of various (and sometimes contrasting) findings, and taking decisions when little or no empirical data is available, are everyday challenges facing conservationists. Bayesian networks (BN) provide a statistical modeling framework that enables analysis and integration of information addressing different aspects of conservation. There has been an increased interest in the use of BNs to model conservation issues, however the development of more sophisticated BNs, utilizing object-oriented (OO) features, is still at the frontier of ecological research. We describe an integrated, parallel modeling process followed during a BN modeling workshop held in Namibia to combine expert knowledge and data about free-ranging cheetahs. The aim of the workshopwas to obtain a more comprehensive view of the current viability of the free-ranging cheetah population in Namibia, and to predict the effect different scenarios may have on the future viability of this free-ranging cheetah population. Furthermore, a complementary aim was to identify influential parameters of themodel to more effectively target those parameters having the greatest impact on population viability. The BN was developed by aggregating diverse perspectives from local and independent scientists, agents from the national ministry, conservation agency members and local fieldworkers. This integrated BN approach facilitates OO modeling in a multi-expert context which lends itself to a series of integrated, yet independent, subnetworks describing different scientific and management components. We created three subnetworks in parallel: a biological, ecological and human factors network, which were then combined to create a complete representation of free-ranging cheetah population viability. Such OOBNs have widespread relevance to the effective and targeted conservation management of vulnerable and endangered species.",""
"As cloud computing becomes popular, intrusion detection has been focused again, since huge amount of network attacks have increased the requirement of efficient network intrusion detection techniques. Currently, lots of methods are used to solve this issue, but lower detection rate of these original models cannot satisfy complex Internet environment. In this paper, we propose a novel intrusion detection model-Bayesian Network-based binary quantum-behaved particle swarm optimization (BQPSO-BN). Since the classical QPSO algorithm only operates in continuous and real-valued space, and the problem of Bayesian networks learning is in discrete space, we redefine the position vector and the distance between two positions, and adjust the iterative equations of QPSO to binary search space. Experiment results with KDD99 dataset show that BQPSO-BN is an efficient and effective algorithm and has better convergence speed compared with BPSO-BN and GA-BN models.",""
"We present language-motivated approaches to detecting, localizing and classifying activities and gestures in videos. In order to obtain statistical insight into the underlying patterns of motions in activities, we develop a dynamic, hierarchical Bayesian model which connects low-level visual features in videos with poses, motion patterns and classes of activities. This process is somewhat analogous to the method of detecting topics or categories from documents based on the word content of the documents, except that our documents are dynamic. The proposed generative model harnesses both the temporal ordering power of dynamic Bayesian networks such as hidden Markov models (HMMs) and the automatic clustering power of hierarchical Bayesian models such as the latent Dirichlet allocation (LDA) model. We also introduce a probabilistic framework for detecting and localizing pre-specified activities (or gestures) in a video sequence, analogous to the use of filler models for keyword detection in speech processing. We demonstrate the robustness of our classification model and our spotting framework by recognizing activities in unconstrained real-life video sequences and by spotting gestures via a one-shot-learning approach.","We demonstrate the robustness of our classification model and our spotting framework by recognizing activities in unconstrained real-life video sequences and by spotting gestures via a one-shot-learning approach."
"Bayesian networks are graphical models that describe dependency relationships between variables, and are powerful tools for studying probability classifiers. At present, the causal Bayesian network learning method is used in constructing Bayesian network classifiers while the contribution of attribute to class is overlooked. In this paper, a Bayesian network specifically for classification-restricted Bayesian classification networks is proposed. Combining dependency analysis between variables, classification accuracy evaluation criteria and a search algorithm, a learning method for restricted Bayesian classification networks is presented. Experiments and analysis are done using data sets from UCI machine learning repository. The results show that the restricted Bayesian classification network is more accurate than other well-known classifiers.","Bayesian networks are graphical models that describe dependency relationships between variables, and are powerful tools for studying probability classifiers."
"The tracking and recognition of facial activities from images or videos have attracted great attention in computer vision field. Facial activities are characterized by three levels. First, in the bottom level, facial feature points around each facial component, i.e., eyebrow, mouth, etc., capture the detailed face shape information. Second, in the middle level, facial action units, defined in the facial action coding system, represent the contraction of a specific set of facial muscles, i.e., lid tightener, eyebrow raiser, etc. Finally, in the top level, six prototypical facial expressions represent the global facial muscle movement and are commonly used to describe the human emotion states. In contrast to the mainstream approaches, which usually only focus on one or two levels of facial activities, and track (or recognize) them separately, this paper introduces a unified probabilistic framework based on the dynamic Bayesian network to simultaneously and coherently represent the facial evolvement in different levels, their interactions and their observations. Advanced machine learning methods are introduced to learn the model based on both training data and subjective prior knowledge. Given the model and the measurements of facial motions, all three levels of facial activities are simultaneously recognized through a probabilistic inference. Extensive experiments are performed to illustrate the feasibility and effectiveness of the proposed model on all three level facial activities.","Given the model and the measurements of facial motions, all three levels of facial activities are simultaneously recognized through a probabilistic inference."
"Probabilistic Boolean network (PBN) modelling is a semi-quantitative approach widely used for the study of the topology and dynamic aspects of biological systems. The combined use of rule-based representation and probability makes PBN appealing for large-scale modelling of biological networks where degrees of uncertainty need to be considered. A considerable expansion of our knowledge in the field of theoretical research on PBN can be observed over the past few years, with a focus on network inference, network intervention and control. With respect to areas of applications, PBN is mainly used for the study of gene regulatory networks though with an increasing emergence in signal transduction, metabolic, and also physiological networks. At the same time, a number of computational tools, facilitating the modelling and analysis of PBNs, are continuously developed. A concise yet comprehensive review of the state-of-the-art on PBN modelling is offered in this article, including a comparative discussion on PBN versus similar models with respect to concepts and biomedical applications. Due to their many advantages, we consider PBN to stand as a suitable modelling framework for the description and analysis of complex biological systems, ranging from molecular to physiological levels.","A considerable expansion of our knowledge in the field of theoretical research on PBN can be observed over the past few years, with a focus on network inference, network intervention and control."
"Job shop scheduling is an important decision process in contemporary manufacturing systems. In this paper, we aim at the job shop scheduling problem in which the total weighted tardiness must be minimized. This objective function is relevant for the make-to-order production mode with an emphasis on customer satisfaction. In order to save the computational time, we focus on the set of non-delay schedules and use a genetic algorithm to optimize the set of dispatching rules used for schedule construction. Another advantage of this strategy is that it can be readily applied in a dynamic scheduling environment which must be investigated with simulation. Considering that the rules selected for scheduling previous operations have a direct impact on the optimal rules for scheduling subsequent operations, Bayesian networks are utilized to model the distribution of high-quality solutions in the population and to produce the new generation of individuals. In addition, some selected individuals are further improved by a special local search module based on systematic perturbations to the operation processing times. The superiority of the proposed approach is especially remarkable when the size of the scheduling problem is large.",""
"Artificial intelligence can be used to recognize and anticipate dynamic situations. Several computational methods based on mathematical tools already exist, but most of the time their implementation is complex and takes a long time to execute. In this article, we propose another learning and anticipation method in order to assist users in dynamic situations. We call it scenario-based reasoning' algorithm. It is inspired by case-based reasoning. It works with symbolic data and its aim is to make real-time predictions. To do so, manipulated knowledge is specially designed to limit our solution's complexity and to facilitate learning and anticipation.",""
"Parameter setting for evolutionary algorithms is still an important issue in evolutionary computation. There are two main approaches to parameter setting: parameter tuning and parameter control. In this paper, we introduce self-adaptive parameter control of a genetic algorithm based on Bayesian network learning and simulation. The nodes of this Bayesian network are genetic algorithm parameters to be controlled. Its structure captures probabilistic conditional (in)dependence relationships between the parameters. They are learned from the best individuals, i.e., the best configurations of the genetic algorithm. Individuals are evaluated by running the genetic algorithm for the respective parameter configuration. Since all these runs are time-consuming tasks, each genetic algorithm uses a small-sized population and is stopped before convergence. In this way promising individuals should not be lost. Experiments with an optimal search problem for simultaneous row and column orderings yield the same optima as state-of-the-art methods but with a sharp reduction in computational time. Moreover, our approach can cope with as yet unsolved high-dimensional problems.",""
"The growing interest in modular and distributed approaches for the design and control of intelligent manufacturing systems gives rise to new challenges. One of the major challenges that have not yet been well addressed is monitoring and diagnosis in distributed manufacturing systems. In this paper we propose the use of a multi-agent Bayesian framework known as Multiply Sectioned Bayesian Networks (MSBNs) as the basis for multi-agent distributed diagnosis in modular assembly systems. We use a close-to-industry case study to demonstrate how MSBNs can be used to build component-based Bayesian sub-models, how to verify the resultant models, and how to compile the multi-agent models into runtime structures to allow consistent multi-agent belief update and inference. (C) 2013 The Society of Manufacturing Engineers. Published by Elsevier Ltd. All rights reserved.","We use a close-to-industry case study to demonstrate how MSBNs can be used to build component-based Bayesian sub-models, how to verify the resultant models, and how to compile the multi-agent models into runtime structures to allow consistent multi-agent belief update and inference."
"This article proposes a methodology for the application of Bayesian networks in conducting quantitative risk assessment of operations in offshore oil and gas industry. The method involves translating a flow chart of operations into the Bayesian network directly. The proposed methodology consists of five steps. First, the flow chart is translated into a Bayesian network. Second, the influencing factors of the network nodes are classified. Third, the Bayesian network for each factor is established. Fourth, the entire Bayesian network model is established. Lastly, the Bayesian network model is analyzed. Subsequently, five categories of influencing factors, namely, human, hardware, software, mechanical, and hydraulic, are modeled and then added to the main Bayesian network. The methodology is demonstrated through the evaluation of a case study that shows the probability of failure on demand in closing subsea ram blowout preventer operations. The results show that mechanical and hydraulic factors have the most important effects on operation safety. Software and hardware factors have almost no influence, whereas human factors are in between. The results of the sensitivity analysis agree with the findings of the quantitative analysis. The three-axiom-based analysis partially validates the correctness and rationality of the proposed Bayesian network model.","Second, the influencing factors of the network nodes are classified."
"The multivariate exponential power family is considered for n-dimensional random variables, Z, with a known partition Z equivalent to (Y, X) of dimensions p and n - p, respectively, with interest focusing on the conditional distribution Y vertical bar X. An infinitesimal variation of any parameter of the joint distribution produces perturbations in both the conditional and marginal distributions. The aim of the study was to determine the local effect of kurtosis deviations using the Kullback-Leibler divergence measure between probability distributions. The additive decomposition of this measure in terms of the conditional and marginal distributions, Y vertical bar X and X, is used to define a relative sensitivity measure of the conditional distribution family {Y vertical bar X = x}. Finally, simulated results suggest that for large dimensions, the measure is approximately equal to the ratio p/n, and then the effect of non-normality with respect to kurtosis depends only on the relative size of the variables considered in the partition of the random vector. (C) 2013 Elsevier Inc. All rights reserved.",""
"We present an integrated modeling framework for simulating land-use decision making under the influence of payments for ecosystem services. The model combines agent-based modeling (ABM) with Bayesian belief networks (BBNs) and opinion dynamics models (ODM). The model endows agents with the ability to make land-use decisions at the household and plot levels. The decision-making process is captured with the BBNs that were constructed and calibrated with both qualitative and quantitative information, i.e., knowledge gained from group discussions with stakeholders and empirical survey data. To represent interpersonal interactions within social networks, the decision process is further modulated by the opinion dynamics model. The goals of the model are to improve the ability of ABM to emulate land-use decision making and thus provide a better understanding of the potential impacts of payments for ecosystem services on land use and household livelihoods. Our approach provides three important innovations. First, decision making is represented in a causal directed graph. Second, the model provides a natural framework for combining knowledge from experts and stakeholders with quantitative data. Third, the modular architecture and the software implementation can be customized with modest efforts. The model is therefore a flexible, general platform that can be tailored to other studies by mounting the appropriate case-specific \"brain\" into the agents. The model was calibrated for the Sloping Land Conversion Program (SLCP) in Yunnan, China using data from participatory mapping, focus group interviews, and a survey of 509 farm households in 17 villages. (c) 2012 Elsevier Ltd. All rights reserved.",""
"To enhance the automatic text classification task, this paper proposes a novel approach for treating the problem of inductive bias incurred by the centroid classifier assumption. This approach is a trainable classifier, which takes into account tfidf as a text feature. The main goal of the proposed approach is to take advantage of the most similar training errors in the classification model for successively updating that model based on a certain threshold. The proposed approach is practical and flexible to implement. The complete performance of the proposed approach is measured at several threshold values on the Reuters-21578 text categorization collection. Experimental results show that the proposed approach can improve the performance of the centroid classifier better than traditional approaches (traditional centroid classifier, support vector machines, decision trees, Bayes nets, and N Bayes) by 1, 1.2, 4.1, 7.5, and 11%, respectively. (c) 2013 Institute of Electrical Engineers of Japan. Published by John Wiley & Sons, Inc.","To enhance the automatic text classification task, this paper proposes a novel approach for treating the problem of inductive bias incurred by the centroid classifier assumption."
"An application of dynamic Bayesian networks for quantitative risk assessment of human factors on offshore blowouts is presented. Human error is described using human factor barrier failure (HFBF), which consists of three categories of factors, including individual factor barrier failure (IFBF), organizational factor barrier failure (OFBF) and group factor barrier failure (GFBF). The structure of human factors is illustrated using pseudo-fault tree, which is defined by incorporating the intermediate options into fault tree in order to eliminate the binary restriction. A methodology of translating pseudo-fault tree into Bayesian networks and dynamic Bayesian networks taking repair into consideration is proposed and the propagation is performed. The results show that the human factor barrier failure probability only increases within the first two weeks and rapidly reaches a stable level when the repair is considered, whereas it increases continuously when the repair action is not considered. The results of mutual information show that the important degree sequences for the three categories of human factors on HFBF are: GFBF, OFBF and IFBF. In addition, each individual human factor contributes different to the HFBF, those which contribute much should given more attention in order to improve the human reliability and prevent the potential accident occurring. (C) 2013 Elsevier Ltd. All rights reserved.",""
"Reliability is the key factor for software system quality. Several models have been introduced to estimate and predict reliability based on results of software testing activities. Software Reliability Growth Models (SRGMs) are considered the most commonly used to achieve this goal. Over the past decades, many researchers have discussed SRGMs' assumptions, applicability, and predictability. They have concluded that SRGMs have many shortcomings related to their unrealistic assumptions, environment-dependent applicability, and questionable predictability. Several approaches based on non-parametric statistics, Bayesian networks, and machine learning methods have been proposed in the literature. Based on their theoretical nature, however, they cannot completely address the SRGMs' limitations. Consequently, addressing these shortcomings is still a very crucial task in order to provide reliable software systems. This paper presents a well-established prediction approach based on time series ARIMA (Autoregressive Integrated Moving Average) modeling as an alternative solution to address the SRGMs' limitations and provide more accurate reliability prediction. Using real-life data sets on software failures, the accuracy of the proposed approach is evaluated and compared to popular existing approaches. (C) 2013 Elsevier Inc. All rights reserved.",""
"With the advances of biomedical techniques in the last decade, the costs of human genomic sequencing and genomic activity monitoring are coming down rapidly. To support the huge genome-based business in the near future, researchers are eager to find killer applications based on human genome information. Causal gene identification is one of the most promising applications, which may help the potential patients to estimate the risk of certain genetic diseases and locate the target gene for further genetic therapy. Unfortunately, existing pattern recognition techniques, such as Bayesian networks, cannot be directly applied to find the accurate causal relationship between genes and diseases. This is mainly due to the insufficient number of samples and the extremely high dimensionality of the gene space. In this paper, we present the first practical solution to causal gene identification, utilizing a new combinatorial formulation over V-Structures commonly used in conventional Bayesian networks, by exploring the combinations of significant V-Structures. We prove the NP-hardness of the combinatorial search problem under a general settings on the significance measure on the V-Structures, and present a greedy algorithm to find suboptimal results. Extensive experiments show that our proposal is both scalable and effective, particularly with interesting findings on the causal genes over real human genome data. (c) 2013 Elsevier Ltd. All rights reserved.",""
"Recursive probability trees (RPTs) are a data structure for representing several types of potentials involved in probabilistic graphical models. The RPT structure improves the modeling capabilities of previous structures (like probability trees or conditional probability tables). These capabilities can be exploited to gain savings in memory space and/or computation time during inference. This paper describes the modeling capabilities of RPTs as well as how the basic operations required for making inference on Bayesian networks operate on them. The performance of the inference process with RPTs is examined with some experiments using the variable elimination algorithm.","These capabilities can be exploited to gain savings in memory space and/or computation time during inference."
"This paper examines optimal policies in a continuous review inventory management system when demand in each time period follows a log-normal distribution. In this scenario, the distribution for demand during the entire lead time period has no known form. The proposed procedure uses the Fenton-Wilkinson method to estimate the parameters for a single log-normal distribution that approximates the probability density function (PDF) for lead time demand, conditional on a specific lead time. Once these parameters are determined, a mixture of truncated exponentials (MTE) function that approximates the lead time demand distribution is constructed. The objective is to include the log-normal distribution in a robust decision support system where the PEW that best fits the historical period demand data is used to construct the lead time demand distribution. Experimental results indicate that when the log-normal distribution is the best fit, the model presented in this paper reduces expected inventory costs by improving optimal policies, as compared to other potential approximations. (C) 2013 Elsevier Ltd. All rights reserved.",""
"The shortcomings of Human Reliability Analysis (HRA) have been a topic of discussion for over two decades. Repeated attempts to address these limitations have resulted in over 50 HRA methods, and the HRA research community continues to develop new methods. However, there remains a gap between the methods developed by HRA researchers and those actually used by HRA practitioners. Bayesian Networks (BNs) have become an increasingly popular part of the risk and reliability analysis framework over the past decade. BNs provide a framework for addressing many of the shortcomings of HRA from a researcher perspective and from a practitioner perspective. Several research groups have developed advanced HRA methods based on BNs, but none of these methods has been adopted by HRA practitioners in the U.S. nuclear power industry or at the U.S. Nuclear Regulatory Commission. In this paper we bridge the gap between HRA research and HRA practice by building a BN version of the widely used SPAR-H method. We demonstrate how the SPAR-H BN can be used by HRA practitioners, and we also demonstrate how it can be modified to incorporate data and information from research to advance HRA practice. The SPAR-H BN can be used as a starting point for translating HRA research efforts and advances in scientific understanding into real, timely benefits for HRA practitioners. (C) 2013 Elsevier Ltd. All rights reserved.",""
"When there are several experts in a specific domain, each may believe in a different Bayesian network (BN) representation of the domain. In order to avoid having to work with several BNs, it is desirable to aggregate them into a single BN. One way of finding the aggregated BN is to start by finding the structure, and then find the parameters. In this paper, we focus on the second step, assuming that the structure has been found by some previous method. DemocraticOP is a new way of combining experts' parameters in a model. The logic behind this approach is borrowed from the concept of democracy in the real world. We assume that there is a ground truth and that each expert represents a deviation from it - the goal is to try to find the ground truth based on the experts' opinions. If the experts do not agree, then taking a simple average of their opinions (as occurs in classical aggregation functions such as LinOP and LogOP) is flawed. Instead, we believe it is better to identify similar opinions through clustering, and then apply averaging, or any other aggregation function, over the cluster with the highest number of members to obtain the aggregated parameters that are closest to the ground truth. In other words, respect the majority as is done in democratic societies instead of averaging over all experts' parameters. The new approach is implemented and tested over several BNs with different numbers of variables and parameters, and with different numbers of experts. The results show that DemocraticOP outperforms two commonly used methods, LinOP and LogOP, in three key metrics: the average of absolute value of the difference between the true probability distribution and the one corresponding to the aggregated parameters, Kullback-Leibler divergence, and running time. (C) 2012 Elsevier Inc. All rights reserved.",""
"Background: Heroin dependence is a debilitating psychiatric disorder with complex inheritance. Since the dopaminergic system has a key role in rewarding mechanism of the brain, which is directly or indirectly targeted by most drugs of abuse, we focus on the effects and interactions among dopaminergic gene variants. Objective: To study the potential association between allelic variants of dopamine D2 receptor (DRD2), ANKK1 (ankyrin repeat and kinase domain containing 1), dopamine D4 receptor (DRD4), catechol-O-methyl transferase (COMT) and dopamine transporter (SLC6A3) genes and heroin dependence in Hungarian patients. Methods: 303 heroin dependent subjects and 555 healthy controls were genotyped for 7 single nucleotide polymorphisms (SNPs) rs4680 of the COMT gene; rs1079597 and rs1800498 of the DRD2 gene; rs1800497 of the ANKK1 gene; rs1800955, rs936462 and rs747302 of the DRD4 gene. Four variable number of tandem repeats (VNTRs) were also genotyped: 120 bp duplication and 48 bp VNTR in exon 3 of DRD4 and 40 bp VNTR and intron 8 VNTR of SLC6A3. We also perform a multivariate analysis of associations using Bayesian networks in Bayesian multilevel analysis (BN-BMLA). Findings and conclusions: In single marker analysis the TaqIA (rs1800497) and TaqIB (rs1079597) variants were associated with heroin dependence. Moreover, -521 C/T SNP (rs1800955) of the DRD4 gene showed nominal association with a possible protective effect of the C allele. After applying the Bonferroni correction TaqIB was still significant suggesting that the minor (A) allele of the TaqIB SNP is a risk component in the genetic background of heroin dependence. The findings of the additional multiple marker analysis are consistent with the results of the single marker analysis, but this method was able to reveal an indirect effect of a promoter polymorphism (rs936462) of the DRD4 gene and this effect is mediated through the -521 C/T (rs1800955) polymorphism in the promoter.",""
"Inferring regulatory networks from experimental data via probabilistic graphical models is a popular framework to gain insights into biological systems. However, the inherent noise in experimental data coupled with a limited sample size reduces the performance of network reverse engineering. Prior knowledge from existing sources of biological information can address this low signal to noise problem by biasing the network inference towards biologically plausible network structures. Although integrating various sources of information is desirable, their heterogeneous nature makes this task challenging. We propose two computational methods to incorporate various information sources into a probabilistic consensus structure prior to be used in graphical model inference. Our first model, called Latent Factor Model (LFM), assumes a high degree of correlation among external information sources and reconstructs a hidden variable as a common source in a Bayesian manner. The second model, a Noisy-OR, picks up the strongest support for an interaction among information sources in a probabilistic fashion. Our extensive computational studies on KEGG signaling pathways as well as on gene expression data from breast cancer and yeast heat shock response reveal that both approaches can significantly enhance the reconstruction accuracy of Bayesian Networks compared to other competing methods as well as to the situation without any prior. Our framework allows for using diverse information sources, like pathway databases, GO terms and protein domain data, etc. and is flexible enough to integrate new sources, if available.","Prior knowledge from existing sources of biological information can address this low signal to noise problem by biasing the network inference towards biologically plausible network structures."
"Cancer can be a result of accumulation of different types of genetic mutations such as copy number aberrations. The data from tumors are cross-sectional and do not contain the temporal order of the genetic events. Finding the order in which the genetic events have occurred and progression pathways are of vital importance in understanding the disease. In order to model cancer progression, we propose Progression Networks, a special case of Bayesian networks, that are tailored to model disease progression. Progression networks have similarities with Conjunctive Bayesian Networks (CBNs) [1], a variation of Bayesian networks also proposed for modeling disease progression. We also describe a learning algorithm for learning Bayesian networks in general and progression networks in particular. We reduce the hard problem of learning the Bayesian and progression networks to Mixed Integer Linear Programming (MILP). MILP is a Non-deterministic Polynomial-time complete (NP-complete) problem for which very good heuristics exists. We tested our algorithm on synthetic and real cytogenetic data from renal cell carcinoma. We also compared our learned progression networks with the networks proposed in earlier publications. The software is available on the website https://bitbucket.org/farahani/diprog.",""
"In this paper, designing a Bayesian network structure to maximize a score function based on learning from data strategy is studied. The scoring function is considered to be a decomposable one such as BDeu, BIC, BD, BDe or AIC. Optimal design of such a network is known to be an NP-hard problem and the solution becomes rapidly infeasible as the number of variables (i.e., nodes in the network) increases. Several methods such as hill-climbing, dynamic programming, and branch and bound techniques are proposed to tackle this problem. However, these techniques either produce sub-optimal solutions or the time required to produce an optimal solution is unacceptable. The challenge of the latter solutions is to reduce the computation time necessary for large-size problems. In this study, a new branch and bound method called PBB (pruned brand and bound) is proposed which is expected to find the globally optimal network structure with respect to a given score function. It is an any-time method, i.e., if it is externally stopped, it gives the best solution found until that time. Several pruning strategies are proposed to reduce the number of nodes created in the branch and bound tree. Practical experiments show the effectiveness of these pruning strategies. The performance of PBB, on several common datasets, is compared with the latest state-of-the-art methods. The results show its superiority in many aspects, especially, in the required running time, and the number of created nodes of the branch and bound tree. (C) 2013 Sharif University of Technology. Production and hosting by Elsevier B.V. All rights reserved.",""
"We study an economic decision problem where the actors are two firms and the Antitrust Authority whose main task is to monitor and prevent firms' potential anti-competitive behaviour and its effect on the market. The Antitrust Authority's decision process is modelled using a Bayesian network where both the relational structure and the parameters of the model are estimated from a data set provided by the Authority itself. A number of economic variables that influence this decision process are also included in the model. We analyse how monitoring by the Antitrust Authority affects firms' strategies about cooperation. Firms' strategies are modelled as a repeated prisoner's dilemma using object-oriented Bayesian networks. We show how the integration of firms' decision process and external market information can be modelled in this way. Various decision scenarios and strategies are illustrated.",""
"We propose Fast Generalized Subset Scan (FGSS), a new method for detecting anomalous patterns in general categorical data sets. We frame the pattern detection problem as a search over subsets of data records and attributes, maximizing a nonparametric scan statistic over all such subsets. We prove that the nonparametric scan statistics possess a novel property that allows for efficient optimization over the exponentially many subsets of the data without an exhaustive search, enabling FGSS to scale to massive and high-dimensional data sets. We evaluate the performance of FGSS in three real-world application domains (customs monitoring, disease surveillance, and network intrusion detection), and demonstrate that FGSS can successfully detect and characterize relevant patterns in each domain. As compared to three other recently proposed detection algorithms, FGSS substantially decreased run time and improved detection power for massive multivariate data sets.",""
"Constraint-based learning of Bayesian networks (BN) from limited data can lead to multiple testing problems when recovering dense areas of the skeleton and to conflicting results in the orientation of edges. In this paper, we present a new constraint-based algorithm, light mutual min (LMM) for improved accuracy of BN learning from small sample data. LMM improves the assessment of candidate edges by using a ranking criterion that considers conditional independence on neighboring variables at both sides of an edge simultaneously. The algorithm also employs an adaptive relaxation of constraints that, selectively, allows some nodes not to condition on some neighbors. This relaxation aims at reducing the incorrect rejection of true edges connecting high degree nodes due to multiple testing. LMM additionally incorporates a new criterion for ranking v-structures that is used to recover the completed partially directed acyclic graph (CPDAG) and to resolve conflicting v-structures, a common problem in small sample constraint-based learning. Using simulated data, each of these components of LMM is shown to significantly improve network inference compared to commonly applied methods when learning from limited data, including more accurate recovery of skeletons and CPDAGs compared to the PC, MaxMin, and MaxMin hill climbing algorithms. A proof of asymptotic correctness is also provided for LMM for recovering the correct skeleton and CPDAG.","Using simulated data, each of these components of LMM is shown to significantly improve network inference compared to commonly applied methods when learning from limited data, including more accurate recovery of skeletons and CPDAGs compared to the PC, MaxMin, and MaxMin hill climbing algorithms."
"Learning causal relations from observational data is a fundamental task in knowledge discovery. Recently, an Information Geometric Causal Inference (IGCI) framework is proposed, which represents causal relations by deterministic functions, defines causal mechanisms by a cause-mapping independence postulate and learns causal directions by an information geometry formulation. Observing IGCI's limitation in representing general causal relations and its ambiguity in inferring causal directions, we generalize IGCI's original postulate, and propose a new dependence causal inference (DCI) method where linear correlation and a new cross likelihood (CL) measure are introduced. We prove that CL dependence measure incorporates IGCI as its special case in some sense. Experimental results on synthetic and real-world data verify the effectiveness of our generalization and methods, and show that our method is more robust to disturbances on real data sets.","Recently, an Information Geometric Causal Inference (IGCI) framework is proposed, which represents causal relations by deterministic functions, defines causal mechanisms by a cause-mapping independence postulate and learns causal directions by an information geometry formulation."
"Research on networks is increasingly popular in a wide range of machine learning fields, and structural inference of networks is a key problem. Unfortunately, network structural inference is time consuming and there is an increasing need to infer the structure of ever-larger networks. This article presents the Dense Structural Expectation Maximisation (DSEM) algorithm, a novel extension of the well-known SEM algorithm. DSEM increases the efficiency of structural inference by using the time-expensive calculations required in each SEM iteration more efficiently, and can be O(N) times faster than SEM, where N is the size of the network. The article has also combined DSEM with parallelisation and evaluated the impact of these improvements over SEM, individually and combined. The possibility of combining these novel approaches with other research on structural inference is also considered. The contributions also appear to be usable for all kinds of structural inference, and may greatly improve the range, variety and size of problems which can be tractably addressed. Code is freely available online at: http://syntilect.com/cgf/pubs:software.","Research on networks is increasingly popular in a wide range of machine learning fields, and structural inference of networks is a key problem."
"The topic of automatically history-matched reservoir models has seen much research activity in recent years. History matching is an example of an inverse problem, and there is significant active research on inverse problems in many other scientific and engineering areas. While many techniques from other fields, such as genetic algorithms, evolutionary strategies, differential evolution, particle swarm optimization, and the ensemble Kalman filter have been tried in the oil industry, more recent and effective ideas have yet to be tested. One of these relatively untested ideas is a class of algorithms known as estimation of distribution algorithms (EDAs). EDAs are population-based algorithms that use probability models to estimate the probability distribution of promising solutions, and then to generate new candidate solutions. EDAs have been shown to be very efficient in very complex high-dimensional problems. An example of a state-of-the-art EDA is the Bayesian optimization algorithm (BOA), which is a multivariate EDA employing Bayesian networks for modeling the relationships between good solutions. The use of a Bayesian network leads to relatively fast convergence as well as high diversity in the matched models. Given the relatively limited number of reservoir simulations used in history matching, EDA-BOA offers the promise of high-quality history matches with a fast convergence rate. In this paper, we introduce EDAs and describe BOA in detail. We show results of the EDA-BOA algorithm on two history-matching problems. First, we tune the algorithm, demonstrate convergence speed, and search diversity on the PUNQ-S3 synthetic case. Second, we apply the algorithm to a real North Sea turbidite field with multiple wells. In both examples, we show improvements in performance over traditional population-based algorithms.",""
"Given a fixed budget and an arbitrary cost for selecting each node, the budgeted influence maximization (BIM) problem concerns selecting a set of seed nodes to disseminate some information that maximizes the total number of nodes influenced (termed as influence spread) in social networks at a total cost no more than the budget. Our proposed seed selection algorithm for the BIM problem guarantees an approximation ratio of (1 - 1/root e). The seed selection algorithm needs to calculate the influence spread of candidate seed sets, which is known to be #P-complex. Identifying the linkage between the computation of marginal probabilities in Bayesian networks and the influence spread, we devise efficient heuristic algorithms for the latter problem. Experiments using both large-scale social networks and synthetically generated networks demonstrate superior performance of the proposed algorithm with moderate computation costs. Moreover, synthetic datasets allow us to vary the network parameters and gain important insights on the impact of graph structures on the performance of different algorithms.",""
"There is accumulating evidence that driver distraction is a leading cause of vehicle crashes and incidents. In particular, increased use of so-called in-vehicle information systems (IVIS) and partially autonomous driving assistance systems (PADAS) have raised important and growing safety concerns. Thus, detecting the driver's state is of paramount importance, to adapt IVIS and PADAS accordingly, therefore avoiding or mitigating their possible negative effects. The purpose of this paper is to show a method for the nonintrusive and real-time detection of visual distraction, using vehicle dynamics data and without using the eye-tracker data as inputs to classifiers. Specifically, we present and compare different models that are based on well-known machine learning (ML) methods. Data for training the models were collected using a static driving simulator, with real human subjects performing a specific secondary task [i.e., a surrogate visual research task (SURT)] while driving. Different training methods, model characteristics, and feature selection criteria have been compared. Based on our results, using a support vector machine (SVM) has outperformed all the other ML methods, providing the highest classification rate for most of the subjects. Potential applications of this paper include the design of an adaptive IVIS and of a \"smarter\" PADAS.","The purpose of this paper is to show a method for the nonintrusive and real-time detection of visual distraction, using vehicle dynamics data and without using the eye-tracker data as inputs to classifiers."
"Introduction: Managing chronic disease through automated systems has the potential to both benefit the patient and reduce health-care costs. We have developed and evaluated a disease management system for patients with chronic obstructive pulmonary disease (COPD). Its aim is to predict and detect exacerbations and, through this, help patients self-manage their disease to prevent hospitalisation. Materials: The carefully crafted intelligent system consists of a mobile device that is able to collect case-specific, subjective and objective, physiological data, and to alert the patient by a patient-specific interpretation of the data by means of probabilistic reasoning. Collected data are also sent to a central server for inspection by health-care professionals. Methods: We evaluated the probabilistic model using cross-validation and ROC analyses on data from an earlier study and by an independent data set. Furthermore a pilot with actual COPD patients has been conducted to test technical feasibility and to obtain user feedback. Results: Model evaluation results show that we can reliably detect exacerbations. Pilot study results suggest that an intervention based on this system could be successful. (C) 2013 Elsevier Inc. All rights reserved.",""
"Importance measures are widely used in reliability engineering. To support system reliability optimization, this paper studies the relationship and changing characteristics of the Birnbaum importance measures (BMs) for different components in binary coherent systems. First, the probabilistic meaning of BM and a modeling method for binary coherent system based on Bayesian network (BN) are presented. Then, the relationship of BMs for different components is explored according to the position of corresponding nodes (components) in BN structures. Later, the changing of BMs for different components, which is caused by the reliability improvements of related root or middle nodes (components) in BN structures, is analyzed respectively. Finally, an illustrative example of a helicopter convertor is implemented to demonstrate the calculation and application process of the relationship and changing characteristics in binary coherent systems with BN. The experimental analysis results substantiate the correctness and effectiveness of the proposed relationship and changing characteristics of BMs for different components.",""
"Objective-Genetic approaches have identified numerous loci associated with coronary heart disease (CHD). The molecular mechanisms underlying CHD gene-disease associations, however, remain unclear. We hypothesized that genetic variants with both strong and subtle effects drive gene subnetworks that in turn affect CHD. Approach and Results-We surveyed CHD-associated molecular interactions by constructing coexpression networks using whole blood gene expression profiles from 188 CHD cases and 188 age-and sex-matched controls. Twenty-four coexpression modules were identified, including 1 case-specific and 1 control-specific differential module (DM). The DMs were enriched for genes involved in B-cell activation, immune response, and ion transport. By integrating the DMs with gene expression-associated single-nucleotide polymorphisms and with results of genome-wide association studies of CHD and its risk factors, the control-specific DM was implicated as CHD causal based on its significant enrichment for both CHD and lipid expression-associated single-nucleotide polymorphisms. This causal DM was further integrated with tissue-specific Bayesian networks and protein-protein interaction networks to identify regulatory key driver genes. Multitissue key drivers (SPIB and TNFRSF13C) and tissue-specific key drivers (eg, EBF1) were identified. Conclusions-Our network-driven integrative analysis not only identified CHD-related genes, but also defined network structure that sheds light on the molecular interactions of genes associated with CHD risk.",""
"Diagnosis and classification of chronic obstructive pulmonary disease (COPD) may be seen as difficult. Causal reasoning can be used to relate clinical measurements with radiological representation of COPD phenotypes airways disease and emphysema. In this paper a causal probabilistic network was constructed that uses clinically available measurements to classify patients suffering from COPD into the main phenotypes airways disease and emphysema. The network grades the severity of disease and for emphysematous COPD, the type of bullae and its location central or peripheral. In four patient cases the network was shown to reach the same conclusion as was gained from the patients' High Resolution Computed Tomography (HRCT) scans. These were: airways disease, emphysema with central small bullae, emphysema with central large bullae, and emphysema with peripheral bullae. The approach may be promising in targeting HRCT in COPD patients, assessing phenotypes of the disease and monitoring its progression using clinical data. (C) 2013 Elsevier Ireland Ltd. All rights reserved.","Diagnosis and classification of chronic obstructive pulmonary disease (COPD) may be seen as difficult."
"A sequential approach to combining two established modeling techniques (systems thinking and Bayesian Belief Networks; BBNs) was developed and applied to climate change adaptation research within the South East Queensland Climate Adaptation Research Initiative (SEQ-CARI). Six participatory workshops involving 66 stakeholders based within SEQ produced six system conceptualizations and 22 alpha-level BBNs. The outcomes of the initial systems modeling exercise successfully allowed the selection of critical determinants of key response variables for in depth analysis within more homogeneous, sector-based groups of participants. Using two cases, this article focuses on the processes and methodological issues relating to the use of the BBN modeling technique when the data are based on expert opinion. The study expected to find both generic and specific determinants of adaptive capacity based on the perceptions of the stakeholders involved. While generic determinants were found (e.g. funding and awareness levels), sensitivity analysis identified the importance of pragmatic, context-based determinants, which also had methodological implications. The article raises questions about the most appropriate scale at which the methodology applied can be used to identify useful generic determinants of adaptive capacity when, at the scale used, the most useful determinants were sector-specific. Comparisons between individual BBN conditional probabilities identified diverging and converging beliefs, and that the sensitivity of response variables to direct descendant nodes was not always perceived consistently. It was often the accompanying narrative that provided important contextual information that explained observed differences, highlighting the benefits of using critical narrative with modeling tools. (C) 2012 Elsevier Ltd. All rights reserved.",""
"The framework of causal Bayes nets, currently influential in several scientific disciplines, provides a rich formalism to study the connection between causality and probability from an epistemological perspective. This article compares three assumptions in the literature that seem to constrain the connection between causality and probability in the style of Occam's razor. The trio includes two minimality assumptions-one formulated by Spirtes, Glymour, and Scheines (SGS) and the other due to Pearl-and the more well-known faithfulness or stability assumption. In terms of logical strength, it is fairly obvious that the three form a sequence of increasingly stronger assumptions. The focus of this article, however, is to investigate the nature of their relative strength. The comparative analysis reveals an important sense in which Pearl's minimality assumption is as strong as the faithfulness assumption and identifies a useful condition under which it is as safe as SGS's relatively secure minimality assumption. Both findings have notable implications for the theory and practice of causal inference.","Both findings have notable implications for the theory and practice of causal inference."
"This study investigated the physiological impact of changing electron donor-acceptor ratios on electron transfer pathways in the metabolically flexible subsurface bacterium Shewanella oneidensis, using batch and chemostat cultures, with an azo dye (ramazol black B) as the model electron acceptor. Altering the growth rate did result in changes in biomass yield, but not in other key physiological parameters including the total cytochrome content of the cells, the production of extracellular flavin redox shuttles or the potential of the organism to reduce the azo dye. Dramatic increases in the ability to reduce the dye were noted when cells were grown under conditions of electron acceptor (fumarate) limitation, although the yields of extracellular redox mediators (flavins) were similar under conditions of electron donor (lactate) or acceptor limitation. FT-IR spectroscopy confirmed shifts in the metabolic fingerprints of cells grown under these contrasting conditions, while spectrophotometric analyses supported a critical role for c-type cytochromes, expressed at maximal concentrations under conditions of electron acceptor limitation. Finally, key intracellular metabolites were quantified in batch experiments at various electron donor and acceptor ratios and analysed using discriminant analysis and a Bayesian network to construct a central metabolic pathway model for cells grown under conditions of electron donor or acceptor limitation. These results have identified key mechanisms involved in controlling electron transfer in Shewanella species, and have highlighted strategies to maximise reductive activity for a range of bioprocesses.",""
"One basic approach to learn Bayesian networks (BNs) from data is to apply a search procedure to explore the set of candidate networks for the database in light of a scoring metric, where the most popular stochastic methods are based on some meta-heuristic mechanisms, such as Genetic Algorithm, Evolutionary Programming and Ant Colony Optimization. In this paper, we have developed a new algorithm for learning BNs which employs a recently introduced meta-heuristic: artificial bee colony (ABC). All the phases necessary to tackle our learning problem using this meta-heuristic are described, and some experimental results to compare the performance of our ABC-based algorithm with other algorithms are given in the paper.",""
"Bayesian networks are important knowledge representation tools for handling uncertain pieces of information. The success of these models is strongly related to their capacity to represent and handle dependence relations. Some forms of Bayesian networks have been successfully applied in many classification tasks. In particular, naive Bayes classifiers have been used for intrusion detection and alerts correlation. This paper analyses the advantage of adding expert knowledge to probabilistic classifiers in the context of intrusion detection and alerts correlation. As examples of probabilistic classifiers, we will consider the well-known Naive Bayes, Tree Augmented Na < ve Bayes (TAN), Hidden Naive Bayes (HNB) and decision tree classifiers. Our approach can be applied for any classifier where the outcome is a probability distribution over a set of classes (or decisions). In particular, we study how additional expert knowledge such as \"it is expected that 80 % of traffic will be normal\" can be integrated in classification tasks. Our aim is to revise probabilistic classifiers' outputs in order to fit expert knowledge. Experimental results show that our approach improves existing results on different benchmarks from intrusion detection and alert correlation areas.","Some forms of Bayesian networks have been successfully applied in many classification tasks."
"Falls in geriatry are associated with important morbidity, mortality and high healthcare costs. Because of the large number of variables related to the risk of falling, determining patients at risk is a difficult challenge. The aim of this work was to validate a tool to detect patients with high risk of fall using only bibliographic knowledge. Thirty articles corresponding to 160 studies were used to modelize fall risk. A retrospective case-control cohort including 288 patients (88 +/- A 7 years) and a prospective cohort including 106 patients (89 +/- A 6 years) from two geriatric hospitals were used to validate the performances of our model. We identified 26 variables associated with an increased risk of fall. These variables were split into illnesses, medications, and environment. The combination of the three associated scores gives a global fall score. The sensitivity and the specificity were 31.4, 81.6, 38.5, and 90 %, respectively, for the retrospective and the prospective cohort. The performances of the model are similar to results observed with already existing prediction tools using model adjustment to data from numerous cohort studies. This work demonstrates that knowledge from the literature can be synthesized with Bayesian networks.",""
"This paper presents and discusses further aspects of the subjectivist interpretation of probability (also known as the 'personalist' view of probabilities) as initiated in earlier forensic and legal literature. It shows that operational devices to elicit subjective probabilities - in particular the so-called scoring rules - provide additional arguments in support of the standpoint according to which categorical claims of forensic individualisation do not follow from a formal analysis under that view of probability theory. (C) 2013 Forensic Science Society. Published by Elsevier Ireland Ltd. All rights reserved.",""
"Sex-related homicides tend to arouse wide media coverage and thus raise the urgency to find the responsible offender. However, due to the low frequency of such crimes, domain knowledge lacks completeness. We have therefore accumulated a large data-set and apply several structural learning algorithms to the data in order to combine their results into a single general graphic model. The graphical model broadly presents a distinction between an offender and a situation-driven crime. A situation-driven crime may be characterised by, amongst others, an offender lacking preparation and typically attacking a known victim in familiar surroundings. On the other hand, offender-driven crimes may be identified by the high level of forensic awareness demonstrated by the offender and the sophisticated measures applied to control the victim. The prediction performance of the graphical model is evaluated via a model averaging approach on the outcome variable offender's age. The combined graph undercuts the error rate of the single algorithms and an appropriate threshold results in an error rate of less than 10%, which describes a promising level for an actual implementation by the police.",""
"Causal relationships among variables can be depicted by a causal network of these variables. We propose a local structure learning approach for discovering the direct causes and the direct effects of a given target variable. In the approach, we first find the variable set of parents, children, and maybe some descendants (PCD) of the target variable, but generally we cannot distinguish the parents from the children in the PCD of the target variable. Next, to distinguish the causes from the effects of the target variable, we find the PCD of each variable in the PCD of the target variable, and we repeat the process of finding PCDs along the paths starting from the target variable. Without constructing a whole network over all variables, we find only a local structure around the target variable. Theoretically, we show the correctness of the proposed approach under the assumptions of faithfulness, causal sufficiency, and that conditional independencies are correctly checked.",""
"Identifying causal structures from observations is fundamental in many applications. Probabilistic graphical models provide a unifying framework for capturing complex causal dependencies among random variables. Recently, a linear non-Gaussian acyclic model (LiNGAM) and some smart algorithms (ICA-LiNGAM, DirectLiNGAM) have been proposed, which outperform previous graphical models and learning methods in identifying variable orders. We propose new solutions (TMFLiNGAM, SchurLiNGAM and RCLiNGAM) to the LiNGAM learning task from the perspective of matrix identification. TMFLiNGAM and SchurLiNGAM recover orders more directly, and RCLiNGAM can improve the accuracy of previous algorithms on uniform and sparse structures. The perspective also facilitates the learning of sparse models where the performance of all independent component analysis-based algorithms can be improved by reconstructing variable orders from the inverse of separation matrices. Experimental results under various settings provide average evaluations over the learning methods, and verify the effectiveness of our perspective and algorithms.",""
"The presence of occlusions in facial images is inevitable in unconstrained scenarios. However recognizing occluded faces remains a partially solved problem in computer vision. In this contribution we propose a novel Bayesian technique inspired by psychophysical mechanisms relevant to face recognition to address the facial occlusion problem. For some individuals certain facial regions, e.g. features comprising of some of the upper face, might be more discriminative than the rest of the features in the face. For others, it might be the features over the mid face and some of the lower face that are important. The proposed approach in this paper, will allow for such a psychophysical analysis to be factored into the recognition process. We have discovered and modeled similarity mappings that exist in facial domains by means of Bayesian Networks. The model can efficiently learn and exploit these mappings from the facial domain and hence capable of tackling uncertainties caused by occlusions. The proposed technique shows improved recognition rates over state of the art techniques. (C) 2012 Elsevier B.V. All rights reserved.",""
"Structure learning of Bayesian Networks (BNs) is an important topic in machine learning. Driven by modern applications in genetics and brain sciences, accurate and efficient learning of large-scale BN structures from high-dimensional data becomes a challenging problem. To tackle this challenge, we propose a Sparse Bayesian Network (SBN) structure learning algorithm that employs a novel formulation involving one L1-norm penalty term to impose sparsity and another penalty term to ensure that the learned BN is a Directed Acyclic Graph (DAG)-a required property of BNs. Through both theoretical analysis and extensive experiments on 11 moderate and large benchmark networks with various sample sizes, we show that SBN leads to improved learning accuracy, scalability, and efficiency as compared with 10 existing popular BN learning algorithms. We apply SBN to a real-world application of brain connectivity modeling for Alzheimer's disease (AD) and reveal findings that could lead to advancements in AD research.",""
"Bayesian networks proved to be a useful tool in many technical fields as well as in forensic sciences. The present paper proposes a novel application of Bayesian networks in forensic engineering, focusing on the analysis of technical causes of a catastrophic bridge downfall. During repair a road bridge over important railway lines suddenly slipped down from temporary supports. Incidentally at the same time an intercity train approached the location and crashed into the collapsed bridge at a high speed. The accident resulted in great societal and economic consequences. Forensic investigation concerning causes of the bridge collapse was complicated due to the additional damage caused by the train. Moreover, the remaining structural elements of the collapsed bridge and temporary supports were shortly after the accident removed to renew railway traffic. Background materials of the investigation and additional detailed structural analyses did not reveal any convincing evidence of the initiation cause. Critical consideration of all possible causes including aerodynamic effects supplemented by a causal (Bayesian) network finally resulted in identification of the most significant causes including insufficient foundation and overall stiffness of temporary supports. (C) 2013 Elsevier Ltd. All rights reserved.",""
"Thanks to their inherent properties, probabilistic graphical models are one of the prime candidates for machine learning and decision making tasks especially in uncertain domains. Their capabilities, like representation, inference and learning, if used effectively, can greatly help to build intelligent systems that are able to act accordingly in different problem domains. Bayesian networks are one of the most widely used class of these models. Some of the inference and learning tasks in Bayesian networks involve complex optimization problems that require the use of meta-heuristic algorithms. Evolutionary algorithms, as successful problem solvers, are promising candidates for this purpose. This paper reviews the application of evolutionary algorithms for solving some NP-hard optimization tasks in Bayesian network inference and learning. (c) 2013 Elsevier Inc. All rights reserved.","Their capabilities, like representation, inference and learning, if used effectively, can greatly help to build intelligent systems that are able to act accordingly in different problem domains."
"Greedy Equivalence Search (GES) is nowadays the state of the art algorithm for learning Bayesian networks (BNs) from complete data. However, from a practical point of view, this algorithm may not be efficient enough to deal with data from high dimensionality and/or complex domains. This paper proposes some modifications to GES aimed at increasing its efficiency. Under the faithfulness assumption, the modified algorithms preserve the same theoretical properties of the original one, that is, they recover a perfect map of the target distribution in the large sample limit. Moreover, experimental results confirm that, although the proposed methods carry out a significantly smaller number of computations, the quality of the BNs learned can be compared with those obtained with GES. (C) 2012 Elsevier Inc. All rights reserved.",""
"The marginal likelihood of the data computed using Bayesian score metrics is at the core of score+search methods when learning Bayesian networks from data. However, common formulations of those Bayesian score metrics rely on free parameters which are hard to assess. Recent theoretical and experimental works have also shown that the commonly employed BDe score metric is strongly biased by the particular assignments of its free parameter known as the equivalent sample size. This sensitivity means that poor choices of this parameter lead to inferred BN models whose structure and parameters do not properly represent the distribution generating the data even for large sample sizes. In this paper we argue that the problem is that the BDe metric is based on assumptions about the BN model parameters distribution assumed to generate the data which are too strict and do not hold in real settings. To overcome this issue we introduce here an approach that tries to marginalize the meta-parameter locally, aiming to embrace a wider set of assumptions about these parameters. It is shown experimentally that this approach offers a robust performance, as good as that of the standard BDe metric with an optimum selection of its free parameter and, in consequence, this method prevents the choice of wrong settings for this widely applied Bayesian score metric. (C) 2012 Elsevier Inc. All rights reserved.",""
"Latent tree models were proposed as a class of models for unsupervised learning, and have been applied to various problems such as clustering and density estimation. In this paper, we study the usefulness of latent tree models in another paradigm, namely supervised learning. We propose a novel generative classifier called latent tree classifier (LTC). An LTC represents each class-conditional distribution of attributes using a latent tree model, and uses Bayes rule to make prediction. Latent tree models can capture complex relationship among attributes. Therefore, LTC is able to approximate the true distribution behind data well and thus achieves good classification accuracy. We present an algorithm for learning LTC and empirically evaluate it on an extensive collection of UCI data. The results show that LTC compares favorably to the state-of-the-art in terms of classification accuracy. We also demonstrate that LTC can reveal underlying concepts and discover interesting subgroups within each class. (C) 2012 Elsevier Inc. All rights reserved.","We propose a novel generative classifier called latent tree classifier (LTC)."
"Over the past few years, fault detection for computer networks has attracted extensive attentions for its importance in network management. Most existing fault detection methods are based on active probing techniques which can detect the occurrence of faults fast and precisely. But these methods suffer from the limitation of traffic overhead, especially in large scale networks. To relieve traffic overhead induced by active probing based methods, a new fault detection method, whose key is to divide the detection process into multiple stages, is proposed in this paper. During each stage, only a small region of the network is detected by using a small set of probes. Meanwhile, it also ensures that the entire network can be covered after multiple detection stages. This method can guarantee that the traffic used by probes during each detection stage is small sufficiently so that the network can operate without severe disturbance from probes. Several simulation results verify the effectiveness of the proposed method. (C) 2013 Elsevier Ltd. All rights reserved.",""
"Traditional emotion models, when tagging single emotions in documents, often ignore the fact that most documents convey complex human emotions. In this paper, we join emotion analysis with topic models to find complex emotions in documents, as well as the intensity of the emotions, and study how the document emotions vary with topics. Hierarchical Bayesian networks are employed to generate the latent topic variables and emotion variables. On average, our model on single emotion classification outperforms the traditional supervised machine learning models such as SVM and Naive Bayes. The other model on the complex emotion classification also achieves promising results. We thoroughly analyze the impact of vocabulary quality and topic quantity to emotion and intensity prediction in our experiments. The distribution of topics such as Friend and Job are found to be sensitive to the documents' emotions, which we call emotion topic variation in this paper. This reveals the deeper relationship between topics and emotions. (c) 2012 Elsevier Ltd. All rights reserved.","On average, our model on single emotion classification outperforms the traditional supervised machine learning models such as SVM and Naive Bayes."
"This paper discusses recurrent multi-criteria, multi-attribute decision problems. Because of the possibility of decision-maker ignorance or low decision-maker involvement the decision problem structuring is done once for all by a group of experts and does not involve the implication of the decision makers. We propose an original model based on Bayesian networks, which provides a decision process that helps the decision-maker to select an appropriate alternative among a set of alternatives, taking into account multiple criteria that are often conflicting. Our model makes it possible to represent in the same model the decision case (i.e., the decision-maker characteristics, contextual characteristics, their needs and preferences), the set of alternatives with the different attributes, and the choice criteria. The model allows us to compute the value of three essential elements: the importance of each criterion, which is based on the decision-case characteristics; each criterion's evaluation index in terms of the alternative; and each criterion's satisfaction index. The recurrent problem of choosing a manual wheelchair (MWC) illustrates the construction and use of our model. (c) 2012 Elsevier Ltd. All rights reserved.",""
"In a previous premises-level case-control study of the 2007 equine influenza outbreak in Australia, the protective effect of several variables representing on-farm biosecurity practices was identified. However, using logistic regression it was not possible to definitively identify individual effects and associations between each of the personal biosecurity measures implemented by horse premises owners and managers in the face of the outbreak. In this study we apply Bayesian network modelling to identify the complex web of associations between these variables, horse premises infection status and other premises-level covariates. We focussed this analysis primarily on the inter-relationship between the nine variables representing on-farm personal biosecurity measures (of people residing on the premises and those visiting), and all other variables from the final logistic regression model of our previous analysis. Exact structure discovery was used to identify the globally optimal model from across the landscape of all directed acyclic graphs possible for our dataset. Bootstrapping was used to adjust the model for over-fitting. Our final Bayesian graphic network model included 18 variables linked by 23 arcs, each arc analogous to a single multivariable generalised linear model, combined in a probabilistically coherent way. Amongst the personal biosecurity measures, having a footbath in place, certain practices of visitors (hand-washing, changing clothes and shoes) in contact with the horses, and the regularity of horse handling were statistically associated with premises infection status. The results of this in-depth analysis provide new insight into the complex web of direct and indirect associations between risk factors and horse premises infection status during the first 7 weeks of the 2007 equine influenza outbreak in Australia. In future outbreaks, unnecessary contact and handling of horses should be avoided, especially by those coming from off the premises. Prior to any such contact, persons handling horses should use a footbath (if present), change their clothes and shoes, and wash their hands. (C) 2013 Elsevier B.V. All rights reserved.","However, using logistic regression it was not possible to definitively identify individual effects and associations between each of the personal biosecurity measures implemented by horse premises owners and managers in the face of the outbreak."
"Many pathogens are sensitive to climatic variables and this is reflected in their seasonality of occurrence and transmission. The identification of environmental conditions that influence disease occurrence can be subtle, particularly considering their complex interdependencies in addition to those relationships between climate and disease. Statistical treatment of environmental variables is often dependent on their correlations and thus descriptions of climate are often restricted to means rather than accounting for the more precise aspects (including mean, maximum, minimum, variability). Here we utilize a novel multivariate statistical modelling approach, additive Bayesian network (ABN) analyses, to identify the inter-linkages of different weather variables to better capture short-term environmental conditions that are important drivers of disease. We present a case study that explores weather as a driver of disease in livestock systems. We utilize quality assurance health scheme data on ten major diseases of pigs from 875 finishing pig herds distributed across the United Kingdom over 7 years (2005-2011). We examine the relationship between the occurrence of these pathologies and contemporary weather conditions measured by local meteorological stations. All ten pathologies were associated with at least 2 other pathologies (maximum 6). Three pathologies were associated directly with temperature variables: papular dermatitis, enzootic pneumonia and milk spots. Latitude was strongly associated with multiple pathologies, though associations with longitude were eliminated when clustering for repeated observations of farms was assessed. The identification of relationships between climatic factors and different (potentially related) diseases offers a more comprehensive insight into the complex role of seasonal drivers and herd health status than traditional analytical methods. (C) 2013 Elsevier B.V. All rights reserved.",""
"While the genesis of antimicrobial resistance (AMR) in animal production is a high profile topic in the media and the scientific community, it is still not well understood. The epidemiology of AMR is complex. This complexity is demonstrated by extensive biological and evolutionary mechanisms which are potentially impacted by farm management and husbandry practices - the risk factors. Many parts of this system have yet to be fully described. Notably, the occurrence of multiple resistance patterns is the rule rather than exception - the multivariate problem. A first essential step in the development of any comprehensive risk factor analysis - whose goal is the prevention or reduction of AMR - is to describe those associations between different patterns of resistance which are systematic. That is, have sufficient statistical support for these patterns to be considered robust features of the underlying epidemiological system, and whose presence must therefore be incorporated into any risk factor analysis of AMR for it to be meaningful with respect to the farm environment. Presented here is a case study that seeks to identify systematic associations between patterns of resistance to 13 different antimicrobials in Escherichia coli isolates obtained from composite finisher (>80 kg) pig faecal samples obtained from Canada's five major pork producing provinces. The use of a Bayesian network analysis approach allowed us to identify many systematic associations between individual antimicrobial resistances. Sixteen of these resistances are corroborated with existing literature. These associations are distributed between several important classes of antimicrobials including the beta-lactams, folate biosynthesis inhibitors, tetracyclines, aminoglycosides and quinolones. This study presents an exciting first step towards the larger and far more ambitious goal of developing generic and holistic risk factor analyses for on-farm occurrence of AMR. Analyses of this nature would combine multivariate response variables (joint patterns of resistance) with multi-factorial causal factors from within the livestock production environment thereby permitting a more complete understanding of the epidemiology of antimicrobial resistance. Crown Copyright (C) 2013 Published by Elsevier B.V. All rights reserved.",""
"Reconstruction of road accidents combines objective and subjective action. The former concerns science, the latter assessment of human behavior in the context of objective findings. It is not uncommon for experts equipped with an arsenal of tools to obtain similar results of calculations, but to present radically different conclusions about the cause of the accident. The use of sophisticated methods of uncertainty analysis does not guarantee improvement in quality of reconstruction, because, increasingly, the most serious source of reduced reliability of reconstruction is problems in logical inference. In the article the structure of uncertainty and reliability of accident reconstruction was described. A definition of reliability of road accident reconstruction based on the theory of conditional probability and Bayesian network, as a function of modeling, data and expert reliability (defined in the text) was proposed. The uncertainty of reconstruction was made dependent only on the uncertainty of the data. This separation makes it possible to conduct a qualitative and quantitative analysis of reconstruction reliability and to analyze its sensitivity to component parameters, independently of the uncertainty analysis. An example of calculation was presented. The proposed formalism constitutes a tool helpful to explain, among other things, the paradox of reliable reconstruction despite its uncertain results or unreliable reconstruction despite high precision of results. This approach is of great importance in the reconstruction of road accidents, which goes far beyond the analysis of a single, homogeneous subsystem. (C) 2013 Elsevier Ireland Ltd. All rights reserved.","The use of sophisticated methods of uncertainty analysis does not guarantee improvement in quality of reconstruction, because, increasingly, the most serious source of reduced reliability of reconstruction is problems in logical inference."
"This paper reviews current research on scenario development in water resource management. We provide an overview of existing techniques, highlight any limitations, and discuss future research directions to improve scenario development practices for water resource planning. In water management, scenarios are used to account for uncertainties associated with climatic, socio-economic, and management conditions that affect the performance of water resource systems. These uncertainties affect future water supply, water demand and management strategy. Several water-related scenarios with qualitative and quantitative techniques are reviewed against a general scenario development procedure. Although the reviewed literature demonstrates that scenario development is an effective tool to deal with uncertain future water systems, two limitations of applied quantitative techniques were identified: (i) the need for extending discrete scenarios to continuous scenarios to more completely cover future conditions, and (ii) the need for introducing probabilistic scenarios to explicitly quantify uncertainties. These issues can be addressed using existing techniques from information theory and statistics, pointing the way forward for scenario development practices in water resource planning and management. (C) 2012 Elsevier Inc. All rights reserved.",""
"Warfarin therapy is known as a complex process because of the variation in the patients' response. Failure to deal with such variation may lead to death as a result of thrombosis or bleeding. The possible sources of variation such as concomitant illnesses and drug interactions have to be investigated by the clinician in order to deal with the variation. This paper describes a decision support system (DSS) using Bayesian networks for assisting clinicians to make better decisions in Warfarin therapy management. The DSS is developed in collaboration with a Swedish hospital group that manages Warfarin therapy for more than 3000 patients. The proposed model can assist the clinician in making dose-adjustment and follow-up interval decisions, investigating variation causes, and evaluating bleeding and thrombosis risks related to therapy. The model is built upon previous findings from medical literature, the knowledge of domain experts, and large dataset of patients. (C) 2012 Elsevier B.V. All rights reserved.",""
"We consider the problem of finding a directed acyclic graph (DAG) that optimizes a decomposable Bayesian network score. While in a favorable case an optimal DAG can be found in polynomial time, in the worst case the fastest known algorithms rely on dynamic programming across the node subsets, taking time and space 2(n), to within a factor polynomial in the number of nodes n. In practice, these algorithms are feasible to networks of at most around 30 nodes, mainly due to the large space requirement. Here, we generalize the dynamic programming approach to enhance its feasibility in three dimensions: first, the user may trade space against time; second, the proposed algorithms easily and efficiently parallelize onto thousands of processors; third, the algorithms can exploit any prior knowledge about the precedence relation on the nodes. Underlying all these results is the key observation that, given a partial order P on the nodes, an optimal DAG compatible with P can be found in time and space roughly proportional to the number of ideals of P, which can be significantly less than 2(n). Considering sufficiently many carefully chosen partial orders guarantees that a globally optimal DAG will be found. Aside from the generic scheme, we present and analyze concrete tradeoff schemes based on parallel bucket orders.",""
NA,""
"Breast cancer is the leading cause of cancer-related death for women in Tunisia and the prognosis of its metastasis remains a major problem for oncologists despite advances in treatment. In this work we use Bayesian networks to develop a decision support system that is based on the modeling of relationships between key signaling proteins and clinical and pathological characteristics of breast tumors and patients. Motivated by the lack of prior information on the parameters of the problem, we use the Implicit inference for the structure and parameter learning. A dataset of 84 Tunisian breast cancer patients was used and new prognosis factors were identified. The system predicts a metastasis risk for different patients by computing a score that is the joint probability of the Bayesian network using parameters estimated on the learning database. Based on the results of the developed system we identified that overexpression of ErbB2, ErbB3, bcl2 as well as of oestrogen and progesterone receptors associated with a low level of ErbB4 was the predominant profile associated with high risk of metastasis.","Motivated by the lack of prior information on the parameters of the problem, we use the Implicit inference for the structure and parameter learning."
"A method of calculating the exact top event probability of a fault tree with dynamic gates and repeated basic events is proposed. The top event probability of such a dynamic fault tree is obtained by converting the tree into an equivalent Markov model. However, the Markov-based method is not realistic for a complex system model because the number of states that should be considered in the Markov analysis increases explosively as the number of basic events in the model increases. To overcome this shortcoming, we propose an alternative method in this paper. It is a hybrid of a Bayesian network (BN) and an algebraic technique. First, modularization is applied to a dynamic fault tree. The detected modules are classified into two types: one satisfies the parental Markov condition and the other does not. The module without the parental Markov condition is replaced with an equivalent single event. The occurrence probability of this event is obtained as the sum of disjoint sequence probabilities. After the contraction of modules without parent Markov condition, the BN algorithm is applied to the dynamic fault tree. The conditional probability tables for dynamic gates are presented. The BN is a standard one and has hierarchical and modular features. Numerical example shows that our method works well for complex systems.","The detected modules are classified into two types: one satisfies the parental Markov condition and the other does not."
"Quantitative risk analysis is in principle an ideal method to map one's risks, but it has limitations due to the complexity of models, scarcity of data, remaining uncertainties, and above all because effort, cost, and time requirements are heavy. Also, software is not cheap, the calculations are not quite transparent, and the flexibility to look at various scenarios and at preventive and protective options is limited. So, the method is considered as a last resort for determination of risks. Simpler methods such as LOPA that focus on a particular scenario and assessment of protection for a defined initiating event are more popular. LOPA may however not cover the whole range of credible scenarios, and calamitous surprises may emerge. In the past few decades, Artificial Intelligence university groups, such as the Decision Systems Laboratory of the University of Pittsburgh, have developed Bayesian approaches to support decision making in situations where one has to weigh gains and costs versus risks. This paper will describe details of such an approach and will provide some examples of both discrete random variables, such as the probability values in a LOPA, and continuous distributions, which can better reflect the uncertainty in data. (c) 2012 Elsevier Ltd. All rights reserved.",""
"Aim Decision-making for conservation management often involves evaluating risks in the face of environmental uncertainty. Models support decision-making by (1) synthesizing available knowledge in a systematic, rational and transparent way and (2) providing a platform for exploring and resolving uncertainty about the consequences of management decisions. Despite their benefits, models are still not used in many conservation decision-making contexts. In this article, we provide evidence of common objections to the use of models in environmental decision-making. In response, we present a series of practical solutions for modellers to help improve the effectiveness and relevance of their work in conservation decision-making. Location Global review. Methods We reviewed scientific and grey literature for evidence of common objections to the use of models in conservation decision-making. We present a set of practical solutions based on theory, empirical evidence and best-practice examples to help modellers substantively address these objections. Results We recommend using a structured decision-making framework to guide good modelling practice in decision-making and highlight a variety of modelling techniques that can be used to support the process. We emphasize the importance of participatory decision-making to improve the knowledge-base and social acceptance of decisions and to facilitate better conservation outcomes. Improving communication and building trust are key to successfully engaging participants, and we suggest some practical solutions to help modellers develop these skills. Main conclusions If implemented, we believe these practical solutions could help broaden the use of models, forging deeper and more appropriate linkages between science and management for the improvement of conservation decision-making.",""
"Independence of Conditionals (IC) has recently been proposed as a basic rule for causal structure learning. If a Bayesian network represents the causal structure, its Conditional Probability Distributions (CPDs) should be algorithmically independent. In this paper we compare IC with causal faithfulness (FF), stating that only those conditional independences that are implied by the causal Markov condition hold true. The latter is a basic postulate in common approaches to causal structure learning. The common spirit of FF and IC is to reject causal graphs for which the joint distribution looks 'non-generic'. The difference lies in the notion of genericity: FF sometimes rejects models just because one of the CPDs is simple, for instance if the CPD describes a deterministic relation. IC does not behave in this undesirable way. It only rejects a model when there is a non-generic relation between different CPDs although each CPD looks generic when considered separately. Moreover, it detects relations between CPDs that cannot be captured by conditional independences. IC therefore helps in distinguishing causal graphs that induce the same conditional independences (i.e., they belong to the same Markov equivalence class). The usual justification for FF implicitly assumes a prior that is a probability density on the parameter space. IC can be justified by Solomonoff's universal prior, assigning non-zero probability to those points in parameter space that have a finite description. In this way, it favours simple CPDs, and therefore respects Occam's razor. Since Kolmogorov complexity is uncomputable, IC is not directly applicable in practice. We argue that it is nevertheless helpful, since it has already served as inspiration and justification for novel causal inference algorithms.","We argue that it is nevertheless helpful, since it has already served as inspiration and justification for novel causal inference algorithms."
"Bow tie diagrams have become a popular method for risk analysis and safety management. This tool describes the whole scenario of a given risk graphically, and proposes preventive and protective barriers to reduce, respectively, its occurrence and its severity. The weakness of bow tie diagrams is that they are restricted to a graphical representation of different scenarios exclusively designed by experts that ignore the dynamic aspect of real systems. Thus, constructing bow tie diagrams in an automatic and dynamic way remains a real challenge. This paper proposes a new Bayesian approach to construct bow tie diagrams from real data and improve them by adding a new numerical that enables us to implement the appropriate preventive and protective barriers in a dynamic manner. (C) 2012 The Institution of Chemical Engineers. Published by Elsevier B.V. All rights reserved.",""
"This paper introduces a novel probabilistic activity modeling approach that mines recurrent sequential patterns called motifs from documents given as word time count matrices (e.g., videos). In this model, documents are represented as a mixture of sequential activity patterns (our motifs) where the mixing weights are defined by the motif starting time occurrences. The novelties are multi fold. First, unlike previous approaches where topics modeled only the co-occurrence of words at a given time instant, our motifs model the co-occurrence and temporal order in which the words occur within a temporal window. Second, unlike traditional Dynamic Bayesian networks (DBN), our model accounts for the important case where activities occur concurrently in the video (but not necessarily in synchrony), i.e., the advent of activity motifs can overlap. The learning of the motifs in these difficult situations is made possible thanks to the introduction of latent variables representing the activity starting times, enabling us to implicitly align the occurrences of the same pattern during the joint inference of the motifs and their starting times. As a third novelty, we propose a general method that favors the recovery of sparse distributions, a highly desirable property in many topic model applications, by adding simple regularization constraints on the searched distributions to the data likelihood optimization criteria. We substantiate our claims with experiments on synthetic data to demonstrate the algorithm behavior, and on four video datasets with significant variations in their activity content obtained from static cameras. We observe that using low-level motion features from videos, our algorithm is able to capture sequential patterns that implicitly represent typical trajectories of scene objects.","The learning of the motifs in these difficult situations is made possible thanks to the introduction of latent variables representing the activity starting times, enabling us to implicitly align the occurrences of the same pattern during the joint inference of the motifs and their starting times."
"There are four primary accident types at steel building construction (SC) projects: falls (tumbles), object falls, object collapse, and electrocution. Several systematic safety risk assessment approaches, such as fault tree analysis (ETA) and failure mode and effect criticality analysis (FMECA), have been used to evaluate safety risks at SC projects. However, these traditional methods ineffectively address dependencies among safety factors at various levels that fail to provide early warnings to prevent occupational accidents. To overcome the limitations of traditional approaches, this study addresses the development of a safety risk-assessment model for SC projects by establishing the Bayesian networks (BN) based on fault tree (FT) transformation. The BN-based safety risk-assessment model was validated against the safety inspection records of six SC building projects and nine projects in which site accidents occurred. The ranks of posterior probabilities from the BN model were highly consistent with the accidents that occurred at each project site. The model accurately provides site safety-management abilities by calculating the probabilities of safety risks and further analyzing the causes of accidents based on their relationships in BNs. In practice, based on the analysis of accident risks and significant safety factors, proper preventive safety management strategies can be established to reduce the occurrence of accidents on SC sites. (C) 2013 Elsevier Ltd. All rights reserved.",""
"Changes in the cortical expression of small non-coding microRNA (miRNA) have been observed in postmortem analysis of psychotic disorders. Antipsychotic drugs (APDs) are the most effective treatment option for these disorders and have been associated with changes in gene expression. MicroRNA regulate numerous genes involved in brain development and function. It is therefore plausible to question whether miRNA expression is also altered and hence whether they take part in the neuroleptic mechanism of action. We sought to investigate whether treatment with APDs induces changes in miRNA expression and query the functional implications of such changes. Furthermore, we investigated the possible functional interplay of miRNA-gene regulatory interactions. High-throughput miRNA profiling of the whole brain of C57BL/6 mice treated with haloperidol, olanzapine or clozapine for 7 days was performed. Functional analysis was conducted on the putative targets of altered microRNA. Significant miRNA-gene regulatory interactions were evaluated by the integration of genome-wide mRNA expression analysis using the Bayesian networks with splitting-averaging strategy and functional analysis conducted. Small subsets of miRNA were altered with each treatment with potential neurologically relevant influence. Metabolic pathways were enriched in olanzapine and clozapine treatments, possibly associated with their weight gain side effects. Neurologically and metabolically relevant miRNA-gene interaction networks were identified in the olanzapine treatment group. This study is the first to suggest a role for miRNA in the mechanism of APD action and the metabolic side effects of the atypical ADPs, and adds support for their consideration in pharmacogenomics.",""
"Although there is a long line of work on identifying duplicates in relational data, only a few solutions focus on duplicate detection in more complex hierarchical structures, like XML data. In this paper, we present a novel method for XML duplicate detection, called XMLDup. XMLDup uses a Bayesian network to determine the probability of two XML elements being duplicates, considering not only the information within the elements, but also the way that information is structured. In addition, to improve the efficiency of the network evaluation, a novel pruning strategy, capable of significant gains over the unoptimized version of the algorithm, is presented. Through experiments, we show that our algorithm is able to achieve high precision and recall scores in several data sets. XMLDup is also able to outperform another state-of-the-art duplicate detection solution, both in terms of efficiency and of effectiveness.",""
"In this study, we discuss and apply a novel and efficient algorithm for learning a local Bayesian network model in the vicinity of the ZNF217 oncogene from breast cancer microarray data without having to decide in advance which genes have to be included in the learning process. ZNF217 is a candidate oncogene located at 20q13, a chromosomal region frequently amplified in breast and ovarian cancer, and correlated with shorter patient survival in these cancers. To properly address the difficulties in managing complex gene interactions given our limited sample, statistical significance of edge strengths was evaluated using bootstrapping and the less reliable edges were pruned to increase the network robustness. We found that 13 out of the 35 genes associated with deregulated ZNF217 expression in breast tumours have been previously associated with survival and/or prognosis in cancers. Identifying genes involved in lipid metabolism opens new fields of investigation to decipher the molecular mechanisms driven by the ZNF217 oncogene. Moreover, nine of the 13 genes have already been identified as putative ZNF217 targets by independent biological studies. We therefore suggest that the algorithms for inferring local BNs are valuable data mining tools for unraveling complex mechanisms of biological pathways from expression data. The source code is available at http://www710.univ-lyon1. fr/similar to aaussem/Software.html. (c) 2012 Elsevier Ltd. All rights reserved.",""
"Latent Dirichlet allocation (LDA) is an important hierarchical Bayesian model for probabilistic topic modeling, which attracts worldwide interest and touches on many important applications in text mining, computer vision and computational biology. This paper represents the collapsed LDA as a factor graph, which enables the classic loopy belief propagation (BP) algorithm for approximate inference and parameter estimation. Although two commonly used approximate inference methods, such as variational Bayes (VB) and collapsed Gibbs sampling (GS), have gained great success in learning LDA, the proposed BP is competitive in both speed and accuracy, as validated by encouraging experimental results on four large-scale document datasets. Furthermore, the BP algorithm has the potential to become a generic scheme for learning variants of LDA-based topic models in the collapsed space. To this end, we show how to learn two typical variants of LDA-based topic models, such as author-topic models (ATM) and relational topic models (RTM), using BP based on the factor graph representations.","This paper represents the collapsed LDA as a factor graph, which enables the classic loopy belief propagation (BP) algorithm for approximate inference and parameter estimation."
"The essential precondition of implementing interventionist techniques of causal reasoning is that particular variables are identified as so-called intervention variables. While the pertinent literature standardly brackets the question how this can be accomplished in concrete contexts of causal discovery, the first part of this paper shows that the interventionist nature of variables cannot, in principle, be established based only on an interventionist notion of causation. The second part then demonstrates that standard observational methods that draw on Bayesian networks identify intervention variables only if they also answer the questions that can be answered by interventionist techniques-which are thus rendered dispensable. The paper concludes by suggesting a way of identifying intervention variables that allows for exploiting the whole inferential potential of interventionist techniques.",""
"In this paper, we deal with the observability problem in traffic networks and the optimal location of counting and scanning devices. After explaining what we mean by observability, the problems of what to observe, how to observe traffic data and how to incorporate prior or obsolete information together with the cases of genuine and pseudo-samples of flow data are discussed. Plate scanning information is dealt with and the flow amount of information measure of information corresponding to a subset of scanned links is analysed. Some pivoting and matrix techniques are given for solving the most common problems of observability of traffic flows in a network. Finally, the problem of optimal location of counters and plate scanning cameras is analysed and several examples are given.",""
"Architecture generation and evaluation are critical points in complex system design. Uncertainties concerning component characteristics and their impact onto overall system performance are often not taken into account in early design stages. In this paper, we propose a Bayesian network (BN) approach for system architecture generation and evaluation. A method relying on Bayesian network templates is proposed in order to represent an architecture design problem integrating uncertainties concerning component characteristics and component compatibility. These templates aim at modeling designers' knowledge concerning system architecture. We also propose an algorithm for architecture generation and evaluation related to the Bayesian network model with the objective of generating all possible architectures and filtering them in view to a defined confidence threshold. Within this algorithm, expert estimations on component compatibilities are used to estimate overall architecture uncertainty as a confidence level. The proposed approach is tested and illustrated on a case study of bicycle design. This first case shows how uncertainties concerning component compatibilities and components characteristics impact bicycle architecture generation. The method is, additionally, tested and implemented in the case of a radar antenna cooling system design in industry. Results highlight the relevance of the proposed approach in view to the generated solutions as well as other benefits such as reduced time for architecture generation, and a better overall understanding of the design problem. However, some limitations have been observed and call for enhancements like integration of designer's preferences and identification of possible trade-offs within the architecture. This method enables generation and evaluation of complex system architecture taking into account initial system requirements and designer's knowledge. Its usability and added-value have been verified on a large-scale system implemented in industry.",""
"Facial action recognition is concerned with recognizing the local facial motions from image or video. In recent years, besides the development of facial feature extraction techniques and classification techniques, prior models have been introduced to capture the dynamic and semantic relationships among facial action units. Previous works have shown that combining the prior models with the image measurements can yield improved performance in AU recognition. Most of these prior models, however, are learned from data, and their performance hence largely depends on both the quality and quantity of the training data. These data-trained prior models cannot generalize well to new databases, where the learned AU relationships are not present. To alleviate this problem, we propose a knowledge-driven prior model for AU recognition, which is learned exclusively from the generic domain knowledge that governs AU behaviors, and no training data are used. Experimental results show that, with no training data but generic domain knowledge, the proposed knowledge-driven model achieves comparable results to the data-driven model for specific database and significantly outperforms the data-driven models when generalizing to new data set.","In recent years, besides the development of facial feature extraction techniques and classification techniques, prior models have been introduced to capture the dynamic and semantic relationships among facial action units."
"We review the proposed mathematical models of the response to osmotic stress in yeast. These models mainly differ in the choice of mathematical representation (e. g. Bayesian networks, ordinary differential equations, or rule-based models), the extent to which the modeling is data-driven, and predictability. The overview exemplifies how one biological system can be modeled with various modeling techniques and at different levels of resolution, and how the choice typically is based on the amount and quality of available data, prior information of the system, and the research question in focus. As a natural part of the overview, we discuss requirements, advantages, and limitations of the different modeling approaches.",""
"Laboratory work is critical in undergraduate engineering courses. It is used to integrate theory and practice. This demands that laboratory activities are synchronized with lectures to maximize their derivable learning outcomes, which are measurable through assessment. The typical high costs of the traditional engineering laboratory, which often militate against the synchronization of laboratory activities and lectures, have catalyzed the increased adoption of virtual laboratories in engineering laboratory education. The principles of assessment in the virtual learning environment are essentially the same as in the traditional learning environment, with the same requirements for fairness, reliability, and validity. This motivated the incorporation, in a Virtual Electronic Laboratory (VEL) environment, of a Bayesian network-based tool for the performance assessment of students' laboratory work in the environment. This paper details a description of the assessment tool, its verification, evaluation (as an assessment tool within the VEL environment), and application processes.",""
"Rapid changes in global climate are likely to alter species assemblages and environmental characteristics resulting in novel ecosystems. The ability to predict characteristics of future ecosystems is crucial for environmental planning and the development of effective climate change adaptation strategies. This paper presents an approach for envisioning novel ecosystems in future climates. Focusing on riparian ecosystems, we use qualitative process models to predict likely abiotic and biotic changes in four case study systems: tropical coastal floodplains, temperate streams, high mountain streams and urban riparian zones. We concentrate on functional groups rather than individual species and consider dispersal constraints and the capacity for genetic adaptation. Our scenarios suggest that climatic changes will reduce indigenous diversity, facilitate non-indigenous invasion (especially C4 graminoids), increase fragmentation and result in simplified and less distinctive riparian ecosystems. Compared to models based on biota-environment correlations, process models built on mechanistic understanding (like Bayesian belief networks) are more likely to remain valid under novel climatic conditions. We posit that predictions based on species' functional traits will facilitate regional comparisons and can highlight effects of climate change on ecosystem structure and function. Ecosystems that have experienced similar modification to that expected under climate change (for example, altered flow regimes of regulated rivers) can be used to help inform and evaluate predictions. By manipulating attributes of these system models (for example, magnitude of climatic changes or adaptation strategies used), implications of various scenarios can be assessed and optimal management strategies identified.",""
"In this paper, a modified and improved quality of experience (QoE) estimation approach has been presented on the basis of the user-behavior modelling and context-aware modelling. We have implemented this proposed approach for peer-to-peer live video streaming systems because of the dynamically adaptive and distributed nature of these systems. The problem of estimating QoE is formulated as a Bayesian network model using user-behavior and context-related variables as nodes. The relationship between these nodes are established to find out estimated QoE, which is based on the user-behavior data and context data, and dynamically adapt to any change in their values. This relationship is further explored to provide the basis for more user-centric, context-aware and dynamically adaptive services and network systems as this in-service QoE is important for network optimization and service provisioning. Copyright (c) 2013 John Wiley & Sons, Ltd.",""
"It is important to predict both observable and hidden behaviors in complex engineering systems. However, compared with observable behavior, it is often difficult to establish a forecastingmodel for hidden behavior. The existing methods for predicting the hidden behavior cannot effectively and simultaneously use the hybrid information with uncertainties that include qualitative knowledge and quantitative data. Although belief rule base (BRB) has been employed to predict the observable behavior using the hybrid information with uncertainties, it is still not applicable to predict the hidden behavior directly. As such, in this paper, a new BRB-based model is proposed to predict the hidden behavior. In the proposed BRB-based model, the initial values of parameters are usually given by experts, thus some of them may not be accurate, which can lead to inaccurate prediction results. In order to solve the problem, a parameter estimation algorithm for training the parameters of the forecasting model is further proposed on the basis of maximum likelihood algorithm. Using the hybrid information with uncertainties, the proposedmodel can combine together with the parameter estimation algorithm and improve the forecasting precision in an integrated and effective manner. A case study is conducted to demonstrate the capability and potential applications of the proposed forecasting model with the parameter estimation algorithm.",""
"Due to the infeasibility of randomized controlled experiments, the existence of unobserved variables and the fact that equivalent direct acyclic graphs obtained generally can not be distinguished, it is difficult to learn the true causal relations of original graph. This paper presents an algorithm called BSPC based on adjacent nodes to learn the structure of Causal Bayesian Networks with unobserved variables by using observational data. It does not have to adjust the structure as the existing algorithms FCI and MBCS*, while it can guarantee to obtain the true adjacent nodes. More important is that algorithm BSPC reduces computational complexity and improves reliability of conditional independence tests. Theoretical results show that the new algorithm is correct. In addition, the advantages of BSPC in terms of the number of conditional independence tests and the number of orientation errors are illustrated with simulation experiments from which we can see that it is more suitable in order to learn the structure of Causal Bayesian Networks with latent variables. Moreover a better latent structure representation is returned.",""
"This paper considers the state-space smoothing problem in a distributed fashion. In the scenario of sensor networks, we assume that the nodes can be ordered in space and have access to noisy measurements relative to different but correlated states. The goal of each node is to obtain the minimum variance estimate of its own state conditional on all the data collected by the network using only local exchanges of information. We present a cooperative smoothing algorithm for Gauss Markov linear models and provide an exact convergence analysis for the algorithm, also clarifying its advantages over the Jacobi algorithm. Extensions of the numerical scheme able to perform field estimation using a grid of sensors are also derived. (C) 2013 Elsevier Ltd. All rights reserved.",""
"Conditional independence graphs are now widely applied in science and industry to display interactions between large numbers of variables. However, the computational load of structure identification grows with the number of nodes in the network and the sample size. A tailored version of the PC algorithm is proposed which is based on mutual information tests with a specified testing order, combined with false negative reduction and false positive control. It is found to be competitive with current structure identification methodologies for both estimation accuracy and computational speed and outperforms these in large scale scenarios. The methodology is also shown to approximate dense networks. The comparisons are made on standard benchmarking data sets and an anonymized large scale real life example.",""
"To relax the homogeneity assumption of classical dynamic Bayesian networks (DBNs), various recent studies have combined DBNs with multiple changepoint processes. The underlying assumption is that the parameters associated with time series segments delimited by multiple changepoints are a priori independent. Under weak regularity conditions, the parameters can be integrated out in the likelihood, leading to a closed-form expression of the marginal likelihood. However, the assumption of prior independence is unrealistic in many real-world applications, where the segment-specific regulatory relationships among the interdependent quantities tend to undergo gradual evolutionary adaptations. We therefore propose a Bayesian coupling scheme to introduce systematic information sharing among the segment-specific interaction parameters. We investigate the effect this model improvement has on the network reconstruction accuracy in a reverse engineering context, where the objective is to learn the structure of a gene regulatory network from temporal gene expression profiles. The objective of the present paper is to expand and improve an earlier conference paper in six important aspects. Firstly, we offer a more comprehensive and self-contained exposition of the methodology. Secondly, we extend the model by introducing an extra layer to the model hierarchy, which allows for information-sharing among the network nodes, and we compare various coupling schemes for the noise variance hyperparameters. Thirdly, we introduce a novel collapsed Gibbs sampling step, which replaces a less efficient uncollapsed Gibbs sampling step of the original MCMC algorithm. Fourthly, we show how collapsing and blocking techniques can be used for developing a novel advanced MCMC algorithm with significantly improved convergence and mixing. Fifthly, we systematically investigate the influence of the (hyper-)hyperparameters of the proposed model. Sixthly, we empirically compare the proposed global information coupling scheme with an alternative paradigm based on sequential information sharing.",""
"Current metrics for predicting bleaching episodes, e.g. NOAA's Coral Reef Watch Program, do not seem to apply well to Brazil's marginal reefs located in Bahia state and alternative predictive approaches must be sought for effective long term management. Bleaching occurrences at Abrolhos have been observed since the 1990s but with a much lower frequency/extent than for other reef systems worldwide. We constructed a Bayesian Belief Network (BN) to back-predict the intensity of bleaching events and learn how local and regional scale forcing factors interact to enhance or alleviate coral bleaching specific to Abrolhos. Bleaching intensity data were collected for several reef sites across Bahia state coast (similar to 12 degrees-20 degrees S; 37 degrees-40 degrees W) during the austral summer 1994-2005 and compared to environmental data: sea surface temperature (SST), diffuse light attenuation coefficient at 490 nm (K-490), rain precipitation, wind velocities, and El Nino Southern Oscillation (ENSO) proxies. Conditional independence tests were calculated to produce four specialized BNs, each with specific factors that likely regulate bleaching intensity. All specialized BNs identified that a five-day accumulated SST proxy (SSTAc5d) was the exclusive parent node for coral bleaching producing a total predictive rate of 88% based on SSTAc5d state. When SSTAc5d was simulated as unknown, the Thermal-Eolic Resultant BN kept the total predictive rate of 88%. Our approach has produced initial means to predict beaching intensity at Abrolhos. However, the robustness of the model required for management purposes must be further (and regularly) operationally tested with new in situ and remote sensing data. (C) 2013 Elsevier Ltd. All rights reserved.",""
"The ability of variation source diagnosis in the auto body assembly process plays an essential role in the success of the manufacturing enterprises. However, it is more challenging to identify the process faults associated with the compliant sheet metal parts based on small measurement data sets. A new Bayesian networks (BN) modeling approach under the condition of small data sets is proposed. The main causal links are identified based on mapping of the variation sensitivity matrix. The interaction effects are detected according to the conditional mutual information tests. After the network structure is determined, the Bayesian approach is used to obtain the conditional probability tables by incorporating prior probability distributions. The evaluation of diagnostic performance concerning evidence number and log-odds noise levels is also presented. A real bracket assembly case was used to illustrate the whole procedures for fixture fault diagnosis. The examined test cases demonstrate the proposed BN approach is practical and effective, even when incomplete evidences are observed and a medium-level noise is present.",""
"Multirelational classification aims to discover patterns across multiple interlinked tables (relations) in a relational database. In many large organizations, such a database often spans numerous departments and/or subdivisions, which are involved in different aspects of the enterprise such as customer profiling, fraud detection, inventory management, financial management, and so on. When considering classification, different phases of the knowledge discovery process are affected by economic utility. For instance, in the data preprocessing process, one must consider the cost associated with acquiring, cleaning, and transforming large volumes of data. When training and testing the data mining models, one has to consider the impact of the data size on the running time of the learning algorithm. In order to address these utility-based issues, the paper presents an approach to create a pruned database for multirelational classification, while minimizing predictive performance loss on the final model. Our method identifies a set of strongly uncorrelated subgraphs from the original database schema, to use for training, and discards all others. The experiments performed show that our strategy is able to, without sacrificing predictive accuracy, significantly reduce the size of the databases, in terms of the number of relations, tuples, and attributes.The approach prunes the sizes of databases by as much as 94 %. Such reduction also results in decreasing computational cost of the learning process. The method improves the multirelational learning algorithms' execution time by as much as 80 %. In particular, our results demonstrate that one may build an accurate model with only a small subset of the provided database.","Multirelational classification aims to discover patterns across multiple interlinked tables (relations) in a relational database."
"In this paper we give evidence that it is possible to characterize and detect those potential users of false invoices in a given year, depending on the information in their tax payment, their historical performance and characteristics, using different types of data mining techniques. First, clustering algorithms like SUM and neural gas are used to identify groups of similar behaviour in the universe of taxpayers. Then decision trees, neural networks and Bayesian networks are used to identify those variables that are related to conduct of fraud and/or no fraud, detect patterns of associated behaviour and establishing to what extent cases of fraud and/or no fraud can be detected with the available information. This will help identify patterns of fraud and generate knowledge that can be used in the audit work performed by the Tax Administration of Chile (in Spanish Servicio de Impuestos Internos (SII)) to detect this type of tax crime. (C) 2012 Elsevier Ltd. All rights reserved.",""
"This paper compares two statistical models of location within a smart flat during the day. The location is then identified with a task executed normally or repeated pathologically, e. g., in case of Alzheimer disease (AD), whereas a task persistence parameter assesses tendency to perseverate. Compared with a Polya's urns derived approach, the Markovian one is more effective and offers up to 98 percent of good prediction using only the last known location but distinguishing days of week. To extend these results to a multisensor context, some difficulties must be overcome. An external knowledge is made from a set of observable random variables provided by body sensors and organized either in a Bayesian network or in a reference knowledge base system (KBS) containing the person's actimetric profile. When data missed or errors occurred, an estimate of the joint probabilities of these random variables and hence the probability of all events appearing in the network or the KBS was developed and corrects the bias of the Lancaster and Zentgraf classical approach which in certain circumstances provides negative estimates. Finally, we introduce a correction corresponding to a possible loss of the person's synchronization with the nycthemeral (day versus night) zeitgebers (synchronizers) to avoid false alarms.",""
"We review the applicability of Bayesian networks (BNs) for discovering relations between genes, environment, and disease. By translating probabilistic dependencies among variables into graphical models and vice versa, BNs provide a comprehensible and modular framework for representing complex systems. We first describe the Bayesian network approach and its applicability to understanding the genetic and environmental basis of disease. We then describe a variety of algorithms for learning the structure of a network from observational data. Because of their relevance to real-world applications, the topics of missing data and causal interpretation are emphasized. The BN approach is then exemplified through application to data from a population-based study of bladder cancer in New Hampshire, USA. For didactical purposes, we intentionally keep this example simple. When applied to complete data records, we find only minor differences in the performance and results of different algorithms. Subsequent incorporation of partial records through application of the EM algorithm gives us greater power to detect relations. Allowing for network structures that depart from a strict causal interpretation also enhances our ability to discover complex associations including gene-gene (epistasis) and gene-environment interactions. While BNs are already powerful tools for the genetic dissection of disease and generation of prognostic models, there remain some conceptual and computational challenges. These include the proper handling of continuous variables and unmeasured factors, the explicit incorporation of prior knowledge, and the evaluation and communication of the robustness of substantive conclusions to alternative assumptions and data manifestations.",""
"Chemical and biological forensic programs focus on the identification of a threat and acquisition of laboratory measurements to determine how a threat agent may have been produced. However, to generate investigative leads, it might also be useful to identify institutions where the same agent has been produced by the same or a very similar process, since the producer of the agent may have learned methods at a university or similar institution. We have developed a Bayesian network framework that fuses hard and soft data sources to assign probability to production practices. It combines the results of laboratory measurements with an automatic text reader to scan scientific literature and rank institutions that had published papers on the agent of interest in order of the probability that the institution has the capability to generate the sample of interest based on laboratory data. We demonstrate the Bayesian network on an example case from microbial forensics, predicting the methods used to produce Bacillus anthracis spores based on mass spectrometric measurements and identifying institutions that have a history of growing Bacillus spores using the same or highly similar methods. We illustrate that the network model can assign a higher posterior probability than expected by random chance to appropriate institutions when trained using only a small set of manually analyzed documents. This is the first example of an automated methodology to integrate experimental and textual data for the purpose of investigative forensics. (C) 2013 Elsevier Ireland Ltd. All rights reserved.",""
"A set of predictor variables is said to be intrinsically multivariate predictive (IMP) for a target variable if all properly contained subsets of the predictor set are poor predictors of the. target but the full set predicts the target with great accuracy. In a previous article, the main properties of IMP Boolean variables have been analytically described, including the introduction of the IMP score, a metric based on the coefficient of determination (CoD) as a measure of predictiveness with respect to the target variable. It was shown that the IMP score depends on four main properties: logic of connection, predictive power, covariance between predictors and marginal predictor probabilities (biases). This paper extends that work to a broader context, in an attempt to characterize properties of discrete Bayesian networks that contribute to the presence of variables (network nodes) with high IMP scores. We have found that there is a relationship between the IMP score of a node and its territory size, i.e., its position along a pathway with one source: nodes far from the source display larger IMP scores than those closer to the source, and longer pathways display larger maximum IMP scores. This appears to be a consequence of the fact that nodes with small territory have larger probability of having highly covariate predictors, which leads to smaller IMP scores. In addition, a larger number of XOR and NXOR predictive logic relationships has positive influence over the maximum IMP score found in the pathway. This work presents analytical results based on a simple structure network and an analysis involving random networks constructed by computational simulations. Finally, results from a real Bayesian network application are provided. (C) 2012 Elsevier Inc. All rights reserved.",""
"This paper presents the application of a framework for reliability analysis of substation automation (SA) system functions. The framework is based on probabilistic relational models which combines probabilistic reasoning offered by Bayesian networks together with architecture models in form of entity relationship diagrams. In the analysis, both the physical infrastructure, and the logical structure of the system, is regarded in terms of qualitative modeling and quantitative analysis. Moreover, the framework treats the aspect of failures caused by software. An example is detailed with the framework applied to an IEC 61850-based SA system. The logical structure, including functions and their relations, is modeled in accordance with Pieces of Information for COMmunication (PICOM) defined in the IEC 61850 standard. By applying PICOMs as frame of reference when modeling functions the model instantiation becomes more standardized compared to subjectively defining functions. A quantitative reliability analysis is performed on a function for tipping a circuit breaker in case of mismatch between currents. The result is presented both in terms of a qualitative architecture model and a quantitative result showing the probability of successful operation during a period of one year.",""
"Eukaryotic gene transcription is a complex process, which requires the orchestrated recruitment of a large number of proteins, such as sequence-specific DNA binding factors, chromatin remodelers and modifiers, and general transcription machinery, to regulatory regions. Previous works have shown that these regulatory proteins favor specific organizational theme along promoters. Details about how they cooperatively regulate transcriptional process, however, remain unclear. We developed a method to reconstruct a Bayesian network (BN) model representing functional relationships among various transcriptional components. The positive/negative influence between these components was measured from protein binding and nucleosome occupancy data and embedded into the model. Application on S. cerevisiae ChIP-Chip data showed that the proposed method can recover confirmed relationships, such as Isw1-Pol II, TFIIH-Pol II, TFIIB-TBP, Pol II-H3K36Me3, H3K4Me3-H3K14Ac, etc. Moreover, it can distinguish colocating components from functionally related ones. Novel relationships, e. g., ones between Mediator and chromatin remodeling complexes (CRCs), and the combinatorial regulation of Pol II recruitment and activity by CRCs and general transcription factors (GTFs), were also suggested. Conclusion: protein binding events during transcription positively influence each other. Among contributing components, GTFs and CRCs play pivotal roles in transcriptional regulation. These findings provide insights into the regulatory mechanism.",""
"With Partially Observable Markov Decision Processes (POMDPs), Intelligent Tutoring Systems (ITSs) can model individual learners from limited evidence and plan ahead despite uncertainty. However, POMDPs need appropriate representations to become tractable in ITSs that model many learner features, such as mastery of individual skills or the presence of specific misconceptions. This article describes two POMDP representations-state queues and observation chains-that take advantage of ITS task properties and let POMDPs scale to represent over 100 independent learner features. A real-world military training problem is given as one example. A human study (n = 14) provides initial validation for the model construction. Finally, evaluating the experimental representations with simulated students helps predict their impact on ITS performance. The compressed representations can model a wide range of simulated problems with instructional efficacy equal to lossless representations. With improved tractability, POMDP ITSs can accommodate more numerous or more detailed learner states and inputs.",""
"Objective: Large health care datasets normally have a hierarchical structure, in terms of levels, as the data have been obtained from different practices, hospitals, or regions. Multilevel regression is the technique commonly used to deal with such multilevel data. However, for the statistical analysis of interactions between entities from a domain, multilevel regression yields little to no insight. While Bayesian networks have proved to be useful for analysis of interactions, they do not have the capability to deal with hierarchical data. In this paper, we describe a new formalism, which we call multilevel Bayesian networks; its effectiveness for the analysis of hierarchically structured health care data is studied from the perspective of multimorbidity. Methods: Multilevel Bayesian networks are formally defined and applied to analyze clinical data from family practices in The Netherlands with the aim to predict interactions between heart failure and diabetes mellitus. We compare the results obtained with multilevel regression. Results: The results obtained by multilevel Bayesian networks closely resembled those obtained by multilevel regression. For both diseases, the area under the curve of the prediction model improved, and the net reclassification improvements were significantly positive. In addition, the models offered considerable more insight, through its internal structure, into the interactions between the diseases. Conclusions: Multilevel Bayesian networks offer a suitable alternative to multilevel regression when analyzing hierarchical health care data. They provide more insight into the interactions between multiple diseases. Moreover, a multilevel Bayesian network model can be used for the prediction of the occurrence of multiple diseases, even when some of the predictors are unknown, which is typically the case in medicine. (C) 2013 Elsevier B.V. All rights reserved.","Multilevel regression is the technique commonly used to deal with such multilevel data."
"Objective: The human immunodeficiency virus (HIV) is one of the fastest evolving organisms in the planet. Its remarkable variation capability makes HIV able to escape from multiple evolutionary forces naturally or artificially acting on it, through the development and selection of adaptive mutations. Although most drug resistance mutations have been well identified, the dynamics and temporal patterns of appearance of these mutations can still be further explored. The use of models to predict mutational pathways as well as temporal patterns of appearance of adaptive mutations could greatly benefit clinical management of individuals under antiretroviral therapy. Methods and material: We apply a temporal nodes Bayesian network (TNBN) model to data extracted from the Stanford HIV drug resistance database in order to explore the probabilistic relationships between drug resistance mutations and antiretroviral drugs unveiling possible mutational pathways and establishing their probabilistic-temporal sequence of appearance. Results: In a first experiment, we compared the TNBN approach with other models such as static Bayesian networks, dynamic Bayesian networks and association rules. TNBN achieved a 64.2% sparser structure over the static network. In a second experiment, the TNBN model was applied to a dataset associating antiretroviral drugs with mutations developed under different antiretroviral regimes. The learned models captured previously described mutational pathways and associations between antiretroviral drugs and drug resistance mutations. Predictive accuracy reached 90.5%. Conclusion: Our results suggest possible applications of TNBN for studying drug-mutation and mutation mutation networks in the context of antiretroviral therapy, with direct impact on the clinical management of patients under antiretroviral therapy. This opens new horizons for predicting HIV mutational pathways in immune selection with relevance for antiretroviral drug development and therapy plan. (C) 2013 Elsevier B.V. All rights reserved.",""
"Objective: One of the hardest technical tasks in employing Bayesian network models in practice is obtaining their numerical parameters. In the light of this difficulty, a pressing question, one that has immediate implications on the knowledge engineering effort, is whether precision of these parameters is important. In this paper, we address experimentally the question whether medical diagnostic systems based on Bayesian networks are sensitive to precision of their parameters. Methods and materials: The test networks include HEPAR II, a sizeable Bayesian network model for diagnosis of liver disorders and six other medical diagnostic networks constructed from medical data sets available through the Irvine Machine Learning Repository. Assuming that the original model parameters are perfectly accurate, we lower systematically their precision by rounding them to progressively courser scales and check the impact of this rounding on the models' accuracy. Results: Our main result, consistent across all tested networks, is that imprecision in numerical parameters has minimal impact on the diagnostic accuracy of models, as long as we avoid zeroes among parameters. Conclusion: The experiments' results provide evidence that as long as we avoid zeroes among model parameters, diagnostic accuracy of Bayesian network models does not suffer from decreased precision of their parameters. (C) 2013 Elsevier B.V. All rights reserved.",""
"Objective: Modelling the associations from high-throughput experimental molecular data has provided unprecedented insights into biological pathways and signalling mechanisms. Graphical models and networks have especially proven to be useful abstractions in this regard. Ad hoc thresholds are often used in conjunction with structure learning algorithms to determine significant associations. The present study overcomes this limitation by proposing a statistically motivated approach for identifying significant associations in a network. Methods and materials: A new method that identifies significant associations in graphical models by estimating the threshold minimising the L norm between the cumulative distribution function (CDF) of the observed edge confidences and those of its asymptotic counterpart is proposed. The effectiveness of the proposed method is demonstrated on popular synthetic data sets as well as publicly available experimental molecular data corresponding to gene and protein expression profiles. Results: The improved performance of the proposed approach is demonstrated across the synthetic data sets using sensitivity, specificity and accuracy as performance metrics. The results are also demonstrated across varying sample sizes and three different structure learning algorithms with widely varying assumptions. In all cases, the proposed approach has specificity and accuracy close to 1, while sensitivity increases linearly in the logarithm of the sample size. The estimated threshold systematically outperforms common ad hoc ones in terms of sensitivity while maintaining comparable levels of specificity and accuracy. Networks from experimental data sets are reconstructed accurately with respect to the results from the original papers. Conclusion: Current studies use structure learning algorithms in conjunction with ad hoc thresholds for identifying significant associations in graphical abstractions of biological pathways and signalling mechanisms. Such an ad hoc choice can have pronounced effect on attributing biological significance to the associations in the resulting network and possible downstream analysis. The statistically motivated approach presented in this study has been shown to outperform ad hoc thresholds and is expected to alleviate spurious conclusions of significant associations in such graphical abstractions. (C) 2012 Elsevier B.V. All rights reserved.",""
"There are many pertinent open issues in the area of star and planet formation. Large statistical samples of young stars across star-forming regions are needed to trigger a breakthrough in our understanding, but most optical studies are based on a wide variety of spectrographs and analysis methods, which introduces large biases. Here we show how graphical Bayesian networks can be employed to construct a hierarchical probabilistic model which allows pre-main-sequence ages, masses, accretion rates and extinctions to be estimated using two widely available photometric survey data bases (Isaac Newton Telescope Photometric Ha Survey r'/Ha/i' and Two Micron All Sky Survey J-band magnitudes). Because our approach does not rely on spectroscopy, it can easily be applied to homogeneously study the large number of clusters for which Gaia will yield membership lists. We explain how the analysis is carried out using the Markov chain Monte Carlo method and provide PYTHON source code. We then demonstrate its use on 587 known low-mass members of the star-forming region NGC 2264 (Cone Nebula), arriving at a median age of 3.0 Myr, an accretion fraction of 20 +/- 2 per cent and a median accretion rate of 10(-8.4) M-circle dot yr(-1). The Bayesian analysis formulated in this work delivers results which are in agreement with spectroscopic studies already in the literature, but achieves this with great efficiency by depending only on photometry. It is a significant step forward from previous photometric studies because the probabilistic approach ensures that nuisance parameters, such as extinction and distance, are fully included in the analysis with a clear picture on any degeneracies.",""
"Fisheries professionals are increasingly tasked with incorporating climate change projections into their decisions. Here we demonstrate how a structured decision framework, coupled with analytical tools and spatial data sets, can help integrate climate and biological information to evaluate management alternatives. We present examples that link down-scaled climate change scenarios to fish populations for two common types of problems: (1) strategic spatial prioritization of limited conservation resources and (2) deciding whether removing migration barriers would benefit a native fish also threatened with invasion by a nonnative competitor. We used Bayesian networks (BNs) to translate each decision problem into a quantitative tool and implemented these models under historical and future climate projections. The spatial prioritization BN predicted a substantial loss of habitat for the target species by the 2080s and provided a means to map habitats and populations most likely to persist under future climate projections. The barrier BN applied to three streams predicted that barrier removal decisions-previously made assuming a stationary climate-were likely robust under the climate scenario considered. The examples demonstrate the benefit of structuring the decision-making process to clarify management objectives, formalize assumptions, synthesize current understanding about climate effects on fish populations, and identify key uncertainties requiring further investigation.",""
"We describe a method, based on the Merton model, to improve credit portfolio models by adding to the underlying distributions forward-looking tails deducted through the Bayesian networks technology. Given the forward-looking stance of the approach, its results give a better quantified picture of the vulnerabilities of an institution under extreme stress and at the same time satisfy the Basel II recommendations for integrating forward-looking stress scenarios in the decision-making process and capital planning. We show the procedure in detail in a stylized case.",""
"This research proposes an integrated Bayesian networks (BN) approach that adopts structural equation modeling (SEM) to discover the knowledge of the belief or causal relationships which are subsequently used as the BN network structure to predict the level of e-learning participation. SEM is an advanced statistical technique in the social and behavioral sciences to verify the hypothesized relationships, but it is seldom combined with other machine-learning algorithms. To overcome the difficulty of constructing a BN structure when learning from data, this study uses SEM to aid BN in discovering a suitable network architecture for prediction. 159 valid samples were collected from college students with online English learning experience. Compared with other prediction methods, the SEM-BN approach yielded better results than those of back-propagation neural networks (BPN) and classification & regression trees (CART).","Compared with other prediction methods, the SEM-BN approach yielded better results than those of back-propagation neural networks (BPN) and classification & regression trees (CART)."
"Histone modifications (HMs) are an important post-translational feature. Different types of HMs are believed to co-exist and co-regulate biological processes such as gene expression and, therefore, are intrinsically dependent on each other. We develop inference for this complex biological network of HMs based on a graphical model using ChIP-Seq data. A critical computational hurdle in the inference for the proposed graphical model is the evaluation of a normalization constant in an autologistic model that builds on the graphical model. We tackle the problem by Monte Carlo evaluation of ratios of normalization constants. We carry out a set of simulations to validate the proposed approach and to compare it with a standard approach using Bayesian networks. We report inference on HM dependence in a case study with ChIP-Seq data from a next generation sequencing experiment. An important feature of our approach is that we can report coherent probabilities and estimates related to any event or parameter of interest, including honest uncertainties. Posterior inference is obtained from a joint probability model on latent indicators for the recorded HMs. We illustrate this in the motivating case study. An R package including an implementation of posterior simulation in C is available from Riten Mitra upon request.","We develop inference for this complex biological network of HMs based on a graphical model using ChIP-Seq data."
"Causal networks are graphically represented by directed acyclic graphs (DAGs). Learning causal networks from data is a challenging problem due to the size of the space of DAGs, the acyclicity constraint placed on the graphical structures, and the presence of equivalence classes. In this article, we develop an L-1-penalized likelihood approach to estimate the structure of causal Gaussian networks. A blockwise coordinate descent algorithm, which takes advantage of the acyclicity constraint, is proposed for seeking a local maximizer of the penalized likelihood. We establish that model selection consistency for causal Gaussian networks can be achieved with the adaptive lasso penalty and sufficient experimental interventions. Simulation and real data examples are used to demonstrate the effectiveness of our method. In particular, our method shows satisfactory performance for DAGs with 200 nodes, which have about 20,000 free parameters. Supplementary materials for this article are available online.",""
"Traffic accidents constitute a major problem worldwide. One of the principal causes of traffic accidents is adverse driving behavior that is inherently influenced by traffic conditions and infrastructure among other parameters. Probabilistic models for the assessment of road accidents risk usually employs machine learning using historical data of accident records. The main drawback of these approaches is limited coverage of traffic data. This study illustrates a prototype approach that escapes from this problem, and highlights the need to enhance historical accident records with traffic information for improved road safety analysis. Traffic conditions estimation is achieved through Dynamic Traffic Assignment (DTA) simulation that utilizes temporal aspects of a transportation system. Accident risk quantification is achieved through a Bayesian Networks (BNs) model learned from the method's enriched accidents dataset. The study illustrates the integration of BN with the DTA-based simulator, Visual Interactive Systems for Transport Algorithms (VISTAs), for the assessment of accident risk index (ARI), used to identify accident black spots on road networks. (C) 2012 Elsevier Ltd. All rights reserved.",""
"One of the principal objectives of traffic accident analyses is to identify key factors that affect the severity of an accident. However, with the presence of heterogeneity in the raw data used, the analysis of traffic accidents becomes difficult. In this paper, Latent Class Cluster (LCC) is used as a preliminary tool for segmentation of 3229 accidents on rural highways in Granada (Spain) between 2005 and 2008. Next, Bayesian Networks (BNs) are used to identify the main factors involved in accident severity for both, the entire database (EDB) and the clusters previously obtained by LCC. The results of these cluster-based analyses are compared with the results of a full-data analysis. The results show that the combined use of both techniques is very interesting as it reveals further information that would not have been obtained without prior segmentation of the data. BN inference is used to obtain the variables that best identify accidents with killed or seriously injured. Accident type and sight distance have been identify in all the cases analysed; other variables such as time, occupant involved or age are identified in EDB and only in one cluster; whereas variables vehicles involved, number of injuries, atmospheric factors, pavement markings and pavement width are identified only in one cluster. (C) 2012 Elsevier Ltd. All rights reserved.","BN inference is used to obtain the variables that best identify accidents with killed or seriously injured."
"Fault diagnosis and assessment is a crucial and difficult problem for power system. Back propagation neural network expert system (BPES) is an often used method in fault diagnosis. However, with the layer numbers increasing, BPES becomes time consuming and even hard to converge. To solve this problem, we divide the whole networks into many sub-BP groups within a short depth and then propose a novel Multi-BP expert system (MBPES) based method for power system fault diagnosis. We use two real power system data sets to test the effectiveness of MBPES. Experimental results show that MBPES obtains higher accuracy than two commonly used methods. (c) 2012 Elsevier Ltd. All rights reserved.",""
"This paper proposes an estimation method for the confidence level of feedback information (CLFI), namely the confidence level of reported information in computer integrated manufacturing (CIM) architecture for logic diagnosis. This confidence estimation provides a diagnosis module with precise reported information to quickly identify the origin of equipment failure. We studied the factors affecting CLFI, such as measurement system reliability, production context, position of sensors in the acquisition chains, type of products, reference metrology, preventive maintenance and corrective maintenance based on historical data and feedback information generated by production equipments. We introduced the new 'CLFI' concept based on the Dynamic Bayesian Network approach and Tree Augmented Naive Bayes model. Our contribution includes an on-line confidence computation module for production equipment data, and an algorithm to compute CLFI. We suggest it to be applied to the semiconductor manufacturing industry. (c) 2012 Elsevier Ltd. All rights reserved.",""
"This paper develops analytical methods for test resource allocation that aid in reducing the uncertainty in the system model prediction for multilevel and multidisciplinary systems. The various component, subsystem, and system-level model predictions; the corresponding inputs and calibration parameters; test data; and model and measurement errors are connected efficiently using a Bayesian network. This provides a unified framework for uncertainty analysis where test data can be integrated along with computational simulations. The Bayesian network is used in an inverse problem where the model parameters of multiple subsystems are calibrated simultaneously. This leads to a decrease in the variance of the model parameters, and hence, in the valiance of the overall system performance prediction. An optimization-based procedure is then used for test resource allocation using the Bayesian network, and those tests that can effectively reduce the uncertainty in the system model prediction are identified. The proposed methods are extended to three types of aerospace systems-testing applications: structural dynamics (multilevel), thermally induced vibration/flutter (multidisciplinary), and simplified space telescope mirror (multilevel, multidisciplinary).",""
"Aquaculture is proposed as a means to income generation and food security in developing nations. Understanding drivers of attitudes and perceptions towards choosing aquaculture as a livelihood is. essential to aid policy makers in promoting its development. This paper takes a new approach to establishing a baseline of these social and economic drivers. We used simple metrics familiar to policy makers collected in face-to-face semi-structured interviews - e.g. education level, time availability to work and income level - to predict willingness of individuals to adopt aquaculture as a livelihood. We compared modelling approaches ability to provide insights into effects of social and economic factors on willingness of 422 household decision-makers in coastal villages in Tanzania to participate in sea cucumber aquaculture as an alternative livelihood. Linear regression identified the factors; time available for a supplementary livelihood, gender, social network strength and material style of life as significantly predicting individuals' willingness to adopt aquaculture. A Bayesian Belief Network (BBN) model of community data created using logistic regression results, open response analysis and critical literature appraisal allowed intuitive manipulation of factors to predict the influence of aquaculture uptake drivers and constraints. The BBN model provided quantified predictions of the effect of specific policy interventions to promote aquaculture uptake within the modelled community. The analysis from the BBN model supports its broader use as an assessment tool for informing policy formulation by highlighting key areas of intervention to increase willingness to uptake aquaculture among target groups, such as low income households and women. BBNs provide a modelling approach that allows policy makers to visualise the influence of socio-economic factors on the success of introducing aquaculture in different local contexts. (c) 2012 Elsevier Ltd. All rights reserved.","Linear regression identified the factors; time available for a supplementary livelihood, gender, social network strength and material style of life as significantly predicting individuals' willingness to adopt aquaculture."
"In this paper, a system of systems (SoS) framework for the reliability analysis of telecommunication networks is proposed. In this framework, two hazard analysis techniques, hazard and operability analysis and fault tree analysis, are combined in a hybrid scheme. This is further enhanced using the Bayesian network model along with sensitivity analysis in order to answer complex probability queries and to estimate the impact of residual mishap risks, unknown events, or events that cannot easily be modeled. The SoS emergent behavior is further revealed using exploratory modeling. The proposed SoS framework is applied in the case of a fiber-to-the-curb VDSL telecommunication network.",""
"New insights into the intrarenal renin-angiotensin (Ang) system have modified our traditional view of the system. However, many finer details of this network of peptides and associated peptidases remain unclear. We hypothesized that a computational systems biology approach, applied to peptidomic data, could help to unravel the network of enzymatic conversions. We built and refined a Bayesian network model and a dynamic systems model starting from a skeleton created with established elements of the renin-Ang system and further developed it with archived matrix-assisted laser desorption ionization-time of flight mass spectra from experiments conducted in mouse podocytes exposed to exogenous Ang substrates. The model-building process suggested previously unrecognized steps, 3 of which were confirmed in vitro, including the conversion of Ang(2-10) to Ang(2-7) by neprilysin, Ang(1-9) to Ang(2-9), and Ang(1-7) to Ang(2-7) by aminopeptidase A. These data suggest a wider role of neprilysin and aminopeptidase A in glomerular formation of bioactive Ang peptides and shunting their formation. Other steps were also suggested by the model, and supporting evidence for those steps was evaluated using model-comparison methods. Our results demonstrate that systems biology methods applied to peptidomic data are effective in identifying novel steps in the Ang peptide processing network, and these findings improve our understanding of the glomerular renin-Ang system. (Hypertension. 2013;61:690-700.). circle Online Data Supplement",""
"The sanitary sewerage connection rate is an important indicator of advanced cities. Following the construction of sanitary sewerages, the maintenance and management systems are required for keeping pipelines and facilities functioning well. These maintenance tasks often require sewer workers to enter the manholes and the pipelines, which are confined spaces short of natural ventilation and have the potential for hazardous substances to be present. Working in sewers could be easily exposed to a risk of adverse health effects. This paper proposes the use of Bayesian belief networks as a higher level of noncarcinogenic health risk assessment of sewer workers. On the basis of the epidemiological studies, the actual hospital attendance records and expert experiences, the Bayesian belief networks is capable of capturing the probabilistic relationships between the hazardous substances in sewers and their adverse health effects, and accordingly inferring the morbidity and mortality of the adverse health effects. The provision of the morbidity and mortality rates of the related diseases is more informative and can alleviate the drawbacks of conventional methods.",""
"Many high-hazard industries around the world have explicitly recognized the critical role that human, management and organizational risk factors play in major accidents. The findings of accident investigations and risk assessments demonstrate a growing recognition that the cultural context of work practices may influence safety just as much as technology. The objective of this paper is to establish a relationship between the concepts of safety culture and organizational culture in a Nuclear Power Plant (NPP). This study permits the identification and quantification of the possible mechanisms for improving the safety culture in the NPP acting on organizational culture. It therefore provides a methodology to identify potential strategies for safety improvement. Probabilistic (Bayesian) Networks (BNs) have been used to determine the relationships between the organizational culture and safety culture in a quantitative form. To this aim, we considered data from a survey conducted of every employee at a Spanish NPP. The resulting data-driven models allow us to establish the probabilistic relationship among organizational culture factors, including the 12 OCI (Organizational Culture Inventory) scales, that have an influence on safety culture. The study yielded a ranking of organizational cultures that can be used to improve safety culture in a NPP. (C) 2012 Elsevier Ltd. All rights reserved.",""
"In 2008, the U.S. Fish and Wildlife Service was petitioned to list the Pacific walrus (Odobenus rosmarus divergens) under the U.S. Endangered Species Act (ESA). Research into stressors that may be negatively affecting walruses is incomplete. We developed a Bayesian belief network model structured around the ESA 5-factor analysis during a workshop attended by walrus and ESA experts to 1) elicit expert opinion on important stressors and their effects, 2) develop the model, and 3) develop and analyze plausible future scenarios. The listing factors and associated stressors were organized as sub-models, capturing the cumulative effects of the factors through model output, which was the probability of negative, neutral, or positive effects. We found that in a time-constrained workshop, the graphical display of Bayesian belief networks allowed for rapid development, assessment, and revision of model structure and parameters. We modeled up to 3 scenarios (most likely-, worst-, and best-case) for each of 4 time periods (recent past, contemporary, mid-century, and late-century). Model output for the recent past (reference condition) was consistent with observations and provided a baseline for comparison of the outcomes of other periods and scenarios; stressor effects became increasingly negative with time. However, scenario analyses indicated that mitigation of relatively few stressors could reduce the cumulative effects of the listing factors. Uncertainty in model output was lowest for the past but differed by only 7% among the other time periods. We used 4 types of sensitivity analyses to identify explanatory variables that had the greatest influence on model outcomes. Published 2012. This article is a U.S. Government work and is in the public domain in the USA.",""
"Faced with the fragmented and heterogeneous character of knowledge regarding complex food systems, we have developed a practical methodology, in the framework of the dynamic Bayesian networks associated with Dirichlet distributions, able to incrementally build and update model parameters each time new information is available whatever its source and format. From a given network structure, the method consists in using a priori Dirichlet distributions that may be assessed from literature, empirical observations, experts opinions, existing models, etc. Next, they are successively updated by using Bayesian inference and the expected a posteriori each time new or additional information is available and can be formulated into a frequentist form. This method also enables to take (1) uncertainties pertaining to the system; (2) the confidence level on the different sources of information into account. The aim is to be able to enrich the model each time a new piece of information is available whatever its source and format in order to improve the representation and thus provide a better understanding of systems. We have illustrated the feasibility and practical using of our approach in a real case namely the modelling of the Camembert-type cheese ripening. (C) 2012 Elsevier Ltd. All rights reserved.","Next, they are successively updated by using Bayesian inference and the expected a posteriori each time new or additional information is available and can be formulated into a frequentist form."
"Oil transport has greatly increased in the Gulf of Finland over the years, and risks of an oil accident occurring have risen. Thus, an effective oil combating strategy is needed. We developed a Bayesian Network (BN) to examine the recovery efficiency and optimal disposition of the Finnish oil combating vessels in the Gulf of Finland (GoF), Eastern Baltic Sea. Four alternative home harbors, five accident points, and ten oil combating vessels were included in the model to find the optimal disposition policy that would maximize the recovery efficiency. With this composition, the placement of the oil combating vessels seems not to have a significant effect on the recovery efficiency. The process seems to be strongly controlled by certain random factors independent of human action, e.g. wave height and stranding time of the oil. Therefore, the success of oil combating is rather uncertain, so it is also important to develop activities that aim for preventing accidents. We found that the model developed is suitable for this type of multidecision optimization. The methodology, results, and practices are further discussed.",""
"Bayesian network is one of the most successful graph models for representing the reactive oxygen species regulatory pathway. With the increasing number of microarray measurements, it is possible to construct the Bayesian network from microarray data directly. Although large numbers of Bayesian network learning algorithms have been developed, when applying them to learn Bayesian networks from microarray data, the accuracies are low due to that the databases they used to learn Bayesian networks contain too few microarray data. In this paper, we propose a consensus Bayesian network which is constructed by combining Bayesian networks from relevant literatures and Bayesian networks learned from microarray data. It would have a higher accuracy than the Bayesian networks learned from one database. In the experiment, we validated the Bayesian network combination algorithm on several classic machine learning databases and used the consensus Bayesian network to model the Escherichia coli's ROS pathway.",""
"In this work we study the effects of model inaccuracies on the description of a Gaussian Bayesian network with a set of variables of interest and a set of evidential variables. Using the Kullback-Leibler divergence measure, we compare the output of two different networks after evidence propagation: the original network, and a network with perturbations representing uncertainties in the quantitative parameters. We describe two methods for analyzing the sensitivity and robustness of a Gaussian Bayesian network on this basis. In the sensitivity analysis, different expressions are obtained depending on which set of parameters is considered inaccurate. This fact makes it possible to determine the set of parameters that most strongly disturbs the network output. If all of the divergences are small, we can conclude that the network output is insensitive to the proposed perturbations. The robustness analysis is similar, but considers all potential uncertainties jointly. It thus yields only one divergence, which can be used to confirm the overall sensitivity of the network. Some practical examples of this method are provided, including a complex, real-world problem. (C) 2012 Elsevier Inc. All rights reserved.",""
"Bayesian Networks (BNs) are powerful tools for assessing and predicting consequences of water management scenarios and uncertain drivers like climate change, integrating available scientific knowledge with the interests of the multiple stakeholders. However, among their major limitations, the non-transient treatment of the cause-effect relationship stands out. A Decision Support System (DSS) based on Dynamic Bayesian Networks (DBNs) is proposed here aimed to palliate that limitation through time slicing technique. The DSS comprises several classes (Object-Oriented BN networks), especially designed for future 5 years length time steps (time slices), covering a total control period of 30 years (2070-2100). The DSS has been developed for assessing impacts generated by different Climate Change (CC) scenarios (generated from several Regional Climatic Models (RCMs) under two emission scenarios, A1B and A2) in an aquifer system (Serral-Salinas) affected by intensive groundwater use over the last 30 years. A calibrated continuous water balance model was used to generate hydrological CC scenarios, and then a groundwater flow model (MODFLOW) was employed in order to analyze the aquifer behavior under CC conditions. Results obtained from both models were used as input for the DSS, considering rainfall, aquifer recharge, variation of piezometric levels and temporal evolution of aquifer storage as the main hydrological components of the aquifer system. Results show the evolution of the aquifer storage for each future time step under different climate change conditions and under controlled water management interventions. This type of applications would allow establishing potential adaptation strategies for aquifer systems as the CC comes into effect. (C) 2012 Elsevier B.V. All rights reserved.",""
"Gene regulation is a key factor in gaining a full understanding of molecular biology. microRNA (miRNA), a novel class of non-coding RNA, has recently been found to be one crucial class of post-transactional regulators, and play important roles in cancer. One essential step to understand the regulatory effect of miRNAs is the reliable prediction of their target mRNAs. Typically, the predictions are solely based on the sequence information, which unavoidably have high false detection rates. Recently, some novel approaches are developed to predict miRNA targets by integrating the typical algorithm with the paired expression profiles of miRNA and mRNA. Here we review and discuss these integrative approaches and propose a new algorithm called HCTarget. Applying HCtarget to the expression data in multiple myeloma, we predict target genes for ten specific miRNAs. The experimental verification and a loss of function study validate our predictions. Therefore, the integrative approach is a reliable and effective way to predict miRNA targets, and could improve our comprehensive understanding of gene regulation.",""
"Patients with vascular cognitive impairment (VCI) commonly exhibit deficits in processing speed. This has been attributed to a disruption of frontal-subcortical neuronal circuits by ischemic lesions, but the exact mechanisms and underlying anatomical structures are poorly understood. We set out to identify a strategic brain network for processing speed by applying graph-based data-mining techniques to MRI lesion maps from patients with small vessel disease. We studied 235 patients with CADASIL, a genetic small vessel disease causing pure VCI. Using a probabilistic atlas in standard space we first determined the regional volumes of white matter hyperintensities (WMH) and lacunar lesions (LL) within major white matter tracts. Conditional dependencies between the regional lesion volumes and processing speed were then examined using Bayesian network analysis. Exploratory analysis identified a network of five imaging variables as the best determinant of processing speed. The network included LL in the left anterior thalamic radiation and the left cingulum as well as WMH in the left forceps minor, the left parahippocampal white matter and the left corticospinal tract. Together these variables explained 34% of the total variance in the processing speed score. Structural equation modeling confirmed the findings obtained from the Bayesian models. In summary, using graph-based models we identified a strategic brain network having the highest predictive value for processing speed in our cohort of patients with pure small vessel disease. Our findings confirm and extend previous results showing a role of frontal-subcortical neuronal circuits, in particular dorsolateral prefrontal and cingulate circuits, in VCI. (C) 2012 Elsevier Inc. All rights reserved.",""
"There are different structure of the network and the variables, and the process of learning Bayesian networks has a lot of different forms. The structure of the network can be unknown or known, and the variables can be observable or hidden in some or all of the data points. Consequently, there are four cases of learning Bayesian networks from data: known structure and observable variables, unknown structure and observable variables, known structure and unobservable variables and unknown structure and unobservable variables. In this paper, we focus on known structure and observable variables, unknown structure and observable variables.",""
"Researchers are becoming more and more interested in the security issues of software engineering. It will effectively reduce the cost of development and maintenance in order to detect and predict security threats. In this paper, attack patterns are analysed in the field of software engineering, and Bayesian Networks is applied to construct attack networks topology, to find the dependencies of attack patterns. It helps to find the vulnerable points, locate the path of security threats effectively, and predict probable attacks reasonably. We use multi-variant statistical analysis for the attack networks, and factor analysis is applied to reduce the relevance. In Dirichlet distribution, the state transition distribution of each attack node is calculated to detect and predict the security threats. In order to verify the effectiveness and robustness of the approach, buffer flow is chosen as the analysis domain, and 14 attack patterns are selected for the experiments. It shows that attack patterns are effectively modelled based on Bayesian Networks and potential attack patterns are discovered, while threats are predicted and located accurately.",""
"We present the Chain Event Graph (CEG) as a complementary graphical model to the Causal Bayesian Network for the representation and analysis of causally manipulated asymmetric problems. Our focus is on causal identifiability - finding conditions for when the effects of a manipulation can be estimated from a subset of events observable in the unmanipulated system. CEG analogues of Pearl's Back Door and Front Door theorems are presented, applicable to the class of singular manipulations, which includes both Pearl's basic Do intervention and the class of functional manipulations possible on Bayesian Networks. These theorems are shown to be more flexible than their Bayesian Network counterparts, both in the types of manipulation to which they can be applied, and in the nature of the conditioning sets which can be used. (C) 2012 Elsevier B.V. All rights reserved.",""
"Pathological and age-related changes may affect an individual's gait, in turn raising the risk of falls. In elderly, falls are common and may eventuate in severe injuries, long-term disabilities, and even death. Thus, there is interest in estimating the risk of falls from gait analysis. Estimation of the risk of falls requires consideration of the longitudinal evolution of different variables derived from human gait. Bayesian networks are probabilistic models which graphically express dependencies among variables. Dynamic Bayesian networks (DBNs) are a type of BN adequate for modeling the dynamics of the statistical dependencies in a set of variables. In this work, a DBN model incorporates gait derived variables to predict the risk of falls in elderly within 6 months subsequent to gait assessment. Two DBNs were developed; the first (DBN1; expert-guided) was built using gait variables identified by domain experts, whereas the second (DBN2; strictly computational) was constructed utilizing gait variables picked out by a feature selection algorithm. The effectiveness of the second model to predict falls in the 6 months following assessment is 72.22 %. These results are encouraging and supply evidence regarding the usefulness of dynamic probabilistic models in the prediction of falls from pathological gait.",""
"PURPOSE: To reveal hidden patterns and knowledge present in nursing care information documented with standardized nursing terminologies on end-of-life (EOL) hospitalized patients. METHOD: 596 episodes of care that included pain as a problem on a patient's care plan were examined using statistical and data mining tools. The data were extracted from the Hands-On Automated Nursing Data System database of nursing care plan episodes (n=40,747) coded with NANDA-I, Nursing Outcomes Classification, and Nursing Intervention Classification (NNN) terminologies. System episode data (episode=care plans updated at every hand-off on a patient while staying on a hospital unit) had been previously gathered in eight units located in four different healthcare facilities (total episodes=40,747; EOL episodes=1,425) over 2 years and anonymized prior to this analyses. RESULTS: Results show multiple discoveries, including EOL patients with hospital stays (<72hr) are less likely (p<.005) to meet the pain relief goals compared with EOL patients with longer hospital stays. CONCLUSIONS: The study demonstrates some major benefits of systematically integrating NNN into electronic health records.","The data were extracted from the Hands-On Automated Nursing Data System database of nursing care plan episodes (n=40,747) coded with NANDA-I, Nursing Outcomes Classification, and Nursing Intervention Classification (NNN) terminologies."
"Expert knowledge in the form of mathematical models can be considered sufficient statistics of all prior experimentation in the domain, embodying generic or abstract knowledge of it. When used in a probabilistic framework, such models provide a sound foundation for data mining, inference, and decision making under uncertainty. We describe a methodology for encapsulating knowledge in the form of ordinary differential equations (ODEs) in dynamic Bayesian networks (DBNs). The resulting DBN framework can handle both data and model uncertainty in a principled manner, can be used for temporal data mining with noisy and missing data, and can be used to re-estimate model parameters automatically using data streams. We propose an alternative to the fixed time step inference used in standard DBNs. In our algorithm, the DBN automatically adapts the time step lengths to suit the dynamics in each step. The resulting system allows us to efficiently infer probable values of hidden variables using multiple time series of evidence, some of which may be sparse, noisy or incomplete. We evaluate our approach with a DBN based on a variant of the van der Pol oscillator, and demonstrate an example where it gives more accurate results than the standard approach, but using only one tenth the number of time steps. We also apply our approach to a real-world example in critical care medicine. By incorporating knowledge in the form of an existing ODE model, we have built a DBN framework for efficiently predicting individualised patient responses using the available bedside and lab data. (c) 2012 Elsevier Inc. All rights reserved.","When used in a probabilistic framework, such models provide a sound foundation for data mining, inference, and decision making under uncertainty."
"High Performance Computing (HPC) is a field concerned with solving large-scale problems in science and engineering. However, the computational infrastructure of HPC systems can also be misused as demonstrated by the recent commoditization of cloud computing resources on the black market As a first step towards addressing this, we introduce a machine learning approach for classifying distributed parallel computations based on communication patterns between compute nodes. We first provide relevant background on message passing and computational equivalence classes called dwarfs and describe our exploratory data analysis using self organizing maps. We then present our classification results across 29 scientific codes using Bayesian networks and compare their performance against Random Forest classifiers. These models, trained with hundreds of gigabytes of communication logs collected at Lawrence Berkeley National Laboratory, perform well without any a priori information and address several shortcomings of previous approaches. (C) 2012 Elsevier B.V. All rights reserved.","We then present our classification results across 29 scientific codes using Bayesian networks and compare their performance against Random Forest classifiers."
"A mixture distribution approach to modelling demand during lead time in a continuous-review inventory model is described. Using this approach, both lead time and demand per unit time can follow state-dependent distributions. By using mixtures of truncated exponentials functions to approximate these distributions, mixture distributions that can be easily manipulated in closed folio can be constructed as the marginal distributions for lead time and demand per unit time. These are then used to approximate the mixture of compound distributions for demand during lead time. The technique is illustrated by first applying it to a 'normal-gamma' inventory problem, then by modelling a problem with empirical distributions for lead time and demand per unit time. Journal of the Operational Research Society (2013) 64, 217-228. doi:10.1057/jors.2012.39; published online 25 April 2012",""
"The proper functioning of any living cell relies on complex networks of gene regulation. These regulatory interactions are not static but respond to changes in the environment and evolve during the life cycle of an organism. A challenging objective in computational systems biology is to infer these time-varying gene regulatory networks from typically short time series of transcriptional profiles. While homogeneous models, like conventional dynamic Bayesian networks, lack the flexibility to succeed in this task, fully flexible models suffer from inflated inference uncertainty due to the limited amount of available data. In the present paper we explore a semi-flexible model based on a piecewise homogeneous dynamic Bayesian network regularized by gene-specific inter-segment information sharing. We explore different choices of prior distribution and information coupling and evaluate their performance on synthetic data. We apply our method to gene expression time series obtained during the life cycle of Drosophila melanogaster, and compare the predicted segmentation with other state-of-the-art techniques. We conclude our evaluation with an application to synthetic biology, where the objective is to predict an in vivo regulatory network of five genes in Saccharomyces cerevisiae subjected to a changing environment.","While homogeneous models, like conventional dynamic Bayesian networks, lack the flexibility to succeed in this task, fully flexible models suffer from inflated inference uncertainty due to the limited amount of available data."
"Bayesian networks (BNs) have become a standard in the field of Artificial Intelligence as a means of dealing with uncertainty and risk modelling. In recent years, there has been particular interest in the simultaneous use of continuous and discrete domains, obviating the need for discretization, using so-called hybrid BNs. In these hybrid environments, Mixtures of Truncated Exponentials (MTEs) provide a suitable solution for working without any restriction. The objective of this study is the assessment of groundwater quality through the design and application of a probabilistic clustering, based on hybrid Bayesian networks with MTEs. Firstly, the results obtained allows the differentiation of three groups of sampling points, indicating three different classes of groundwater quality. Secondly, the probability that a sampling point belongs to each cluster allows the uncertainty in the clusters to be assessed, as well as the risks associated in terms of water quality management. The methodology developed could be applied to other fields in environmental sciences.",""
"Malignant peritoneal mesothelioma (MPM) is a rare disease treated with cytoreductive surgery (CRS) and hyperthermic intraperitoneal chemotherapy (HIPEC). Estimation of personalized survival times can potentially guide treatment and surveillance. We analyzed 104 patients who underwent CRS and cisplatin-based HIPEC for MPM. By means of 25 demographic, laboratory, operative, and histopathological variables, we developed a novel nomogram using machine-learned Bayesian belief networks with stepwise training, testing, and cross-validation. The mean peritoneal carcinomatosis index (PCI) was 15, and 66 % of patients had a completeness of cytoreduction (CC) score of 0 or 1. Eighty-seven percent of patients had epithelioid histology. The median follow-up time was 49 (1-195) months. The 3- and 5-year overall survivals (OS) were 58 and 46 %, respectively. The histological subtype, pre-CRS PCI, and preoperative serum CA-125 had the greatest impact on OS and were included in the nomogram. The mean areas under the receiver operating characteristic curve for the 10-fold cross-validation of the 3- and 5-year models were 0.77 and 0.74, respectively. The graphical calculator or nomogram uses color coding to assist the clinician in quickly estimating individualized patient-specific survival before surgery. Machine-learned Bayesian belief network analysis generated a novel nomogram predicting 3- and 5-year OS in patients treated with CRS and HIPEC for MPM. Pre-CRS estimation of survival times may potentially individualize patient care by influencing the use of systemic therapy and frequency of diagnostic imaging, and might prevent CRS in patients unlikely to achieve favorable outcomes despite surgical intervention.",""
"Wastewater treatment is a complicated dynamic process, the effectiveness of which is affected by microbial, chemical, and physical factors. At present, predicting the effluent quality of wastewater treatment systems is difficult because of complex biological reaction mechanisms that vary with both time and the physical attributes of the system. Bayesian networks are useful for addressing uncertainties in artificial intelligence applications. Their powerful inferential capability and convenient decision support mechanisms provide flexibility and applicability for describing and analyzing factors affecting wastewater treatment systems. In this study, a Bayesian network-based approach for modeling and predicting a wastewater treatment system based on Modified Sequencing Batch Reactor (MSBR) was proposed. Using the presented approach, a Bayesian network model for MSBR can be constructed using experiential information and physical data relating to influent loads, operating conditions, and effluent concentrations. Additionally, MSBR prediction analysis, wherein effluent concentration can be predicted from influent loads and operational conditions, can be performed. This approach can be applied, with minimal modifications, to other types of wastewater treatment plants. (C) 2012 Elsevier Ltd. All rights reserved.",""
"A multi-species approach to fisheries management requires taking into account the interactions between species in order to improve recruitment forecasting of the fish species. Recent advances in Bayesian networks direct the learning of models with several interrelated variables to be forecasted simultaneously. These models are known as multi-dimensional Bayesian network classifiers (MDBNs). Preprocessing steps are critical for the posterior learning of the model in these kinds of domains. Therefore, in the present study, a set of 'state-of-the-art' uni-dimensional pre-processing methods, within the categories of missing data imputation, feature discretization and feature subset selection, are adapted to be used with MDBNs. A framework that includes the proposed multi-dimensional supervised preprocessing methods, coupled with a MDBN classifier, is tested with synthetic datasets and the real domain of fish recruitment forecasting. The correctly forecasting of three fish species (anchovy, sardine and hake) simultaneously is doubled (from 173% to 29.5%) using the multi-dimensional approach in comparison to mono-species models. The probability assessments also show high improvement reducing the average error (estimated by means of Brier score) from 0.35 to 0.27. Finally, these differences are superior to the forecasting of species by pairs. (C) 2012 Elsevier Ltd. All rights reserved.","These models are known as multi-dimensional Bayesian network classifiers (MDBNs)."
"Software testing is a crucial activity during software development and fault prediction models assist practitioners herein by providing an upfront identification of faulty software code by drawing upon the machine learning literature. While especially the Naive Bayes classifier is often applied in this regard, citing predictive performance and comprehensibility as its major strengths, a number of alternative Bayesian algorithms that boost the possibility of constructing simpler networks with fewer nodes and arcs remain unexplored. This study contributes to the literature by considering 15 different Bayesian Network (BN) classifiers and comparing them to other popular machine learning techniques. Furthermore, the applicability of the Markov blanket principle for feature selection, which is a natural extension to BN theory, is investigated. The results, both in terms of the AUC and the recently introduced H-measure, are rigorously tested using the statistical framework of Demsar. It is concluded that simple and comprehensible networks with less nodes can be constructed using BN classifiers other than the Naive Bayes classifier. Furthermore, it is found that the aspects of comprehensibility and predictive performance need to be balanced out, and also the development context is an item which should be taken into account during model selection.","While especially the Naive Bayes classifier is often applied in this regard, citing predictive performance and comprehensibility as its major strengths, a number of alternative Bayesian algorithms that boost the possibility of constructing simpler networks with fewer nodes and arcs remain unexplored."
"Molecular classification of colorectal cancer (CRC) is currently based on microsatellite instability (MSI), KRAS or BRAF mutation and, occasionally, chromosomal instability (CIN). Whilst useful, these categories may not fully represent the underlying molecular subgroups. We screened 906 stage II/III CRCs from the VICTOR clinical trial for somatic mutations. Multivariate analyses (logistic regression, clustering, Bayesian networks) identified the primary molecular associations. Positive associations occurred between: CIN and TP53 mutation; MSI and BRAF mutation; and KRAS and PIK3CA mutations. Negative associations occurred between: MSI and CIN; MSI and NRAS mutation; and KRAS mutation, and each of NRAS, TP53 and BRAF mutations. Some complex relationships were elucidated: KRAS and TP53 mutations had both a direct negative association and a weaker, confounding, positive association via TP53CINMSIBRAFKRAS. Our results suggested a new molecular classification of CRCs: (1) MSI+ and/or BRAF-mutant; (2) CIN+ and/or TP53 mutant, with wild-type KRAS and PIK3CA; (3) KRAS- and/or PIK3CA-mutant, CIN+, TP53-wild-type; (4) KRAS and/or PIK3CA-mutant, CIN, TP53-wild-type; (5) NRAS-mutant; (6) no mutations; (7) others. As expected, group 1 cancers were mostly proximal and poorly differentiated, usually occurring in women. Unexpectedly, two different types of CIN+ CRC were found: group 2 cancers were usually distal and occurred in men, whereas group 3 showed neither of these associations but were of higher stage. CIN+ cancers have conventionally been associated with all three of these variables, because they have been tested en masse. Our classification also showed potentially improved prognostic capabilities, with group 3, and possibly group 1, independently predicting disease-free survival. Copyright (C) 2012 Pathological Society of Great Britain and Ireland. Published by John Wiley & Sons, Ltd.","Molecular classification of colorectal cancer (CRC) is currently based on microsatellite instability (MSI), KRAS or BRAF mutation and, occasionally, chromosomal instability (CIN)."
"This article describes a probabilistic model that quantifies hazards that arise from Staphylococcus aureus in milk that is sold as pasteurized in the United Kingdom. The model is centered on coupled dynamics for S. aureus populations, staphylococcal enterotoxins, and the concentration of alkaline phosphatase throughout the milk chain. The chain includes farm collection and storage of pooled milk, further pooling for off-farm processing, high temperature short time thermal processing, and possible postprocess contamination. The model is implemented as a Bayesian belief network. The results indicate that milk sold as pasteurized is relatively safe with respect to the hazards associated with S. aureus and that most risk is associated with small scale on-farm processing. An additional analysis of likelihood ratios shows that alkaline phosphatase concentrations in filler tank milk are a good indicator of potential hazards and that these concentrations, in conjunction with other measurements, can be used effectively to discriminate over possible failure modes. The ability to discriminate over potential failure modes can support preemptive actions, such as maintenance or hygiene, which assist with milk chain management and, over extended periods, accumulate to drive improved safety, efficiency, and security.",""
"A new methodology is introduced based on Bayesian network both to model domino effect propagation patterns and to estimate the domino effect probability at different levels. The flexible structure and the unique modeling techniques offered by Bayesian network make it possible to analyze domino effects through a probabilistic framework, considering synergistic effects, noisy probabilities, and common cause failures. Further, the uncertainties and the complex interactions among the domino effect components are captured using Bayesian network. The probabilities of events are updated in the light of new information, and the most probable path of the domino effect is determined on the basis of the new data gathered. This study shows how probability updating helps to update the domino effect model either qualitatively or quantitatively. The methodology is applied to a hypothetical example and also to an earlier-studied case study. These examples accentuate the effectiveness of Bayesian network in modeling domino effects in processing facility.",""
"This paper presents a comparative analysis of System Dynamics Modelling (SDM) and Object-Oriented Bayesian Networks (OOBN). Both techniques are extensively used for water resources modelling due to their flexibility, effectiveness in assessing different management options, ease of operation and suitability for encouraging stakeholder involvement. Conversely, both approaches have several important differences that make them complementary. For example, while SDM is more suitable for simulating the feedback dynamics of processes, OOBN modelling is a powerful tool for modelling systems with uncertain inputs (or outputs) characterised by probability distributions. This comparative analysis is applied to the Kairouan aquifer system, Tunisia, where the aquifer plays an essential role for socio-economic development in the region. Both models produced comparable results using baseline data, and show their complementarity through a suite of scenario tests. It is shown that reducing pumping of groundwater to coastal cities may prove the key to reducing the current aquifer deficit, though local demand reduction must be considered to preserve the agricultural economy. It is suggested that water management assessment should be tackled using both approaches to complement each other, adding depth and insight, and giving a more coherent picture of the problem being addressed, allowing for robust policy decisions to be made.",""
"Automatically learning the graph structure of a single Bayesian network (BN) which accurately represents the underlying multivariate probability distribution of a collection of random variables is a challenging task. But obtaining a Bayesian solution to this problem based on computing the posterior probability of the presence of any edge or any directed path between two variables or any other structural feature is a much more involved problem, since it requires averaging over all the possible graph structures. For the former problem, recent advances have shown that search + score approaches find much more accurate structures if the search is constrained by a previously inferred skeleton (i.e. a relaxed structure with undirected edges which can be inferred using local search based methods). Based on similar ideas, we propose two novel skeleton-based approaches to approximate a Bayesian solution to the BN learning problem: a new stochastic search which tries to find directed acyclic graph (DAG) structures with a non-negligible score; and a new Markov chain Monte Carlo method over the DAG space. These two approaches are based on the same idea. In a first step, both employ a previously given skeleton and build a Bayesian solution constrained by this skeleton. In a second step, using the preliminary solution, they try to obtain a new Bayesian approximation but this time in an unconstrained graph space, which is the final outcome of the methods. As shown in the experimental evaluation, this new approach strongly boosts the performance of these two standard techniques proving that the idea of employing a skeleton to constrain the model space is also a successful strategy for performing Bayesian structure learning of BNs. (C) 2012 Elsevier B.V. All rights reserved.",""
"We investigate various techniques for keyword spotting which are exclusively based on acoustic modeling and do not presume the existence of an in-domain language model. Since adequate context modeling is nevertheless necessary for word spotting, we show how the principle of Long Short-Term Memory (LSTM) can be incorporated into the decoding process. We propose a novel technique that exploits LSTM in combination with Connectionist Temporal Classification in order to improve performance by using a self-learned amount of contextual information. All considered approaches are evaluated on read speech as contained in the TIMIT corpus as well as on the SEMAINE database which consists of spontaneous and emotionally colored speech. As further evidence for the effectiveness of LSTM modeling for keyword spotting, results on the CHiME task are shown. (C) 2012 Elsevier B.V. All rights reserved.","We propose a novel technique that exploits LSTM in combination with Connectionist Temporal Classification in order to improve performance by using a self-learned amount of contextual information."
"We present a new framework for learning high-dimensional multivariate probability distributions from estimated marginals. The approach is motivated by compositional models and Bayesian networks, and designed to adapt to small sample sizes. We start with a large, overlapping set of elementary statistical building blocks, or \"primitives,\" which are low-dimensional marginal distributions learned from data. Each variable may appear in many primitives. Subsets of primitives are combined in a Lego-like fashion to construct a probabilistic graphical model; only a small fraction of the primitives will participate in any valid construction. Since primitives can be precomputed, parameter estimation and structure search are separated. Model complexity is controlled by strong biases; we adapt the primitives to the amount of training data and impose rules which restrict the merging of them into allowable compositions. The likelihood of the data decomposes into a sum of local gains, one for each primitive in the final structure. We focus on a specific subclass of networks which are binary forests. Structure optimization corresponds to an integer linear program and the maximizing composition can be computed for reasonably large numbers of variables. Performance is evaluated using both synthetic data and real datasets from natural language processing and computational biology.",""
"During the last three decades, several techniques have been developed for the quantitative study of human reliability. In the 1980s, techniques were developed to model systems by means of binary trees, which did not allow for the representation of the context in which human actions occur. Thus, these techniques cannot model the representation of individuals, their interrelationships, and the dynamics of a system. These issues make the improvement of methods for Human Reliability Analysis (HRA) a pressing need. To eliminate or at least attenuate these limitations, some authors have proposed modeling systems using Bayesian Belief Networks (BBNs). The application of these tools is expected to address many of the deficiencies in current approaches to modeling human actions with binary trees. This paper presents a methodology based on BBN for analyzing human reliability and applies this method to the operation of an oil tanker, focusing on the risk of collision accidents. The obtained model was used to determine the most likely sequence of hazardous events and thus isolate critical activities in the operation of the ship to study Internal Factors (IFs), Skills, and Management and Organizational Factors (MOFs) that should receive more attention for risk reduction. (C) 2012 Elsevier Ltd. All rights reserved.",""
"Water resource management is often characterized by conflicts, as a result of the heterogeneity of interests associated with a shared resource. Many water conflicts arise on a global scale and, in particular, an increasing level of conflicts can be observed in the Mediterranean basin, characterized by water scarcity. In the present work, in order to assist the conflict analysis process, and thus outline a proper groundwater management, stakeholders were involved in the process and suitable tools were used in a Mediterranean area (the Apulia region, in Italy). In particular, this paper seeks to elicit and structure farmers' mental models influencing their decision over the main water source for irrigation. The more crucial groundwater is for farmers' objectives, the more controversial is the groundwater protection strategy. Bayesian Belief Networks were developed to simulate farmers' behavior with regard to groundwater management and to assess the impacts of protection strategy. These results have been used to calculate the conflict degree in the study area, derived from the introduction of policies for the reduction of groundwater exploitation for irrigation purposes. The less acceptable the policy is, the more likely it is that conflict will develop between farmers and the Regional Authority. The results of conflict analysis were also used to contribute to the debate concerning potential conflict mitigation measures. The approach adopted in this work has been discussed with a number of experts in groundwater management policies and irrigation management, and its main strengths and weaknesses have been identified. Increasing awareness of the existence of potential conflicts and the need to deal with them can be seen as an interesting initial shift in the Apulia region's water management regime, which is still grounded in merely technical approaches. (C) 2012 Elsevier Ltd. All rights reserved.",""
"A Bayesian network model of Anabaena blooms in Grahamstown Dam, near Newcastle, Australia is described. This model meets the criteria of being decision-focused, data driven, transparent, and capable of being used by non-expert modellers. Monitored data were arranged in a consistently formatted database from which the model could 'learn' probabilistic relationships between model elements such as pumped nutrient load, lake water column nutrient concentrations, and Anabaena concentrations. This 'minimal model' produced useful insights into ecosystem relationships and provided a basic model for later development. Subsequent modelling and elicitation of conditional probabilities from experts strengthened components of the model for which there is little data available. The approach to incorporating elicited data is described and some simple scenario testing is also presented. Management outcomes resulting from application of the model are presented. (C) 2012 Elsevier B.V. All rights reserved.",""
"Estimation of distribution algorithms (EDAs), as an extension of genetic algorithms, samples new solutions from the probabilistic model, which characterizes the distribution of promising solutions in the search space at each generation. This paper introduces and evaluates a novel estimation of a distribution algorithm, called L1-regularized Bayesian optimization algorithm, L1BOA. In L1BOA, Bayesian networks as probabilistic models are learned in two steps. First, candidate parents of each variable in Bayesian networks are detected by means of L1-regularized logistic regression, with the aim of leading a sparse but nearly optimized network structure. Second, the greedy search, which is restricted to the candidate parent-child pairs, is deployed to identify the final structure. Compared with the Bayesian optimization algorithm (BOA), L1BOA improves the efficiency of structure learning due to the reduction and automated control of network complexity introduced with L1-regularized learning. Experimental studies on different types of benchmark problems show that L1BOA not only outperforms BOA when no prior knowledge about problem structure is available, but also achieves and even exceeds the best performance of BOA that applies explicit controls on network complexity. Furthermore, Bayesian networks built by L1BOA and BOA during evolution are analysed and compared, which demonstrates that L1BOA is able to build simpler, yet more accurate probabilistic models.","First, candidate parents of each variable in Bayesian networks are detected by means of L1-regularized logistic regression, with the aim of leading a sparse but nearly optimized network structure."
"Along with the increase of complexity in engineering systems, there exist many dynamic characteristics within the system failure process, such as sequence dependency, functional dependency and spares. Markov-based dynamic fault trees can figure out the modeling of systems with these characteristics. However, when confronted with the issue of state space explosion resulted from the growth of system complexity, the Markov-based approach is no longer efficient. In this paper, we combine the Bayesian networks with the dynamic fault trees to model the reliability of such types of systems. The inference technique of Bayesian network is utilized for reliability assessment and fault probability estimation. The solar array drive assembly is used to demonstrate the effectiveness of this method.","The inference technique of Bayesian network is utilized for reliability assessment and fault probability estimation."
"Taking account of the influence of common cause failure (CCF) to system reliability and the widespread presence of multi-state system (MSS) in engineering practices, a method for reliability modeling and assessment of a multi-state system with common cause failure is proposed by taking the advantage of graphic representation and uncertainty reasoning of Bayesian Network (BN). The model is applied to a two-axis positioning mechanism transmission system to demonstrate its effectiveness and capability for directly calculating the system reliability on the basis of multi-state probabilities of components. Firstly, the reliability block diagram is built according to the hierarchy of structure and function of multi-state system. Then, the traditional Bayesian Networks model of the transmission system is constructed based on the reliability block diagram, failure logic between components and the failure probability distribution of them. In this paper, the beta-factor model is used to analyze the CCF of the transmission system, and a new Bayesian network combining with CCF is established following by the implementation of reliability analysis. Finally, the comparison between the proposed method and the one without considering CCF is made to verify the efficiency and accuracy of the proposed method.",""
"Bayesian networks and their associated methods are especially suited for capturing and dealing with uncertainty. They can be successfully applied both in engineering sciences and in reliability analyses of water distribution systems. In this paper we propose an interactive Bayesian network and a decision-theoretic system which intend to monitor water loss, predict likely outcome and select appropriate decisions.",""
"In gear or rolling bearing systems, it is difficult to extract symptoms from vibration signals where shock vibration signals are present. However, the neural network method cannot provide satisfactory diagnosis results without adequate training samples. Bayesian networks provide an effective approach for fault diagnosis in cases given uncertain and incomplete information. In this study, the statistical factors of vibration signals in the time-domain were used and the diagnosis results by using Bayesian networks were superior to other neural network methods.",""
"Advances in digital media technology have increased in multimedia content. Tagging is one of the most effective methods to manage a great volume of multimedia content. However, manual tagging has limitations such as human fatigue and subjective and ambiguous keywords. In this paper, we present an automatic tagging method to generate semantic annotation on a mobile phone. In order to overcome the constraints of the mobile environment, the method uses two layered Bayesian networks. In contrast to existing techniques, this approach attempts to design probabilistic models with fixed tree structures and intermediate nodes. To evaluate the performance of this method, an experiment is conducted with data collected over a month. The result shows the efficiency and effectiveness of our proposed method. Furthermore, a simple graphic user interface is developed to visualize and evaluate recognized activities and probabilities.",""
"Over the past decade, studies applying data-driven modeling approaches have demonstrated significant contributions toward the integrative understanding of multivariate cell regulatory system operation. Here we review applications of several of these approaches, including principal component analysis, partial least squares regression, partial least squares discriminant analysis, decision trees, and Bayesian networks, and describe the advances they have offered in systems-level understanding of immune cell signaling and communication. We show how these approaches generate novel insights from high-throughput proteomic data, from classification to association to influence to mechanisms. Looking forward, new experimental technologies involving single-cell measurements of cytokine expression beckon extension of these modeling techniques to inference of immune cell-cell communication networks, with a goal of aiding development of improved vaccine therapeutics.","Here we review applications of several of these approaches, including principal component analysis, partial least squares regression, partial least squares discriminant analysis, decision trees, and Bayesian networks, and describe the advances they have offered in systems-level understanding of immune cell signaling and communication."
"This paper presents a method to model a driver's en route learning process and changes in route choice at each decision node. A model based on Bayesian networks is proposed to describe the en route updating of the driver's knowledge of the traffic state. A random utility based model is developed to predict en route choices. A case study based on probe data is carried out to illustrate the development of the model and analyze the dynamic route choice problem. The results show that the model in,which a driver's choice of making decisions en route is taken into account has a better goodness of fit. The probability of making a choice en route is related to the distance from the origin and the spatial scale of the intersection at the decision node.",""
"Flight delays and safety are the principal contradictions in the sound development of civil aviation. Flight delays often come up and induce civil aviation safety risk simultaneously. Based on flight delays, the random characteristics of civil aviation safety risk are analyzed. Flight delays have been deemed to a potential safety hazard. The change rules and characteristics of civil aviation safety risk based on flight delays have been analyzed. Bayesian networks (BN) have been used to build the aviation operation safety assessment model based on flight delay. The structure and parameters learning of the model have been researched. By using BN model, some airline in China has been selected to assess safety risk of civil aviation. The civil aviation safety risk of BN model has been assessed by GeNIe software. The research results show that flight delay, which increases the safety risk of civil aviation, can be seen as incremental safety risk. The effectiveness and correctness of the model have been tested and verified.",""
"During the last two decades, molecular genetic studies and the completion of the sequencing of the Arabidopsis thaliana genome have increased knowledge of hormonal regulation in plants. These signal transduction pathways act in concert through gene regulatory and signalling networks whose main components have begun to be elucidated. Our understanding of the resulting cellular processes is hindered by the complex, and sometimes counter-intuitive, dynamics of the networks, which may be interconnected through feedback controls and cross-regulation. Mathematical modelling provides a valuable tool to investigate such dynamics and to perform in silico experiments that may not be easily carried out in a laboratory. In this article, we firstly review general methods for modelling gene and signalling networks and their application in plants. We then describe specific models of hormonal perception and cross-talk in plants. This mathematical analysis of sub-cellular molecular mechanisms paves the way for more comprehensive modelling studies of hormonal transport and signalling in a multi-scale setting.",""
"Owing to its complexity, the traveling salesman problem (TSP) is one of the most intensively studied problems in computational mathematics. The TSP is defined as the provision of minimization of total distance, cost, and duration by visiting the n number of points only once in order to arrive at the starting point. Various heuristic algorithms used in many fields have been developed to solve this problem. In this study, a solution was proposed for the TSP using the ant colony system and parameter optimization was taken from the Taguchi method. The implementation was tested by various data sets in the Traveling Salesman Problem Library and a performance analysis was undertaken. In addition to these, a variance analysis was undertaken in order to identify the effect values of the parameters on the system. Implementation software was developed using the MATLAB program, which has a useful interface and simulation support.",""
"Age-related macular degeneration (AMD) is the leading cause of irreversible blindness in the elderly. The aim of this study was therefore to explore the relationship between the presence of multiple gene polymorphisms and 2 distinct advanced 'dry and wet' AMD phenotypes, and to assess gene interactions with the influence of personal factors in a Turkish population as a pilot study. For the analysis, the data were collected from 73 unrelated participants, grouped as 29 wet and 26 dry AMD patients, and 18 healthy controls. They were all genotyped for the multiple gene polymorphisms in 12 different genes. The data set collected was then analyzed using the Bayesian inference methods and visualized by means of the Bayesian networks. The results suggest that: 1) the PAI-1 4G/5G and FV G1691A genes have joint roles in the separation of the 3 groups; 2) both wet and dry AMD can be separated from the control group using the genes PAI-1 4G/5G, FV G1691A, FXH V34L, and PT G20210A; 3) although the wet AMD and control groups can be separated by the combination of the ACE I I D and B-fibrinogen-455 G-A gene polymorphisms, there seems to be no significant effect of the genes on the separation between the dry AMD and control groups; 4) the wet AMD and control groups can be distinguished by the combination of body mass index and the MTHFR-C677T and PAI-1 genes; and 5) there is a correlation between wet AMD and a high body mass index (>30 kg/m(2)). It was also found that the impact of body mass index on the disease development seems only in question with the connective availability of the genes MTHFR C677T and PAI-1. It can be concluded that the combination of the MTHFR C677T and PAI-1 4G/5G gene polymorphisms in the presence of obesity may increase the risk of wet AMD. In addition, the results further support a complex interplay among genetic and environmental factors in the development of different phenotypes.","The data set collected was then analyzed using the Bayesian inference methods and visualized by means of the Bayesian networks."
"Prediction of traffic demand is essential, either for an understanding of the future traffic state or so necessary measures can be taken to alleviate congestion. Usually, an origin-destination (O-D) matrix is used to represent traffic demand between two zones in transportation planning. Vehicles are assumed to be homogeneous; the trips of each vehicle are examined separately. This traditional O-D matrix lacks a behavioral basis and trip-based model structure. Another research stream of travel activity-based research addresses individual travel behaviors. This stream addresses the trip chain for travelers, but the research scope is attributes of trips, which ignores the road network. The concept of the O-D tuple, a sequence of dependent O-D pairs, is proposed for linking these two fields and for predicting traffic demand better. Through advanced monitoring systems that identify and track vehicles in the road network, the additional uncertainties of O-D tuples can be mitigated and thus reduce the underspecification more specifically. The hierarchical Bayesian networks mechanism in Gaussian space with multiprocesses is proposed for gaining the posterior of uncertain parameters. The model includes level and trend components for predicting future traffic volumes. A case study demonstrates that the proposed method can predict demand, and the path flow from cameras can reduce uncertainty in the estimation and prediction process, especially for O-D tuples.",""
"In this paper, learning a Bayesian network structure that optimizes a scoring function for a given dataset is viewed as a shortest path problem in an implicit state-space search graph. This perspective highlights the importance of two research issues: the development of search strategies for solving the shortest path problem, and the design of heuristic functions for guiding the search. This paper introduces several techniques for addressing the issues. One is an A* search algorithm that learns an optimal Bayesian network structure by only searching the most promising part of the solution space. The others are mainly two heuristic functions. The first heuristic function represents a simple relaxation of the acyclicity constraint of a Bayesian network. Although admissible and consistent, the heuristic may introduce too much relaxation and result in a loose bound. The second heuristic function reduces the amount of relaxation by avoiding directed cycles within some groups of variables. Empirical results show that these methods constitute a promising approach to learning optimal Bayesian network structures.",""
"Despite the efforts made towards the Millennium Development Goals targets during the last decade, improved access to water supply or basic sanitation still remains unavailable for millions of people across the world. This paper proposes a set of models that use 25 key variables and country profiles from the WatSan4Dev data set involving water supply and sanitation (Dondeynaz et al., 2012). This paper suggests the use of Bayesian network modelling methods because they are more easily adapted to deal with non-normal distributions, and integrate a qualitative approach for data analysis. They also offer the advantage of integrating preliminary knowledge into the probabilistic models. The statistical performance of the proposed models ranges between 20 and 5% error rates, which are very satisfactory taking into account the strong heterogeneity of variables. Probabilistic scenarios run from the models allow an assessment of the relationships between human development, external support, governance aspects, economic activities and water supply and sanitation (WSS) access. According to models proposed in this paper, gaining a strong poverty reduction will require the WSS access to reach 75-76% through: (1) the management of ongoing urbanisation processes to avoid slums development; and (2) the improvement of health care, for instance for children. Improving governance, such as institutional efficiency, capacities to make and apply rules, or control of corruption is positively associated with WSS sustainable development. The first condition for an increment of the HDP (human development and poverty) remains of course an improvement of the economic conditions with higher household incomes. Moreover, a significant country commitment to the environment, associated with civil society freedom of expression constitutes a favourable setting for sustainable WSS services delivery. Intensive agriculture using irrigation practises also appears as a mean for sustainable WSS thanks to multi-uses and complementarities. With a WSS sector organised at national level, irrigation practices can support the structuring and efficiency of the agriculture sector. It may then induce rural development in areas where WSS access often is set back compared to urban areas(1). External financial support, called Official Development Assistance (ODA CI), plays a role in WSS improvement but comes last in the sensitivity analyses of models. An overall 47% of the Official Development Assistance goes first to poor countries, and is associated to governance aspects: (1) political stability and (2) country commitment to the environment and civil society degree of freedom. These governance aspects constitute a good framework for aid implementation in recipient countries. Modelling is run with the five groups of countries as defined in Dondeynaz et al. (2012). Models for profile 4 (essential external support) and profile 5 (primary material consumption) are specifically detailed and analysed in this paper. For countries in profile 4, fighting against water scarcity and progressing desertification should be the priority. However, for countries in profile 5, efforts should first concentrate on consolidation of political stability while supporting diversification of the economic activities. Nevertheless, for both profiles, reduction of poverty should remain the first priority as previously indicated.",""
"Ports are exposed to various risks in their internal operations and external interactions with inland transport carriers and sea-going vessels within maritime logistics systems. While conventional safety management techniques may be capable of dealing with accidental, hazard-based risks in port, new vulnerability analysis methods are urgently required for tackling those caused by threats such as terrorist attacks. The motivation for identifying the vulnerabilities is the need for prioritising activities and resources on port security investments and risk reduction processes. This paper develops an advanced threat-based criticality analysis methodology designed for the identification and prioritisation of vulnerable port facilities under uncertainties. The model relies on the combination of fuzzy Bayesian reasoning and analytical hierarchy process (AHP) analysis in a complementary way so as to facilitate the treatment of uncertainty in data, thus realising effective quantitative analysis of the vulnerabilities under different threat modes in ports. The outcomes can be used either as a stand-alone technique for prioritising critical systems such as port facilitates with high values and significant functions or as part of an integrated decision making method for evaluating the effectiveness of security control options.",""
"Wireless sensor networks (WSNs) are a fundamental building block of many pervasive applications. Nevertheless the use of such technology raises new challenges regarding the development of reliable and fault-tolerant systems. One of the most critical issues is the detection of corrupted readings amidst the huge amount of gathered sensory data. Indeed, such readings could significantly affect the quality of service (QoS) of the WSN, and thus it is highly desirable to automatically discard them. This issue is usually addressed through \"fault detection\" algorithms that classify readings by exploiting temporal and spatial correlations. Generally, these algorithms do not take into account QoS requirements other than the classification accuracy. This paper proposes a fully distributed algorithm for detecting data faults, taking into account the response time besides the classification accuracy. We adopt the Bayesian networks to perform classification of readings and the Pareto optimization to allow QoS requirements to be simultaneously satisfied. Our approach has been tested on a synthetic dataset in order to evaluate its behavior with respect to different values of QoS constraints. The experimental evaluation produced good results, showing that our algorithm is able to greatly reduce the response time at the cost of a small reduction in classification accuracy.","Generally, these algorithms do not take into account QoS requirements other than the classification accuracy."
"Graphical model learning and inference are often performed using Bayesian techniques. In particular, learning is usually performed in two separate steps. First, the graph structure is learned from the data; then the parameters of the model are estimated conditional on that graph structure. While the probability distributions involved in this second step have been studied in depth, the ones used in the first step have not been explored in as much detail. In this paper, we will study the prior and posterior distributions defined over the space of the graph structures for the purpose of learning the structure of a graphical model. In particular, we will provide a characterisation of the behaviour of those distributions as a function of the possible edges of the graph. We will then use the properties resulting from this characterisation to define measures of structural variability for both Bayesian and Markov networks, and we will point out some of their possible applications.","Graphical model learning and inference are often performed using Bayesian techniques."
"The paper gives an introduction into graphical models and their use in specifying stochastic models in geodesy and photogrammetry. Basic task in adjustment theory can intuitively be described and analysed using graphical models. The paper shows that geodetic networks and bundle adjustments can be interpreted as graphical models, both as Bayesian networks or as conditional random fields. Especially hidden Markov random fields and conditional random fields are demonstrated to be versatile models for parameter estimation and classification.","Especially hidden Markov random fields and conditional random fields are demonstrated to be versatile models for parameter estimation and classification."
"Throughout the last couple of years several approaches for the feedback-based improvement of capital goods have been developed. Based on previous research work the authors are proposing a new methodology of decision support for the improvement of existing mass-produced standard products. This approach is based on prescriptive decision theory and uses feedback data in addition to product-specific characteristics and properties. For the prediction and evaluation of different improvement alternatives, the presented solution uses object-oriented Bayesian networks (OOBN). The validation of the proposed solution has been demonstrated on the basis of decision processes for the improvement of centrifugal pumps. (c) 2013 CIRP.",""
"The purpose of this paper is to identify probabilistic relationships of mobile service usage behaviour, and especially to understand the probabilistic relationship between overall service usage diversity and average daily service usage intensity. These are topical themes due to the high number of services available in application stores which may or may not lead to high usage diversity of mobile services. Four analytical methods are used in the study, all are based on Bayesian Networks; 1) Visual analysis of Bayesian Networks to find initially interesting patterns, variables and their relationships, 2) user segmentation analysis, 3) node force analysis and 4) a combination of expert-based service clustering and machine learning for usage diversity vs. intensity analysis. All the analyses were conducted with handset-based data collected from university students and staff. The analysis indicates that services exist, which mediate usage of other services. In other words, usage of these services increases the probability of using also other services. A service called Installer is an example of this kind of a service. In addition, probabilistic relationships can be found within certain service cluster pairs in their usage diversity and intensity values. Based on these relationships, similar mediation type of behaviour can be found for service clusters as for individual services. This is most visible in the relation between System/Utilities and Business/Productivity service clusters. They do not have a direct relationship but usage diversity is a mediator between them. Furthermore, segmentation analysis shows that the user segment called \"experimentalists\" uses more mediator services than other user segments. Furthermore, \"experimentalists\" use a much broader set of services daily, than the other segments. This study demonstrates that a Bayesian Network is a straightforward way to express model characteristics on high level. Moreover, Node Force, Direct and Total effect are useful metrics to measure the mediation effects. The clustering implemented as a hybrid of machine learning and expert-based clustering process is also a useful way to calculate relationships between clusters of more than a hundred individual services.",""
"This paper proposes a new method, conditional probability table(CPT) decomposition, to analyze the independent and deterministic components of CPT. This method can be used to approximate and analyze Baysian networks. The decomposition of Bayesian networks is accomplished by representing CPTs as a linear combination of extreme CPTs, which forms a new framework to conduct inference. Based on this new framework, inference in Bayesian networks can be done by decomposing them into less connected and weighted subnetworks. We can achieve exact inference if the original network is decomposed into singly-connected subnetworks. Besides, approximate inference can be done by discarding the subnetworks with small weights or by a partial decomposition and application of belief propagation (BP) on the still multiply-connected subnetworks. Experiments show that the decomposition-based approximation outperforms BP in most cases.","The decomposition of Bayesian networks is accomplished by representing CPTs as a linear combination of extreme CPTs, which forms a new framework to conduct inference."
"There has been some tendency to view decision science and resilience theory as opposing approaches, or at least as contending perspectives, for natural resource management. Resilience proponents have been especially critical of optimization in decision science, at least for those cases where it is focused on the aggressive pursuit of efficiency. In general, optimization of resource systems is held to reduce spatial, temporal, or organizational heterogeneity that would otherwise limit efficiency, leading to homogenization of a system and making it less able to cope with unexpected changes or disturbances. For their part, decision analysts have been critical of resilience proponents for not providing much practical advice to decision makers. We believe a key source of tension between resilience thinking and application of decision science is the pursuit of efficiency in the latter (i.e., choosing the \"best\" management action or strategy option to maximize productivity of one or few resource components), vs. a desire in the former to keep options open (i.e., maintaining and enhancing diversity). It seems obvious, however, that with managed natural systems, there must be a principle by which to guide decision making, which at a minimum allows for a comparison of projected outcomes associated with decision alternatives. This is true even if the primary concern of decision making is the preservation of system resilience. We describe how a careful framing of conservation problems, especially in terms of management objectives and predictive models, can help reduce the purported tension between resilience thinking and decision analysis. In particular, objective setting in conservation problems needs to be more attuned to the dynamics of ecological systems and to the possibility of deep uncertainties that underlie the risk of unintended, if not irreversible, outcomes. Resilience thinking also leads to the suggestion that model development should focus more on process rather than pattern, on multiple scales of influence, and on phenomena that can create alternative stability regimes. Although we acknowledge the inherent difficulties in modeling ecological processes, we stress that formulation of useful models need not depend on a thorough mechanistic understanding or precise parameterization, assuming that uncertainty is acknowledged and treated in a systematic manner.",""
"A maximum likelihood based model selection of discrete Bayesian networks is considered. The structure learning is performed by employing a scoring function S, which, for a given network G and n-sample D-n,is defined as the maximum marginal log-likelihood l minus a penalization term lambda(n)h proportional to network complexity h(G), S(G vertical bar D-n) = l(G vertical bar D-n)-lambda(n)h(G). An available case analysis is developed with the standard log-likelihood replaced by the sum of sample average node log-likelihoods. The approach utilizes partially missing data records and allows for comparison of models fitted to different samples. In missing completely at random settings the estimation is shown to be consistent if and only if the sequence lambda(n) converges to zero at as lower than n(-1/2) rate. In particular, the BIC model selection (lambda(n)=0.5log(n)/n) applied to the node-average log-likelihood is shown to be inconsistent in general. This is in contrast to the complete data case when BIC is known to be consistent. The conclusions are confirmed by numerical experiments.",""
"Service management is becoming more and more important within the area of IT management. How to efficiently manage and organize service in complicated IT service environments with frequent changes is a challenging issue. IT service and the related information from different sources are characterized as diverse, incomplete, heterogeneous, and geographically distributed. It is hard to consume these complicated services without knowledge assistant. To address this problem, a systematic way (with proposed toolsets and process) is proposed to tackle the challenges of acquisition, structuring, and refinement of structured knowledge. An integrated knowledge process is developed to guarantee the whole engineering procedure which utilizes Bayesian networks (BNs) as the knowledge model. This framework can be successfully applied on key tasks in service management, such as problem determination and change impact analysis, and a real example of Cisco VoIP system is introduced to show the usefulness of this method.",""
"The conflict that exists between the competing needs of biological conservation and pastoral production is well recognised but few studies have examined these conflicts due to their complexity and the uncertainty that surrounds these relationships. The development of a Bayesian network model that examines the trade-offs between the conservation value of the landscape for a range of taxa (flora, mammals, birds and herpetofauna) and its primary production value under alternative land uses is described. The model emphasises structural diversity of vegetation and ecosystem productivity as key drivers of both biodiversity and agricultural production. Simple scenarios, used to examine the influence of different land uses on multiple components of biodiversity and agricultural productivity, demonstrated the potential for the analysis of the trade-offs associated with alternative landscape designs. The potential of the model, as a planning or policy development tool for land management agencies or regional Natural Resource Management bodies, at multiple scales, is identified.",""
"Fault diagnosis of power systems is an important task in power system operation. In this paper, fuzzy reasoning spiking neural P systems (FRSN P systems) are implemented for fault diagnosis of power systems for the first time. As a graphical modeling tool, FRSN P systems are able to represent fuzzy knowledge and perform fuzzy reasoning well. When the cause-effect relationship between candidate faulted section and protective devices is represented by the FRSN P systems, the diagnostic conclusion can be drawn by means of a simple parallel matrix based reasoning algorithm. Three different power systems are used to demonstrate the feasibility and effectiveness of the proposed fault diagnosis approach. The simulations show that the developed FRSN P systems based diagnostic model has notable characteristics of easiness in implementation, rapidity in parallel reasoning, and capability in handling uncertainties. In addition, it is independent of the scale of power system and can be used as a reliable tool for fault diagnosis of power systems.",""
"In data analysis, latent variables play a central role because they help provide powerful insights into a wide variety of phenomena, ranging from biological to human sciences. The latent tree model, a particular type of probabilistic graphical models, deserves attention. Its simple structure - a tree - allows simple and efficient inference, while its latent variables capture complex relationships. In the past decade, the latent tree model has been subject to significant theoretical and methodological developments. In this review, we propose a comprehensive study of this model. First we summarize key ideas underlying the model. Second we explain how it can be efficiently learned from data. Third we illustrate its use within three types of applications: latent structure discovery, multidimensional clustering, and probabilistic inference. Finally, we conclude and give promising directions for future researches in this field.","Its simple structure - a tree - allows simple and efficient inference, while its latent variables capture complex relationships."
"Recently much work in Machine Learning has concentrated on using expressive representation languages that combine aspects of logic and probability. A whole field has emerged, called Statistical Relational Learning, rich of successful applications in a variety of domains. In this paper we present a Machine Learning technique targeted to Probabilistic Logic Programs, a family of formalisms where uncertainty is represented using Logic Programming tools. Among various proposals for Probabilistic Logic Programming, the one based on the distribution semantics is gaining popularity and is the basis for languages such as ICL, PRISM, ProbLog and Logic Programs with Annotated Disjunctions. This paper proposes a technique for learning parameters of these languages. Since their equivalent Bayesian networks contain hidden variables, an Expectation Maximization (EM) algorithm is adopted. In order to speed the computation up, expectations are computed directly on the Binary Decision Diagrams that are built for inference. The resulting system, called EMBLEM for \"EM over Bdds for probabilistic Logic programs Efficient Mining\", has been applied to a number of datasets and showed good performances both in terms of speed and memory usage. In particular its speed allows the execution of a high number of restarts, resulting in good quality of the solutions.","In order to speed the computation up, expectations are computed directly on the Binary Decision Diagrams that are built for inference."
"Intention recognition is the process of becoming aware of the intentions of other agents, inferring them through observed actions or effects on the environment. Intention recognition enables pro-activeness, in cooperating or promoting cooperation, and in pre-empting danger. Technically, intention recognition can be performed incrementally as you go along, which amounts to learning. Intention recognition can also use past experience from a database of past interactions, not necessarily with the same agent. Bayesian Networks (BN) can be employed to dynamically summarize general statistical evidence, furnishing heuristic information to link with the situation specific information, about which logical reasoning can take place, and decisions made on actions to be performed, possibly involving actions to obtain new observations. This situated reasoning feeds into the BN to tune it, and back again into the logic component. In this article, we provide a review bearing on the state-of-the-art work on intention and plan recognition, which includes a comparison with our recent research, where we address a number of important issues of intention recognition. We also argue for an integrative approach to intention-based decision-making that uses a combination of Logic Programming and Bayesian Networks.",""
"Agricultural intensification often has complex effects on a wide range of environmental and economic values, presenting planners with challenging decisions for optimising sustainable benefits. Bayesian Belief Networks (BBNs) can be used as a decision-support tool for evaluating the influence of development scenarios across a range of values. A BBN was developed to guide decisions on water abstraction and irrigation-driven land use intensification in the Hurunui River catchment, New Zealand. The BBN examines the combined effects of different irrigation water sources and four land development scenarios, with and without a suite of on-farm mitigations, on ground and surface water quality, key socioeconomic values (i.e. farm earnings and jobs, and contribution to regional gross domestic production (GDP)) and aquatic values (i.e. salmon, birds, waterscape, contact recreation, periphyton and invertebrates). It predicts high farm earnings, jobs and regional GDP with 150% increase in irrigated area, but a range of positive and negative aquatic environmental outcomes, depending on the location of water storage dams and the application of a suite of on-farm mitigations. This BBN synthesis of a complex system enhanced the ability to include aquatic values alongside economic and social values in land-use and water resource planning and decision making, and has influenced objective setting in Hurunui planning processes.",""
"One problem faced in knowledge engineering for Bayesian networks (BNs) is the exponential growth of the number of parameters in their conditional probability tables (CPTs). The most common practical solution is the application of the so-called canonical gates and, among them, the noisy-OR (or their generalization, the noisy-MAX) gates, which take advantage of the independence of causal interactions and provide a logarithmic reduction of the number of parameters required to specify a CPT. In this paper, we propose an algorithm that fits a noisy-MAX distribution to an existing CPT, and we apply this algorithm to search for noisy-MAX gates in three existing practical BN models: ALARM, HAILFINDER, and HEPAR II. We show that the noisy-MAX gate provides a surprisingly good fit for as many as 50% of CPTs in two of these networks. We observed this in both distributions elicited from experts and those learned from data. The importance of this finding is that it provides an empirical justification for the use of the noisy-MAX gate as a powerful knowledge engineering tool.",""
". Proteins are the workhorses of all living systems, and protein bioinformatics deals with analysis of protein sequences (one dimensional) and structures (three dimensional). The paper reviews statistical advances in three major active areas of protein structural bioinformatics: structure comparison, Ramachandran plots and structure prediction. These topics play a key role in understanding one of the greatest unsolved problems in biology, how proteins fold from one dimension to three dimensions, and have relevance to protein functionality, drug discovery and evolutionary biology. For each area, we give the biological background and review one of the main bioinformatics solutions to a specific problem in that area. We then present statistical tools recently developed to investigate these problems, consisting of Bayesian alignment, directional distributions and hidden Markov models. We illustrate each problem with a new case-study and describe what statistics can offer to these problems. We highlight challenges facing these areas and conclude with an overall discussion.",""
"An evacuation decision for dam breaks is a very serious issue. A late decision may lead to loss of lives and properties, but a very early evacuation will incur unnecessary expenses. This paper presents a risk-based framework of dynamic decision making for dam-break emergency management (DYDEM). The dam-break emergency management in both time scale and space scale is introduced first to define the dynamic decision problem. The probability of dam failure is taken as a stochastic process and estimated using a time-series analysis method. The flood consequences are taken as functions of warning time and evaluated with a human risk analysis model (HURAM) based on Bayesian networks. A decision criterion is suggested to decide whether to evacuate the population at risk (PAR) or to delay the decision. The optimum time for evacuating the PAR is obtained by minimizing the expected total loss, which integrates the time-related probabilities and flood consequences. When a delayed decision is chosen, the decision making can be updated with available new information. A specific dam-break case study is presented in a companion paper to illustrate the application of this framework to complex dam-breaching problems.",""
"Tangjiashan landslide dam, which was triggered by the M-s = 8.0 Wenchuan earthquake in 2008 in China, threatened 1.2 million people downstream of the dam. All people in Beichuan Town 3.5 km downstream of the dam and 197 thousand people in Mianyang City 85 km downstream of the dam were evacuated 10 days before the breaching of the dam. Making such an important decision under uncertainty was difficult. This paper applied a dynamic decision-making framework for dam-break emergency management (DYDEM) to help rational decision in the emergency management of the Tangjiashan landslide dam. Three stages are identified with different levels of hydrological, geological and social-economic information along the timeline of the landslide dam failure event. The probability of dam failure is taken as a time series. The dam breaching parameters are predicted with a set of empirical models in stage 1 when no soil property information is known, and a physical model in stages 2 and 3 when knowledge of soil properties has been obtained. The flood routing downstream of the dam in these three stages is analyzed to evaluate the population at risk (PAR). The flood consequences, including evacuation costs, flood damage and monetized loss of life, are evaluated as functions of warning time using a human risk analysis model based on Bayesian networks. Finally, dynamic decision analysis is conducted to find the optimal time to evacuate the population at risk with minimum total loss in each of these three stages.",""
"Objectives: To obtain a balanced view on the role and place of expert knowledge and learning methods in building Bayesian networks for medical image interpretation. Methods and materials: The interpretation of mammograms was selected as the example medical image interpretation problem. Medical image interpretation has its own common standards and procedures. The impact of these on two complementary methods for Bayesian network construction was explored. Firstly, methods for the discretisation of continuous features were investigated, yielding multinomial distributions that were compared to the original Gaussian probabilistic parameters of the network. Secondly, the structure of a manually constructed Bayesian network was tested by structure learning from image data. The image data used for the research came from screening mammographic examinations of 795 patients, of whom 344 were cancerous. Results: The experimental results show that there is an interesting interplay of machine learning results and background knowledge in medical image interpretation. Networks with discretised data lead to better classification performance (increase in the detected cancers of up to 11.7%), easier interpretation, and a better fit to the data in comparison to the expert-based Bayesian network with Gaussian probabilistic parameters. Gaussian probability distributions are often used in medical image interpretation because of the continuous nature of many of the image features. The structures learnt supported many of the expert-originated relationships but also revealed some novel relationships between the mammographic features. Using discretised features and performing structure learning on the mammographic data has further improved the cancer detection performance of up to 17% compared to the manually constructed Bayesian network model. Conclusion: Finding the right balance between expert knowledge and data-derived knowledge, both at the level of network structure and parameters, is key to using Bayesian networks for medical image interpretation. A balanced approach to building Bayesian networks for image interpretation yields more accurate and understandable Bayesian network models. (C) 2012 Elsevier B.V. All rights reserved.","Networks with discretised data lead to better classification performance (increase in the detected cancers of up to 11."
"A strong normative development in Europe, including the Nitrate Directive (1991) and the Water Framework Directive (WFD) (2000), has been promulgated. The WFD states that all water bodies have to reach a good quantitative and chemical status by 2015. It is necessary to consider different objectives, often in conflict, for tackling a suitable assessment of the impacts generated by water policies aimed to reduce nitrate pollution in groundwater. For that, an annual lumped probabilistic model based on Bayesian networks (BNs) has been designed for hydro-economic modelling of groundwater quality control under uncertain conditions. The information introduced in the BN model comes from different sources such as previous groundwater flow and mass transport simulations, hydro-economic models, stakeholders and expert opinion, etc. The methodology was applied to the El Salobral-Los Llanos aquifer unit within the 'Easter Mancha' groundwater body, which is one of the largest aquifers in Spain (7,400 km(2)), included in the Jucar River Basin. Over the past 30 years, socioeconomic development within the region has been mainly depending on intensive use of groundwater resources for irrigating crops. This has provoked a continuous groundwater level fall in the last two decades and significant streamflow depletion in the connected Jucar River. This BN model has proved to be a robust Decision Support System for helping water managers in the decision making process.",""
"This paper presents an approach for modelling Systems Integration Technical Risks (SITR) assessment using Bayesian Belief Networks (BBN). SITR represent a significant part of project risks associated with a development of large software intensive systems. We propose conceptual modelling framework to address the problem of SITR assessment at early stages of a system life cycle. This framework includes a set of BBN models, representing the risk contributing factors, and complementing Parametric Models (PM), used for providing input data to the BBN models. In particular we describe SITR identification approach explaining corresponding BBN models' topologies and relevant conceptual model framework. This framework includes a set of BBN models, representing the risk contributing factors, fused with complementary PMs providing input data to the BBN models. Heuristic approaches for easing Conditional Probabilities Tables (CPT) generation are described. We briefly discuss preliminary results of model testing. In conclusion we summarise benefits and constraints for SITR assessment based on BBN models, and provide suggestions for further research directions for model improvement.",""
"A Bayesian network (BN) is a graphical model of uncertainty that is especially well suited to legal arguments. It enables us to visualize and model dependencies between different hypotheses and pieces of evidence and to calculate the revised probability beliefs about all uncertain factors when any piece of new evidence is presented. Although BNs have been widely discussed and recently used in the context of legal arguments, there is no systematic, repeatable method for modeling legal arguments as BNs. Hence, where BNs have been used in the legal context, they are presented as completed pieces of work, with no insights into the reasoning and working that must have gone into their construction. This means the process of building BNs for legal arguments is ad hoc, with little possibility for learning and process improvement. This article directly addresses this problem by describing a method for building useful legal arguments in a consistent and repeatable way. The method complements and extends recent work by Hepler, Dawid, and Leucari (2007) on object-oriented BNs for complex legal arguments and is based on the recognition that such arguments can be built up from a small number of basic causal structures (referred to as idioms). We present a number of examples that demonstrate the practicality and usefulness of the method.",""
"Sudden valve closure in pipeline systems can cause high pressures that may lead to serious damages. Using an optimal valve closing rule can play an important role in managing extreme pressures in sudden valve closure. In this paper, an optimal closing rule curve is developed using a multi-objective optimization model and Bayesian networks (BNs) for controlling water pressure in valve closure instead of traditional step functions or single linear functions. The method of characteristics is used to simulate transient flow caused by valve closure. Non-dominated sorting genetic algorithms-II is also used to develop a Pareto front among three objectives related to maximum and minimum water pressures, and the amount of water passes through the valve during the valve-closing process. Simulation and optimization processes are usually time-consuming, thus results of the optimization model are used for training the BN. The trained BN is capable of determining optimal real-time closing rules without running costly simulation and optimization models. To demonstrate its efficiency, the proposed methodology is applied to a reservoir-pipe-valve system and the optimal closing rule curve is calculated for the valve. The results of the linear and BN-based valve closure rules show that the latter can significantly reduce the range of variations in water hammer pressures.",""
"Risk-related knowledge gained from past construction projects is regarded as potentially extremely useful in risk management. This article describes a proposed approach to capture and integrate risk-related knowledge to support decision making in construction projects. To ameliorate the problem related to the scarcity of risks information often encountered in construction projects, Bayesian Belief Networks are used and expert judgment is elicited to augment available information. Particularly, the article provides an overview of judgment-based biases that can appear in the elicitation of judgments for constructing Bayesian Networks and the provisos that can be made in this respect to minimize these types of bias. The proposed approach is successfully applied to develop six models for top risks in tunnel works. More than 30 tunneling experts in the Netherlands and Germany were involved in the investigation to provide information on identifying relevant scenarios than can lead to failure events associated with tunneling risks. The article has provided an illustration of the applicability of the developed approach for the case of face instability in soft soils using slurry shields.",""
"The application of structural health monitoring (SHM) often employs multiple sensors to monitor the state of health and usage of the structures. The fault of any sensor may lead to an inaccurate or even incorrect inference with the collected sensor data, which will accordingly create a negative impact on higher-level decisions for maintenance and services. Thus, sensor validation becomes a critical process to the performance of the whole SHM system. This paper presents the use of Bayesian belief network to validate the reading of strain gauges on an aluminum plate for loading monitoring. The Bayesian belief network is constructed with the training data. The factors investigated in this paper, which may affect the validation process, include sensor configuration, sensor redundancy, and sensor data range for the discretization. The feasibility of using a Bayesian belief network for SHM sensor validation is demonstrated with the experimental results.","The fault of any sensor may lead to an inaccurate or even incorrect inference with the collected sensor data, which will accordingly create a negative impact on higher-level decisions for maintenance and services."
"In this paper a probabilistic approach to sensor fault diagnosis is presented. The proposed method is applicable to systems whose dynamic can be approximated with only few active states, especially in process control where we usually have a relatively slow dynamics. Unlike most existing probabilistic approaches to fault diagnosis, which are based on Bayesian Belief Networks, in this approach the probabilistic model is directly extracted from a parity equation. The relevant parity equation can be found using a model of the system or through principal component analysis of data measured from the system. In addition, a sensor detectability index is introduced that specifies the level of detectability of sensor faults in a set of analytically redundant sensors. This index depends only on the internal relationships of the variables of the system and noise level. The method is tested on a model of the Tennessee Eastman process and the result shows a fast and reliable prediction of fault in the detectable sensors. (C) 2012 Elsevier Ltd. All rights reserved.",""
"Dependency networks (DNs) have been receiving more attention recently because their structures and parameters can be easily learned from data. The full conditional distributions (FCDs) are known conditions of DNs. Gibbs sampling is currently the most popular inference method on DNs. However, sampling methods converge slowly and it can be hard to diagnose their convergence. In this article, we introduce a set of linear equations to describe the relations between joint probability distributions (JPDs) and FCDs. These equations provide a novel perspective to understand reasoning on DNs. Based on these linear equations, we develop both exact and approximate algorithms for inference on DNs. Experiments show that the proposed approximate algorithms can produce effective results by maintaining low computational complexity.","Gibbs sampling is currently the most popular inference method on DNs."
"Within probabilistic classification problems, learning the Markov boundary of the class variable consists in the optimal approach for feature subset selection. In this paper we propose two algorithms that learn the Markov boundary of a selected variable. These algorithms are based on the score+search paradigm for learning Bayesian networks. Both algorithms use standard scoring functions but they perform the search in constrained spaces of class-focused directed acyclic graphs, going through the space by means of operators adapted for the problem. The algorithms have been validated experimentally by using a wide spectrum of databases, and their results show a performance competitive with the state-of-the-art.","Within probabilistic classification problems, learning the Markov boundary of the class variable consists in the optimal approach for feature subset selection."
"Variable selection is an important problem for cluster analysis of high-dimensional data. It is also a difficult one. The difficulty originates not only from the lack of class information but also the fact that high-dimensional data are often multifaceted and can be meaningfully clustered in multiple ways. In such a case the effort to find one subset of attributes that presumably gives the \"best\" clustering may be misguided. It makes more sense to identify various facets of a data set (each being based on a subset of attributes), cluster the data along each one, and present the results to the domain experts for appraisal and selection. In this paper, we propose a generalization of the Gaussian mixture models and demonstrate its ability to automatically identify natural facets of data and cluster data along each of those facets simultaneously. We present empirical results to show that facet determination usually leads to better clustering results than variable selection. (C) 2012 Elsevier Inc. All rights reserved.",""
"Large population biobanks of unrelated individuals have been highly successful in detecting common genetic variants affecting diseases of public health concern. However, they lack the statistical power to detect more modest gene-gene and gene-environment interaction effects or the effects of rare variants for which related individuals are ideally required. In reality, most large population studies will undoubtedly contain sets of undeclared relatives, or pedigrees. Although a crude measure of relatedness might sometimes suffice, having a good estimate of the true pedigree would be much more informative if this could be obtained efficiently. Relatives are more likely to share longer haplotypes around disease susceptibility loci and are hence biologically more informative for rare variants than unrelated cases and controls. Distant relatives are arguably more useful for detecting variants with small effects because they are less likely to share masking environmental effects. Moreover, the identification of relatives enables appropriate adjustments of statistical analyses that typically assume unrelatedness. We propose to exploit an integer linear programming optimisation approach to pedigree learning, which is adapted to find valid pedigrees by imposing appropriate constraints. Our method is not restricted to small pedigrees and is guaranteed to return a maximum likelihood pedigree. With additional constraints, we can also search for multiple high-probability pedigrees and thus account for the inherent uncertainty in any particular pedigree reconstruction. The true pedigree is found very quickly by comparison with other methods when all individuals are observed. Extensions to more complex problems seem feasible.",""
"Since eukaryotic transcription is regulated by sets of Transcription Factors (TFs) having various transcriptional time delays, identification of temporal combinations of activated TFs is important to reconstruct Transcriptional Regulatory Networks (TRNs). Our methods combine time course microarray data, information on physical binding between the TFs and their targets and the regulatory sequences of genes using a log-linear model to reconstruct dynamic functional TRNs of the yeast cell cycle and human apoptosis. In conclusion, our results suggest that the proposed dynamic motif search method is more effective in reconstructing TRNs than the static motif search method.",""
"In this paper, we describe the integration and evaluation of an existing generic Bayesian student model (GBSM) into an existing computerized testing system within the Mathematics Education Project (PmatE - Projecto Matematica Ensino) of the University of Aveiro. This generic Bayesian student model had been previously evaluated with simulated students, but a real application was still missing. In the work presented here, we have used the GBSM to define Bayesian Student Models (BSMs) for a concrete domain: first degree equations. In order to test the diagnosis capabilities of such BSMs, an evaluation with 152 students has been performed. Each of the 152 students took both a computerized test within PMatE and a written exam, both of them designed to measure students' knowledge in 12 concepts related to first degree equations. The written exam was graded by three experts. Then two BSMs were developed, one for the computer test and another one for the written exam. These BSMs were used to obtain estimations of student's knowledge on the same 12 concepts, and the inter-rater agreement among the different measures was computed. Results show a high degree of agreement among the scores given by the experts and also among the diagnosis provided by the BSM in the written exam and expert's average, but a low degree of agreement among the diagnosis provided by the BSM in the computer test and expert's average. (C) 2012 Elsevier Ltd. All rights reserved.",""
"Objective: (1) To determine the brain connectivity pattern associated with clinical rigidity scores in Parkinson's disease (PD) and (2) to determine the relation between clinically assessed rigidity and quantitative metrics of motor performance. Background: Rigidity, the resistance to passive movement, is exacerbated in PD by asking the subject to move the contralateral limb, implying that rigidity involves a distributed brain network. Rigidity mainly affects subjects when they attempt to move; yet the relation between clinical rigidity scores and quantitative aspects of motor performance are unknown. Methods: Ten clinically diagnosed PD patients (off-medication) and 10 controls were recruited to perform an fMRI squeeze-bulb tracking task that included both visually guided and internally guided features. The direct functional connectivity between anatomically defined regions of interest was assessed with Dynamic Bayesian Networks (DBNs). Tracking performance was assessed by fitting Linear Dynamical System (LDS) models to the motor performance, and was compared to the clinical rigidity scores. A cross-validated Least Absolute Shrinkage and Selection Operator (LASSO) regression method was used to determine the brain connectivity network that best predicted clinical rigidity scores. Results:The damping ratio of the LDS models significantly correlated with clinical rigidity scores (p = 0.014). An fMRI connectivity network in subcortical and primary and premotor cortical regions accurately predicted clinical rigidity scores (p < 10(-5)). Conclusion: A widely distributed cortical/subcortical network is associated with rigidity observed in PD patients, which reinforces the importance of altered functional connectivity in the pathophysiology of PD. PD subjects with higher rigidity scores tend to have less overshoot in their tracking performance, and damping ratio may represent a robust, quantitative marker of the motoric effects of increasing rigidity.","A cross-validated Least Absolute Shrinkage and Selection Operator (LASSO) regression method was used to determine the brain connectivity network that best predicted clinical rigidity scores."
"Quantitative software reliability measurement approaches have some limitations in demonstrating the proper level of reliability in cases of safety-critical software. One of the more promising alternatives is the use of software development quality information. Particularly in the nuclear industry, regulatory bodies in most countries use both probabilistic and deterministic measures for ensuring the reliability of safety-grade digital computers in NPPs. The point of deterministic criteria is to assess the whole development process and its related activities during the software development life cycle for the acceptance of safety-critical software. In addition software Verification and Validation (V&V) play an important role in this process. In this light, we propose a V&V-based fault estimation method using Bayesian Nets to estimate the remaining faults for safety-critical software after the software development life cycle is completed. By modeling the fault insertion and elimination processes during the whole development phases, the proposed method systematically estimates the expected number of remaining faults. (C) 2012 Elsevier Ltd. All rights reserved.",""
"Temporal Bayesian networks have gained popularity as a robust technique to model dynamic systems in which the components' sequential dependency, as well as their functional dependency, cannot be ignored. In this regard, discrete-time Bayesian networks have been proposed as a viable alternative to solve dynamic fault trees without resort to Markov chains. This approach overcomes the drawbacks of Markov chains such as the state-space explosion and the error-prone conversion procedure from dynamic fault tree. It also benefits from the inherent advantages of Bayesian networks such as probability updating. However, effective mapping of the dynamic gates of dynamic fault trees into Bayesian networks while avoiding the consequent huge multi-dimensional probability tables has always been a matter of concern. In this paper, a new general formalism has been developed to model two important elements of dynamic fault tree, i.e., cold spare gate and sequential enforcing gate, with any arbitrary probability distribution functions. Also, an innovative Neutral Dependency algorithm has been introduced to model dynamic gates such as priority-AND gate, thus reducing the dimension of conditional probability tables by an order of magnitude. The second part of the paper is devoted to the application of discrete-time Bayesian networks in the risk assessment and safety analysis of complex process systems. It has been shown how dynamic techniques can effectively be applied for optimal allocation of safety systems to obtain maximum risk reduction. (C) 2012 Elsevier Ltd. All rights reserved.",""
"Biotracing, a new method developed to assist with food chain management and to control food safety, is presented to highlight practical considerations, including logistic issues for its implementation. The main differences between traceability and biotracing and between predictive microbiology and biotracing are explained and examples of situations in which biotracing could be of real help are listed (foodborne outbreaks, liability issues, HACCP, risk evaluation and decision making, education and training). Indications on how to access and interrogate two prototype models, called SimpleTrace and SimpleMatch, as well as some other Bayesian networks, are given to encourage using of biotracing, while operational biotracing is illustrated by an agent based model called AgentChain. The main types of inferences, which point to sources that generate potential problems within a particular food chain, are revealed. Biotracing is strongly recommended for introduction into continuous operations that include in line data collection, and can be operated, alongside existing safety systems, without additional burden. (C) 2012 Elsevier Ltd. All rights reserved.","The main types of inferences, which point to sources that generate potential problems within a particular food chain, are revealed."
"The popularity of Bayesian Network modelling of complex domains using expert elicitation has raised questions of how one might validate such a model given that no objective dataset exists for the model. Past attempts at delineating a set of tests for establishing confidence in an entirely expert-elicited model have focused on single types of validity stemming from individual sources of uncertainty within the model. This paper seeks to extend the frameworks proposed by earlier researchers by drawing upon other disciplines where measuring latent variables is also an issue. We demonstrate that even in cases where no data exist at all there is a broad range of validity tests that can be used to establish confidence in the validity of a Bayesian Belief Network. (C) 2012 Elsevier Ltd. All rights reserved.",""
"In the United States and worldwide, runway incursions are acknowledged as a critical concern for aviation safety. Despite efforts to the contrary, however, the rate at which these events occur in the United States has steadily risen. Analyses of the causes of runway incursions have frequently been limited to discrete events and have not addressed the dynamic interactions that led to breaches of runway safety. This paper emphasizes the need for cross-domain methods of causation analysis applied to runway incursions in the United States. A holistic modeling technique using Bayesian belief networks to interpret causation in the presence of sparse data is outlined, with intended application at the systems level. Further, the importance of investigating runway incursions probabilistically and of incorporating information from human factors, technological, and organizational perspectives is supported. A method for structuring Bayesian networks with quantitative and qualitative event analyses in conjunction with structured expert probability estimation is outlined, and results are presented for propagation of evidence through the model as well as for causal analysis. The model provides a dynamic, inferential platform for future evaluation of the causes of runway incursions. The results in part confirm what is known about the causes of runway incursions, but more important shed light on multifaceted causal interactions in a modeling space that allows causal inference and evaluation of changes to the system in a dynamic setting. Suggestions for additional research are discussed; the prominent suggestion is a need for future testing coupled with a focus on higher levels of quantification while exploring means of enhancing availability of relevant data.","The results in part confirm what is known about the causes of runway incursions, but more important shed light on multifaceted causal interactions in a modeling space that allows causal inference and evaluation of changes to the system in a dynamic setting."
"Background: Detecting epistatic interactions plays a significant role in improving pathogenesis, prevention, diagnosis, and treatment of complex human diseases. Applying machine learning or statistical methods to epistatic interaction detection will encounter some common problems, e.g., very limited number of samples, an extremely high search space, a large number of false positives, and ways to measure the association between disease markers and the phenotype. Results: To address the problems of computational methods in epistatic interaction detection, we propose a score-based Bayesian network structure learning method, EpiBN, to detect epistatic interactions. We apply the proposed method to both simulated datasets and three real disease datasets. Experimental results on simulation data show that our method outperforms some other commonly-used methods in terms of power and sample-efficiency, and is especially suitable for detecting epistatic interactions with weak or no marginal effects. Furthermore, our method is scalable to real disease data. Conclusions: We propose a Bayesian network-based method, EpiBN, to detect epistatic interactions. In EpiBN, we develop a new scoring function, which can reflect higher-order epistatic interactions by estimating the model complexity from data, and apply a fast Branch-and-Bound algorithm to learn the structure of a two-layer Bayesian network containing only one target node. To make our method scalable to real data, we propose the use of a Markov chain Monte Carlo (MCMC) method to perform the screening process. Applications of the proposed method to some real GWAS (genome-wide association studies) datasets may provide helpful insights into understanding the genetic basis of Age-related Macular Degeneration, late-onset Alzheimer's disease, and autism.",""
"Background: TNF (Tumor Necrosis Factor-alpha) induces HUVEC (Human Umbilical Vein Endothelial Cells) to proliferate and form new blood vessels. This TNF-induced angiogenesis plays a key role in cancer and rheumatic disease. However, the molecular system that underlies TNF-induced angiogenesis is largely unknown. Methods: We analyzed the gene expression changes stimulated by TNF in HUVEC over a time course using microarrays to reveal the molecular system underlying TNF-induced angiogenesis. Traditional k-means clustering analysis was performed to identify informative temporal gene expression patterns buried in the time course data. Functional enrichment analysis using DAVID was then performed for each cluster. The genes that belonged to informative clusters were then used as the input for gene network analysis using a Bayesian network and nonparametric regression method. Based on this TNF-induced gene network, we searched for sub-networks related to angiogenesis by integrating existing biological knowledge. Results: k-means clustering of the TNF stimulated time course microarray gene expression data, followed by functional enrichment analysis identified three biologically informative clusters related to apoptosis, cellular proliferation and angiogenesis. These three clusters included 648 genes in total, which were used to estimate dynamic Bayesian networks. Based on the estimated TNF-induced gene networks, we hypothesized that a sub-network including IL6 and IL8 inhibits apoptosis and promotes TNF-induced angiogenesis. More particularly, IL6 promotes TNF-induced angiogenesis by inducing NF-kappa B and IL8, which are strong cell growth factors. Conclusions: Computational gene network analysis revealed a novel molecular system that may play an important role in the TNF-induced angiogenesis seen in cancer and rheumatic disease. This analysis suggests that Bayesian network analysis linked to functional annotation may be a powerful tool to provide insight into disease.","The genes that belonged to informative clusters were then used as the input for gene network analysis using a Bayesian network and nonparametric regression method."
"Background: Co-expression measures are often used to define networks among genes. Mutual information (MI) is often used as a generalized correlation measure. It is not clear how much MI adds beyond standard (robust) correlation measures or regression model based association measures. Further, it is important to assess what transformations of these and other co-expression measures lead to biologically meaningful modules (clusters of genes). Results: We provide a comprehensive comparison between mutual information and several correlation measures in 8 empirical data sets and in simulations. We also study different approaches for transforming an adjacency matrix, e. g. using the topological overlap measure. Overall, we confirm close relationships between MI and correlation in all data sets which reflects the fact that most gene pairs satisfy linear or monotonic relationships. We discuss rare situations when the two measures disagree. We also compare correlation and MI based approaches when it comes to defining co-expression network modules. We show that a robust measure of correlation (the biweight midcorrelation transformed via the topological overlap transformation) leads to modules that are superior to MI based modules and maximal information coefficient (MIC) based modules in terms of gene ontology enrichment. We present a function that relates correlation to mutual information which can be used to approximate the mutual information from the corresponding correlation coefficient. We propose the use of polynomial or spline regression models as an alternative to MI for capturing non-linear relationships between quantitative variables. Conclusion: The biweight midcorrelation outperforms MI in terms of elucidating gene pairwise relationships. Coupled with the topological overlap matrix transformation, it often leads to more significantly enriched co-expression modules. Spline and polynomial networks form attractive alternatives to MI in case of non-linear relationships. Our results indicate that MI networks can safely be replaced by correlation networks when it comes to measuring co-expression relationships in stationary data.","It is not clear how much MI adds beyond standard (robust) correlation measures or regression model based association measures."
"Detect highlight event is an important step for semantic-based video retrieval. Hidden conditional random field (HCRF) is a discriminative model, which is effective in fusing observations for event inference. Mid-level semantics and their refinements are more robust than low-level visual features in event detection for learning models. To make full use of the contextual information, two aspects are taken into account during soccer video event detection. The first is parsing video sequences into event clips. The second is fusing the temporal transitions of the mid-level semantics of an event clip to determine the event type. In this study, HCRF is utilised to model the observations of mid-level semantics of an event clip for event detection. Comparisons are made with the dynamic Bayesian networks, hidden Markov model (HMM), enhanced HMM and conditional random field-based event detection approaches. Experimental results show the effectiveness of the proposed method.","Hidden conditional random field (HCRF) is a discriminative model, which is effective in fusing observations for event inference."
"Introduction: Occupational stress is a common phenomenon in our society, and generates problems for both workers' health and the functioning of organizations. Over past decades numerous studies have examined occupational stress from the perspective of gender, offering somewhat contradictory results. Some of them found no differences and others indicated that either men or women suffer from greater amounts of occupational stress. Method: The purpose of this study was to analyze gender differences in stress in situations that involve certain occupational demands. The data used were taken from a random sample population of 11,054 (5,917 men and 5,137 women) from the VI National Survey on Working Conditions (NSWC) which was conducted in Spain in 2007. To carry out this study, a probabilistic model was constructed using Bayesian networks, with the following variables related to task demands: working with tight deadlines, quick work, intellectually demanding work, complicated tasks, repetitive tasks, excessive work, and work demanding high attention levels. Results: The results of this study reveal that: the indicators studied significantly increased stress levels; women initially had higher stress levels than men; and when exposed to determined task demands, stress differences between genders tended to increase. Impact on Industry: Companies need to consider the gender of their workers when assigning tasks in high demand/stress jobs. (C) 2012 National Safety Council and Elsevier Ltd. All rights reserved.",""
"Bayesian networks (BNs) have attained widespread use in data analysis and decision making. Well-studied topics include efficient inference, evidence, propagation, Parameter learning from data for complete and incomplete data scenarios, expert elicitation for calibrating BN probabilities, and structure learning. It is common for the researcher to assume the structure of the BN or to glean the structure from expert elicitation or domain knowledge. In this scenario, the model may be calibrated through learning the parameters from relevant data. There is a lack of work on model diagnostics for fitted BNs; this is the contribution of this article. We key on the definition of (conditional) independence to develop a graphical diagnostic that indicates whether the conditional independence assumptions imposed, when one assumes the structure of the BN, are supported by the data. We develop the approach theoretically and describe a Monte Carlo method to generate uncertainty measures for the consistency of the data with conditional independence assumptions under the model structure. We describe how this theoretical information and the data are presented in a graphical diagnostic tool. We demonstrate the approach through data simulated from BNs under different conditional independence assumptions. We also apply the diagnostic to a real-world dataset. The results presented in this article show that this approach is most feasible for smaller BNs-this is not peculiar to the proposed diagnostic graphic, but rather is related to the general difficulty of combining large BNs with data in any manner (such as through parameter estimation). It is the authors' hope that this article helps highlight the need for more research into BN model diagnostics. This article has supplementary materials online.","Well-studied topics include efficient inference, evidence, propagation, Parameter learning from data for complete and incomplete data scenarios, expert elicitation for calibrating BN probabilities, and structure learning."
"Precise patterns of spatial and temporal gene expression are central to metazoan complexity and act as a driving force for embryonic development. While there has been substantial progress in dissecting and predicting cis-regulatory activity, our understanding of how information from multiple enhancer elements converge to regulate a gene's expression remains elusive. This is in large part due to the number of different biological processes involved in mediating regulation as well as limited availability of experimental measurements for many of them. Here, we used a Bayesian approach to model diverse experimental regulatory data, leading to accurate predictions of both spatial and temporal aspects of gene expression. We integrated whole-embryo information on transcription factor recruitment to multiple cis-regulatory modules, insulator binding and histone modification status in the vicinity of individual gene loci, at a genome-wide scale during Drosophila development. The model uses Bayesian networks to represent the relation between transcription factor occupancy and enhancer activity in specific tissues and stages. All parameters are optimized in an Expectation Maximization procedure providing a model capable of predicting tissue-and stage-specific activity of new, previously unassayed genes. Performing the optimization with subsets of input data demonstrated that neither enhancer occupancy nor chromatin state alone can explain all gene expression patterns, but taken together allow for accurate predictions of spatio-temporal activity. Model predictions were validated using the expression patterns of more than 600 genes recently made available by the BDGP consortium, demonstrating an average 15-fold enrichment of genes expressed in the predicted tissue over a naive model. We further validated the model by experimentally testing the expression of 20 predicted target genes of unknown expression, resulting in an accuracy of 95% for temporal predictions and 50% for spatial. While this is, to our knowledge, the first genome-wide approach to predict tissue-specific gene expression in metazoan development, our results suggest that integrative models of this type will become more prevalent in the future.",""
"Density evolution is often used to determine the performance of an ensemble of low-density parity-check (LDPC) codes under iterative message-passing algorithms. Conventional density evolution techniques over memoryless channels are based on the assumption that messages at iteration l are only a function of the messages at iteration l - 1 and possibly the channel output. This assumption is valid for many algorithms such as standard belief propagation (BP) and min-sum (MS) algorithms. However, there are other important iterative algorithms such as successive relaxation (SR) versions of BP and MS, and differential decoding with binary message passing (DD-BMP) algorithm of Mobini et al., for which this assumption is not valid. The reason is the introduction of memory in these algorithms. In this work, we propose a model for iterative decoding algorithms with memory which covers SR and DD-BMP algorithms as special cases. Based on this model, we derive a Bayesian network for iterative algorithms with memory over memoryless channels and use this representation to analyze the performance of the algorithms using density evolution. The density evolution technique is developed based on truncating the memory of the decoding process and approximating it with a finite order Markov process, and can be implemented efficiently. As an example, we apply our technique to analyze the performance of DD-BMP on regular LDPC code ensembles, and make a number of interesting observations with regard to the performance/complexity tradeoff of DD-BMP in comparison with BP and MS algorithms. The model presented in this paper is based on certain simplifying assumptions about the memory structure of iterative algorithms such as the existence of memory only at the output of variable nodes in the code's Tanner graph rather than at both outputs of variable and check nodes. The Bayesian network framework introduced here however, can still be used to analyze the more general scenarios.",""
"A method combining Department of Defense Architecture Framework (DODAF) 2.0 and Bayesian networks (BN) is introduced. This method can not only provide qualitative description for large systems using DODAF models, but also analyze mission reliability quantitatively based on Bayesian networks. The steps are given to build DODAF-BN models and a missile defense system (MDS) DODAF-BN model is built. Furthermore, the mission reliability of MDS is calculated and analyzed under several scenarios.",""
"Remotely sensed data is the main source of vegetation leaf area index (LAI) information on the regional to global scale. Many validation results have revealed that the accuracy of the retrieved LAI is often affected by the cloud cover of imagery, instrument problems, and inversion algorithms. Ground meteorological station data, characterized by relatively high accuracy and time continuity compared with remote sensing data, can provide complementary information to remote sensing observations. In this paper, we combine the potential advantages of both types of data in order to improve LAI retrievals in the Heihe River Basin, an arid and semi-arid area in northwest China where Moderate Resolution Imaging Spectroradiometer (MODIS) LAI values are significantly underestimated. A dynamic Bayesian network (DBN) is used to integrate these two data types for time series LAI estimation. Results show that the square of correlation coefficient between LAI values estimated by our DBN method (referred to as DBN LAI) and field measured LAI values is 0.76, with a root mean square error of 0.78. The DBN LAI are closer to field measurements than the MODIS LAI standard product values. Moreover, by introducing ground meteorological station data using a dynamic process model, DBN LAI show better temporal consistency than the MODIS LAI. It is concluded that the quality of LAI retrievals can be improved by combining remote sensing data and ground meteorological station data using a filtering inference algorithm in a DBN framework. More importantly, the study provides a basis and method for utilizing ground meteorological station network data to estimate land surface parameters on a regional scale. (c) 2012 Elsevier Inc. All rights reserved.","It is concluded that the quality of LAI retrievals can be improved by combining remote sensing data and ground meteorological station data using a filtering inference algorithm in a DBN framework."
"MOTIVATION: When analysing gene expression time series data, an often overlooked but crucial aspect of the model is that the regulatory network structure may change over time. Although some approaches have addressed this problem previously in the literature, many are not well suited to the sequential nature of the data. RESULTS: Here, we present a method that allows us to infer regulatory network structures that may vary between time points, using a set of hidden states that describe the network structure at a given time point. To model the distribution of the hidden states, we have applied the Hierarchical Dirichlet Process Hidden Markov Model, a non-parametric extension of the traditional Hidden Markov Model, which does not require us to fix the number of hidden states in advance. We apply our method to existing microarray expression data as well as demonstrating is efficacy on simulated test data.",""
"Stress is a major growing concern in our day and age adversely impacting both individuals and society. Stress research has a wide range of benefits from improving personal operations, learning, and increasing work productivity to benefiting society - making it an interesting and socially beneficial area of research. This survey reviews sensors that have been used to measure stress and investigates techniques for modelling stress. It discusses non-invasive and unobtrusive sensors for measuring computed stress, a term we coin in the paper. Sensors that do not impede everyday activities that could be used by those who would like to monitor stress levels on a regular basis (e.g. vehicle drivers, patients with illnesses linked to stress) is the focus of the discussion. Computational techniques have the capacity to determine optimal sensor fusion and automate data analysis for stress recognition and classification. Several computational techniques have been developed to model stress based on techniques such as Bayesian networks, artificial neural networks, and support vector machines, which this survey investigates. The survey concludes with a summary and provides possible directions for further computational stress research. (C) 2012 Elsevier Ireland Ltd. All rights reserved.","Computational techniques have the capacity to determine optimal sensor fusion and automate data analysis for stress recognition and classification."
"This paper presents a simple and novel curve fitting approach for generating simple gene regulatory subnetworks from time series gene expression data. Microarray experiments simultaneously generate massive data sets and help immensely in the large-scale study of gene expression patterns. Initial biclustering reduces the search space in the high-dimensional microarray data. The least-squares error between fitting of gene pairs is minimized to extract a set of gene-gene interactions, involving transcriptional regulation of genes. The higher error values are eliminated to retain only the strong interacting gene pairs in the resultant gene regulatory subnetwork. Next the algorithm is extended to a generalized framework to enhance its capability. The methodology takes care of the higher-order dependencies involving multiple genes co-regulating a single gene, while eliminating the need for user-defined parameters. It has been applied to the time-series Yeast data, and the experimental results biologically validated using standard databases and literature.",""
"We present an automated classification of stars exhibiting periodic, non-periodic and irregular light variations. The Hipparcos catalogue of unsolved variables is employed to complement the training set of periodic variables of Dubath et al. with irregular and non-periodic representatives, leading to 3881 sources in total which describe 24 variability types. The attributes employed to characterize light-curve features are selected according to their relevance for classification. Classifier models are produced with random forests and a multistage methodology based on Bayesian networks, achieving overall misclassification rates under 12 per cent. Both classifiers are applied to predict variability types for 6051 Hipparcos variables associated with uncertain or missing types in the literature.","We present an automated classification of stars exhibiting periodic, non-periodic and irregular light variations."
"An epidemiologic systems analysis of diarrhea in children in Pakistan is presented. Application of additive Bayesian network modeling to 20052006 data from the Pakistan Social and Living Standards Measurement Survey reveals the complexity of child diarrhea as a disease system. The key distinction between standard analytical approaches, such as multivariable regression, and Bayesian network analyses is that the latter attempt to not only identify statistically associated variables but also, additionally and empirically, separate these into those directly and indirectly dependent upon the outcome variable. Such discrimination is vastly more ambitious but has the potential to reveal far more about key features of complex disease systems. Additive Bayesian network analyses across 41 variables from the Pakistan Social and Living Standards Measurement Survey identified 182 direct dependencies but with only 3 variables: 1) access to a dry pit latrine (protective; odds ratio 0.67); 2) access to an atypical water source (protective; odds ratio 0.49); and 3) no formal garbage collection (unprotective; odds ratio 1.32), supported as directly dependent with the presence of diarrhea. All but 2 of the remaining variables were also, in turn, directly or indirectly dependent upon these 3 key variables. These results are contrasted with the use of a standard approach (multivariable regression).","The key distinction between standard analytical approaches, such as multivariable regression, and Bayesian network analyses is that the latter attempt to not only identify statistically associated variables but also, additionally and empirically, separate these into those directly and indirectly dependent upon the outcome variable."
"Causal structure learning algorithms construct Bayesian networks from observational data. Using non-interventional data, existing constraint-based algorithms may return I-equivalent partially directed acyclic graphs. However, these algorithms do not fully exploit the graphical properties of Bayesian networks, and require many redundant tests that reduce both speed and accuracy. In this paper, we introduce ideas to exploit such properties to increase the speed and accuracy of causal structure learning for multivariate normal data. In numerical experiments on five benchmarking networks our proposed algorithm was faster and more accurate than recently-developed algorithms. (c) 2012 Elsevier B.V. All rights reserved.",""
"Background The drug supply chain is a cross-disciplinary process involving numerous actors. This can create difficulties in attempts to successfully analyse and manage its execution. Objective To produce a tool allowing easy evaluation and optimisation of the hospital drug supply chain. Material and methods A supervised Bayesian network was built to model a hospital drug supply chain. Two learning patterns were used: literature and experimental data. The network was tested by using data separate from the data used for its construction. Two hundred scenarios were simulated to evaluate the impact of organisational modalities. Results The model estimates a need for 3.2 clinical pharmacists per 100 patients. The model gave an appropriate estimation of technician workforce of three hospital pharmacies and showed that one of them was understaffed. Simulations showed that a unit-dose drug distribution system and prescription analysis seem to have comparable impact on quality index, with a maximum increase of 28 for unit- dose drug distribution and 25 for prescription control (depending on control methods, the increase in quality varies between 31 and 89). In addition, changing from a global to a unit- dose distribution system results in an increase of 14% in the hospital pharmacy's payroll whereas changing from no prescription control to the daily control of 100% of prescriptions leads to a rise of 218% in the pharmacy's wage bill. Conclusions The Bayesian approach appears to be a valuable decision tool to gauge the influence of organisational changes on care quality.",""
"This paper presents an extension to the Conservative PC algorithm which is able to detect violations of adjacency faithfulness under causal sufficiency and triangle faithfulness. Violations can be characterized by pseudo-independent relations and equivalent edges, both generating a pattern of conditional independencies that cannot be modeled faithfully. Both cases lead to uncertainty about specific parts of the skeleton of the causal graph. These ambiguities are modeled by an f-pattern. We prove that our Adjacency Conservative PC algorithm is able to correctly learn the f-pattern. We argue that the solution also applies for the finite sample case if we accept that only strong edges can be identified. Experiments based on simulations and the ALARM benchmark model show that the rate of false edge removals is significantly reduced, at the expense of uncertainty on the skeleton and a higher sensitivity for accidental correlations. (C) 2012 Elsevier Inc. All rights reserved.",""
"To perform efficient inference in Bayesian networks by means of a Junction Tree method, the network graph needs to be triangulated. The quality of this triangulation largely determines the efficiency of the subsequent inference, but the triangulation problem is unfortunately NP-hard. It is common for existing methods to use the treewidth criterion for optimality of a triangulation. However, this criterion may lead to a somewhat harder inference problem than the total table size criterion. We therefore investigate new methods for depth-first search and best-first search for finding optimal total table size triangulations. The search methods are made faster by efficient dynamic maintenance of the cliques of a graph. This problem was investigated by Stix, and in this paper we derive a new simple method based on the Bron-Kerbosch algorithm that compares favourably to Stix' approach. The new approach is generic in the sense that it can be used with other algorithms than just Bron-Kerbosch. The algorithms for finding optimal triangulations are mainly supposed to be off-line methods, but they may form the basis for efficient any-time heuristics. Furthermore, the methods make it possible to evaluate the quality of heuristics precisely and allow us to discover parts of the search space that are most important to direct randomized sampling to. (C) 2012 Elsevier Inc. All rights reserved.","To perform efficient inference in Bayesian networks by means of a Junction Tree method, the network graph needs to be triangulated."
"A Recursive Probability Tree (RPT) is a data structure for representing the potentials involved in Probabilistic Graphical Models (PGMs). This structure is developed with the aim of capturing some types of independencies that cannot be represented with previous structures. This capability leads to improvements in memory space and computation time during inference. This paper describes a learning algorithm for building RPTs from probability distributions. The experimental analysis shows the proper behavior of the algorithm: it produces RPTs encoding good approximations of the original probability distributions. (C) 2012 Elsevier Inc. All rights reserved.","This capability leads to improvements in memory space and computation time during inference."
"Sensitivity analysis in hidden Markov models (HMMs) is usually performed by means of a perturbation analysis where a small change is applied to the model parameters, upon which the output of interest is re-computed. Recently it was shown that a simple mathematical function describes the relation between HMM parameters and an output probability of interest; this result was established by representing the HMM as a (dynamic) Bayesian network. To determine this sensitivity function, it was suggested to employ existing Bayesian network algorithms. Up till now, however, no special purpose algorithms for establishing sensitivity functions for HMMs existed. In this paper we discuss the drawbacks of computing HMM sensitivity functions, building only upon existing algorithms. We then present a new and efficient algorithm, which is specially tailored for determining sensitivity functions in HMMs. (C) 2012 Elsevier Inc. All rights reserved.",""
"We consider in this paper the robustness of decisions based on probabilistic thresholds. To this effect, we propose the same-decision probability as a query that can be used as a confidence measure for threshold-based decisions. More specifically, the same-decision probability is the probability that we would have made the same threshold-based decision, had we known the state of some hidden variables pertaining to our decision. We study a number of properties about the same-decision probability. First, we analyze its computational complexity. We then derive a bound on its value, which we can compute using a variable elimination algorithm that we propose. Finally, we consider decisions based on noisy sensors in particular, showing through examples that the same-decision probability can be used to reason about threshold-based decisions in a more refined way. (C) 2012 Elsevier Inc. All rights reserved.",""
"Financial exploitation of forests comprises an important part of man activity. There are efforts being made to conserve the sustainable exploitation while simultaneously avoiding degradation of the environment. One tool used in these efforts is the modeling of tree features, such as total tree height, sawn-timber tree height, merchantable tree height, and total or sawn-timber tree volume, which yields an estimate of the forest in finance recoverable goods. Sustainable forest management design must be supported by the adjustment of computational techniques. The purpose of this paper is to assess a reliable modeling approach for estimating individual tree heights for the maturity of trees for logging through determining the applicability of different types of neural network models and identifying a neural network procedure for accurate estimation of these variables. These models serve as an alternative to the traditional regression approach. All types of model estimations are evaluated and compared in this paper. Back Propagation Artificial Neural Network (BPANN), Cascade Correlation Artificial Neural Network (CCANN), and Generalized Regression Neural Network (GRNN) models are developed to estimate individual tree heights for the logging of mature trees, such as sawn-timber height and merchantable height. The results reported in this research suggest that the selected BPANN and CCANN models are reliable and demonstrate their adequacy and potential for estimating sawn-timber and merchantable tree height. The results also illustrate that the CCANN models are superior to the BPANN and GRNN models and lead to higher estimation accuracy. Moreover, the NN models were found to be superior to the tested nonlinear regression models. (C) 2012 Elsevier Inc. All rights reserved.","These models serve as an alternative to the traditional regression approach."
"Many of our decisions pertain to causal systems. Nevertheless, only recently has it been claimed that people use causal models when making judgments, decisions and predictions, and that causal Bayes nets allow us to formally describe these inferences. Experimental research has been limited to simple, artificial problems, which are unrepresentative of the complex dynamic systems we successfully deal with in everyday life. For instance, in social interactions, we can explain the actions of other's on the fly and we can generalize from limited observations to predict future actions and their consequences. Our main argument is that none of these inferences (i.e., induction, generalization, explanation, and prediction) can be achieved without causal reasoning. As a case in point we use the popular television series desperate housewives and show how causal Bayes nets are able to explain the inferences made in social contexts. Crucially, causal Bayes nets also allow us to understand why we can infer so much from so little when making sense of a protagonist's behavior.","Nevertheless, only recently has it been claimed that people use causal models when making judgments, decisions and predictions, and that causal Bayes nets allow us to formally describe these inferences."
"This paper presents a new game theoretic methodology for equitable waste load allocation in rivers utilizing fuzzy bi-matrix games, Non-dominated Sorting Genetic Algorithms II (NSGA-II), cooperative game theory, Bayesian Networks (BNs) and Probabilistic Support Vector Machines (PSVMs). In this methodology, at first, a trade-off curve between objectives, which are average treatment level of dischargers and fuzzy risk of low water quality, is obtained using NSGA-II. Then, the best non-dominated solution is selected using a non-zero-sum bi-matrix game with fuzzy goals. In the next step, to have an equitable waste load allocation, some possible coalitions among dischargers are formed and treatment costs are reallocated to discharges and side payments are calculated. To develop probabilistic rules for real-time waste load allocation, the proposed model is applied considering several scenarios of pollution loads and the results are used for training and testing BNs and PSVMs. The applicability and efficiency of the methodology are examined in a real-world case study of the Zarjub River in the northern part of Iran. The results show that the average relative errors of the proposed rules in estimating the treatment levels of dischargers are less than 5 %.",""
"The Bayesian network models of redundant systems including parallel system and voting system, taking account of common cause failure and imperfect coverage, are proposed. The Triple Modular Redundancy (TMR) and Double Dual Modular Redundancy (DDMR) control systems for subsea Blowout Preventer (BOP) are presented. By applying the proposed Bayesian network models, the reliability of subsea BOP control systems are evaluated at any given time, and the difference between posterior and prior probabilities of each single component given the system failure is obtained. The effects of coverage factor of redundant subsystem and failure rate of single component on reliability of systems are also researched. The results show that the DDMR control system has a little higher reliability than TMR system. To improve the reliability of subsea BOP control systems, the component failure rates of Ethernet switch (ES), programmable logic controller (PLC) and personal computer (PC) should be reduced for TMR system, whereas the failure rates of ES and PC should be reduced for DDMR system. The recovery mechanism of PLC, PC and ES subsystems, and PC and ES subsystems should be paid more attention for TMR and DDMR control systems, respectively. (C) 2012 Elsevier Ltd. All rights reserved.",""
"In the forensic examination of DNA mixtures, the question of how to set the total number of contributors (N) presents a topic of ongoing interest. Part of the discussion gravitates around issues of bias, in particular when assessments of the number of contributors are not made prior to considering the genotypic configuration of potential donors. Further complication may stem from the observation that, in some cases, there may be numbers of contributors that are incompatible with the set of alleles seen in the profile of a mixed crime stain, given the genotype of a potential contributor. In such situations, procedures that take a single and fixed number contributors as their output can lead to inferential impasses. Assessing the number of contributors within a probabilistic framework can help avoiding such complication. Using elements of decision theory, this paper analyses two strategies for inference on the number of contributors. One procedure is deterministic and focuses on the minimum number of contributors required to 'explain' an observed set of alleles. The other procedure is probabilistic using Bayes' theorem and provides a probability distribution for a set of numbers of contributors, based on the set of observed alleles as well as their respective rates of occurrence. The discussion concentrates on mixed stains of varying quality (i.e., different numbers of loci for which genotyping information is available). A so-called qualitative interpretation is pursued since quantitative information such as peak area and height data are not taken into account. The competing procedures are compared using a standard scoring rule that penalizes the degree of divergence between a given agreed value for N, that is the number of contributors, and the actual value taken by N. Using only modest assumptions and a discussion with reference to a casework example, this paper reports on analyses using simulation techniques and graphical models (i.e., Bayesian networks) to point out that setting the number of contributors to a mixed crime stain in probabilistic terms is, for the conditions assumed in this study, preferable to a decision policy that uses categoric assumptions about N. (C) 2012 Elsevier Ireland Ltd. All rights reserved.","Using elements of decision theory, this paper analyses two strategies for inference on the number of contributors."
"Evaluation of series of PCR experiments referring to the same evidence is not infrequent in a forensic casework. This situation is met when 'series of results in mixture' (EPGs produced by reiterating PCR experiments over the same DNA mixture extract) have to be interpreted or when 'potentially related traces' (mixtures that can have contributors in common) require a combined interpretation. In these cases, there can be uncertainty on the genotype assignment, since: (a) more than one genotype combination fall under the same peak profile; (b) PCR preferential amplification alters pre-PCR allelic proportions; (c) other, more unpredictable technical problems (dropouts/dropins, etc.) take place. The uncertainty in the genotype assignment is in most cases addressed by empirical methods (selection of just one particular profile; extraction of consensual or composite profiles) that disregard part of the evidence. Genotype assignment should conversely take advantage from a joint Bayesian analysis (JBA) of all STRs peak areas generated at each experiment. This is the typical case of Bayesian analysis in which adoption of object-oriented Bayesian networks (OOBNs) could be highly helpful. Starting from experimentally designed mixtures, we created typical examples of 'series of results in mixture' of 'potentially related traces'. JBA was some administered to the whole peak area evidence, by specifically tailored OOBNs models, which enabled genotype assignment reflecting all the available evidence. Examples of a residual ambiguity in the genotype assignment came to light at assumed genotypes with partially overlapping alleles (for example: AB + AC -> ABC). In the 'series of results in mixture', this uncertainty was in part refractory to the joint evaluation. Ambiguity was conversely dissipated at the 'potentially related' trace example, where the ABC allelic scheme at the first trace was interpreted together with other unambiguous combinations (ABCD; AB) at the related trace. We emphasize the need to carry out extensive, blind sensitivity tests specifically addressing the residual ambiguity that arises from overlapping results mixed at various quantitative ratios. (C) 2012 Elsevier Ireland Ltd. All rights reserved.",""
"Context: When adapting a system to new usage patterns, processes or technologies, it is necessary to foresee the implications of the architectural design changes on system quality. Examination of quality outcomes through implementation of the different architectural design alternatives is often unfeasible. We have developed a method called PREDIQT with the aim to facilitate model-based prediction of impacts of architectural design changes on system quality. A recent case study indicated feasibility of the PREDIQT method when applied on a real-life industrial system. The promising results encouraged further and more structured evaluation of PREDIQT. Objective: This paper reports on the experiences from applying the PREDIQT method in a second and more recent case study - on a real-life industrial system from another domain and with different system characteristics, as compared to the previous case study. The objective was to evaluate the method in a fully realistic setting and with respect to carefully defined criteria. Method: The case study conducted the first two phases of PREDIQT in their entirety, while the last (third) phase was partially covered. In addition, the method was assessed through a thought experiment-based evaluation of predictions and a postmortem review. All prediction models were developed during the analysis and the entire target system was analyzed in a fully realistic setting. Results: The evaluation argues that the prediction models are sufficiently expressive and comprehensible. It is furthermore argued that PREDIQT: facilitates predictions such that informed decisions can be made; is cost-effective; and facilitates knowledge management. Conclusion: The experiences and results obtained indicate that the PREDIQT method can be carried out with limited resources, on a real-life system, and result in useful predictions. Furthermore, the observations indicate that the method, particularly its process, facilitates understanding of the system architecture and its quality characteristics, and contributes to structured knowledge management. (C) 2012 Elsevier B.V. All rights reserved.",""
"We comment on a recent article by Newton (Environ. Model. Softw. (2010), 25, 15-23), which proposed a method, based on a Bayesian belief networks, for classifying the threat status of species under the IUCN Red List Categories and Criteria, and compared this method to an earlier one that we had developed that is based on fuzzy logic. There are three types of differences between the results of the two methods, the most consequential of which is different threat status categories assigned to some species for which the input data were uncertain. We demonstrate that the results obtained using the fuzzy logic approach are consistent with IUCN Red List criteria and guidelines. The application of Bayesian Networks to the IUCN Red List criteria to assist uncertain risk assessments may yet have merit. However, in order to be consistent with IUCN Red List assessments, applications of Bayesian approaches to actual Red List assessments would need an explicit and objective method for assigning likelihoods based on uncertain data. (C) 2012 Elsevier Ltd. All rights reserved.",""
"Continuous time Bayesian networks are used to diagnose cardiogenic heart failure and to anticipate its likely evolution. The proposed model overcomes the strong modeling and computational limitations of dynamic Bayesian networks. It consists of both unobservable physiological variables, and clinically and instrumentally observable events which might support diagnosis like myocardial infarction and the future occurrence of shock. Three case studies related to cardiogenic heart failure are presented. The model predicts the occurrence of complicating diseases and the persistence of heart failure according to variations of the evidence gathered from the patient. Predictions are shown to be consistent with current pathophysiological medical understanding of clinical pictures.",""
"Statistical-relational learning combines logical syntax with probabilistic methods. Markov Logic Networks (MLNs) are a prominent model class that generalizes both first-order logic and undirected graphical models (Markov networks). The qualitative component of an MLN is a set of clauses and the quantitative component is a set of clause weights. Generative MLNs model the joint distribution of relationships and attributes. A state-of-the-art structure learning method is the moralization approach: learn a set of directed Horn clauses, then convert them to conjunctions to obtain MLN clauses. The directed clauses are learned using Bayes net methods. The moralization approach takes advantage of the high-quality inference algorithms for MLNs and their ability to handle cyclic dependencies. A weakness of moralization is that it leads to an unnecessarily large number of clauses. In this paper we show that using decision trees to represent conditional probabilities in the Bayes net is an effective remedy that leads to much more compact MLN structures. In experiments on benchmark datasets, the decision trees reduce the number of clauses in the moralized MLN by a factor of 5-25, depending on the dataset. The accuracy of predictions is competitive with the models obtained by standard moralization, and in many cases superior.","The moralization approach takes advantage of the high-quality inference algorithms for MLNs and their ability to handle cyclic dependencies."
"Recently, there has been an increasing interest in generative models that represent probabilistic patterns over both links and attributes. A common characteristic of relational data is that the value of a predicate often depends on values of the same predicate for related entities. For directed graphical models, such recursive dependencies lead to cycles, which violates the acyclicity constraint of Bayes nets. In this paper we present a new approach to learning directed relational models which utilizes two key concepts: a pseudo likelihood measure that is well defined for recursive dependencies, and the notion of stratification from logic programming. An issue for modelling recursive dependencies with Bayes nets are redundant edges that increase the complexity of learning. We propose a new normal form format that removes the redundancy, and prove that assuming stratification, the normal form constraints involve no loss of modelling power. Empirical evaluation compares our approach to learning recursive dependencies with undirected models (Markov Logic Networks). The Bayes net approach is orders of magnitude faster, and learns more recursive dependencies, which lead to more accurate predictions.",""
"Background: HIV-associated dementia (HAD) is the most common dementia type in young adults less than 40 years of age. Although the neurotoxins, oxidative/metabolic stress and impaired activity of neurotrophic factors are believed to be underlying reasons for the development of HAD, the genomic basis, which ultimately defines the virus-host interaction and leads to neurologic manifestation of HIV disease is lacking. Therefore, identifying HIV fingerprints on the host gene machinery and its regulation by microRNA holds a great promise and potential for improving our understanding of HAD pathogenesis, its diagnosis and therapy. Results: A parallel profiling of mRNA and miRNA of the frontal cortex autopsies from HIV positive patients with and without dementia was performed using Illumina Human-6 BeadChip and Affymetrix version 1.0 miRNA array, respectively. The gene ontology and pathway analysis of the two data sets showed high concordance between miRNA and mRNAs, revealing significant interference with the host axon guidance and its downstream signalling pathways in HAD brains. Moreover, the differentially expressed (DE) miRNAs identified in this study, in particular miR-137, 153 and 218, based on which most correlations were built cumulatively targeted neurodegeneration related pathways, implying their future potential in diagnosis, prognosis and possible therapies for HIV-mediated and possibly other neurodegenerative diseases. Furthermore, this relationship between DE miRNAs and DE mRNAs was also reflected in correlation analysis using Bayesian networks by splitting-averaging strategy (SA-BNs), which revealed 195 statistically significant correlated miRNA-mRNA pairs according to Pearson's correlation test (P<0.05). Conclusions: Our study provides the first evidence on unambiguous support for intrinsic functional relationship between mRNA and miRNA in the context of HIV-mediated neurodegeneration, which shows that neurologic manifestation in HIV patients possibly occurs through the interference with the host axon guidance and its downstream signalling pathways. These data provide an excellent avenue for the development of new generation of diagnostic/prognostic biomarkers and therapeutic intervention strategies for HIV-associated neurodegeneration.",""
"Background: The compound LY303511 (LY30) has been proven to induce production of ROS and to sensitize cancer cells to TRAIL-induced apoptosis, but the mechanisms and mediators of LY30-induced effects are potentially complex. Bayesian networks are a modelling technique for making probabilistic inferences about complex networks of uncertain causality. Methods: Fluorescent indicators for ROS, reactive nitrogen species (RNS), and free calcium were measured in time-series after LY30 treatment. This \"correlative\" dataset was used as input for Bayesian modelling to predict the causal dependencies among the measured species. Predictions were compared against a separate \"causal\" dataset, in which cells had been treated with FeTPPS to scavenge peroxynitrite. EGTA-am to chelate calcium, and Tiron to scavenge O-2(center dot-). Finally, cell viability measurements were integrated into an extended model of LY30 effects. Results: LY30 treatment caused a rapid increase of ROS (measured by DCFDA) as well as a significant increase in RNS and calcium. Bayesian modelling predicted that Ca2+ was a partial cause of the ROS induced by short incubations with LY30, and that RNS was strongly responsible for the ROS induced by long incubations with LY30. Validation experiments confirmed the predicted roles of RNS and calcium, and also demonstrated a causal role for O-2(center dot-). In cell viability experiments, the additive effects of calcium and peroxynitrite were responsible for 90% of LY30-mediated sensitization to TRAIL-induced apoptosis. Conclusions: We conclude that LY30 induces interdependent pathways of reactive species and stress signalling, with peroxynitrite and calcium contributing most significantly to apoptosis sensitization. (C) 2012 Elsevier Inc. All rights reserved.","Bayesian networks are a modelling technique for making probabilistic inferences about complex networks of uncertain causality."
"Hierarchical generative models, such as Bayesian networks, and belief propagation have been shown to provide a theoretical framework that can account for perceptual processes, including feedforward recognition and feedback modulation. The framework explains both psychophysical and physiological experimental data and maps well onto the hierarchical distributed cortical anatomy. However, the complexity required to model cortical processes makes inference, even using approximate methods, very computationally expensive. Thus, existing object perception models based on this approach are typically limited to tree-structured networks with no loops, use small toy examples or fail to account for certain perceptual aspects such as invariance to transformations or feedback reconstruction. In this study we develop a Bayesian network with an architecture similar to that of HMAX, a biologically-inspired hierarchical model of object recognition, and use loopy belief propagation to approximate the model operations (selectivity and invariance). Crucially, the resulting Bayesian network extends the functionality of HMAX by including top-down recursive feedback. Thus, the proposed model not only achieves successful feedforward recognition invariant to noise, occlusions, and changes in position and size, but is also able to reproduce modulatory effects such as illusory contour completion and attention. Our novel and rigorous methodology covers key aspects such as learning using a layerwise greedy algorithm, combining feedback information from multiple parents and reducing the number of operations required. Overall, this work extends an established model of object recognition to include high-level feedback modulation, based on state-of-the-art probabilistic approaches. The methodology employed, consistent with evidence from the visual cortex, can be potentially generalized to build models of hierarchical perceptual organization that include top-down and bottom-up interactions, for example, in other sensory modalities.","However, the complexity required to model cortical processes makes inference, even using approximate methods, very computationally expensive."
"To learn about the progression of a complex disease, it is necessary to understand the physiology and function of many genes operating together in distinct interactions as a system. In order to significantly advance our understanding of the function of a system, we need to learn the causal relationships among its modeled genes. To this end, it is desirable to compare experiments of the system under complete interventions of some genes, e. g., knock-out of some genes, with experiments of the system without interventions. However, it is expensive and difficult (if not impossible) to conduct wet lab experiments of complete interventions of genes in animal models, e. g., a mouse model. Thus, it will be helpful if we can discover promising causal relationships among genes with observational data alone in order to identify promising genes to perturb in the system that can later be verified in wet laboratories. While causal Bayesian networks have been actively used in discovering gene pathways, most of the algorithms that discover pairwise causal relationships from observational data alone identify only a small number of significant pairwise causal relationships, even with a large dataset. In this article, we introduce new causal discovery algorithms-the Equivalence Local Implicit latent variable scoring Method (EquLIM) and EquLIM with Markov chain Monte Carlo search algorithm (EquLIM-MCMC)-that identify promising causal relationships even with a small observational dataset.",""
"Building Bayesian networks (BNs) for problems of risk attributable to natural hazards is a complex problem. Risk analysis makes use of domain experts' knowledge, which is an essential element of input for the analysis. This knowledge is acquired from fragmented sources of expertise. The effectiveness of the analysis depends on the integration of the knowledge and data, which is of vital significance to decision makers. Setting up the flow of information for the acquired knowledge, which is from different fragmented sources, is a difficult task. Moreover, the process of dealing with the fragmented knowledge of domain experts may be equally complicated because of the large number of variables involved in the case of problems of risk attributable to natural hazards. To this end, a new approach using graph-theoretic techniques is proposed in this paper for integrating experts' knowledge and data for setting up the flow of information for building BNs, so that it can be used in risk analysis. To demonstrate the approach, a case study on windstorm-induced damage of a roof structure is considered. DOI:10.1061/(ASCE)NH.1527-6996.0000076. (C) 2012 American Society of Civil Engineers.",""
"Inferring the combinatorial regulatory code of transcription factors (TFs) from genome-wide TF binding profiles is challenging. A major reason is that TF binding profiles significantly overlap and are therefore highly correlated. Clustered occurrence of multiple TFs at genomic sites may arise from chromatin accessibility and local cooperation between TFs, or binding sites may simply appear clustered if the profiles are generated from diverse cell populations. Overlaps in TF binding profiles may also result from measurements taken at closely related time intervals. It is thus of great interest to distinguish TFs that directly regulate gene expression from those that are indirectly associated with gene expression. Graphical models, in particular Bayesian networks, provide a powerful mathematical framework to infer different types of dependencies. However, existing methods do not perform well when the features (here: TF binding profiles) are highly correlated, when their association with the biological outcome is weak, and when the sample size is small. Here, we develop a novel computational method, the Neighbourhood Consistent PC (NCPC) algorithms, which deal with these scenarios much more effectively than existing methods do. We further present a novel graphical representation, the Direct Dependence Graph (DDGraph), to better display the complex interactions among variables. NCPC and DDGraph can also be applied to other problems involving highly correlated biological features. Both methods are implemented in the R package ddgraph, available as part of Bioconductor (http://bioconductor.org/packages/2.11/bioc/html/ddgraph.html). Applied to real data, our method identified TFs that specify different classes of cis-regulatory modules (CRMs) in Drosophila mesoderm differentiation. Our analysis also found depletion of the early transcription factor Twist binding at the CRMs regulating expression in visceral and somatic muscle cells at later stages, which suggests a CRM-specific repression mechanism that so far has not been characterised for this class of mesodermal CRMs.",""
"It is well known that for long-duration space missions, there is growing need for efficient utilization of telemetry data to enhance diagnostic performance and assist the less-experienced personnel in performing monitoring and diagnosis tasks. To address this need, we have, recently, developed a systematic and transparent fault diagnosis methodology within a hierarchical fault diagnosis framework for satellites formation flight. We developed our proposed hierarchical decomposition framework through a novel Bayesian network-based model, namely component dependence model (CDM). In this paper, we investigate the verification and validation of the CDM for fault diagnosis in satellites formation flight. We propose and develop a sensitivity analysis to verify the CDM by taking advantage of our systematic CDM development methodology. The proposed verification method satisfies the unique requirement of identifying CDM sensitivity when diagnostic performances of the algorithms that are deployed at one or more nodes of the CDM change. This implies that our verification approach and analysis are different from traditional sensitivity analysis that uses proportional scaling which is not applicable to the CDM methodology. Furthermore, in such analysis, a change in the model parameters under consideration is, typically, due to a change in the subjective judgment of an expert whose opinion is used in model development as opposed to the changes due to diagnostic performance variations. We demonstrate the proposed verification approach by using synthetic formation flight data, and show that our CDM development method does not lead to a fault diagnosis model that is sensitive to small variation in its parameters.",""
"Multiple sclerosis is an autoimmune disorder of the central nervous system and potentially the most common cause of neurological disability in young adults. The clinical disease course is highly variable and different multiple sclerosis subtypes can be defined depending on the progression of the severity of the disease. In the early stages, the disease subtype is unknown, and there is no information about how the severity is going to evolve. As there are different treatment options available depending on the progression of the disease, early identification has become highly relevant. Thus, given a new patient, it is important to diagnose the disease subtype. Another relevant information to predict is the expected time to reach a severity level indicating that assistance for walking is required. Given that we have to predict two correlated class variables: disease subtype and time to reach certain severity level, we use multidimensional Bayesian network classifiers because they can model and exploit the relations among both variables. Besides, the obtained models can be validated by the physicians using their expert knowledge due to the interpretability of Bayesian networks. The learning of the classifiers is made by means of a novel multi-objective approach which tries to maximize the accuracy of both class variables simultaneously. The application of the methodology proposed in this paper can help a physician to identify the expected progression of the disease and to plan the most suitable treatment.","Given that we have to predict two correlated class variables: disease subtype and time to reach certain severity level, we use multidimensional Bayesian network classifiers because they can model and exploit the relations among both variables."
"In this paper a Bayesian Networks-based solution for dialogue modelling is presented. This solution is combined with carefully designed contextual information handling strategies. With the purpose of validating these solutions, and introducing a spoken dialogue system for controlling a Hi-Fi audio system as the selected prototype, a real-user evaluation has been conducted. Two different versions of the prototype are compared. Each version corresponds to a different implementation of the algorithm for the management of the actuation order, the algorithm for deciding the proper order to carry out the actions required by the user. The evaluation is carried out in terms of a battery of both subjective and objective metrics collected from speakers interacting with the Hi-Fi audio box through predefined scenarios. Defined metrics have been specifically adapted to measure: first, the usefulness and the actual relevance of the proposed solutions, and, secondly, their joint performance through their intelligent combination mainly measured as the level achieved with regard to the user satisfaction. A thorough and comprehensive study of the main differences between both approaches is presented. Two-way analysis of variance (ANOVA) tests are also included to measure the effects of both: the system used and the type of scenario factors, simultaneously. Finally, the effect of bringing this flexibility, robustness and naturalness into our home dialogue system is also analyzed through the results obtained. These results show that the intelligence of our speech interface has been well perceived, highlighting its excellent ease of use and its good acceptance by users, therefore validating the approached dialogue management solutions and demonstrating that a more natural, flexible and robust dialogue is possible thanks to them. (C) 2012 British Informatics Society Limited. All rights reserved.",""
"Entrepreneurship research is receiving increasing attention in our context, as entrepreneurs are key social agents involved in economic development. We compare the success of the dichotomic logistic regression model and the Bayes simple classifier to predict entrepreneurship, after manipulating the percentage of missing data and the level of categorization in predictors. A sample of undergraduate university students (N = 1230) completed five scales (motivation, attitude towards business creation, obstacles, deficiencies, and training needs) and we found that each of them predicted different aspects of the tendency to business creation. Additionally, our results show that the receiver operating characteristic (ROC) curve is affected by the rate of missing data in both techniques, but logistic regression seems to be more vulnerable when faced with missing data, whereas Bayes nets underperform slightly when categorization has been manipulated. Our study sheds light on the potential entrepreneur profile and we propose to use Bayesian networks as an additional alternative to overcome the weaknesses of logistic regression when missing data are present in applied research.","We compare the success of the dichotomic logistic regression model and the Bayes simple classifier to predict entrepreneurship, after manipulating the percentage of missing data and the level of categorization in predictors."
"We present a method for synthesizing 3D object arrangements from examples. Given a few user-provided examples, our system can synthesize a diverse set of plausible new scenes by learning from a larger scene database. We rely on three novel contributions. First, we introduce a probabilistic model for scenes based on Bayesian networks and Gaussian mixtures that can be trained from a small number of input examples. Second, we develop a clustering algorithm that groups objects occurring in a database of scenes according to their local scene neighborhoods. These contextual categories allow the synthesis process to treat a wider variety of objects as interchangeable. Third, we train our probabilistic model on a mix of user-provided examples and relevant scenes retrieved from the database. This mixed model learning process can be controlled to introduce additional variety into the synthesized scenes. We evaluate our algorithm through qualitative results and a perceptual study in which participants judged synthesized scenes to be highly plausible, as compared to hand-created scenes.",""
"Rational decision making in land use planning and licensing of H-2 infrastructure surrounded by other industrial activities and by population should take account of individual and societal risks. QRA software packages produce a risk matrix of potential consequences versus event probabilities without indicating uncertainty, and results are therefore shrouded in ambiguity. Due to the 'black-box' effect of a package, the calculations also lack transparency. Bayesian Belief Network (BBN) software modeling cause-consequence chains allow easy inspection of intermediate results and sensitivity tracking, and it can take account of data distributions instead of point values. For support of decision making, risk analysts should in addition determine the utilities of decision alternatives. Utilities describe desirability of benefits on a single scale. Rationally weighing risks versus benefits results in more transparent and defendable decisions. Example risk analyses of two types of refueling stations and three hydrogen supply transportation types applying BBNs are worked out. Copyright (C) 2012, Hydrogen Energy Publications, LLC. Published by Elsevier Ltd. All rights reserved.",""
"A multivariate statistical technique was applied to the design of an orally disintegrating tablet and to clarify the causal correlation among variables of the manufacturing process and pharmaceutical responses. Orally disintegrating tablets (ODTs) composed mainly of mannitol were prepared via the wet-granulation method using crystal transition from the delta to the beta form of mannitol. Process parameters (water amounts (X-1), kneading time (X-2), compression force (X-3), and amounts of magnesium stearate (X-4)) were optimized using a nonlinear response surface method (RSM) incorporating a thin plate spline interpolation (RSM-S). The results of a verification study revealed that the experimental responses, such as tensile strength and disintegration time, coincided well with the predictions. A latent structure analysis of the pharmaceutical formulations of the tablet performed using a Bayesian network led to the clear visualization of a causal connection among variables of the manufacturing process and tablet characteristics. The quantity of beta-mannitol in the granules (Q(beta)) was affected by X-2 and influenced all granule properties. The specific surface area of the granules was affected by X-1 and Q(beta) and had an effect on all tablet characteristics. Moreover, the causal relationships among the variables were clarified by inferring conditional probability distributions. These techniques provide a better understanding of the complicated latent structure among variables of the manufacturing process and tablet characteristics.",""
"We present a cosmography analysis of the local Universe based on the recently released Two-Micron All-Sky Redshift Survey catalogue. Our method is based on a Bayesian Networks Machine Learning algorithm (the Kigen-code) which self-consistently samples the initial density fluctuations compatible with the observed galaxy distribution and a structure formation model given by second-order Lagrangian perturbation theory (2LPT). From the initial conditions we obtain an ensemble of reconstructed density and peculiar velocity fields which characterize the local cosmic structure with high accuracy unveiling non-linear structures like filaments and voids in detail. Coherent redshift-space distortions are consistently corrected within 2LPT. From the ensemble of cross-correlations between the reconstructions and the galaxy field and the variance of the recovered density fields, we find that our method is extremely accurate up to k similar to 1 h Mpc(-1) and still yields reliable results down to scales of about 34 h(-1) Mpc. The motion of the Local Group we obtain within similar to 80 h(-1) Mpc (v(LG) = 522 +/- 86 km s(-1), l(LG) = 291 degrees +/- 16 degrees, b(LG) = 34 degrees +/- 8 degrees) is in good agreement with measurements derived from the cosmic microwave background and from direct observations of peculiar motions and is consistent with the predictions of Lambda CDM.",""
"Motivation: Protein signaling networks play a key role in cellular function, and their dysregulation is central to many diseases, including cancer. To shed light on signaling network topology in specific contexts, such as cancer, requires interrogation of multiple proteins through time and statistical approaches to make inferences regarding network structure. Results: In this study, we use dynamic Bayesian networks to make inferences regarding network structure and thereby generate testable hypotheses. We incorporate existing biology using informative network priors, weighted objectively by an empirical Bayes approach, and exploit a connection between variable selection and network inference to enable exact calculation of posterior probabilities of interest. The approach is computationally efficient and essentially free of user-set tuning parameters. Results on data where the true, underlying network is known place the approach favorably relative to existing approaches. We apply these methods to reverse-phase protein array time-course data from a breast cancer cell line (MDA-MB-468) to predict signaling links that we independently validate using targeted inhibition. The methods proposed offer a general approach by which to elucidate molecular networks specific to biological context, including, but not limited to, human cancers.","To shed light on signaling network topology in specific contexts, such as cancer, requires interrogation of multiple proteins through time and statistical approaches to make inferences regarding network structure."
"Syndrome differentiation is the character of Chinese medicine (CM). Disease differentiation is the principle of Western medicine (WM). Identifying basic syndromes feature and structure of disease of WM is an important avenue for prevention and treatment of integrated Chinese and Western medicine. The idea here is first to divide all patients suffering from a disease of WM into several groups in the light of the stage of the disease, and secondly to identify basic syndromes feature in a distinct stage, and finally to achieve the purpose of syndrome differentiation. Syndrome differentiation is simply taken as a classifier that classifies patients into distinct classes primarily based on overall observation of their symptoms. Previous clustering methods are unable to cope with the complexity of CM. We therefore show a new multi-dimensional clustering method in the form of general latent structure (GLS) model, which is a suitable statistical learning technique of latent class analysis. In this paper, we learn an optimal GLS model which reflects much better model quality compared with other latent class models from the osteoporosis patient of community women (OPCW) real data including 40-65 year-old women whose bone mineral density (BMD) is less than mean-2.0 standard deviation (M-2.0SD). Further, we illustrate a case analysis of statistical identification of CM syndromes feature and structure of OPCW from qualitative and quantitative contents through the GLS model. Our analysis has discovered natural clusters and structures that correspond well to CM basic syndrome and factors of osteoporosis patients (OP). The GLS model suggests the possibility of establishing objective and quantitative diagnosis standards for syndrome differentiation on OPCW. Hence, for the future it can provide a reference for the similar study from the perspective of a combination of disease differentiation and syndrome differentiation.","Syndrome differentiation is simply taken as a classifier that classifies patients into distinct classes primarily based on overall observation of their symptoms."
"Automatic generation control (AGC) is one of the important control problems in electric power system design and operation, and is becoming more significant today because of increasing renewable energy sources such as wind farms. The power fluctuation caused by a high penetration of wind farms negatively contributes to the power imbalance and frequency deviation. In this paper, a new intelligent agent-based control scheme, using Bayesian networks (BNs), is addressed to design AGC system in a multiarea power system. Model independence and flexibility in specifying the control objectives identify the proposed approach as an attractive solution for AGC design in a real-world power system. The BN also provides a robust probabilistic method of reasoning under uncertainty, and moreover, using multiagent structure in the proposed control framework realizes parallel computation and a high degree of scalability. The proposed control scheme is examined on the 10-machine New England test power system. An experimental real-time implementation is also performed on the aggregated model of West Japan power system.",""
"Credal networks relax the precise probability requirement of Bayesian networks, enabling a richer representation of uncertainty in the form of closed convex sets of probability measures. The increase in expressiveness comes at the expense of higher computational costs. In this paper, we present a new variable elimination algorithm for exactly computing posterior inferences in extensively specified credal networks, which is empirically shown to outperform a state-of-the-art algorithm. The algorithm is then turned into a provably good approximation scheme, that is, a procedure that for any input is guaranteed to return a solution not worse than the optimum by a given factor. Remarkably, we show that when the networks have bounded treewidth and bounded number of states per variable the approximation algorithm runs in time polynomial in the input size and in the inverse of the error factor, thus being the first known fully polynomial-time approximation scheme for inference in credal networks. (C) 2012 Elsevier Inc. All rights reserved.","In this paper, we present a new variable elimination algorithm for exactly computing posterior inferences in extensively specified credal networks, which is empirically shown to outperform a state-of-the-art algorithm."
"Integrated interdisciplinary modeling techniques, providing reliable and accurate estimates for wave characteristics, have gained attention in recent years. With the ability to express knowledge in a rule-based form, the Rough Set Theory (RST) has been successfully employed in many fields. However the application of RST has not been investigated in wave height (WH) prediction. In this paper, the RST is applied to Lake Superior in North America to find some simple rules, called decision rules, for WH prediction. Decision rules are derived by expressing WH as functions of wind data gathered by the National Data Buoy Center (NDBC). Comparing results of RST with results of other soft computing techniques such as Support Vector Machines (SVMs), Bayesian Networks (BNs), Artificial Neural Networks (ANNs) and Adaptive Neuro-Fuzzy Inference System (ANFIS) indicates that the RST outperforms other soft computing techniques in WH prediction and provide some simple decision rules which can be accurately used by decision makers and engineers. (C) 2012 Elsevier Ltd. All rights reserved.","Comparing results of RST with results of other soft computing techniques such as Support Vector Machines (SVMs), Bayesian Networks (BNs), Artificial Neural Networks (ANNs) and Adaptive Neuro-Fuzzy Inference System (ANFIS) indicates that the RST outperforms other soft computing techniques in WH prediction and provide some simple decision rules which can be accurately used by decision makers and engineers."
"We propose an approach to analyze and synthesize a set of human facial and vocal expressions, and then use the classified expressions to decide the robot's response in a human-robot-interaction. During a human-to-human conversation, a person senses the interlocutor's face and voice, perceives her/his emotional expressions, and processes this information in order to decide which response to give. Moreover, observed emotions are taken into account and the response may be aggressive, funny (henceforth meaning humorous) or just neutral according to not only the observed emotions, but also the personality of the person. The purpose of our proposed structure is to endow robots with the capability to model human emotions, and thus several subproblems need to be solved: feature extraction, classification, decision and synthesis. In the proposed approach we integrate two classifiers for emotion recognition from audio and video, and then use a new method for fusion with the social behavior profile. To keep the person engaged in the interaction, after each iterance of analysis, the robot synthesizes human voice with both lips synchronization and facial expressions. The social behavior profile conducts the personality of the robot. The structure and work flow of the synthesis and decision are addressed, and the Bayesian networks are discussed. We also studied how to analyze and synthesize the emotion from the facial expression and vocal expression. A new probabilistic structure that enables a higher level of interaction between a human and a robot is proposed.","We propose an approach to analyze and synthesize a set of human facial and vocal expressions, and then use the classified expressions to decide the robot's response in a human-robot-interaction."
"The Tangjiashan landslide dam was formed during the Ms8.0 Wenchuan earthquake in 2008 and posed high risks to 1.2 million people downstream the dam. A human risk analysis model (HURAM) reported in the companion paper is applied to evaluate the human risk in the Tangjiashan landslide dam failure. The characteristics of this landslide dam are introduced first. The breaching parameters in two cases (i.e., the actual case and a high erodibility case) are predicted with a physically based model, and the flood routing processes in these two cases are simulated using numerical analysis. The population at risk downstream of the landslide dam is then obtained based on the results of the flood routing simulations. Subsequently, the human risks are analyzed with HURAM using Bayesian networks. Fourteen influence parameters and their interrelationships are considered in a systematic structure in the case study. A change in anyone of them may affect the other parameters and leads to loss of life. HURAM allows not only cause-to-result inference, but also result-to-cause inference by updating the Bayesian network with specific information from the study case. The uncertainties of the parameters and their relationships are studied both at the global level using multiple sources of information and at the local level by updating the prior probabilities.","HURAM allows not only cause-to-result inference, but also result-to-cause inference by updating the Bayesian network with specific information from the study case."
"Each day thousands of workers suffer occupational accidents of varying degrees of severity. Accidents at work render workers incapable of carrying out their day to day activities, either temporarily or permanently, and they also have detrimental effects on family life, the company, and the general public. In order to reduce the occupational accident rate, it is necessary to determine the causes of those accidents. Although there are many different types of accidents, they generally stem from poor working conditions. The purpose of this study was to analyze the influence of working conditions on occupational accidents from data gathered in the VI National Survey of Working Conditions (VI NSWCs) in 2007. This survey utilized a random sample of the active population of Spain. The sample comprised 11,054 people (5917 males and 5137 females). In order to carry out the study, a probabilistic model was built using Bayesian networks. The model included the following variables: hygiene conditions, ergonomic conditions, job demands, physical symptoms, psychological symptoms, and occupational accidents. The study demonstrated that there were strong relationships between hygiene conditions and occupational accidents; it has been shown that poor hygienic conditions duplicate the probability of accident. Physical symptoms increased almost 50% due to poor ergonomic conditions. And finally, high job demands almost duplicated the psychological symptoms. The investigation also showed a high degree of interdependence between physical and psychological symptomatologies and the relationship between these and occupational accidents. (C) 2012 Elsevier Ltd. All rights reserved.",""
"The advent of online services, social networks, crowdsourcing, and serious Web games has promoted the emergence of a novel computation paradigm, where complex tasks are solved by exploiting the capacity of human beings and computer platforms in an integrated way. Water Resources Management systems can take advantage of human and social computation in several ways: collecting and validating data, complementing the analytic knowledge embodied in models with tacit knowledge from individuals and communities, using human sensors to monitor the variation of conditions at a fine grain and in real time, activating human networks to perform search tasks or actuate management actions. This exploratory paper overviews different forms of human and social computation and analyzes how they can be exploited to enhance the effectiveness of ICT-based Water Resources Management. (C) 2012 Elsevier Ltd. All rights reserved.",""
"Bayesian networks (BNs) are increasingly being used to model environmental systems, in order to: integrate multiple issues and system components; utilise information from different sources; and handle missing data and uncertainty. BNs also have a modular architecture that facilitates iterative model development. For a model to be of value in generating and sharing knowledge or providing decision support, it must be built using good modelling practice. This paper provides guidelines to developing and evaluating Bayesian network models of environmental systems, and presents a case study habitat suitability model for juvenile Astacopsis gouldi, the giant freshwater crayfish of Tasmania. The guidelines entail clearly defining the model objectives and scope, and using a conceptual model of the system to form the structure of the BN, which should be parsimonious yet capture all key components and processes. After the states and conditional probabilities of all variables are defined, the BN should be assessed by a suite of quantitative and qualitative forms of model evaluation. All the assumptions, uncertainties, descriptions and reasoning for each node and linkage, data and information sources, and evaluation results must be clearly documented. Following these standards will enable the modelling process and the model itself to be transparent, credible and robust, within its given limitations. (C) 2012 Elsevier Ltd. All rights reserved.",""
"Background: We recently developed two Bayesian networks, referred to as the Bayesian-Estimated Tools for Survival (BETS) models, capable of estimating the likelihood of survival at 3 and 12 months following surgery for patients with operable skeletal metastases (BETS-3 and BETS-12, respectively). In this study, we attempted to externally validate the BETS-3 and BETS-12 models using an independent, international dataset. Methods: Data were collected from the Scandinavian Skeletal Metastasis Registry for patients with extremity skeletal metastases surgically treated at eight major Scandinavian referral centers between 1999 and 2009. These data were applied to the BETS-3 and BETS-12 models, which generated a probability of survival at 3 and 12 months for each patient. Model robustness was assessed using the area under the receiver-operating characteristic curve (AUC). An analysis of incorrect estimations was also performed. Results: Our dataset contained 815 records with adequate follow-up information to establish survival at 12 months. All records were missing data including the surgeon's estimate of survival, which was previously shown to be a first-degree associate of survival in both models. The AUCs for the BETS-3 and BETS-12 models were 0.79 and 0.76, respectively. Incorrect estimations by both models were more commonly optimistic than pessimistic. Conclusions: The BETS-3 and BETS-12 models were successfully validated using an independent dataset containing missing data. These models are the first validated tools for accurately estimating postoperative survival in patients with operable skeletal metastases of the extremities and can provide the surgeon with valuable information to support clinical decisions in this patient population.",""
"Background: Inference of biological networks has become an important tool in Systems Biology. Nowadays it is becoming clearer that the complexity of organisms is more related with the organization of its components in networks rather than with the individual behaviour of the components. Among various approaches for inferring networks, Bayesian Networks are very attractive due to their probabilistic nature and flexibility to incorporate interventions and extra sources of information. Recently various attempts to infer networks with different Bayesian Networks approaches were pursued. The specific interest in this paper is to compare the performance of three different inference approaches: Bayesian Networks without any modification; Bayesian Networks modified to take into account specific interventions produced during data collection; and a probabilistic hierarchical model that allows the inclusion of extra knowledge in the inference of Bayesian Networks. The inference is performed in three different types of data: (i) synthetic data obtained from a Gaussian distribution, (ii) synthetic data simulated with Netbuilder and (iii) Real data obtained in flow cytometry experiments. Results: Bayesian Networks with interventions and Bayesian Networks with inclusion of extra knowledge outperform simple Bayesian Networks in all data sets when considering the reconstruction accuracy and taking the edge directions into account. In the Real data the increase in accuracy is also observed when not taking the edge directions into account. Conclusions: Although it comes with a small extra computational cost the use of more refined Bayesian network models is justified. Both the inclusion of extra knowledge and the use of interventions have outperformed the simple Bayesian network model in simulated and Real data sets. Also, if the source of extra knowledge used in the inference is not reliable the inferred network is not deteriorated. If the extra knowledge has a good agreement with the data there is no significant difference in using the Bayesian networks with interventions or Bayesian networks with the extra knowledge.","Background: Inference of biological networks has become an important tool in Systems Biology."
"Overflows from sanitary sewers during wet weather, which occur when the hydraulic capacity of the sewer system is exceeded, are considered a potential threat to the ecological and public health of the waterways which receive these overflows. As a result, water retailers in Australia and internationally commit significant resources to manage and abate sewer overflows. However, whilst some studies have contributed to an increased understanding of the impacts and risks associated with these events, they are relatively few in number and there still is a general lack of knowledge in this area. A Bayesian network model to assess the public health risk associated with wet weather sewer overflows is presented in this paper. The Bayesian network approach is shown to provide significant benefits in the assessment of public health risks associated with wet weather sewer overflows. In particular, the ability for the model to account for the uncertainty inherent in sewer overflow events and subsequent impacts through the use of probabilities is a valuable function. In addition, the paper highlights the benefits of the probabilistic inference function of the Bayesian network in prioritising management options to minimise public health risks associated with sewer overflows. (C) 2012 Published by Elsevier Ltd.","In addition, the paper highlights the benefits of the probabilistic inference function of the Bayesian network in prioritising management options to minimise public health risks associated with sewer overflows."
"Fraud is a global problem that has required more attention due to an accentuated expansion of modern technology and communication. When statistical techniques are used to detect fraud, whether a fraud detection model is accurate enough in order to provide correct classification of the case as a fraudulent or legitimate is a critical factor. In this context, the concept of bootstrap aggregating (bagging) arises. The basic idea is to generate multiple classifiers by obtaining the predicted values from the adjusted models to several replicated datasets and then combining them into a single predictive classification in order to improve the classification accuracy. In this paper, for the first time, we aim to present a pioneer study of the performance of the discrete and continuous k-dependence probabilistic networks within the context of bagging predictors classification. Via a large simulation study and various real datasets, we discovered that the probabilistic networks are a strong modeling option with high predictive capacity and with a high increment using the bagging procedure when compared to traditional techniques. (C) 2012 Elsevier Ltd. All rights reserved.","When statistical techniques are used to detect fraud, whether a fraud detection model is accurate enough in order to provide correct classification of the case as a fraudulent or legitimate is a critical factor."
"In less than a decade, the European context on industrial risk management has evolved in order to propose frameworks to improve knowledge of both hazardous events and systemic analysis. Some frameworks are addressing more precisely socio-technical systems, considered as complex systems, operating under environmental constraints and for which multiple risks exist. Indeed, the physical and regulatory environment strongly influence the different stakes of a socio-technical system, mainly its availability, but also its safety. Nevertheless as these systems cannot be studied as a set of independent sub-systems owing to complexity, the conventional risk analysis is not applicable to them. More integrated risk analysis development is required, globally covering all the risks in a same view, taking into account system models (e. g. functional and organizational), system life cycle phase, system environment, the potential role of maintenance, and the human actions. In relation to this context, Electricite De France (EDF), which is managing socio-technical systems dedicated to energy production, took the opportunity to contribute to this issue. Thus, this article is defending a 'system thinking'-based integrated risk analysis approach. Integrated risk analysis covers different disciplines (i.e. dependability, human reliability, and organizational analysis) and is designed for developing methods and appropriate tools in order to support innovative risks analysis of such systems. This approach is justified with regard to other risk analysis approaches in order to highlight the benefits of an integrated approach compared with the usual studies that are specific (technical or environmental or human centred). These main concepts and principles, and the adequacy of Bayesian networks to integrated risk analysis models, are demonstrated by applying them to an industrial case that is a sub-set of an EDF energy power plant (a heat sink). Finally, based on the results of sensitivity studies performed to ensure the robustness of Bayesian network-based integrated risk analysis models, major prospects development and ways to tackle them are identified mainly related to the robustness of risk assessment, the modelling of the human barrier, and the resilient aspects of the organization.",""
"This paper aims to deal with uncertainties occurring in preventive maintenance strategies. After analysing the corrective maintenance data, a decision model that integrates the most important maintenance indicators and their probability distributions is provided. A case study illustrating the proposed methodology that utilizes Bayesian Networks is presented in the final part of the paper.",""
"The present paper describes a Bayesian network approach to Information Retrieval (IR) from Web documents. The network structure provides an intuitive representation of uncertainty relationships and the embedded conditional probability table is used by inference algorithms in an attempt to identify documents that are relevant to the user's needs, expressed in the form of Boolean queries. Our research has been directed in constructing a probabilistic IR framework that focus on assisting users to perform Ad-hoc retrieval of documents from the various domains such as economics, news, sports, etc. Furthermore, users can integrate feedback regarding the relevance of the retrieved documents in an attempt to improve performance on upcoming requests. Towards these goals, we have expanded the traditional Bayesian network IR system and tested it on several Greek web corpora on different application domains. We have developed two different approaches with regards to the structure: a simple one, where the structure is manually provided, and an automated one, where data mining is used in order to extract the network's structure. Results have depicted competitive performance against successful IR models of different theoretical backgrounds, such as the vector space utilizing tf-idf and the probabilistic model of BM25 in terms of precision-recall curves. In order to further improve the performance of the IR system, we have implemented a novel similarity-based lemmatization framework, reducing thus the ambiguity posed by the plethora of morphological variations of the languages in question. The employed lemmatization framework comprises of 3 core components (i.e. the word segregation, the data cleansing and the lemmatization modules) and is language-independent (i.e. can be applied to other languages with morphological peculiarities and thus improve Ad-hoc retrieval) since it achieves the mapping of an input word to its normalized form by employing two state-of-the-art language independent distance metric models, meaning the Levenshtein Edit distance and the Dice coefficient similarity measure, combined with a language model describing the most frequent inflectional suffixes of the examined language. Experimental results support our claim on the significance of this incorporation to Greek texts web retrieval as results improve by a factor of 4% to 11%.","The network structure provides an intuitive representation of uncertainty relationships and the embedded conditional probability table is used by inference algorithms in an attempt to identify documents that are relevant to the user's needs, expressed in the form of Boolean queries."
"Dependency analysis is a typical approach for Bayesian network learning, which infers the structures of Bayesian networks by the results of a series of conditional independence (CI) tests. In practice, testing independence conditioning on large sets hampers the performance of dependency analysis algorithms in terms of accuracy and running time for the following reasons. First, testing independence on large sets of variables with limited samples is not stable. Second, for most dependency analysis algorithms, the number of CI tests grows at an exponential rate with the sizes of conditioning sets, and the running time grows of the same rate. Therefore, determining how to reduce the number of CI tests and the sizes of conditioning sets becomes a critical step in dependency analysis algorithms. In this article, we address a two-phase algorithm based on the observation that the structures of Markov random fields are similar to those of Bayesian networks. The first phase of the algorithm constructs a Markov random field from data, which provides a close approximation to the structure of the true Bayesian network; the second phase of the algorithm removes redundant edges according to CI tests to get the true Bayesian network. Both phases use Markov blanket information to reduce the sizes of conditioning sets and the number of CI tests without sacrificing accuracy. An empirical study shows that the two-phase algorithm performs well in terms of accuracy and efficiency.",""
"Structure learning of Bayesian networks is a well-researched but computationally hard task. For learning Bayesian networks, this paper proposes an improved algorithm based on unconstrained optimization and ant colony optimization (U-ACO-B) to solve the drawbacks of the ant colony optimization (ACO-B). In this algorithm, firstly, an unconstrained optimization problem is solved to obtain an undirected skeleton, and then the ACO algorithm is used to orientate the edges, thus returning the final structure. In the experimental part of the paper, we compare the performance of the proposed algorithm with ACO-B algorithm. The experimental results show that our method is effective and greatly enhance convergence speed than ACO-B algorithm.",""
"For the troubleshooting domain, machine learning is used to suggest model updates using in-service troubleshooting and component testing records. One of the challenges of using these updates is how to justify each change to the system experts; a novel approach for the justification of updates to a Bayesian network troubleshooting model is presented. The results of experiments into the performance of the approach suggest that the changes suggested by maximum likelihood learning improve a model by fitting to the new records and a good justification for each change can be obtained from quantities already computed during the learning process. Copyright (C) 2012 John Wiley & Sons, Ltd.",""
"Motivation: Ontologies provide a structured representation of the concepts of a domain of knowledge as well as the relations between them. Attribute ontologies are used to describe the characteristics of the items of a domain, such as the functions of proteins or the signs and symptoms of disease, which opens the possibility of searching a database of items for the best match to a list of observed or desired attributes. However, naive search methods do not perform well on realistic data because of noise in the data, imprecision in typical queries and because individual items may not display all attributes of the category they belong to. Results:: We present a method for combining ontological analysis with Bayesian networks to deal with noise, imprecision and attribute frequencies and demonstrate an application of our method as a differential diagnostic support system for human genetics.",""
"This article presents KReator, a versatile integrated development environment for probabilistic inductive logic programming currently under development. The area of probabilistic inductive logic programming (or statistical relational learning) aims at applying probabilistic methods of inference and learning in relational or first-order representations of knowledge. In the past ten years the community brought forth a lot of proposals to deal with problems in that area, which mostly extend existing propositional probabilistic methods like Bayes Nets and Markov Networks on relational settings. Only few developers provide prototypical implementations of their approaches and the existing applications are often difficult to install and to use. Furthermore, due to different languages and frameworks used for the development of different systems the task of comparing various approaches becomes hard and tedious. KReator aims at providing a common and simple interface for representing, reasoning and learning with different relational probabilistic approaches. It is a general integrated development environment which enables the integration of various frameworks within the area of probabilistic inductive logic programming and statistical relational learning. Currently, KReator implements Bayesian logic programs, Markov logic networks and relational maximum entropy under grounding semantics. More approaches will be implemented in the near future or can be implemented by researchers themselves as KReator is open-source and available under public license. In this article, we provide some background on probabilistic inductive logic programming and statistical relational learning and illustrate the usage of KReator on several examples using the three approaches currently implemented in KReator. Furthermore, we give an overview on its system architecture.","The area of probabilistic inductive logic programming (or statistical relational learning) aims at applying probabilistic methods of inference and learning in relational or first-order representations of knowledge."
"The principle of maximum entropy has proven to be a powerful approach for commonsense reasoning in probabilistic conditional logics on propositional languages. Due to this principle, reasoning is performed based on the unique model of a knowledge base that has maximum entropy. This kind of model-based inference fulfils many desirable properties for inductive inference mechanisms and is usually the best choice for reasoning from an information theoretical point of view. However, the expressive power of propositional formalisms for probabilistic reasoning is limited and in the past few years many proposals have been given for probabilistic reasoning in relational settings. It seems to be a common view that in order to interpret probabilistic first-order sentences, either a statistical approach that counts (tuples of) individuals has to be used, or the knowledge base has to be grounded to make a possible worlds semantics applicable, for a subjective interpretation of probabilities. Most of these proposals of the second type rely on extensions of traditional probabilistic models like Bayes nets or Markov networks whereas there are only few works on first-order extensions of probabilistic conditional logic. Here, we take an approach of lifting maximum entropy methods to the relational case by employing a relational version of probabilistic conditional logic. First, we propose two different semantics and model theories for interpreting first-order probabilistic conditional logic. We address the problems of ambiguity that are raised by the difference between subjective and statistical views, and develop a comprehensive list of desirable properties for inductive model-based probabilistic inference in relational frameworks. Finally, by applying the principle of maximum entropy in the two different semantical frameworks, we obtain inference operators that fulfill these properties and turn out to be reasonable choices for reasoning in first-order probabilistic conditional logic.","This kind of model-based inference fulfils many desirable properties for inductive inference mechanisms and is usually the best choice for reasoning from an information theoretical point of view."
"This paper presents an approach based on network theory to deal with risk interactions in large engineering projects. Indeed, such projects are exposed to numerous and interdependent risks of various nature, which makes their management more difficult. In this paper, a topological analysis based on network theory is presented, which aims at identifying key elements in the structure of interrelated risks potentially affecting a large engineering project. This analysis serves as a powerful complement to classical project risk analysis. Its originality lies in the application of some network theory indicators to the project risk management field. The construction of the risk network requires the involvement of the project manager and other team members assigned to the risk management process. Its interpretation improves their understanding of risks and their potential interactions. The outcomes of the analysis provide a support for decision-making regarding project risk management. An example of application to a real large engineering project is presented. The conclusion is that some new insights can be found about risks, about their interactions and about the global potential behavior of the project. (C) 2012 Elsevier Ltd. All rights reserved.",""
"A predictive model is presented to optimize deep drilling operations under high speed conditions for the manufacture of steel components such as moulds and dies. The input data include cutting parameters and axial cutting forces measured by sensors on the milling centres where the tests are performed. The novelty of the paper lies in the use of Bayesian Networks that consider the cooling system as an input variable for the optimization of roughness quality in deep drilling operations. Two different coolant strategies are tested: traditional working fluid and MQL (Minimum Quantity Lubrication). The model is based on a machine learning classification method known as Bayesian networks. Various measures used to assess the model demonstrate its suitability to control this type of industrial task. Its ease of interpretation is a further advantage in comparison with other artificial intelligence tools, which makes it a user-friendly application for machine operators.","The model is based on a machine learning classification method known as Bayesian networks."
"Dam breaks have catastrophic consequences for human lives. This paper presents a new human risk analysis model (HURAM) using Bayesian networks for estimating human risks due to dam-break floods. A Bayesian network is constructed according to a logic structure of loss-of-life mechanisms. The nodes (parameters) and the arcs (inter-relationships) of the network are quantified with historical data, existing models and physical analyses. A dataset of 343 dam-failure cases with records of fatality is compiled for this purpose. Comparison between two existing models and the new model is made to test the new model. Finally, sensitivity analysis is conducted to identify the important parameters that lead to loss of life. The new model is able to take into account a large number of important parameters and their inter-relationships in a systematic structure; include the uncertainties of these parameters and their inter-relationships; incorporate information derived from physical analysis, empirical models and historical data; and update the predictions when information in specific cases is available. The application of this model to the study of human risks in a specific dam-break case is presented in a companion paper.",""
"During human communication, every spoken message is intrinsically modulated within different verbal and nonverbal cues that are externalized through various aspects of speech and facial gestures. These communication channels are strongly interrelated, which suggests that generating human-like behavior requires a careful study of their relationship. Neglecting the mutual influence of different communicative channels in the modeling of natural behavior for a conversational agent may result in unrealistic behaviors that can affect the intended visual perception of the animation. This relationship exists both between audiovisual information and within different visual aspects. This paper explores the idea of using joint models to preserve the coupling not only between speech and facial expression, but also within facial gestures. As a case study, the paper focuses on building a speech-driven facial animation framework to generate natural head and eyebrow motions. We propose three dynamic Bayesian networks (DBNs), which make different assumptions about the coupling between speech, eyebrow and head motion. Synthesized animations are produced based on the MPEG-4 facial animation standard, using the audiovisual IEMOCAP database. The experimental results based on perceptual evaluations reveal that the proposed joint models (speech/eyebrow/head) outperform audiovisual models that are separately trained (speech/head and speech/eyebrow).",""
"Probabilistic Decision Graphs (PDGs) are probabilistic graphical models that represent a factorisation of a discrete joint probability distribution using a \"decision graph\"-like structure over local marginal parameters. The structure of a PDG enables the model to capture some context specific independence relations that are not representable in the structure of more commonly used graphical models such as Bayesian networks and Markov networks. This sometimes makes operations in PDGs more efficient than in alternative models. PDGs have previously been defined only in the discrete case, assuming a multinomial joint distribution over the variables in the model. We extend PDGs to incorporate continuous variables, by assuming a Conditional Gaussian (CG) joint distribution. We also show how inference can be carried out in an efficient way. (C) 2011 Elsevier Inc. All rights reserved.","We also show how inference can be carried out in an efficient way."
"Probabilistic inference is among the main topics with reasoning in uncertainty in AI. For this purpose, Bayesian Networks (BNs) is one of the most successful and efficient Probabilistic Graphical Model (PGM) so far. Since the mid-90s, a growing number of BNs extensions have been proposed. Object-oriented, entity-relationship and first-order logic are the main representation paradigms used to extend BNs. While entity-relationship and first-order models have been successfully used for machine learning in defining lifted probabilistic inference, object-oriented models have been mostly underused. Structured inference, which exploits the structural knowledge encoded in an object-oriented PGM, is a surprisingly unstudied technique. In this paper we propose a full object-oriented framework for Probabilistic Relational Models (PRMs) and propose two extensions of the state-of-the-art structured inference algorithm: SPI which removes the major flaws of existing algorithms and SPISBB which largely enhances SPI by using d-separation. (C) 2012 Published by Elsevier Inc.","Probabilistic inference is among the main topics with reasoning in uncertainty in AI."
"We consider efficient indexing methods for conditioning graphs, which are a form of recursive decomposition for Bayesian networks. We compare two well-known methods for indexing, a top-down method and a bottom-up method, and discuss the redundancy that each of these suffer from. We present a new method for indexing that combines the advantages of each model in order to reduce this redundancy. We also introduce the concept of an update manager, which is a node in the conditioning graph that controls when other nodes update their current index. Empirical evaluations over a suite of standard test networks show a considerable reduction both in the amount of indexing computation that takes place, and the overall runtime required by the query algorithm. (C) 2012 Elsevier Inc. All rights reserved.",""
"To specify a Bayesian network (BN), a conditional probability table (CPT), often of an effect conditioned on its n causes, must be assessed for each node. Its complexity is generally exponential in n. Noisy-OR and a number of extensions reduce the complexity to linear, but can only represent reinforcing causal interactions. Non-impeding noisy-AND (NIN-AND) trees are the first causal models that explicitly express reinforcement, undermining, and their mixture. Their acquisition has a linear complexity, in terms of both the number of parameters and the size of the tree topology. As originally proposed, however, they allow only binary effects and causes. This work generalizes binary NIN-AND tree models to multi-valued effects and causes. It is shown that the generalized NIN-AND tree models express reinforcement, undermining, and their mixture at multiple levels, relative to each active value of the effect. The model acquisition is still efficient. For binary variables, they degenerate into binary NIN-AND tree models. Hence, this contribution enables CPTs of discrete BNs of arbitrary variables (binary or multi-valued) to be specified efficiently through the intuitive concepts of reinforcement and undermining. (C) 2012 Elsevier Inc. All rights reserved.",""
"Bayesian networks were used to combine raw datasets from two independently performed but related studies. Both studies investigated sensory satiation by measuring ad libitum intake of a tomato soup model. The Aroma study varied aroma concentration and aroma duration as the explanatory variables, and the Taste study varied salt intensity. To combine the data from the two studies, the Aroma study needed information on salt aspects for all of its observations. Equally, the Taste study needed information on aroma aspects. This information was used to link the two single networks, each representing one study, into a combined network; therefore, it is referred to as structural linking information. The approach taken is seen as an example for the potential benefit and the challenges when combining raw datasets from different studies. The combined network is able to generate additional insights into complex relationships encountered with research on satiation. The main challenge results from the missing of structural linking information. In this paper, we (1) suggest solutions for obtaining the structural linking information, and (2) propose an approach to global experimental design to prevent this situation. The nature of the paper is theoretical rather than analytical due to the limitations caused by the small size of datasets. (C) 2012 Elsevier Ltd. All rights reserved.",""
"The inevitable though frequently informal use of expert opinion in modelling, the increasing number of models that incorporate formally expert opinion from a diverse range of experience and stakeholders, arguments for participatory modelling and analytic-deliberative-adaptive approaches to managing complex environmental problems, and an expanding but uneven literature prompt this critical review and analysis. Aims are to propose common definitions, identify and categorise existing concepts and practice, and provide a frame of reference and guidance for future environmental modelling. The extensive literature review and classification conducted demonstrate that a broad and inclusive definition of experts and expert opinion is both required and part of current practice. Thus an expert can be anyone with relevant and extensive or in-depth experience in relation to a topic of interest. The literature review also exposes informal model assumptions and modeller subjectivity, examines in detail the formal uses of expert opinion and expert systems, and critically analyses the main concepts of, and issues arising in, expert elicitation and the modelling of associated uncertainty. It is noted that model scrutiny and use of expert opinion in modelling will benefit from formal, systematic and transparent procedures that include as wide a range of stakeholders as possible. Enhanced awareness and utilisation of expert opinion is required for modelling that meets the informational needs of deliberative fora. These conclusions in no way diminish the importance of conventional science and scientific opinion but recognise the need for a paradigmatic shift from traditional ideals of unbiased and impartial experts towards unbiased processes of expert contestation and a plurality of expertise and eventually models. Priority must be given to the quality of the enquiry for those responsible for environmental management and policy formulation, and this review emphasises the role for science to maintain and enhance the rigour and formality of the information that informs decision making. (C) 2012 Elsevier Ltd. All rights reserved.","The extensive literature review and classification conducted demonstrate that a broad and inclusive definition of experts and expert opinion is both required and part of current practice."
"Motivation: Cancer development is driven by the accumulation of advantageous mutations and subsequent clonal expansion of cells harbouring these mutations, but the order in which mutations occur remains poorly understood. Advances in genome sequencing and the soon-arriving flood of cancer genome data produced by large cancer sequencing consortia hold the promise to elucidate cancer progression. However, new computational methods are needed to analyse these large datasets. Results: We present a Bayesian inference scheme for Conjunctive Bayesian Networks, a probabilistic graphical model in which mutations accumulate according to partial order constraints and cancer genotypes are observed subject to measurement noise. We develop an efficient MCMC sampling scheme specifically designed to overcome local optima induced by dependency structures. We demonstrate the performance advantage of our sampler over traditional approaches on simulated data and show the advantages of adopting a Bayesian perspective when reanalyzing cancer datasets and comparing our results to previous maximum-likelihood-based approaches.","Results: We present a Bayesian inference scheme for Conjunctive Bayesian Networks, a probabilistic graphical model in which mutations accumulate according to partial order constraints and cancer genotypes are observed subject to measurement noise."
"With the aim of a more effective representation of reliability assessment for real industry, in the last years concepts like dynamic fault trees (DFT) have gained the interest of many researchers and engineers (dealing with problems concerning safety management, design and development of new products, decision analysis and project management, maintenance of industrial plant, etc.). With the increased computational power of modern calculators is possible to achieve results with low modeling efforts and calculating time. Supported by the strong mathematical basis of state space models, the DFT technique has increased its popularity. Nevertheless, DFT analysis of real application has been more likely based on a specific case to case resolution procedure that often requires a great effort in terms of modeling by the human operator. Moreover, limitations like the state space explosion for increasing number of components, the constrain of using exponential distribution for all kind of basic events constituting any analyzed system and the ineffectiveness of modularization for DFT which exhibit dynamic gates at top levels without incurring in calculation and methodological errors are faces of these methodologies. In this paper we present a high level modeling framework that exceeds all these limitations, based on Monte Carlo simulation. It makes use of traditional DFT systemic modeling procedure and by replicating the true casual nature of the system can produce relevant results with low effort in term of modeling and computational time. A Simulink library that integrates Monte Carlo and FT methodologies for the calculation of DFT reliability has been developed, revealing new insights about the meaning of spare gates. (C) 2011 Elsevier Ltd. All rights reserved.",""
"The purpose of this study is to analyze the relations between the factors that enable national competitive advantage and the establishment of competitive superiority in automotive industry through a comprehensive analytical model. Bayesian networks (BN) are used to investigate the associations of different factors in the automotive industry which lead to competitive advantage. The results of the study focus on building a road map for the automotive sector policy makers in their way to improve the competitiveness through scenario analysis. Using the probabilistic dependency structure of the Bayesian network all of the variables in the model can be estimated. Thus, with the proposed model the automotive industry can be analyzed as a whole system and not only in terms of single variables. Findings of the model indicate that technological developments in automotive industry can alter the nature of competition in this industry. (C) 2012 Elsevier Ltd. All rights reserved.",""
"As cell phones have become more common, personalized intelligent services in smartphones have become more highly desired. The mobile intelligent synthetic character is an example of one of these desired services. It is hard to apply an intelligent synthetic character to the smartphone environment because of its dynamism and complexity. This paper proposes a method for generating behaviors of a smart synthetic character that infers user contexts with Bayesian networks. In order to generate more realistic behaviors, the OCC model is utilized to create the character's emotion. Behaviors are produced through large-scale modular behavior networks with inferred contexts. A working progress is the mobile log collected with a Samsung SPH-M4650 smartphone that is used to verify the naturalness and flexibility of the generated behaviors. (C) 2012 Elsevier Ltd. All rights reserved.",""
"In this work, we empirically evaluate the capability of various scoring functions of Bayesian networks for recovering true underlying structures. Similar investigations have been carried out before, but they typically relied on approximate learning algorithms to learn the network structures. The suboptimal structures found by the approximation methods have unknown quality and may affect the reliability of their conclusions. Our study uses an optimal algorithm to learn Bayesian network structures from datasets generated from a set of gold standard Bayesian networks. Because all optimal algorithms always learn equivalent networks, this ensures that only the choice of scoring function affects the learned networks. Another shortcoming of the previous studies stems from their use of random synthetic networks as test cases. There is no guarantee that these networks reflect real-world data. We use real-world data to generate our gold-standard structures, so our experimental design more closely approximates real-world situations. A major finding of our study suggests that, in contrast to results reported by several prior works, the Minimum Description Length (MDL) (or equivalently, Bayesian information criterion (BIC)) consistently outperforms other scoring functions such as Akaike's information criterion (AIC), Bayesian Dirichlet equivalence score (BDeu), and factorized normalized maximum likelihood (fNML) in recovering the underlying Bayesian network structures. We believe this finding is a result of using both datasets generated from real-world applications rather than from random processes used in previous studies and learning algorithms to select high-scoring structures rather than selecting random models. Other findings of our study support existing work, e. g., large sample sizes result in learning structures closer to the true underlying structure; the BDeu score is sensitive to the parameter settings; and the fNML performs pretty well on small datasets. We also tested a greedy hill climbing algorithm and observed similar results as the optimal algorithm.",""
"In a number of applications there is a need to determine the most likely pedigree for a group of persons based on genetic markers. Adequate models are needed to reach this goal. The markers used to perform the statistical calculations can be linked and there may also be linkage disequilibrium (LD) in the population. The purpose of this paper is to present a graphical Bayesian Network framework to deal with such data. Potential LD is normally ignored and it is important to verify that the resulting calculations are not biased. Even if linkage does not influence results for regular paternity cases, it may have substantial impact on likelihood ratios involving other, more extended pedigrees. Models for LD influence likelihoods for all pedigrees to some degree and an initial estimate of the impact of ignoring LD and/or linkage is desirable, going beyond mere rules of thumb based on marker distance. Furthermore, we show how one can readily include a mutation model in the Bayesian Network; extending other programs or formulas to include such models may require considerable amounts of work and will in many case not be practical. As an example, we consider the two STR markers vWa and D12S391. We estimate probabilities for population haplotypes to account for LD using a method based on data from trios, while an estimate for the degree of linkage is taken from the literature. The results show that accounting for haplotype frequencies is unnecessary in most cases for this specific pair of markers. When doing calculations on regular paternity cases, the markers can be considered statistically independent. In more complex cases of disputed relatedness, for instance cases involving siblings or so-called deficient cases, or when small differences in the LR matter, independence should not be assumed. (The networks are freely available at http://arken.umb.no/similar to dakl/BayesianNetworks.)",""
"Network inference approaches are now widely used in biological applications to probe regulatory relationships between molecular components such as genes or proteins. Many methods have been proposed for this setting, but the connections and differences between their statistical formulations have received less attention. In this paper, we show how a broad class of statistical network inference methods, including a number of existing approaches, can be described in terms of variable selection for the linear model. This reveals some subtle but important differences between the methods, including the treatment of time intervals in discretely observed data. In developing a general formulation, we also explore the relationship between single-cell stochastic dynamics and network inference on averages over cells. This clarifies the link between biochemical networks as they operate at the cellular level and network inference as carried out on data that are averages over populations of cells. We present empirical results, comparing thirty-two network inference methods that are instances of the general formulation we describe, using two published dynamical models. Our investigation sheds light on the applicability and limitations of network inference and provides guidance for practitioners and suggestions for experimental design.","Network inference approaches are now widely used in biological applications to probe regulatory relationships between molecular components such as genes or proteins."
"Background: Clostridium difficile (C-Diff) infection following colorectal resection is an increasing source of morbidity and mortality. Objective: We sought to determine if machine-learned Bayesian belief networks (ml-BBNs) could preoperatively provide clinicians with postoperative estimates of C-Diff risk. Methods: We performed a retrospective modeling of the Nationwide Inpatient Sample (NIS) national registry dataset with independent set validation. The NIS registries for 2005 and 2006 were used for initial model training, and the data from 2007 were used for testing and validation. International Classification of Diseases, 9th Revision, Clinical Modification (ICD-9-CM) codes were used to identify subjects undergoing colon resection and postoperative C-Diff development. The ml-BBNs were trained using a stepwise process. Receiver operating characteristic (ROC) curve analysis was conducted and area under the curve (AUC), positive predictive value (PPV), and negative predictive value (NPV) were calculated. Results: From over 24 million admissions, 170,363 undergoing colon resection met the inclusion criteria. Overall, 1.7% developed postoperative C-Diff. Using the ml-BBN to estimate C-Diff risk, model AUC is 0.75. Using only known a priori features, AUC is 0.74. The model has two configurations: a high sensitivity and a high specificity configuration. Sensitivity, specificity, PPV, and NPV are 81.0%, 50.1%, 2.6%, and 99.4% for high sensitivity and 55.4%, 81.3%, 3.5%, and 99.1% for high specificity. C-Diff has 4 first-degree associates that influence the probability of C-Diff development: weight loss, tumor metastases, inflammation/infections, and disease severity. Conclusions: Machine-learned BBNs can produce robust estimates of postoperative C-Diff infection, allowing clinicians to identify high-risk patients and potentially implement measures to reduce its incidence or morbidity. (Interact J Med Res 2012;1(2):e6) doi:10.2196/ijmr.2131","International Classification of Diseases, 9th Revision, Clinical Modification (ICD-9-CM) codes were used to identify subjects undergoing colon resection and postoperative C-Diff development."
"Due to the rapid progress of information and communication technologies, networking between individuals and machines has become necessary and crucial. Some phenomenon already exists in the real world, such as epidemics, social networks, ecology food web, metabolic, genetic or protein interaction networks and database maintained by telecom operator can also be observed via scientific approaches based on networking aspect, which may be useful for solving problems or providing killer applications in real world. In this paper we first review some basic concepts of characterizing a network from different aspects by various structural properties. Then we describe several important network models and approaches developed in the past decade. Furthermore, in order to better obtain network structural properties and topology from data in an effective way, we visualized the result of network analysis by open-source software library, JUNG as a tool, and take cognitive radio networks as a target for evaluation of the structural properties. Finally we apply Bayesian method to the data from real mobile service network to estimate the preference of subscribers for achieving precise marketing as an example of probabilistic network construction from data.",""
"Predictive maintenance (PdM) focuses on failure prediction in order to prevent failure in advance and offer sufficient information to improve inherent safety and maintenance planning. A novel opportunistic predictive maintenance-decision (OPM) method integrating of machinery prognostic and opportunistic maintenance model is proposed in this paper to indicate the optimal maintenance time with minimal cost and safety constrains. DBN-HAZOP model quantifies hazard and operability analysis by dynamic Bayesian networks to provide prospective degradation trends of each component and the overall system for maintenance decision making. It is developed by integrating the prior knowledge of the interactions and dependencies among components and also the external environment, while the online condition monitoring data which is further to update the parameters of the model. Based on the future degradation trends given by DBN-HAZOP model, a local optimal proactive maintenance practice can be determined for each component by minimizing the expected maintenance cost per time unit. Understanding that for a complex system, whenever one of the components stops to perform a predictive maintenance action, the whole complex system must be stopped, at this moment, PdM opportunities arise for the other degraded components in the system at a reduced additional cost. Therefore, this paper further proposes an opportunistic PdM strategy for global cost optimization of predictive maintenance for the whole system, which considers failure probabilities, repair costs, down time cost and set-up cost. Case studies are given throughout to show how this approach works, and the sensitivity of the results to some of the driving cost parameters has also been examined. (C) 2012 The Institution of Chemical Engineers. Published by Elsevier B.V. All rights reserved.",""
"Prudent decision making in subsurface assets requires reservoir uncertainty quantification. In a typical uncertainty-quantification study, reservoir models must be updated using the observed response from the reservoir by a process known as history matching. This involves solving an inverse problem, finding reservoir models that produce, under simulation, a similar response to that of the real reservoir. However, this requires multiple expensive multiphase-flow simulations. Thus, uncertainty-quantification studies employ optimization techniques to find acceptable models to be used in prediction. Different optimization algorithms and search strategies are presented in the literature, but they are generally unsatisfactory because of slow convergence to the optimal regions of the global search space, and, more importantly, failure in finding multiple acceptable reservoir models. In this context, a new approach is offered by estimation-of-distribution algorithms (EDAs). EDAs are population-based algorithms that use models to estimate the probability distribution of promising solutions and then generate new candidate solutions. This paper explores the application of EDAs, including univariate and multivariate models. We discuss two histogram-based univariate models and one multivariate model, the Bayesian optimization algorithm (BOA), which employs Bayesian networks for modeling. By considering possible interactions between variables and exploiting explicitly stored knowledge of such interactions, EDAs can accelerate the search process while preserving search diversity. Unlike most existing approaches applied to uncertainty quantification, the Bayesian network allows the BOA to build solutions using flexible rules learned from the models obtained, rather than fixed rules, leading to better solutions and improved convergence. The BOA is naturally suited to finding good solutions in complex high-dimensional spaces, such as those typical in reservoir-uncertainty quantification. We demonstrate the effectiveness of EDA by applying the well-known synthetic PUNQ-S3 case with multiple wells. This allows us to verify the methodology in a well-controlled case. Results show better estimation of uncertainty when compared with some other traditional population-based algorithms.",""
"In the reliability modeling field, we sometimes encounter systems with uncertain structures, and the use of fault trees and reliability diagrams is not possible. To overcome this problem, Bayesian approaches offer a considerable efficiency in this context. This paper introduces recent contributions in the field of reliability modeling with the Bayesian network approach. Bayesian reliability models are applied to systems with Weibull distribution of failure. To achieve the formulation of the reliability model, Bayesian estimation of Weibull parameters and the model's goodness-of-fit are evoked. The advantages of this modelling approach are presented in the case of systems with an unknown reliability structure, those with a common cause of failures and redundant ones. Finally, we raise the issue of the use of BNs in the fault diagnosis area.",""
"Causal discovery seeks to recover cause-effect relationships from statistical data using graphical models. One goal of this paper is to provide an accessible introduction to causal discovery methods for climate scientists, with a focus on constraint-based structure learning. Second, in a detailed case study constraint-based structure learning is applied to derive hypotheses of causal relationships between four prominent modes of atmospheric low-frequency variability in boreal winter including the Western Pacific Oscillation (WPO), Eastern Pacific Oscillation (EPO), Pacific-North America (PNA) pattern, and North Atlantic Oscillation (NAO). The results are shown in the form of static and temporal independence graphs also known as Bayesian Networks. It is found that WPO and EPO are nearly indistinguishable from the cause-effect perspective as strong simultaneous coupling is identified between the two. In addition, changes in the state of EPO (NAO) may cause changes in the state of NAO (PNA) approximately 18 (3-6) days later. These results are not only consistent with previous findings on dynamical processes connecting different low-frequency modes (e. g., interaction between synoptic and low-frequency eddies) but also provide the basis for formulating new hypotheses regarding the time scale and temporal sequencing of dynamical processes responsible for these connections. Last, the authors propose to use structure learning for climate networks, which are currently based primarily on correlation analysis. While correlation-based climate networks focus on similarity between nodes, independence graphs would provide an alternative viewpoint by focusing on information flow in the network.",""
"The relationships among organisms and their surroundings can be of immense complexity. To describe and understand an ecosystem as a tangled bank, multiple ways of interaction and their effects have to be considered, such as predation, competition, mutualism and facilitation. Understanding the resulting interaction networks is a challenge in changing environments, e.g. to predict knock-on effects of invasive species and to understand how climate change impacts biodiversity. The elucidation of complex ecological systems with their interactions will benefit enormously from the development of new machine learning tools that aim to infer the structure of interaction networks from field data. In the present study, we propose a novel Bayesian regression and multiple changepoint model (BRAM) for reconstructing species interaction networks from observed species distributions. The model has been devised to allow robust inference in the presence of spatial autocorrelation and distributional heterogeneity. We have evaluated the model on simulated data that combines a trophic niche model with a stochastic population model on a 2-dimensional lattice, and we have compared the performance of our model with L1-penalized sparse regression (LASSO) and non-linear Bayesian networks with the BDe scoring scheme. In addition, we have applied our method to plant ground coverage data from the western shore of the Outer Hebrides with the objective to infer the ecological interactions. (C) 2012 Elsevier B.V. All rights reserved.","In the present study, we propose a novel Bayesian regression and multiple changepoint model (BRAM) for reconstructing species interaction networks from observed species distributions."
"This paper presents a novel approach to the CI interdependencies analysis, based on the DBN formalism. An original modeling procedure is illustrated, which divides the DBN in three distinct levels: an atomic events level, a propagation level, and a services level. The first level models the adverse events that may impact on the analyzed CIs, the second one properly captures interdependencies among CIs' services and devices, and the last one allows to monitor the state of provided services. The resulting DBN permits to perform three kinds of analysis: a reliability study, which allows to find structural weaknesses of interconnected CIs, an adverse events propagation study, which highlights the role interdependency plays in the propagation of adverse events, and a failure prediction analysis, that can serve as an useful guide to the fault localization process (failures may have many different explanations due to interdependency). A specific case study provided by Israel Electric Corporation is considered, and explicative simulations are presented and discussed in detail.",""
"Causal discovery is highly desirable in science and technology. In this paper, we study a new research problem of discovery of causal relationships in the context of streaming features, where the features steam in one by one. With a Bayesian network to represent causal relationships, we propose a novel algorithm called causal discovery from streaming features (CDFSF) which consists of a two-phase scheme. In the first phase, CDFSF dynamically discovers causal relationships between each feature seen so far with an arriving feature, while in the second phase CDFSF removes the false positives of each arrived feature from its current set of direct causes and effects. To improve the efficiency of CDFSF, using the symmetry properties between parents (causes) and children (effects) in a faithful Bayesian network, we present a variant of CDFSF, S-CDFSF. Experimental results validate our algorithms in comparison with the existing algorithms of causal relationship discovery.",""
"Dynamic Bayesian Networks (DBNs) can serve as succinct probabilistic dynamic models of biochemical networks [1]. To analyze these models, one must compute the probability distribution over system states at a given time point. Doing this exactly is infeasible for large models; hence one must use approximate algorithms. The Factored Frontier algorithm (FF) is one such algorithm [2]. However FF as well as the earlier Boyen-Koller (BK) algorithm [3] can incur large errors. To address this, we present a new approximate algorithm called the Hybrid Factored Frontier (HFF) algorithm. At each time slice, in addition to maintaining probability distributions over local states-as FF does-HFF explicitly maintains the probabilities of a number of global states called spikes. When the number of spikes is 0, we get FF and with all global states as spikes, we get the exact inference algorithm. We show that by increasing the number of spikes one can reduce errors while the additional computational effort required is only quadratic in the number of spikes. We validated the performance of HFF on large DBN models of biopathways. Each pathway has more than 30 species and the corresponding DBN has more than 3,000 nodes. Comparisons with FF and BK show that HFF is a useful and powerful approximate inferencing algorithm for DBNs.","When the number of spikes is 0, we get FF and with all global states as spikes, we get the exact inference algorithm."
"A better understanding of disease progression is beneficial for early diagnosis and appropriate individual therapy. Many different approaches for statistical modelling of cumulative disease progression have been proposed in the literature, including simple path models up to complex restricted Bayesian networks. Important fields of application are diseases such as cancer and HIV. Tumour progression is measured by means of chromosome aberrations, whereas people infected with HIV develop drug resistances because of genetic changes of the HI-virus. These two very different diseases have typical courses of disease progression, which can be modelled partly by consecutive and partly by independent steps. This paper gives an overview of the different progression models and points out their advantages and drawbacks. Different models are compared via simulations to analyse how they work if some of their assumptions are violated. In a simulation study, we evaluate how models perform in terms of fitting induced multivariate probability distributions and topological relationships. We often find that the true model class used for generating data is outperformed by either a less or a more complex model class. The more flexible conjunctive Bayesian networks can be used to fit oncogenetic trees, whereas mixtures of oncogenetic trees with three tree components can be well fitted by mixture models with only two tree components.",""
"Forensic scientists face increasingly complex inference problems for evaluating likelihood ratios (LRs) for an appropriate pair of propositions. Up to now, scientists and statisticians have derived LR formulae using an algebraic approach. However, this approach reaches its limits when addressing cases with an increasing number of variables and dependence relationships between these variables. In this study, we suggest using a graphical approach, based on the construction of Bayesian networks (BNs). We first construct a BN that captures the problem, and then deduce the expression for calculating the LR from this model to compare it with existing LR formulae. We illustrate this idea by applying it to the evaluation of an activity level LR in the context of the two-trace transfer problem. Our approach allows us to relax assumptions made in previous LR developments, produce a new LR formula for the two-trace transfer problem and generalize this scenario to n traces.","Forensic scientists face increasingly complex inference problems for evaluating likelihood ratios (LRs) for an appropriate pair of propositions."
"Reverse engineering of gene regulatory networks (GRNs) is one of the most challenging tasks in systems biology and bioinformatics. It aims at revealing network topologies and regulation relationships between components from biological data. Owing to the development of biotechnologies, various types of biological data are collected from experiments. With the availability of these data, many methods have been developed to infer GRNs. This paper firstly provides an introduction to the basic biological background and the general idea of GRN inferences. Then, different methods are surveyed from two aspects: models that those methods are based on and inference algorithms that those methods use. The advantages and disadvantages of these models and algorithms are discussed. (c) 2012 Wiley Periodicals, Inc.","This paper firstly provides an introduction to the basic biological background and the general idea of GRN inferences."
"This paper proposes the On-line Asynchronous Compensation Methods (OACM) for static/quasi-static error caused by thermal deformation and machine geometry. The proposed method reduces the complexity of compensation system, which can be implemented on rough interpolation stage or by offset method, depending on the applications. For Position-Dependent Error (PDTE), the rough interpolation compensation is proposed which can deal with volumetric error in an efficient and economical way. Considering the non-uniform error in the whole machining space, the whole error space is divided into subsections along the given tool path, and then combine the subsections as a non-uniform compensation table. Simulation results validate the feasibility of the proposed method based on the rough interpolation data from Non-Uniform Rational B-spline (NURBS) tool path. For Position-Independent Error (PITE), the offset compensation method through CNC-PLC is proposed. Offset compensation method adopts Bayesian Networks to predict error instead of the linear model for thermal compensation inside CNC, but utilizing its interface between CNC and PLC. Machining experiments and successful industry application on Roller Guide Grinding Machine show that this technology can reduce more than 70% of the machining error caused by thermal deformation. (C) 2012 Elsevier Ltd. All rights reserved.",""
"Both, Bayesian networks and probabilistic evaluation are gaining more and more widespread use within many professional branches, including forensic science. Notwithstanding, they constitute subtle topics with definitional details that require careful study. While many sophisticated developments of probabilistic approaches to evaluation of forensic findings may readily be found in published literature, there remains a gap with respect to writings that focus on foundational aspects and on how these may be acquired by interested scientists new to these topics. This paper takes this as a starting point to report on the learning about Bayesian networks for likelihood ratio based, probabilistic inference procedures in a class of master students in forensic science. The presentation uses an example that relies on a casework scenario drawn from published literature, involving a questioned signature. A complicating aspect of that case study - proposed to students in a teaching scenario - is due to the need of considering multiple competing propositions, which is an outset that may not readily be approached within a likelihood ratio based framework without drawing attention to some additional technical details. Using generic Bayesian networks fragments from existing literature on the topic, course participants were able to track the probabilistic underpinnings of the proposed scenario correctly both in terms of likelihood ratios and of posterior probabilities. In addition, further study of the example by students allowed them to derive an alternative Bayesian network structure with a computational output that is equivalent to existing probabilistic solutions. This practical experience underlines the potential of Bayesian networks to support and clarify foundational principles of probabilistic procedures for forensic evaluation. (C) 2012 Forensic Science Society. Published by Elsevier Ireland Ltd. All rights reserved.","This paper takes this as a starting point to report on the learning about Bayesian networks for likelihood ratio based, probabilistic inference procedures in a class of master students in forensic science."
"This paper presents an approach for real-time video event recognition that combines the accuracy and descriptive capabilities of, respectively, probabilistic and semantic approaches. Based on a state-of-art knowledge representation, we define a methodology for building recognition strategies from event descriptions that consider the uncertainty of the low-level analysis. Then, we efficiently organize such strategies for performing the recognition according to the temporal characteristics of events. In particular, we use Bayesian Networks and probabilistically-extended Petri Nets for recognizing, respectively, simple and complex events. For demonstrating the proposed approach, a framework has been implemented for recognizing human-object interactions in the video monitoring domain. The experimental results show that our approach improves the event recognition performance as compared to the widely used deterministic approach. (C) 2012 Elsevier Inc. All rights reserved.",""
"Assessing the passage of aquatic organisms through culvert road crossings has become increasingly common in efforts to restore stream habitat. Several federal and state agencies and local stakeholders have adopted assessment approaches based on literature-derived criteria for culvert impassability. However, criteria differ and are typically specific to larger-bodied fishes. In an analysis to prioritize culverts for remediation to benefit imperiled, small-bodied fishes in the Upper Coosa River system in the southeastern United States, we assessed the sensitivity of prioritization to the use of differing but plausible criteria for culvert impassability. Using measurements at 256 road crossings, we assessed culvert impassability using four alternative criteria sets represented in Bayesian belief networks. Two criteria sets scored culverts as either passable or impassable based on alternative thresholds of culvert characteristics (outlet elevation, baseflow water velocity). Two additional criteria sets incorporated uncertainty concerning ability of small-bodied fishes to pass through culverts and estimated a probability of culvert impassability. To prioritize culverts for remediation, we combined estimated culvert impassability with culvert position in the stream network relative to other barriers to compute prospective gain in connected stream habitat for the target fish species. Although four culverts ranked highly for remediation regardless of which criteria were used to assess impassability, other culverts differed widely in priority depending on criteria. Our results emphasize the value of explicitly incorporating uncertainty into criteria underlying remediation decisions. Comparing outcomes among alternative, plausible criteria may also help to identify research most needed to narrow management uncertainty.",""
"This article introduces a novel approach towards the recognition of typical driving maneuvers in structured highway scenarios and shows some key benefits of traffic scene modeling with object-oriented Bayesian networks (OOBNs). The approach exploits the advantages of an introduced lane-related coordinate system together with individual occupancy schedule grids for all modeled vehicles. This combination allows an efficient classification of the existing vehicle-lane and vehicle-vehicle relations in traffic scenes and thus substantially improves the understanding of complex traffic scenes. Probabilities and variances within the network are propagated systematically which results in probabilistic sets of the modeled driving maneuvers. Using this generic approach, the network is able to classify a total of 27 driving maneuvers including merging and object following.","This combination allows an efficient classification of the existing vehicle-lane and vehicle-vehicle relations in traffic scenes and thus substantially improves the understanding of complex traffic scenes."
"Patients in the intensive care unit (ICU) who require mechanical ventilation due to acute respiratory failure also frequently require the administration of sedative agents. The need for sedation arises both from patient anxiety due to the loss of personal control and the unfamiliar and intrusive environment of the ICU, and also due to pain or other variants of noxious stimuli. While physicians select the agent(s) used for sedation and cardiovascular function, the actual administration of these agents is the responsibility of the nursing staff. If clinical decision support systems and closed-loop control systems could be developed for critical care monitoring and lifesaving interventions as well as the administration of sedation and cardiopulmonary management, the ICU nurse could be released from the intense monitoring of sedation, allowing her/him to focus on other critical tasks. One particularly attractive strategy is to utilize the knowledge and experience of skilled clinicians, capturing explicitly the rules expert clinicians use to decide on how to titrate drug doses depending on the level of sedation. In this paper, we extend the deterministic rule-based expert system for cardiopulmonary management and ICU sedation framework presented in [1] to a stochastic setting by using probability theory to quantify uncertainty and hence deal with more realistic clinical situations.",""
"Many machine learning applications that involve relational databases incorporate first-order logic and probability. Relational extensions of graphical models include Parametrized Bayes Net (Poole in IJCAI, pp. 985-991, 2003), Probabilistic Relational Models (Getoor et al. in Introduction to statistical relational learning, pp. 129-173, 2007), and Markov Logic Networks (MLNs) (Domingos and Richardson in Introduction to statistical relational learning, 2007). Many of the current state-of-the-art algorithms for learning MLNs have focused on relatively small datasets with few descriptive attributes, where predicates are mostly binary and the main task is usually prediction of links between entities. This paper addresses what is in a sense a complementary problem: learning the structure of a graphical model that models the distribution of discrete descriptive attributes given the links between entities in a relational database. Descriptive attributes are usually nonbinary and can be very informative, but they increase the search space of possible candidate clauses. We present an efficient new algorithm for learning a Parametrized Bayes Net that performs a level-wise search through the table join lattice for relational dependencies. From the Bayes net we obtain an MLN structure via a standard moralization procedure for converting directed models to undirected models. Learning MLN structure by moralization is 200-1000 times faster and scores substantially higher in predictive accuracy than benchmark MLN algorithms on five relational databases.",""
"Case-based reasoning (CBR) method has been widely used to study environmental and spatial problems since the 1990s. Spatial relations among geographic cases and between case and environment (hereinafter to be referred as spatial relations) were not well considered in most of the previous studies. However, these relations are extremely important in geographic problems solving as spatially closer geographic phenomena are more likely to be similar than those are disperse in space. This paper presents a generic application paradigm based on CBR to solve geographic problems. To better consider the spatial relations, a new component of \"Geographic Environment\" was added into the standard CBR case representation model. A rough set-based algorithm was used to prune essential spatial relations, which were then used to extract key decision rules and retrieve similar past cases for the new problem. Standard CBR directly accepts the solution that was derived from the past similar cases to solve the new problem. In this study, however, solution was not accepted unless it also satisfied the key decision rules. An illustrating example was used to demonstrate how the general framework and algorithm could be applied to solve geographic problems. The algorithm was then evaluated by examining two datasets, the 2003 land use in the Pearl River Estuary and land use change from 1995 to 2000 in Zhuhai city of China. These two datasets were also examined by the standard CBR and Bayesian Networks (BNs) methods. Comparison of the stratified 10-fold cross-validation results indicated that the algorithm proposed in this study yielded statistically significant higher overall validation accuracy than the standard CBR and BNs methods. (C) 2012 Elsevier B.V. All rights reserved.",""
"Highlight detection is a fundamental step in semantics based video retrieval and personalized sports video browsing. In this paper, an effective hidden Markov models (HMMs) based soccer video event detection method based on a hierarchical video analysis framework is proposed. Soccer video shots are classified into four coarse mid-level semantics: global, median, close-up and audience. Global and local motion information is utilized for the refinement of coarse mid-level semantics. Sequential soccer video is segmented into event clips. Both the temporal transitions of the mid-level semantics and the overall features of an event clip are fused using HMMs to determine the type of event. Highlight detection performance of dynamic Bayesian networks (DBN), conditional random fields (CRF) and the proposed HMM based approach are compared. The average F-score of our highlights (including goal, shoot, foul and placed kick) detection approach is 82.92%, which outperforms that of DBN and CRF by 9.85% and 11.12% respectively. The effects of number of hidden states, overall features, and the refinement of mid-level semantics on the event detection performance are also discussed.","Soccer video shots are classified into four coarse mid-level semantics: global, median, close-up and audience."
"Pittsburgh-style learning classifier systems (LCSs), in which an entire candidate solution is represented as a set of variable number of rules, combine supervised learning with genetic algorithms (GAS) to evolve rule-based classification models. It has been shown that standard crossover operators in GAs do not guarantee an effective evolutionary search in many sophisticated problems that contain strong interactions between features. In this paper, we propose a Pittsburgh-style learning classifier system based on the Bayesian optimization algorithm with the aim of improving the effectiveness and efficiency of the rule structure exploration. In the proposed method, classifiers are generated and recombined at two levels. At the lower level, single rules contained in classifiers are produced by sampling Bayesian networks which characterize the global statistical information extracted from the current promising rules in the search space. At the higher level, classifiers are recombined by rule-wise uniform crossover operators to keep the semantics of rules in each classifier. Experimental studies on both artificial and real world binary classification problems show that the proposed method converges faster while achieving solutions with the same or even higher accuracy compared with the original Pittsburgh-style LCSs. (C) 2012 Elsevier Inc. All rights reserved.","Pittsburgh-style learning classifier systems (LCSs), in which an entire candidate solution is represented as a set of variable number of rules, combine supervised learning with genetic algorithms (GAS) to evolve rule-based classification models."
"Mystery shopping is a well known marketing technique used by companies and marketing analysts to measure quality of service, and gather information about products and services. In this article, we analyse data from mystery shopping surveys via Bayesian Networks in order to examine and evaluate the quality of service offered by the loan departments of Greek Banks. We use mystery shopping visits to collect information about loan products and services and, by this way, evaluate the customer satisfaction and plan improvement strategies that will assist banks to reach their internal standards. Bayesian Networks not only provide a pictorial representation of the dependence structure between the characteristics of interest but also allow to evaluate, interpret and understand the effects of possible improvement strategies. (c) 2012 Elsevier Ltd. All rights reserved.",""
"Background: Abattoir detected pathologies are of crucial importance to both pig production and food safety. Usually, more than one pathology coexist in a pig herd although it often remains unknown how these different pathologies interrelate to each other. Identification of the associations between different pathologies may facilitate an improved understanding of their underlying biological linkage, and support the veterinarians in encouraging control strategies aimed at reducing the prevalence of not just one, but two or more conditions simultaneously. Results: Multi-dimensional machine learning methodology was used to identify associations between ten typical pathologies in 6485 batches of slaughtered finishing pigs, assisting the comprehension of their biological association. Pathologies potentially associated with septicaemia (e.g. pericarditis, peritonitis) appear interrelated, suggesting on-going bacterial challenges by pathogens such as Haemophilus parasuis and Streptococcus suis. Furthermore, hepatic scarring appears interrelated with both milk spot livers (Ascaris suum) and bacteria-related pathologies, suggesting a potential multi-pathogen nature for this pathology. Conclusions: The application of novel multi-dimensional machine learning methodology provided new insights into how typical pig pathologies are potentially interrelated at batch level. The methodology presented is a powerful exploratory tool to generate hypotheses, applicable to a wide range of studies in veterinary research.",""
"One great challenge of genomic research is to efficiently and accurately identify complex gene regulatory networks. The development of high-throughput technologies provides numerous experimental data such as DNA sequences, protein sequence, and RNA expression profiles makes it possible to study interactions and regulations among genes or other substance in an organism. However, it is crucial to make inference of genetic regulatory networks from gene expression profiles and protein interaction data for systems biology. This study will develop a new approach to reconstruct time delay Boolean networks as a tool for exploring biological pathways. In the inference strategy, we will compare all pairs of input genes in those basic relationships by their corresponding p-scores for every output gene. Then, we will combine those consistent relationships to reveal the most probable relationship and reconstruct the genetic network. Specifically, we will prove that O(log n) state transition pairs are sufficient and necessary to reconstruct the time delay Boolean network of n nodes with high accuracy if the number of input genes to each gene is bounded. We also have implemented this method on simulated and empirical yeast gene expression data sets. The test results show that this proposed method is extensible for realistic networks.","However, it is crucial to make inference of genetic regulatory networks from gene expression profiles and protein interaction data for systems biology."
"Background: Considerable progress has been made on algorithms for learning the structure of Bayesian networks from data. Model averaging by using bootstrap replicates with feature selection by thresholding is a widely used solution for learning features with high confidence. Yet, in the context of limited data many questions remain unanswered. What scoring functions are most effective for model averaging? Does the bias arising from the discreteness of the bootstrap significantly affect learning performance? Is it better to pick the single best network or to average multiple networks learnt from each bootstrap resample? How should thresholds for learning statistically significant features be selected? Results: The best scoring functions are Dirichlet Prior Scoring Metric with small lambda and the Bayesian Dirichlet metric. Correcting the bias arising from the discreteness of the bootstrap worsens learning performance. It is better to pick the single best network learnt from each bootstrap resample. We describe a permutation based method for determining significance thresholds for feature selection in bagged models. We show that in contexts with limited data, Bayesian bagging using the Dirichlet Prior Scoring Metric (DPSM) is the most effective learning strategy, and that modifying the scoring function to penalize complex networks hampers model averaging. We establish these results using a systematic study of two well-known benchmarks, specifically ALARM and INSURANCE. We also apply our network construction method to gene expression data from the Cancer Genome Atlas Glioblastoma multiforme dataset and show that survival is related to clinical covariates age and gender and clusters for interferon induced genes and growth inhibition genes. Conclusions: For small data sets, our approach performs significantly better than previously published methods.",""
"Background: Inference about regulatory networks from high-throughput genomics data is of great interest in systems biology. We present a Bayesian approach to infer gene regulatory networks from time series expression data by integrating various types of biological knowledge. Results: We formulate network construction as a series of variable selection problems and use linear regression to model the data. Our method summarizes additional data sources with an informative prior probability distribution over candidate regression models. We extend the Bayesian model averaging (BMA) variable selection method to select regulators in the regression framework. We summarize the external biological knowledge by an informative prior probability distribution over the candidate regression models. Conclusions: We demonstrate our method on simulated data and a set of time-series microarray experiments measuring the effect of a drug perturbation on gene expression levels, and show that it outperforms leading regression-based methods in the literature.","Background: Inference about regulatory networks from high-throughput genomics data is of great interest in systems biology."
"Hepatitis C virus/human immunodeficiency virus (HCV/HIV) coinfected patients demonstrate accelerated progression to severe liver injury in comparison to HCV monoinfected patients, although the underlying mechanisms are unclear owing to infection of separate tissue compartments with two distinct viral pathogens. Microarray analysis of paired liver biopsy and peripheral blood mononuclear cell (PBMC) specimens from HCV/HIV coinfected and HCV monoinfected patients identified a gene expression signature associated with increased inflammation and immune activation that was present only in liver and PBMC samples from coinfected patients. We also identified in these samples liver- and PBMC-specific signatures enriched with fibrogenic/hepatic stellate activation and proinflammatory genes, respectively. Finally. Bayesian networks were constructed by assimilating these data with existing data from liver and PBMC samples from other cohorts, augmenting enrichment of biologically important pathways and further indicating that chronic immune activation in HCV/HIV coinfection may exacerbate liver disease progression in coinfected patients. (c) 2012 Elsevier Inc. All rights reserved.",""
"This paper describes the development of the EU Water Framework Directive central water quality elements from 1970 to 2010 in the Gulf of Finland, a eutrophied sub-basin of the Baltic Sea. The likelihood of accomplishing the management objectives simultaneously is assessed using Bayesian networks. The objectives of good ecological status in winter-time total nitrogen and phosphorus, summer-time chlorophyll-a and summer-time Secchi depth have not been met yet. In addition, the results indicate that it is unlikely for them to be achieved in the near future, despite the decreasing trend in nutrient concentrations over the past few years. It was demonstrated that neither phosphorus nor nitrogen alone controls summertime plankton growth. Reaching good ecological status in nutrients does not necessarily lead to good ecological status of chlorophyll-a, even though a dependency between the parameters does exist. In addition, secchi-depth status is strongly related to chlorophyll-a status in three of the four study-areas.",""
"Within the probabilistic risk assessment community, there is a widely acknowledged need to improve the scientific basis of human reliability analysis (HRA). This has resulted in a number of independent research efforts to gather empirical data to validate HRA methods and a number of independent research efforts to improve theoretical models of human performance used in HRA. This paper introduces a methodology for carefully combining multiple sources of empirical data with validated theoretical models to enhance both qualitative and quantitative HRA applications. The methodology uses a comprehensive set of performance influencing factors to combine data from different sources. Further, the paper describes how to use data to gather insights into the relationships among performance influencing factors and to build a quantitative HRA causal model. To illustrate how the methodology is applied, we introduce the Bayesian network model that resulted from applying the methodology to two sources of human performance data from nuclear power plant operations. The proposed model is introduced to demonstrate how to develop causal insights from HRA data and how to incorporate these insights into a quantitative HRA model. The methodology in this paper provides a path forward for carefully incorporating emerging sources of human performance data into an improved HRA method. The proposed model is a starting point for the next generation of data-informed, theoretically-validated HRA methods.",""
"The fundamental goal of conservation planning is biodiversity persistence, yet most reserve selection methods prioritize sites using occurrence data. We describe a method that integrates correlates of persistence for multiple species into a single currency - site quality. Site quality is, in turn, an explicit measure of performance used in optimization. We develop a Bayesian network to assess site quality, which assigns an expected value to a property based on criteria arrayed into a causal diagram. We then use stochastic dynamic programming to determine whether an organization should acquire or reject a site placed on the public market. Our framework for assessing sites and making land acquisition decisions represents a compromise between the use of generic spatial design criteria and more intensive computational tools, like spatially-explicit population models. There is certainly a loss of precision by using site quality as a surrogate for more direct measures of persistence. However, we believe this simplification is defensible when sufficient data, expertise, or other resources are lacking. (C) 2012 Elsevier Ltd. All rights reserved.",""
"Habitat suitability index (HSI) models rarely characterize the uncertainty associated with their estimates of habitat quality despite the fact that uncertainty can have important management implications. The purpose of this paper was to explore the use of Bayesian belief networks (BBNs) for representing and propagating 3 types of uncertainty in HSI modelsuncertainty in the suitability index relationships, the parameters of the HSI equation, and measurement of habitat variables (i.e., model inputs). I constructed a BBNHSI model, based on an existing HSI model, using Netica (TM) software. I parameterized the BBN's conditional probability tables via Monte Carlo methods, and developed a discretization scheme that met specifications for numerical error. I applied the model to both real and dummy sites in order to demonstrate the utility of the BBNHSI model for 1) determining whether sites with different habitat types had statistically significant differences in HSI, and 2) making decisions based on rules that reflect different attitudes toward riskmaximum expected value, maximin, and maximax. I also examined effects of uncertainty in the habitat variables on the model's output. Some sites with different habitat types had different values for E[HSI], the expected value of HSI, but habitat suitability was not significantly different based on the overlap of 90% confidence intervals for E[HSI]. The different decision rules resulted in different rankings of sites, and hence, different decisions based on risk. As measurement uncertainty in habitat variables increased, sites with significantly different (a?=?0.1) E[HSI] became statistically more similar. Incorporating uncertainty in HSI models enables explicit consideration of risk and more robust habitat management decisions. (c) 2012 The Wildlife Society.",""
"We propose to map a dynamic Bayesian network (DBN) to an ordered family of alpha-shapes to improve DBNs classification power. This mission is achieved by: 1) embedding a DBN into a topological manifold and 2) applying the a-shape geometric constructor to build hierarchical structures assigned to the DBN. This continuous representation of traditional DBNs as alpha-shapes allows more information to be obtained about the objects to be classified. These latter are viewed as hierarchies of geometrical objects with different levels of detail. Topological signatures are therefore unraveled and classification accuracy is enhanced. We have applied the proposed formalism to the task of facial identification across ages. Preliminary results demonstrate that the proposed formalism is a powerful tool since it has outperformed some DBN models, the k-NN classifier, and some recent approaches.","We propose to map a dynamic Bayesian network (DBN) to an ordered family of alpha-shapes to improve DBNs classification power."
"Modeling and identification of high dimensional systems, involving signals with many components, poses severe challenges to off-the-shelf techniques for system identification. This is particularly so when relatively small data sets, as compared to the number signal components, have to be used. It is often the case that each component of the measured signal can be described in terms of a few other measured variables and these dependences can be encoded in a graphical way via so called \"Dynamic Bayesian Networks\". The problem of finding the interconnection structure as well as estimating the dynamic models can be posed as a system identification problem which involves variable selection. While this variable selection could be performed via standard selection techniques, computational complexity may however be a critical issue, being combinatorial in the number of inputs and outputs. In this paper we introduce two new nonparametric techniques which borrow ideas from a recently introduced kernel estimator called \"stable-spline\" as well as from sparsity inducing priors which use l(1)-type penalties. Numerical experiments regarding estimation of large scale sparse (ARMAX) models show that this technique provides a definite advantage over a group LAR algorithm and state-of-the-art parametric identification techniques based on prediction error minimization. (C) 2012 Elsevier Ltd. All rights reserved.",""
"In this paper, we propose a new algorithm based on semantic model for inference in CLG Bayesian networks which is strongly inspired by the architecture of Madsen (in, Int J Approx Reason 49:503-521, 2008). By performing semantic modeling before physical computation, the proposed algorithm takes advantage of the semantic knowledge induced by the structure of the graph and the evidence. Thus, iteration between semantic modeling and physical computation can be avoided. Also,the presented architecture can exploit some remaining independencies in the relevant potentials which were ignored by the previous architecture. The correctness of the proposed algorithm has been proved and the resulting benefits are illustrated by examples. The results indicate a significant potential in semantic knowledge.","In this paper, we propose a new algorithm based on semantic model for inference in CLG Bayesian networks which is strongly inspired by the architecture of Madsen (in, Int J Approx Reason 49:503-521, 2008)."
"While loopy belief propagation (LBP) performs reasonably well for inference in some Gaussian graphical models with cycles, its performance is unsatisfactory for many others. In particular for some models LBP does not converge, and in general when it does converge, the computed variances are incorrect (except for cycle-free graphs for which belief propagation (BP) is non-iterative and exact). In this paper we propose feedback message passing (FMP), a message-passing algorithm that makes use of a special set of vertices (called a feedback vertex set or FVS) whose removal results in a cycle-free graph. In FMP, standard BP is employed several times on the cycle-free subgraph excluding the FVS while a special message-passing scheme is used for the nodes in the FVS. The computational complexity of exact inference is, O(k(2)n) where k is the number of feedback nodes, and n is the total number of nodes. When the size of the FVS is very large, FMP is computationally costly. Hence we propose approximate FMP, where a pseudo-FVS is used instead of an FVS, and where inference in the non-cycle-free graph obtained by removing the pseudo-FVS is carried out approximately using LBP. We show that, when approximate FMP converges, it yields exact means and variances on the pseudo-FVS and exact means throughout the remainder of the graph. We also provide theoretical results on the convergence and accuracy of approximate FMP. In particular, we prove error bounds on variance computation. Based on these theoretical results, we design efficient algorithms to select a pseudo-FVS of bounded size. The choice of the pseudo-FVS allows us to explicitly trade off between efficiency and accuracy. Experimental results show that using a pseudo-FVS of size no larger than log(n), this procedure converges much more often, more quickly, and provides more accurate results than LBP on the entire graph.","While loopy belief propagation (LBP) performs reasonably well for inference in some Gaussian graphical models with cycles, its performance is unsatisfactory for many others."
"Robust updating of parametric probabilistic models in the context of nonlinear structural mechanics represents a great challenge. A framework based on the combined use of structural reliability theory and Bayesian networks is proposed. The methodology is applied to practical engineering problems in the field of civil engineering. This approach appears as being useful to better estimate mechanical properties of an existing structure and may avoid carrying out in situ destructive tests. One of the major feature is that only in situ information available at the member scale (displacements, rotations, etc.) are required. Several structural cases are discussed to point out the main features of the methodology. (C) 2012 Elsevier Ltd. All rights reserved.",""
"We show that one can perform causal inference in a natural way for continuous-time scenarios using tools from stochastic analysis. This provides new alternatives to the positivity condition for inverse probability weighting. The probability distribution that would govern the frequency of observations in the counterfactual scenario can be characterized in terms of a so-called martingale problem. The counterfactual and factual probability distributions may be related through a likelihood ratio given by a stochastic differential equation. We can perform inference for counterfactual scenarios based on the original observations, re-weighted according to this likelihood ratio. This is possible if the solution of the stochastic differential equation is uniformly integrable, a property that can be determined by comparing the corresponding factual and counterfactual short-term predictions. Local independence graphs are directed, possibly cyclic, graphs that represent short-term prediction among sufficiently autonomous stochastic processes. We show through an example that these graphs can be used to identify and provide consistent estimators for counterfactual parameters in continuous time. This is analogous to how Judea Pearl uses graphical information to identify causal effects in finite state Bayesian networks.","We show that one can perform causal inference in a natural way for continuous-time scenarios using tools from stochastic analysis."
"Aim To develop and test a general framework for estimating weed invasion potential (suitability and susceptibility) that utilized expert knowledge of dispersal, establishment and persistence and considered the influence of land management. Location The semi-arid Desert Channels Region of Queensland, Australia (476,000 km2). Methods We developed a general framework that integrated knowledge and empirical data of the environmental and land management variables influencing the dispersal, establishment and persistence of the invasive shrub parkinsonia (Parkinsonia aculeata) using a Bayesian network linked to a Geographic Information System (GIS). We evaluated the influence of different land management scenarios on landscape suitability for parkinsonia. Model performance was assessed by comparing predicted landscape suitability with mapped parkinsonia locations and estimated parkinsonia density. Results Our predictions of moderate to high suitability corresponded reasonably well with mapped parkinsonia locations (71% match) and areas of common to abundant estimated density (92% match). They also suggested that parkinsonia has not reached its potential distribution within the study region. Under current land management conditions, 77,000 km2 of land was found to be highly or moderately suitable for parkinsonia. Scenario analysis indicated that maintaining moderate herbaceous ground cover levels, and using sheep to browse juvenile parkinsonia, reduced the predicted moderate to high suitability area to 27,000 km2, offering a potential management strategy for limiting parkinsonia invasion. Main conclusions Weed invasion potential can be reasonably estimated using expert knowledge of dispersal, establishment and persistence, integrated using a Bayesian network linked to a GIS. This modelling approach can be an alternative to process-based and phenomenological modelling, which can be problematic for modelling new and emerging weed invasions, particularly where data are patchy. The modelling approach also allows the influence of land management change on invasion potential to be investigated through scenario analysis.",""
"Accident probability estimation is a common and central step to all quantitative risk assessment methods. Among many techniques available, bow-tie model (BT) is very popular because it represent the accident scenario altogether including causes and consequences. However, it suffers a static structure limiting its application in real-time monitoring and probability updating which are key factors in dynamic risk analysis. The present work is focused on using BT approach in a dynamic environment in which the occurrence probability of accident consequences changes. In this method, on one hand, failure probability of primary events of BT, leading to the top event, are developed using physical reliability models, and constantly revised as physical parameters (e.g., pressure, velocity, dimension, etc) change. And, on the other hand, the failure probability of safety barriers of the BT are periodically updated using Bayes' theorem as new information becomes available over time. Finally, the resulting, updated BT is used to estimate the posterior probability of the consequences which in turn results in an updated risk profile. (C) 2012 Elsevier Ltd. All rights reserved.",""
"Grid computing has become relevant due to its applications to large-scale resource sharing, wide-area information transfer, and multi-institutional collaborating. In general, in grid computing a service requests the use of a set of resources, available in a grid, to complete certain tasks. Although analysis tools and techniques for these types of systems have been studied, grid reliability analysis is generally computation-intensive to obtain due to the complexity of the system. Moreover, conventional reliability models have some common assumptions that cannot be applied to the grid systems. Therefore, new analytical methods are needed for effective and accurate assessment of grid reliability. This study presents a new method for estimating grid service reliability, which does not require prior knowledge about the grid system structure unlike the previous studies. Moreover, the proposed method does not rely on any assumptions about the link and node failure rates. This approach is based on a data-mining algorithm, the K2, to discover the grid system structure from raw historical system data, that allows to find minimum resource spanning trees (MRST) within the grid then, uses Bayesian networks (BN) to model the MRST and estimate grid service reliability. (C) 2012 Elsevier Ltd. All rights reserved.",""
"Mobile devices can perceive greater details of user states with the increasing integration of mobile sensors into a pervasive computing framework, yet they consume large amounts of batteries and computational resources. This paper proposes a semantic management method which efficiently integrates multiple contexts into the mobile system by analyzing the semantic hierarchy and temporal relations. The proposed method semantically decides the recognition order of the contexts and identifies each context using a corresponding dynamic Bayesian network (DBN). To sort out the contexts, we designed a semantic network using a knowledge-driven approach, whereas DBNs are constructed with a data-driven approach. The proposed method was validated on a pervasive computing framework, which included multiple mobile sensors (such as motion sensors, data-gloves, and bio-signal sensors). Experimental results showed that the semantic management of multiple contexts dramatically reduced the recognition cost. (c) 2012 Elsevier Ltd. All rights reserved.",""
"Antipatterns provide information on commonly occurring solutions to problems that generate negative consequences. The antipattern ontology has been recently proposed as a knowledge base for SPARSE, an intelligent system that can detect the antipatterns that exist in a software project. However, apart from the plethora of antipatterns that are inherently informal and imprecise, the information used in the antipattern ontology itself is many times imprecise or vaguely defined. For example, the certainty in which a cause, symptom or consequence of an antipattern exists in a software project. Taking into account probabilistic information would yield more realistic, intelligent and effective ontology-based applications to support the technology of antipatterns. However, ontologies are not capable of representing uncertainty and the effective detection of antipatterns taking into account the uncertainty that exists in software project antipatterns still remains an open issue. Bayesian Networks (BNs) have been previously used in order to measure, illustrate and handle antipattern uncertainty in mathematical terms. In this paper, we explore the ways in which the antipattern ontology can be enhanced using Bayesian networks in order to reinforce the existing ontology-based detection process. This approach allows software developers to quantify the existence of an antipattern using Bayesian networks, based on probabilistic knowledge contained in the antipattern ontology regarding relationships of antipatterns through their causes, symptoms and consequences. The framework is exemplified using a Bayesian network model of 13 antipattern attributes, which is constructed using BNTab, a plug-in developed for the Protege ontology editor that generates BNs based on ontological information. (C) 2012 Elsevier Ltd. All rights reserved.",""
"Objective: Over the past decades, many studies have used data mining technology to predict the 5-year survival rate of colorectal cancer, but there have been few reports that compared multiple data mining algorithms to the TNM classification of malignant tumors (TNM) staging system using a dataset in which the training and testing data were from different sources. Here we compared nine data mining algorithms to the TNM staging system for colorectal survival analysis. Methods: Two different datasets were used: 1) the National Cancer Institute's Surveillance, Epidemiology, and End Results dataset; and 2) the dataset from a single Chinese institution. An optimization and prediction system based on nine data mining algorithms as well as two variable selection methods was implemented. The TNM staging system was based on the 7th edition of the American Joint Committee on Cancer TNM staging system. Results: When the training and testing data were from the same sources, all algorithms had slight advantages over the TNM staging system in predictive accuracy. When the data were from different sources, only four algorithms (logistic regression, general regression neural network, Bayesian networks, and Naive Bayes) had slight advantages over the TNM staging system. Also, there was no significant differences among all the algorithms (p>0.05). Conclusions: The TNM staging system is simple and practical at present, and data mining methods are not accurate enough to replace the TNM staging system for colorectal cancer survival prediction. Furthermore, there were no significant differences in the predictive accuracy of all the algorithms when the data were from different sources. Building a larger dataset that includes more variables may be important for furthering predictive accuracy.","Objective: Over the past decades, many studies have used data mining technology to predict the 5-year survival rate of colorectal cancer, but there have been few reports that compared multiple data mining algorithms to the TNM classification of malignant tumors (TNM) staging system using a dataset in which the training and testing data were from different sources."
"Planning of regional and urban water resources, and in particular with Integrated Urban Water Management approaches, often considers inter-relationships between human uses of water, the health of the natural environment as well as the cost of various management strategies. Decision makers hence typically need to consider a combination of social, environmental and economic goals. The types of strategies employed can include water efficiency measures, water sensitive urban design, stormwater management, or catchment management. Therefore, decision makers need to choose between different scenarios and to evaluate them against a number of criteria. This type of problem has a discipline devoted to it, i.e. Multi-Criteria Decision Analysis, which has often been applied in water management contexts. This paper describes the application of Subjective Logic in a basic Bayesian Network to a Multi-Criteria Decision Analysis problem. By doing this, it outlines a novel methodology that explicitly incorporates uncertainty and information reliability. The application of the methodology to a known case study context allows for exploration. By making uncertainty and reliability of assessments explicit, it allows for assessing risks of various options, and this may help in alleviating cognitive biases and move towards a well formulated risk management policy. (C) 2012 Elsevier B.V. All rights reserved.",""
"Many computational methods have been widely used to identify transcription regulatory interactions based on gene expression profiles. The selection of dependency measure is very important for successful regulatory network inference. In this paper, we develop a new method-DBoMM (Difference in BIC of Mixture Models)-for estimating dependency of gene by fitting the gene expression profiles into mixture Gaussian models. We show that DBoMM out-performs 4 other existing methods, including Kendall's tau correlation (TAU), Pearson Correlation (COR), Euclidean distance (EUC) and Mutual information (MI) using Escherichia coli, Saccharomyces cerevisiae, Drosophila melanogaster, Arabidopsis thaliana data and synthetic data. DBoMM can also identify condition-dependent regulatory interactions and is robust to noisy data. Of the 741 Escherichia coli regulatory interactions inferred by DBoMM at a 60% true positive rate, 65 are previously known interactions and 676 are novel predictions. To validate the new prediction, the promoter sequences of target genes regulated by the same transcription factors were analyzed and significant motifs were identified.","The selection of dependency measure is very important for successful regulatory network inference."
"This paper focuses on likelihood ratio based evaluations of fibre evidence in cases in which there is uncertainty about whether or not the reference item available for analysis - that is, an item typically taken from the suspect or seized at his home - is the item actually worn at the time of the offence. A likelihood ratio approach is proposed that, for situations in which certain categorical assumptions can be made about additionally introduced parameters, converges to formula described in existing literature. The properties of the proposed likelihood ratio approach are analysed through sensitivity analyses and discussed with respect to possible argumentative implications that arise in practice. (c) 2012 Elsevier Ireland Ltd. All rights reserved.",""
"The establishment of more efficient in vitro approaches has been widely acknowledged as a critical need for toxicity testing. In this study, we examined the effects of methylmercury (MeHg), which is a well-known developmental neurotoxicant, in two neuronal differentiation systems of mouse and human embryonic stem cells (mESCs and hESCs, respectively). Embryoid bodies were generated from gathering of mESCs and hESCs using a micro-device and seeded onto ornithine-laminin-coated plates to promote proliferation and neuronal differentiation. The cells were exposed to MeHg from the start of neuronal induction until the termination of cultures, and significant reductions of mESCs and hESCs were observed in the cell viability assays at 1,10,100 and 1000 nM, respectively. Although the mESC derivatives were more sensitive than the hESC derivatives to MeHg exposure in terms of cell viability, the morphological evaluation demonstrated that the neurite length and branch points of hESC derivatives were more susceptible to a low concentration of MeHg. Then, the mRNA levels of differentiation markers were examined using quantitative RT-PCR analysis and the interactions between MeHg exposure and gene expression levels were visualized using a network model based on a Bayesian algorithm. The Bayesian network analysis showed that a MeHg-node was located on the highest hierarchy in the hESC derivatives, but not in the mESC derivatives, suggesting that MeHg directly affect differentiation marker genes in hESCs. Taken together, effects of MeHg were observed in our neuronal differentiation systems of mESCs and hESCs using a combination of morphological and molecular markers. Our study provided possible, but limited, evidences that human ESC models might be more sensitive in particular endpoints in response to MeHg exposure than that in mouse ESC models. Further investigations that expand on the findings of the present paper may solve problems that occur when the outcomes from laboratory animals are extrapolated for human risk evaluation. (C) 2012 Elsevier Ireland Ltd. All rights reserved.",""
"Probabilistic Logic Programming (PLP), exemplified by Sato and Kameya's PRISM, Poole's ICL, Raedt et al.'s ProbLog and Vennekens et al.'s LPAD, is aimed at combining statistical and logical knowledge representation and inference. However, the inference techniques used in these works rely on enumerating sets of explanations for a query answer. Consequently, these languages permit very limited use of random variables with continuous distributions. In this paper, we present a symbolic inference procedure that uses constraints and represents sets of explanations without enumeration. This permits us to reason over PLPs with Gaussian or Gamma-distributed random variables (in addition to discrete-valued random variables) and linear equality constraints over reals. We develop the inference procedure in the context of PRISM; however the procedure's core ideas can be easily applied to other PLP languages as well. An interesting aspect of our inference procedure is that PRISM's query evaluation process becomes a special case in the absence of any continuous random variables in the program. The symbolic inference procedure enables us to reason over complex probabilistic models such as Kalman filters and a large subclass of Hybrid Bayesian networks that were hitherto not possible in PLP frameworks.","s LPAD, is aimed at combining statistical and logical knowledge representation and inference."
"With recent advances in mobile learning (m-learning), it is becoming possible for learning activities to occur everywhere. The learner model presented in our earlier work was partitioned into smaller elements in the form of learner profiles, which collectively represent the entire learning process. This paper presents an Adaptive Neuro-Fuzzy Inference System (ANFIS) for delivering adapted learning content to mobile learners. The ANFIS model was designed using trial and error based on various experiments. This study was conducted to illustrate that ANFIS is effective with hybrid learning, for the adaptation of learning content according to learners' needs. Study results show that ANFIS has been successfully implemented for learning content adaptation within different learning context scenarios. The performance of the ANFIS model was evaluated using standard error measurements which revealed the optimal setting necessary for better predictability. The MATLAB simulation results indicate that the performance of the ANFIS approach is valuable and easy to implement. The study results are based on analysis of different model settings; they confirm that the m-learning application is functional. However, it should be noted that an increase in the number of inputs being considered by the model will increase the system response time, and hence the delay for the mobile learner.","This paper presents an Adaptive Neuro-Fuzzy Inference System (ANFIS) for delivering adapted learning content to mobile learners."
"Recent advances in high-throughput biotechnologies have led to the rapid growing research interest in reverse engineering of biomolecular systems (REBMS). 'Data-driven' approaches, i.e. data mining, can be used to extract patterns from large volumes of biochemical data at molecular-level resolution while 'design-driven' approaches, i.e. systems modeling, can be used to simulate emergent system properties. Consequently, both data- and design-driven approaches applied to -omic data may lead to novel insights in reverse engineering biological systems that could not be expected before using low-throughput platforms. However, there exist several challenges in this fast growing field of reverse engineering biomolecular systems: (i) to integrate heterogeneous biochemical data for data mining, (ii) to combine top-down and bottom-up approaches for systems modeling and (iii) to validate system models experimentally. In addition to reviewing progress made by the community and opportunities encountered in addressing these challenges, we explore the emerging field of synthetic biology, which is an exciting approach to validate and analyze theoretical system models directly through experimental synthesis, i.e. analysis-by-synthesis. The ultimate goal is to address the present and future challenges in reverse engineering biomolecular systems (REBMS) using integrated workflow of data mining, systems modeling and synthetic biology.",""
"Fault tree analysis (FTA), as one of the powerful tools in reliability engineering, has been widely used to enhance system quality attributes. In most fault tree analyses, precise values are adopted to represent the probabilities of occurrence of those events. Due to the lack of sufficient data or imprecision of existing data at the early stage of product design, it is often difficult to accurately estimate the failure rates of individual events or the probabilities of occurrence of the events. Therefore, such imprecision and uncertainty need to be taken into account in reliability analysis. In this paper, the evidential networks (EN) are employed to quantify and propagate the aforementioned uncertainty and imprecision in fault tree analysis. The detailed conversion processes of some logic gates to EN are described in fault tree (FT). The figures of the logic gates and the converted equivalent EN, together with the associated truth tables and the conditional belief mass tables, are also presented in this work. The new epistemic importance is proposed to describe the effect of ignorance degree of event. The fault tree of an aircraft engine damaged by oil filter plugs is presented to demonstrate the proposed method.",""
"Common inflammatome gene signatures as well as disease-specific signatures were identified by analyzing 12 expression profiling data sets derived from 9 different tissues isolated from 11 rodent inflammatory disease models. The inflammatome signature significantly overlaps with known drug targets and co-expressed gene modules linked to metabolic disorders and cancer. A large proportion of genes in this signature are tightly connected in tissue-specific Bayesian networks (BNs) built from multiple independent mouse and human cohorts. Both the inflammatome signature and the corresponding consensus BNs are highly enriched for immune response-related genes supported as causal for adiposity, adipokine, diabetes, aortic lesion, bone, muscle, and cholesterol traits, suggesting the causal nature of the inflammatome for a variety of diseases. Integration of this inflammatome signature with the BNs uncovered 151 key drivers that appeared to be more biologically important than the non-drivers in terms of their impact on disease phenotypes. The identification of this inflammatome signature, its network architecture, and key drivers not only highlights the shared etiology but also pinpoints potential targets for intervention of various common diseases. Molecular Systems Biology 8: 594; published online 17 July 2012; doi:10.1038/msb.2012.24 Subject Categories: metabolic and regulatory networks; molecular biology of disease",""
"Online automated quality assessment is critical to determine a sensor's fitness for purpose in real-time applications. A Dynamic Bayesian Network (DBN) framework is proposed to produce probabilistic quality assessments and represent the uncertainty of sequentially correlated sensor readings. This is a novel framework to represent the causes, quality state and observed effects of individual sensor errors without imposing any constraints upon the physical deployment or measured phenomenon. It represents the casual relationship between quality tests and combines them in a way to generate uncertainty estimates of samples. The DBN was implemented for a particular marine deployment of temperature and conductivity sensors in Hobart, Australia. The DBN was shown to offer a substantial average improvement (34%) in replicating the error bars that were generated by experts when compared to a fuzzy logic approach.",""
"The paper shows why and how an empirical study of fast-and-frugal heuristics can provide norms of good reasoning, and thus how (and how far) rationality can be naturalized. We explain the heuristics that humans often rely on in solving problems, for example, choosing investment strategies or apartments, placing bets in sports, or making library searches. We then show that heuristics can lead to judgments that are as accurate as or even more accurate than strategies that use more information and computation, including optimization methods. A standard way to defend the use of heuristics is by reference to accuracy-effort trade-offs. We take a different route, emphasizing ecological rationality (the relationship between cognitive heuristics and environment), and argue that in uncertain environments, more information and computation are not always better (the \"less-can-be-more\" doctrine). The resulting naturalism about rationality is thus normative because it not only describes what heuristics people use, but also in which specific environments one should rely on a heuristic in order to make better inferences. While we desist from claiming that the scope of ecological rationality is unlimited, we think it is of wide practical use.","The resulting naturalism about rationality is thus normative because it not only describes what heuristics people use, but also in which specific environments one should rely on a heuristic in order to make better inferences."
"1. Stream reaches found to be impaired by physical, chemical or biological assessment generally are associated with greater extent of urban and agricultural land uses, and lesser amount of undeveloped lands. However, because stream condition commonly is influenced by multiple stressors as well as underlying natural gradients, it can be difficult to establish mechanistic relationships between altered land use and impaired stream condition. 2. This study explores the use of Bayesian belief networks (BBNs) to model presumed causal relationships between stressors and response variables. A BBN depicts the chain of causal relationships resulting in some outcome such as environmental impairment and can make use of evidence from expert judgment as well as observational and experimental data. 3. Three case studies illustrate the flexibility of BBN models. Expert elicitation in a workshop setting was employed to model the effects of sedimentation on benthic invertebrates. A second example used empirical data to explore the influence of natural and anthropogenic gradients on stream habitat in a highly agricultural watershed. The third application drew on several forms of evidence to develop a decision support tool linking grazing and forestry practices to stream reach condition. 4. Although data limitations challenge model development and our ability to narrow the range of possible outcomes, model formulation forces participants to conceptualise causal mechanisms and consider how to resolve data shortfalls. With sufficient effort and resources, models with greater evidentiary strength from observational and experimental data may become practical tools to guide management decisions. 5. Such models may be used to explore possible outcomes associated with a range of scenarios, thus benefiting management decision-making, and to improve insight into likely causal relationships.",""
"The explosion of genomic, transcriptomic, proteomic, metabolomic, and other omics data is challenging the research community to develop rational models for their organization and interpretation to generate novel biological knowledge. The development and use of gene regulatory networks to mechanistically interpret this data is an important development in molecular biology, usually captured under the banner of systems biology. As a result, the repertoire of methods for the reconstruction of comprehensive and cell-context-specific maps of regulatory interactions, or interactomes, has also exploded in the past few years. In this review, we focus on Network Biology and more specifically on methods for reverse engineering transcriptional, post-transcriptional, and post-translational human interaction networks and show how their interrogation is starting to impact our understanding of cellular pathophysiology and one's ability to predict cellular phenotypes from genome-wide molecular observations. WIREs Syst Biol Med 2012 doi: 10.1002/wsbm.1159 For further resources related to this article, please visit the .",""
"In this paper, we combine Leimer's algorithm with MCS-M algorithm to decompose graphical models into marginal models on prime blocks. It is shown by experiments that our method has an easier and faster implementation than Leimer's algorithm.",""
"This overview article for the special series, Bayesian Networks in Environmental and Resource Management, reviews 7 case study articles with the aim to compare Bayesian network (BN) applications to different environmental and resource management problems from around the world. The article discusses advances in the last decade in the use of BNs as applied to environmental and resource management. We highlight progress in computational methods, best-practices for model design and model communication. We review several research challenges to the use of BNs in environmental and resource management that we think may find a solution in the near future with further research attention. Integr Environ Assess Manag 2012; 8: 418429. (c) 2012 SETAC",""
"In integrated groundwater management, different knowledge frames and uncertainties need to be communicated and handled explicitly. This is necessary in order to select efficient adaptive groundwater management strategies. In this connection, Bayesian belief networks allow for integration of knowledge, for engaging stakeholders and for dealing with multiple knowledge frame uncertainties. This is illustrated for the case of the Upper Guadiana Basin, Spain, where Bayesian belief networks with stakeholder involvement were used for dealing with the ambiguities related to sustainable groundwater exploitation. Integr Environ Assess Manag 2012; 8: 430444. (c) SETAC",""
"As rehabilitation of previously channelized rivers becomes more common worldwide, flexible integrative modeling tools are needed to help predict the morphological, hydraulic, economic, and ecological consequences of the rehabilitation activities. Such predictions can provide the basis for planning and long-term management efforts that attempt to balance the diverse interests of river system stakeholders. We have previously reported on a variety of modeling methods and decision support concepts that can assist with various aspects of the river rehabilitation process. Here, we bring all of these tools together into a probability network model that links management actions, through morphological and hydraulic changes, to the ultimate ecological and economic consequences. Although our model uses a causal graph representation common to Bayesian networks, we do not limit ourselves to discrete-valued nodes or conditional Gaussian distributions as required by most Bayesian network implementations. This precludes us from carrying out easy probabilistic inference but gives us the advantages of functional and distributional flexibility and enhanced predictive accuracy, which we believe to be more important in most environmental management applications. We exemplify model application to a large, recently completed rehabilitation project in Switzerland. Integr Environ Assess Manag 2012; 8: 462472. (c) SETAC","This precludes us from carrying out easy probabilistic inference but gives us the advantages of functional and distributional flexibility and enhanced predictive accuracy, which we believe to be more important in most environmental management applications."
"Bayesian networks (BNs) are becoming increasingly common in problems with spatial aspects. The degree of spatial involvement may range from spatial mapping of BN outputs based on nodes in the BN that explicitly involve geographic features, to integration of different networks based on geographic information. In these situations, it is useful to consider how geographic information systems (GISs) could be used to enhance the conceptualization, quantification, and prediction of BNs. Here, we discuss some techniques that may be used to integrate GIS and BN models, with reference to some recent literature which illustrate these approaches. We then reflect on 2 case studies based on our own experience. The first involves the integration of GIS and a BN to assess the scientific factors associated with initiation of Lyngbya majuscula, a cyanobacterium that occurs in coastal waterways around the world. The 2nd case study involves the use of GISs as an aid for eliciting spatially informed expert opinion and expressing this information as prior distributions for a Bayesian model and as input into a BN. Elicitator, the prototype software package we developed for achieving this, is also briefly described. Whereas the 1st case study demonstrates a GIS-data driven specification of conditional probability tables for BNs with complete geographical coverage for all the data layers involved, the 2nd illustrates a situation in which we do not have complete coverage and we are forced to extrapolate based on expert judgement. Integr Environ Assess Manag 2012; 8: 473479. (c) SETAC",""
"The use of Bayesian Belief Networks (BBNs) in modeling of environmental and natural resources systems has gradually grown, and they have become one of the mainstream approaches in the field. They are typically used in modeling complex systems in which policy or management decisions must be made under high uncertainties. This article documents an approach to constructing large and highly complex BBNs using a matrix representation of the model structure. This approach allows smooth construction of highly complicated models with intricate likelihood structures. A case study of the Ganges river basin, the most populated river basin of the planet, is presented. Four different development scenarios were investigated with the purpose of reaching the Millennium Development Goals and Integrated Water Resources Management goals, both promoted by the United Nations Agencies. The model results warned against the promotion of economic development policies that do not place strong emphasis on social and environmental concerns. Integr Environ Assess Manag 2012; 8: 491502. (c) SETAC",""
"In this work a novel technique for cognitive behavioural data acquisition via computer/console games is introduced by which the user feels more relax than s/he is in a formal environment (e.g., labs and clinics) and has less disruption as s/he provides cognitive data sequence by playing a game. The method can be adapted into any game and is based on the assumption that in this way more efficient analysis of mind can be made to unveil the cognitive or mental characteristics of an individual. In experiments of the proposed work a commercial console game was utilised by different users to complete the tasks in which each game player followed his/her own optional scenarios of the game for a certain period of time. The attributes were then extracted from the behavioural video data sequence by visual inspection where each one corresponds to user's behavioural characteristics spotted throughout the game and then analysed by the Bayesian network utility. At the end of all the experiments, two types of results were obtained: semantic representation of behavioural attributes and classification of personal behavioural characteristics. The approach is proved to be a unique way and helped identify general and specific behavioural characteristics of the individuals and is likely to open new areas of applications. (C) 2012 Elsevier Ltd. All rights reserved.","At the end of all the experiments, two types of results were obtained: semantic representation of behavioural attributes and classification of personal behavioural characteristics."
"Sensors are becoming increasingly critical elements in contemporary transportation systems, gathering essential (real-time) traffic information for the planning, management and control of these complex systems. In a recent paper, Hu, Peeta and Chu introduced the interesting problem of determining the smallest subset of links in a traffic network for counting sensor installation, in such a way that it becomes possible to infer the flows on all remaining links. The problem is particularly elegant because of its limited number of assumptions. Unfortunately, path enumeration was required, which - as recognized by the authors - is infeasible for large-scale networks without further simplifying assumptions (that would destroy the assumption-free nature of the problem). In this paper, we present a reformulation of this link observability problem, requiring only node enumeration. Using this node-based approach, we prove a conjecture made by Hu, Peeta and Chu by deriving an explicit relationship between the number of nodes and links in a transportation network, and the minimum number of sensors to install in order to be able to infer all link flows. In addition, we demonstrate how the proposed method can be employed for road networks that already have sensors installed on them. Numerical examples are presented throughout. (C) 2012 Elsevier Ltd. All rights reserved.",""
"Urban water management can be challenging, but in Small Island Developing States it is particularly difficult due to resource constraints and isolation. This is the situation in the town of Tarawa in Kiribati, where attempts to improve water services have often not led to the desired outcomes. The reasons are varied, and include widely a lack of consideration of local circumstances, process requirements, and inadequate involvement of affected stakeholders, and inadequate cross-sectoral coordination. In light of the tendency in urban water planning to assume only the idealized performance of strategies, the authors argue that there is a need to also formally consider the likelihood of realizing this idealized performance. It is difficult to assess such likelihoods, other than via the use of judgments by expert and local stakeholders. Such judgments are typically qualitative and fairly abstract and often not directly concerning a particular strategy. The current paper provides a methodology to assess the likelihood of the idealized performance of strategies, based on Bayesian Networks (BNs) and Subjective Logic (SL) utilizing expert and local knowledge, creating a capacity to capture and apply previous experiences, and dispersed knowledge in decision making and planning. The methodology has been developed and tested on water management strategies in the town of Tarawa, Kiribati. As such, this paper provides a method for mapping the causal explanations for why developments do not achieve their set goals, and the approach may form the basis for assessments to be more widely applied when evaluating urban water strategies in similar contexts. In this paper, the approach has been applied by using existing data from interviews and literature to evaluate one strategy, reserve extensions and groundwater extraction. Other strategies, i.e. rainwater harvesting, desalination and have also been evaluated but have not been described in this paper because of limited space. (C) 2012 Elsevier Ltd. All rights reserved.",""
"Recent years have witnessed a rapid rise in the development of deterministic and non-deterministic models to estimate human impacts on the environment. An important failing of these models is the difficulty that most people have understanding the results generated by them, the implications to their way of life and also that of future generations. Within the field, the measurement of greenhouse gas emissions (GHG) is one such result. The research described in this paper evaluates the potential of Bayesian Network (BN) models for the task of managing GHG emissions in the British agricultural sector. Case study farms typifying the British agricultural sector were inputted into both, the BN model and CALM, a Carbon accounting tool used by the Country Land and Business Association (CLA) in the UK for the same purpose. Preliminary results show that the BN model provides a better understanding of how the tasks carried out on a farm impact the environment through the generation of GHG emissions. This understanding is achieved by translating the emissions information into their cost in monetary terms using the Shadow Price of Carbon (SPC), something that is not possible using the CALM tool. In this manner, the farming sector should be more inclined to deploy measures for reducing its impact. At the same time, the output of the analysis can be used to generate a business plan that will not have a negative effect on a farm's capital income. (C) 2012 Elsevier Ltd. All rights reserved.",""
"Reconstruction of a biological system from its experimental time series data is a challenging task in systems biology. The S-system which consists of a group of nonlinear ordinary differential equations (ODEs) is an effective model to characterize molecular biological systems and analyze the system dynamics. However, inference of S-systems without the knowledge of system structure is not a trivial task due to its nonlinearity and complexity. In this paper, a pruning separable parameter estimation algorithm (PSPEA) is proposed for inferring S-systems. This novel algorithm combines the separable parameter estimation method (SPEM) and a pruning strategy, which includes adding an l(1) regularization term to the objective function and pruning the solution with a threshold value. Then, this algorithm is combined with the continuous genetic algorithm (CGA) to form a hybrid algorithm that owns the properties of these two combined algorithms. The performance of the pruning strategy in the proposed algorithm is evaluated from two aspects: the parameter estimation error and structure identification accuracy. The results show that the proposed algorithm with the pruning strategy has much lower estimation error and much higher identification accuracy than the existing method.","However, inference of S-systems without the knowledge of system structure is not a trivial task due to its nonlinearity and complexity."
"Reducing the computational complexity of inference in Bayesian Networks (BNs) is a key challenge. Current algorithms for inference convert a BN to a junction tree structure made up of clusters of the BN nodes and the resulting complexity is time exponential in the size of a cluster. The need to reduce the complexity is especially acute where the BN contains continuous nodes. We propose a new method for optimizing the calculation of Conditional Probability Tables (CPTs) involving continuous nodes, approximated in Hybrid Bayesian Networks (HBNs), using an approximation algorithm called dynamic discretization. We present an optimized solution to this problem involving binary factorization of the arithmetical expressions declared to generate the CPTs for continuous nodes for deterministic functions and statistical distributions. The proposed algorithm is implemented and tested in a commercial Hybrid Bayesian Network software package and the results of the empirical evaluation show significant performance improvement over unfactorized models.","Reducing the computational complexity of inference in Bayesian Networks (BNs) is a key challenge."
"We discuss two issues in using mixtures of polynomials (MOPS) for inference in hybrid Bayesian networks. MOPs were proposed by Shenoy and West for mitigating the problem of integration in inference in hybrid Bayesian networks. First, in defining MOP for multidimensional functions, one requirement is that the pieces where the polynomials are defined are hypercubes. In this paper, we discuss relaxing this condition so that each piece is defined on regions called hyper-rhombuses. This relaxation means that MOPs are closed under transformations required for multi-dimensional linear deterministic conditionals, such as Z = X + Y, etc. Also, this relaxation allows us to construct MOP approximations of the probability density functions (PDFs) of the multi-dimensional conditional linear Gaussian distributions using a MOP approximation of the PDF of the univariate standard normal distribution. Second, Shenoy and West suggest using the Taylor series expansion of differentiable functions for finding MOP approximations of PDFs. In this paper, we describe a new method for finding MOP approximations based on Lagrange interpolating polynomials (LIP) with Chebyshev points. We describe how the LIP method can be used to find efficient MOP approximations of PDFs. We illustrate our methods using conditional linear Gaussian PDFs in one, two, and three dimensions, and conditional log-normal PDFs in one and two dimensions. We compare the efficiencies of the hyper-rhombus condition with the hypercube condition. Also, we compare the LIP method with the Taylor series method. (C) 2012 Elsevier Inc. All rights reserved.","We discuss two issues in using mixtures of polynomials (MOPS) for inference in hybrid Bayesian networks."
"The selection supplier problem has received a lot of attention from academics in recent years. Several models were developed in the literature, combining consolidated operations research and artificial intelligence methods and techniques. However, the tools presented in the literature neglected learning and adaptation, since this decision making process is approached as a static one rather than a highly dynamic process. Delays, lack of capacity, quality related issues are common examples of dynamic aspects that have a direct impact on long-term relationships with suppliers. This paper presents a novel method based on the integration of influence diagram and fuzzy logic to rank and evaluate suppliers. The model was developed to support managers in exploring the strengths and weaknesses of each alternative, to assist the setting of priorities between conflicting criteria, to study the sensitivity of the behavior of alternatives to changes in underlying decision situations, and finally to identify a preferred course of action. To be effective, the computational implementation of the method was embedded into an information system that includes several functionalities such as supply chain simulation and supplier's databases. A case study in the biodiesel supply chain illustrates the effectiveness of the developed method. (C) 2012 Elsevier Ltd. All rights reserved.",""
"A computer system based on Monte-Carlo simulation and fuzzy logic has been designed, developed and tested to: (i) identify covariates that influence remittances received in a specific country and (ii) explain their behavior throughout the time span involved. The resulting remittance model was designed theoretically, identifying the variables which determined remittances and their dependence relationships, and then developed into a computer cluster. This model aims to be global and is useful for assessing the long term evolution of remittances in scenarios where a rich country is the host (United States of America) while a poor country is the where the migrant is from (El Salvador). By changing the socio-economic characteristics of the countries involved, experts can analyze new socio-economic frameworks to obtain useful conclusions for decision-making processes involving development and sustainability. (C) 2012 Elsevier Ltd. All rights reserved.",""
"This paper describes a Bayesian method for learning causal Bayesian networks through networks that contain latent variables from an arbitrary mixture of observational and experimental data. The paper presents Bayesian methods (including a new method) for learning the causal structure and parameters of the underlying causal process that is generating the data, given that the data contain a mixture of observational and experimental cases. These learning methods were applied using as input various mixtures of experimental and observational data that were generated from the ALARM causal Bayesian network. The paper reports how these structure predictions and parameter estimates compare with the true causal structures and parameters as given by the ALARM network. The paper shows that (1) the new method for learning Bayesian network structure from a mixture of data that this paper introduce, the Gibbs Volume method, best estimates the probability of the data, given the latent variable model and (2) using large data (> 10,000 cases), another model, the implicit latent variable method, is asymptotically correct and efficient. (C) 2012 Elsevier B.V. All rights reserved.",""
"Predicting the biological function of all the genes of an organism is one of the fundamental goals of computational system biology. In the last decade, high-throughput experimental methods for studying the functional interactions between gene products (GPs) have been combined with computational approaches based on Bayesian networks for data integration. The result of these computational approaches is an interaction network with weighted links representing connectivity likelihood between two functionally related GPs. The weighted network generated by these computational approaches can be used to predict annotations for functionally uncharacterized GPs. Here we introduce Weighted Network Predictor (WNP), a novel algorithm for function prediction of biologically uncharacterized GPs. Tests conducted on simulated data show that WNP outperforms other 5 state-of-the-art methods in terms of both specificity and sensitivity and that it is able to better exploit and propagate the functional and topological information of the network. We apply our method to Saccharomyces cerevisiae yeast and Arabidopsis thaliana networks and we predict Gene Ontology function for about 500 and 10000 uncharacterized GPs respectively.",""
"Background: Transcription factors and microRNAs act in concert to regulate gene expression in eukaryotes. Numerous computational methods based on sequence information are available for the prediction of target genes of transcription factors and microRNAs. Although these methods provide a static snapshot of how genes may be regulated, they are not effective for the identification of condition-specific regulators. Results: We propose a new method that combines: a) transcription factors and microRNAs that are predicted to target genes in pathways, with b) microarray expression profiles of microRNAs and mRNAs, in conjunction with c) the known structure of molecular pathways. These elements are integrated into a Bayesian network derived from each pathway that, through probability inference, allows for the prediction of the key regulators in the pathway. We demonstrate 1) the steps to discretize the expression data for the computation of conditional probabilities in a Bayesian network, 2) the procedure to construct a Bayesian network using the structure of a known pathway and the transcription factors and microRNAs predicted to target genes in that pathway, and 3) the inference results as potential regulators of three signaling pathways using microarray expression profiles of microRNA and mRNA in estrogen receptor positive and estrogen receptor negative tumors. Conclusions: We displayed the ability of our framework to integrate multiple sets of microRNA and mRNA expression data, from two phenotypes, with curated molecular pathway structures by creating Bayesian networks. Moreover, by performing inference on the network using known evidence, e. g., status of differentially expressed genes, or by entering hypotheses to be tested, we obtain a list of potential regulators of the pathways. This, in turn, will help increase our understanding about the regulatory mechanisms relevant to the two phenotypes.","These elements are integrated into a Bayesian network derived from each pathway that, through probability inference, allows for the prediction of the key regulators in the pathway."
"Background: Understanding gene interactions is a fundamental question in systems biology. Currently, modeling of gene regulations using the Bayesian Network (BN) formalism assumes that genes interact either instantaneously or with a certain amount of time delay. However in reality, biological regulations, both instantaneous and time-delayed, occur simultaneously. A framework that can detect and model both these two types of interactions simultaneously would represent gene regulatory networks more accurately. Results: In this paper, we introduce a framework based on the Bayesian Network (BN) formalism that can represent both instantaneous and time-delayed interactions between genes simultaneously. A novel scoring metric having firm mathematical underpinnings is also proposed that, unlike other recent methods, can score both interactions concurrently and takes into account the reality that multiple regulators can regulate a gene jointly, rather than in an isolated pair-wise manner. Further, a gene regulatory network (GRN) inference method employing an evolutionary search that makes use of the framework and the scoring metric is also presented. Conclusion: By taking into consideration the biological fact that both instantaneous and time-delayed regulations can occur among genes, our approach models gene interactions with greater accuracy. The proposed framework is efficient and can be used to infer gene networks having multiple orders of instantaneous and time-delayed regulations simultaneously. Experiments are carried out using three different synthetic networks (with three different mechanisms for generating synthetic data) as well as real life networks of Saccharomyces cerevisiae, E. coli and cyanobacteria gene expression data. The results show the effectiveness of our approach.","Further, a gene regulatory network (GRN) inference method employing an evolutionary search that makes use of the framework and the scoring metric is also presented."
"Background: The half-life of a protein is regulated by a range of system properties, including the abundance of components of the degradative machinery and protein modifiers. It is also influenced by protein-specific properties, such as a protein's structural make-up and interaction partners. New experimental techniques coupled with powerful data integration methods now enable us to not only investigate what features govern protein stability in general, but also to build models that identify what properties determine each protein's metabolic stability. Results: In this work we present five groups of features useful for predicting protein stability: (1) post-translational modifications, (2) domain types, (3) structural disorder, (4) the identity of a protein's N-terminal residue and (5) amino acid sequence. We incorporate these features into a predictive model with promising accuracy. At a 20% false positive rate, the model exhibits an 80% true positive rate, outperforming the only previously proposed stability predictor. We also investigate the impact of N-terminal protein tagging as used to generate the data set, in particular the impact it may have on the measurements for secreted and transmembrane proteins; we train and test our model on a subset of the data with those proteins removed, and show that the model sustains high accuracy. Finally, we estimate system-wide metabolic stability by surveying the whole human proteome. Conclusions: We describe a variety of protein features that are significantly over-or under-represented in stable and unstable proteins, including phosphorylation, acetylation and destabilizing N-terminal residues. Bayesian networks are ideal for combining these features into a predictive model with superior accuracy and transparency compared to the only other proposed stability predictor. Furthermore, our stability predictions of the human proteome will find application in the analysis of functionally related proteins, shedding new light on regulation by protein synthesis and degradation.",""
"The estimation of Bayesian networks given high-dimensional data, in particular gene expression data, has been the focus of much recent research. Whilst there are several methods available for the estimation of such networks, these typically assume that the data consist of independent and identically distributed samples. It is often the case, however, that the available data have a more complex mean structure, plus additional components of variance, which must then be accounted for in the estimation of a Bayesian network. In this paper, score metrics that take account of such complexities are proposed for use in conjunction with score-based methods for the estimation of Bayesian networks. We propose first, a fully Bayesian score metric, and second, a metric inspired by the notion of restricted maximum likelihood. We demonstrate the performance of these new metrics for the estimation of Bayesian networks using simulated data with known complex mean structures. We then present the analysis of expression levels of grape-berry genes adjusting for exogenous variables believed to affect the expression levels of the genes. Demonstrable biological effects can be inferred from the estimated conditional independence relationships and correlations amongst the grape-berry genes.",""
"In this paper we propose a Bayesian approach for inference about dependence of high throughput gene expression. Our goals are to use prior knowledge about pathways to anchor inference about dependence among genes; to account for this dependence while making inferences about differences in mean expression across phenotypes; and to explore differences in the dependence itself across phenotypes. Useful features of the proposed approach are a model-based parsimonious representation of expression as an ordinal outcome, a novel and flexible representation of prior information on the nature of dependencies, and the use of a coherent probability model over both the structure and strength of the dependencies of interest. We evaluate our approach through simulations and in the analysis of data on expression of genes in the Complement and Coagulation Cascade pathway in ovarian cancer.","In this paper we propose a Bayesian approach for inference about dependence of high throughput gene expression."
"In this paper we propose an algorithm for answering queries in hybrid Bayesian networks where the underlying probability distribution is of class MTE (mixture of truncated exponentials). The algorithm is based on importance sampling simulation. We show how, like existing importance sampling algorithms for discrete networks, it is able to provide answers to multiple queries simultaneously using a single sample. The behaviour of the new algorithm is experimentally tested and compared with previous methods existing in the literature. (C) 2012 Elsevier B.V. All rights reserved.",""
"We introduce a graphical framework for Bayesian inference that is sufficiently general to accommodate not just the standard case but also recent proposals for a theory of quantum Bayesian inference wherein one considers density operators rather than probability distributions as representative of degrees of belief. The diagrammatic framework is stated in the graphical language of symmetric monoidal categories and of compact structures and Frobenius structures therein, in which Bayesian inversion boils down to transposition with respect to an appropriate compact structure. We characterize classical Bayesian inference in terms of a graphical property and demonstrate that our approach eliminates some purely conventional elements that appear in common representations thereof, such as whether degrees of belief are represented by probabilities or entropic quantities. We also introduce a quantum-like calculus wherein the Frobenius structure is noncommutative and show that it can accommodate Leifer's calculus of 'conditional density operators'. The notion of conditional independence is also generalized to our graphical setting and we make some preliminary connections to the theory of Bayesian networks. Finally, we demonstrate how to construct a graphical Bayesian calculus within any dagger compact category.","We introduce a graphical framework for Bayesian inference that is sufficiently general to accommodate not just the standard case but also recent proposals for a theory of quantum Bayesian inference wherein one considers density operators rather than probability distributions as representative of degrees of belief."
"Objective To develop a tool for evaluating the risk that an outbreak of meningitis will occur in a particular district of the Niger after outbreaks have been reported in other, specified districts of the country. Methods A Bayesian network was represented by a graph composed of 38 nodes (one for each district in the Niger) connected by arrows. In the graph, each node directly influenced each of the \"child\" nodes that lay at the ends of the arrows arising from that node, according to conditional probabilities. The probabilities between \"influencing\" and \"influenced\" districts were estimated by analysis of databases that held weekly records of meningitis outbreaks in the Niger between 1986 and 2005. For each week of interest, each district was given a Boolean-variable score of 1 (if meningitis incidence in the district reached an epidemic threshold in that week) or 0. Findings The Bayesian network approach provided important and original information, allowing the identification of the districts that influence meningitis risk in other districts (and the districts that are influenced by any particular district) and the evaluation of the level of influence between each pair of districts. Conclusion Bayesian networks offer a promising approach to understanding the dynamics of epidemics, estimating the risk of outbreaks in particular areas and allowing control interventions to be targeted at high-fisk areas.",""
"Reliability block diagrams (RBDs) depict the functional relationships between components comprising a system, tem, whereas Bayesian networks (BNs) represent probabilistic relationships between uncertain variables. Previous research has described how one can transform an RBD into a BN. In parallel, developments in the artificial intelligence literature have shown how a BN can be transformed into another graphical representation, an arithmetic circuit, which can subsequently be used for efficient inference. In this paper, we introduce a new graphical representation that we call a fault tree circuit, which is a special kind of arithmetic circuit constructed specifically for an RBD. A fault tree circuit can be constructed directly from an RBD and is more efficient than an arithmetic circuit that is compiled from the BN corresponding to that RBD. We develop several methods for fault tree circuits, highlighting how they can aid the analyst in efficient diagnosis, sensitivity analysis, and decision support for many typical reliability problems. The circuit framework can complement tools that are popular in the reliability analysis community. We use a simple pump system example to illustrate the concepts.","In parallel, developments in the artificial intelligence literature have shown how a BN can be transformed into another graphical representation, an arithmetic circuit, which can subsequently be used for efficient inference."
"We describe the use of Bayesian networks as a tool for nanomaterial risk forecasting and develop a baseline probabilistic model that incorporates nanoparticle specific characteristics and environmental parameters, along with elements of exposure potential, hazard, and risk related to nanomaterials. The baseline model, FINE (Forecasting the Impacts of Nanomaterials in the Environment), was developed using expert elicitation techniques. The Bayesian nature of FINE allows for updating as new data become available, a critical feature for forecasting risk in the context of nanomaterials. The specific case of silver nanoparticles (AgNPs) in aquatic environments is presented here (FINEAgNP). The results of this study show that Bayesian networks provide a robust method for formally incorporating expert judgments into a probabilistic measure of exposure and risk to nanoparticles, particularly when other knowledge bases may be lacking. The model is easily adapted and updated as additional experimental data and other information on nanoparticle behavior in the environment become available. The baseline model suggests that, within the bounds of uncertainty as currently quantified, nanosilver may pose the greatest potential risk as these particles accumulate in aquatic sediments. (C) 2012 Elsevier B.V. All rights reserved.",""
"We propose a probabilistic framework in which different types of information pertaining to the recurrence of large earthquakes on a fault can be combined in order to constrain the parameter space of candidate recurrence models and provide the best combination of models knowing the chosen data set and priors. We use Bayesian inference for parameter and error estimation, graphical models (Bayesian networks) for modeling, and stochastic modeling to link cumulative offsets (CO) to coseismic slip. The cumulative offset-based Bayesian approach (COBBRA) method (Fitzenz et al., 2010) was initially developed to use CO data to further constrain and discriminate between recurrence models built from historical and archaeological catalogs of large earthquakes (CLE). We discuss this method and present an extension of it that incorporates trench data (TD). For our case study, the Jordan Valley fault (JVF), the relative evidence of each model slightly favors the Brownian passage time (BPT) and lognormal models. We emphasize that (1) the time variability of fault slip rate is critical to constrain recurrence models; (2) the shape of the probability density functions (PDF) of paleoseismic events is very important, in most cases not Gaussian, and should be reported in its complexity; (3) renewal models are in terms of intervals between consecutive earthquakes, not dates, and the algorithms should account for that fact; and (4) maximum-likelihood methods are inadequate for parameter uncertainty evaluation and model combination or ranking. Finally, more work is needed to define proper priors and to model the relationship between cumulative slip and coseismic slip, in particular, when the fault behavior is more complex.","We use Bayesian inference for parameter and error estimation, graphical models (Bayesian networks) for modeling, and stochastic modeling to link cumulative offsets (CO) to coseismic slip."
"Precise comprehension of a file system state at any given time is vital for performing digital forensic analyses. To uncover evidence of the digital crime, the logical representation of file system activities helps reconstruct post-event timeline of the unauthorized or malicious accesses made on a system. This paper describes a comparative performance analysis of the Bayesian networks and neural networks techniques to classify the state of file system activities in terms of execution of applications based on the pattern of manipulation of specific files during certain period of time. In particular, this paper discusses the construction of a Bayesian networks and neural networks from the predetermined knowledge of the manipulation of file system artifacts and their corresponding metadata information by a set of software applications. The variability amongst the execution patterns of various applications indicate that the Bayesian network-based model is a more appropriate tool as compared to neural networks because of its ability to learn and detect patterns even from an incomplete dataset. The focus of this paper is to highlight intrinsic significance of the learning approach of Bayesian network methodology in comparison to the techniques used for supervised learning in ordinary neural networks. The paper also highlights the efficacy of Bayesian network technique to proficiently handle large volumes of datasets. (C) 2012 Elsevier Ltd. All rights reserved.",""
"A stochastic demand dynamic traffic model is presented to predict some traffic variables, such as link travel times, link flows, or link densities, and their time evolution in real networks. The model considers that the variables are generalized beta variables such that when they are marginally transformed to standard normal, they become multivariate normal. This gives sufficient degrees of freedom to reproduce (approximate) the considered variables at a discrete set of time-location pairs. Two options to learn the parameters of the model are provided-one based on previous observations of the same variables and one based on simulated data using existing dynamic models. The model is able to provide a point estimate, a confidence interval, or the density of the variable being predicted. To this end, a closed formula for the conditional future variable values (link travel times or flows), given the available past variable information, is provided. Since only local information is relevant to short-term link flow predictions, the model is applicable to very large networks. The following three examples of application are given: 1) the Nguyen-Dupuis network; 2) the Ciudad Real network; and 3) the Vermont state network. The resulting traffic predictions seem to be promising for real traffic networks and can be done in real time.",""
"Prognostics activity deals with the estimation of the Remaining Useful Life (RUL) of physical systems based on their current health state and their future operating conditions. RUL estimation can be done by using two main approaches, namely model-based and dat-driven approaches. The first approach is based on the utilization of physics of failure models of the degradation, while the second approach is based on the transformation of the data provided by the sensors into models that represent the behavior of the degradation. This paper deals with a dat-driven prognostics method, where the RUL of the physical system is assessed depending on its critical component. Once the critical component is identified, and the appropriate sensors installed, the data provided by these sensors are exploited to model the degradation's behavior. For this purpose, Mixture of Gaussians Hidden Markov Models (MoG-HMMs), represented by Dynamic Bayesian Networks (DBNs), are used as a modeling tool. MoG-HMMs allow us to represent the evolution of the component's health condition by hidden states by using temporal or frequency features extracted from the raw signals provided by the sensors. The prognostics process is then done in two phases: a learning phase to generate the behavior model, and an exploitation phase to estimate the current health state and calculate the RUL. Furthermore, the performance of the proposed method is verified by implementing prognostics performance metrics, such as accuracy, precision, and prediction horizon. Finally, the proposed method is applied to real data corresponding to the accelerated life of bearings, and experimental results are discussed.",""
"There is a growing need for both science and practice domains to collaboratively and systematically seek knowledge-based strategies for sustainable development. In recent years, transdisciplinary research has emerged as a new approach that enables joint problem solving among scientists and stakeholders in various fields. In this paper, we aim to introduce transdisciplinary research for supporting the integration of the concept of ecosystem services into land and water management in the Tarim River Basin, Xinjiang, Northwest China. While a large number of ecosystem service studies have helped to raise the awareness for the value of nature in China, a number of challenges remain, including an improved understanding of the relationships between ecosystem structure, functions and services, and the interaction of the various ecosystem services. A meaningful valuation of ecosystem services also requires the consideration of their strong spatial heterogeneity. In addition, ways to introduce the concept of ecosystem services into decision-making in China need to be explored. Thus, successful integration of the concept of ecosystem services into actual land and water management requires a broad knowledge base that only a number of scientific disciplines and stakeholders can provide jointly, via a transdisciplinary research process. We regard transdisciplinary research as a recursive process to support adaptive management that includes joint knowledge generation and integration among scientists and stakeholders. System, target, and transformation knowledge are generated and integrated during the process of (1) problem (re)definition, (2) problem analysis and strategy development, and (3) evaluation of the impact of the derived strategy. Methods to support transdisciplinary research comprise participatory modelling (actor-based modelling and Bayesian Networks modelling) and participatory scenario development. Actor-based modelling is a semi-quantitative method that is based on the analysis of problem perspectives of individual stakeholders as depicted in perception graphs. With Bayesian Networks, complex problem fields are modelled probabilistically in a simplified manner, using both quantitative data and qualitative expert judgments. These participatory methods serve to integrate diverse scientific and stakeholder knowledge and to support the generation of actually implementable management strategies for sustainable development. For the purpose of integrating ecosystem services in land and water management in the Tarim River Basin through transdisciplinary research, collaboration among scientists and institutional stakeholders from different sectors including water, agriculture, forestry, and nature conservation is required. The challenge is to adapt methods of transdisciplinary research to socio-cultural conditions in China, particularly regarding ways of communication and decision-making.",""
"In this study, quality grading of raisins using image processing and data mining based classifiers was investigated. Images from four different classes of raisins (green, green with tail, black, and black with tail) were acquired using a color CCD camera. After pre-processing and segmentation of images, 44 features including 36 color and eight shape features were extracted. Correlation-based feature selection was used to select best features for grading the raisins. Seven features were found superior. To classify raisins, four different data mining-based techniques including artificial neural networks (ANNs), support vector machines (SVMs), decision trees (DTs) and Bayesian networks (BNs) were investigated. Results of validation stage showed ANN with 7-6-4 topology had the highest classification accuracy, 96.33%. After ANN, SVM with polynomial kernel function (95.67%), DT with J48 algorithm (94.67%) and BN with simulated annealing learning (94.33%) had higher accuracy, respectively. Results of this research can be adapted for developing an efficient raisin sorting system. (C) 2012 Elsevier By. All rights reserved.","In this study, quality grading of raisins using image processing and data mining based classifiers was investigated."
"We address the problem of bootstrapping language acquisition for an artificial system similarly to what is observed in experiments with human infants. Our method works by associating meanings to words in manipulation tasks, as a robot interacts with objects and listens to verbal descriptions of the interactions. The model is based on an affordance network, i.e., a mapping between robot actions, robot perceptions, and the perceived effects of these actions upon objects. We extend the affordance model to incorporate spoken words, which allows us to ground the verbal symbols to the execution of actions and the perception of the environment. The model takes verbal descriptions of a task as the input and uses temporal co-occurrence to create links between speech utterances and the involved objects, actions, and effects. We show that the robot is able form useful word-to-meaning associations, even without considering grammatical structure in the learning process and in the presence of recognition errors. These word-to-meaning associations are embedded in the robot's own understanding of its actions. Thus, they can be directly used to instruct the robot to perform tasks and also allow to incorporate context in the speech recognition task. We believe that the encouraging results with our approach may afford robots with a capacity to acquire language descriptors in their operation's environment as well as to shed some light as to how this challenging process develops with human infants.",""
"Background Identification of key factors associated with the risk of adverse cardiovascular events and quantification of this risk using multivariable prediction algorithms are among the major advances made in preventive cardiology and cardiovascular epidemiology. Methods In the present paper, we examined clinical predictors of adverse cardiovascular events among 228 individuals with symptoms suggestive of coronary artery disease (CAD) undergoing functional (stress echocardiography) and anatomical (coronary angiography) assessment of CAD. Particularly, we evaluate the possibility to integrate simple measures that have known prognostic value and more recently discovered predictors of risk, such as stress-related ventricular function data and angiographic data, in a unique model implementing a Bayesian network (BN). Moreover, we compared the performance of BN and the covariates hierarchy with those obtained from logistic regression model and from a set of alternative tools becoming popular in various clinical settings, including random forest classification tree analysis, artificial neural networks and support vector machine. Results Network graph and results coming from sensitivity analysis, where variables are ranked according to the gain they provided in variance reduction, seem have an easily intuitive lecture: variables that are measure of ventricular disfunction or of the extent of CAD show a greater impact in predicting event. On the other hand, anamnestic data such as diabetes, dyslipidaemia, hypertension, smoke habits, which are related to the outcome throughout a process of intermediate variables, per se have a small role in outcome prediction. BNs are able to explain a relevant part of variance (70%) and have discrimination ability superior or comparable with those to random forest classification tree analysis, artificial neural networks and support vector machine. Discussion Despite the complexity of interactions, model obtained implementing a BN seems to be able to adequately describe the relationships existing among the analysed variables. BN, being able to predict scenarios in which new variables can be incorporated as health process evolves, can measure individual's risks for adverse cardiovascular events, providing a permanent second opinion to the medical practitioner and assisting diagnostic and therapeutic process.","Moreover, we compared the performance of BN and the covariates hierarchy with those obtained from logistic regression model and from a set of alternative tools becoming popular in various clinical settings, including random forest classification tree analysis, artificial neural networks and support vector machine."
"To cite this article: Sharma S, Poon A, Himes BE, Lasky-Su J, Sordillo JE, Belanger K, Milton DK, Bracken MB, Triche EW, Leaderer BP, Gold DR, Litonjua AA. Association of variants in innate immune genes with asthma and eczema. Pediatric Allergy Immunology 2012: 23: 315323. Abstract Background: The innate immune pathway is important in the pathogenesis of asthma and eczema. However, only a few variants in these genes have been associated with either disease. We investigate the association between polymorphisms of genes in the innate immune pathway with childhood asthma and eczema. In addition, we compare individual associations with those discovered using a multivariate approach. Methods: Using a novel method, case control based association testing (C2BAT), 569 single nucleotide polymorphisms (SNPs) in 44 innate immune genes were tested for association with asthma and eczema in children from the Boston Home Allergens and Asthma Study and the Connecticut Childhood Asthma Study. The screening algorithm was used to identify the top SNPs associated with asthma and eczema. We next investigated the interaction of innate immune variants with asthma and eczema risk using Bayesian networks. Results: After correction for multiple comparisons, 7 SNPs in 6 genes (CARD25, TGFB1, LY96, ACAA1, DEFB1, and IFNG) were associated with asthma (adjusted p-value<0.02), while 5 SNPs in 3 different genes (CD80, STAT4, and IRAKI) were significantly associated with eczema (adjusted p-value < 0.02). None of these SNPs were associated with both asthma and eczema. Bayesian network analysis identified 4 SNPs that were predictive of asthma and 10 SNPs that predicted eczema. Of the genes identified using Bayesian networks, only CD80 was associated with eczema in the single-SNP study. Using novel methodology that allows for screening and replication in the same population, we have identified associations of innate immune genes with asthma and eczema. Bayesian network analysis suggests that additional SNPs influence disease susceptibility via SNP interactions. Conclusion: Our findings suggest that innate immune genes contribute to the pathogenesis of asthma and eczema, and that these diseases likely have different genetic determinants.",""
"It is well known that the accuracy of importance sampling can be improved by reducing the variance of its sample mean and therefore variance reduction schemes have been the subject of much research. In this paper, we introduce a family of variance reduction schemes that generalize the sample mean from the conventional OR search space to the AND/OR search space for graphical models. The new AND/OR sample means allow trading time and space with variance. At one end is the AND/OR sample tree mean which has the same time and space complexity as the conventional OR sample tree mean but has smaller variance. At other end is the AND/OR sample graph mean which requires more time and space to compute but has the smallest variance. Theoretically, we show that the variance is smaller in the AND/OR space because the AND/OR sample mean is defined over a larger virtual sample size compared with the OR sample mean. Empirically, we demonstrate that the AND/OR sample mean is far closer to the true mean than the OR sample mean. (C) 2012 Elsevier B.V. All rights reserved.",""
"This paper deals with an important probabilistic knowledge integration problem: revising a Bayesian network (BN) to satisfy a set of probability constraints representing new or more specific knowledge. We propose to solve this problem by adopting IPFP (iterative proportional fitting procedure) to BN. The resulting algorithm E-IPFP integrates the constraints by only changing the conditional probability tables (CPT) of the given BN while preserving the network structure; and the probability distribution of the revised BN is as close as possible to that of the original BN. Two variations of E-IPFP are also proposed: 1) E-IPFP-SMOOTH which deals with the situation where the probabilistic constraints are inconsistent with each other or with the network structure of the given BN; and 2) D-IPFP which reduces the computational cost by decomposing a global E-IPFP into a set of smaller local E-IPFP problems.",""
"Endovascular aneurysm repair (EVAR) is an advanced minimally invasive surgical technology that is helpful for reducing patients' recovery time, postoperative morbidity and mortality. This study proposes an ensemble model to predict postoperative morbidity after EVAR. The ensemble model was developed using a training set of consecutive patients who underwent EVAR between 2000 and 2009. All data required for prediction modeling, including patient demographics, preoperative, co-morbidities, and complication as outcome variables, was collected prospectively and entered into a clinical database. A discretization approach was used to categorize numerical values into informative feature space. Then, the Bayesian network (BN), artificial neural network (ANN), and support vector machine (SVM) were adopted as base models, and stacking combined multiple models. The research outcomes consisted of an ensemble model to predict postoperative morbidity after EVAR, the occurrence of postoperative complications prospectively recorded, and the causal effect knowledge by BNs with Markov blanket concept.",""
"Personal agents gather information about users in a user profile. In this work, we propose a novel ontology-based user profile learning. Particularly, we aim to learn context-enriched user profiles using data mining techniques and ontologies. We are interested in knowing to what extent data mining techniques can be used for user profile generation, and how to utilize ontologies for user profile improvement. The objective is to semantically enrich a user profile with contextual information by using association rules, Bayesian networks and ontologies in order to improve agent performance. At runtime, we learn which the relevant contexts to the user are based on the user's behavior observation. Then, we represent the relevant contexts learnt as ontology segments. The encouraging experimental results show the usefulness of including semantics into a user profile as well as the advantages of integrating agents and data mining using ontologies.",""
"In this paper, a bibliographical review over the last decade is presented on the application of Bayesian networks to dependability, risk analysis and maintenance. It is shown an increasing trend of the literature related to these domains. This trend is due to the benefits that Bayesian networks provide in contrast with other classical methods of dependability analysis such as Markov Chains, Fault Trees and Petri Nets. Some of these benefits are the capability to model complex systems, to make predictions as well as diagnostics, to compute exactly the occurrence probability of an event, to update the calculations according to evidences, to represent multi-modal variables and to help modeling user-friendly by a graphical and compact approach. This review is based on an extraction of 200 specific references in dependability, risk analysis and maintenance applications among a database with 7000 Bayesian network references. The most representatives are presented, then discussed and some perspectives of work are provided. (C) 2010 Elsevier Ltd. All rights reserved.",""
"In recent years, the growing interest toward complex critical infrastructures and their interdependencies have solicited new efforts in the area of modeling and analysis of large interdependent systems. Cascading effects are a typical phenomenon of dependencies of components inside a system or among systems. The present paper deals with the modeling of cascading effects in a power grid. In particular, we propose to model such effects in the form of dynamic Bayesian networks (DBN) which can be derived by means of specific rules, from the power grid structure expressed in terms of series and parallel modules. In contrast with the available techniques, DBN offer a good trade-off between the analytical tractability and the representation of the propagation of the cascading event. A case study taken from the literature, is considered as running example. (C) 2010 Elsevier Ltd. All rights reserved.",""
"We present a hybrid Bayesian network (HBN) framework to model the availability of renewable systems. We use an approximate inference algorithm for HBNs that involves dynamically discretizing the domain of all continuous variables and use this to obtain accurate approximations for the renewal or repair time distributions for a system. We show how we can use HBNs to model corrective repair time, logistics delay times and scheduled maintenance time distributions and combine these with time-to-failure distributions to derive system availability. Example models are presented and are accompanied by detailed descriptions of how repair (renewal) distributions might be modelled using HBNs. (C) 2010 Elsevier Ltd. All rights reserved.","We use an approximate inference algorithm for HBNs that involves dynamically discretizing the domain of all continuous variables and use this to obtain accurate approximations for the renewal or repair time distributions for a system."
"Computer assisted troubleshooting with external interventions is considered. The work is motivated by the task of repairing an automotive vehicle at lowest possible expected cost. The main contribution is a decision theoretic troubleshooting system that is developed to handle external interventions. In particular, practical issues in modeling for troubleshooting are discussed, the troubleshooting system is described, and a method for the efficient probability computations is developed. The troubleshooting systems consists of two parts; a planner that relies on AO* search and a diagnoser that utilizes Bayesian networks (BN). The work is based on a case study of an auxiliary braking system of a modern truck. Two main challenges in troubleshooting automotive vehicles are the need for disassembling the vehicle during troubleshooting to access parts to repair, and the difficulty to verify that the vehicle is fault free. These facts lead to that probabilities for faults and for future observations must be computed for a system that has been subject to external interventions that cause changes in the dependency structure. The probability computations are further complicated due to the mixture of instantaneous and non-instantaneous dependencies. To compute the probabilities, we develop a method based on an algorithm, updateBN, that updates a static BN to account for the external interventions. (C) 2011 Elsevier Ltd. All rights reserved.",""
"Ensuring the availability of enterprise IT systems is a challenging task. The factors that can bring systems down are numerous, and their impact on various system architectures is difficult to predict. At the same time, maintaining high availability is crucial in many applications, ranging from control systems in the electric power grid, over electronic trading systems on the stock market to specialized command and control systems for military and civilian purposes. This paper describes a Bayesian decision support model, designed to help enterprise IT system decision-makers evaluate the consequences of their decisions by analyzing various scenarios. The model is based on expert elicitation from 50 experts on IT systems availability, obtained through an electronic survey. The Bayesian model uses a leaky Noisy-OR method to weigh together the expert opinions on 16 factors affecting systems availability. Using this model, the effect of changes to a system can be estimated beforehand, providing decision support for improvement of enterprise IT systems availability. The Bayesian model thus obtained is then integrated within a standard, reliability block diagram-style, mathematical model for assessing availability on the architecture level. In this model, the IT systems play the role of building blocks. The overall assessment framework thus addresses measures to ensure high availability both on the level of individual systems and on the level of the entire enterprise architecture. Examples are presented to illustrate how the framework can be used by practitioners aiming to ensure high availability.",""
"To mimic human tutors and provide optimal training, a cognitive tutoring agent should be able to continuously learn from its interactions with learners. An important element that helps a tutor better understand learner's mistake is finding the causes of the learners' mistakes. In this paper, we explain how we have designed and integrated a causal learning mechanism in a cognitive agent named CELTS (Conscious Emotional Learning Tutoring System) that assists learners during learning activities. Unlike other works in cognitive agents that used Bayesian Networks to deal with causality, CELTS's causal learning mechanism is implemented using data mining algorithms that can be used with large amount of data. The integration of a causal learning mechanism within CELTS allows it to predict learners' mistakes. Experiments showed that the causal learning mechanism help CELTS improve learners' performance. (C) 2011 Published by Elsevier B.V.",""
"Inference in Bayesian networks (BNs) is NP-hard. We proposed the concept of a node set namely Maximum Quadruple-Constrained subset MQC(A,a - e) to improve the efficiency of exact inference in diagnostic Bayesian networks (DBNs). Here, A denotes a node set in a DBN and a - e represent five real numbers. The improvement in efficiency is achieved by computation sharing. That is, we divide inference in a DBN into the computation of eliminating MQC(A, a - e) and the subsequent computation. For certain complex DBNs and (A, a - e), the former computation covers a major part of the whole computation, and the latter one is highly efficient after sharing the former computation. Searching for MQC(A, a - e) is a combinatorial optimization problem. A backtracking-based exact algorithm Backtracking-Search (BS) was proposed, however the time complexity of BS is O(n(3)2(n)) (n = vertical bar A vertical bar). In this article, we propose the following algorithms for searching for MQC(A, a - e) especially in complex DBNs where vertical bar A vertical bar is large. (i) A divide-and-conquer algorithm Divide-and-Conquer (DC) for dividing the problem of searching for MQC(A, a - e) into sub-problems of searching for MQC(B-1, a - e), ...,MQC(B-m, a - e), where B-i subset of A(1 <= i m, 1 <= m <= vertical bar A vertical bar). (ii) A DC-based heuristic algorithm Heuristic-Search (HS) for searching for MQC(B-i, a - e). The time complexity of HS is O(n(6)) (n = vertical bar B-i vertical bar). Empirical results show that, HS outperforms BS over a range of networks.(C) 2011 Elsevier B.V. All rights reserved.","Inference in Bayesian networks (BNs) is NP-hard."
"The influences of the variables in a Bayesian belief network model for estimating the role of human factors on ship collision probability in the Gulf of Finland are studied for discovering the variables with the largest influences and for examining the validity of the network. The change in the so-called causation probability is examined while observing each state of the network variables and by utilizing sensitivity and mutual information analyses. Changing course in an encounter situation is the most influential variable in the model, followed by variables such as the Officer of the Watch's action, situation assessment, danger detection, personal condition and incapacitation. The least influential variables are the other distractions on bridge, the bridge view, maintenance routines and the officer's fatigue. In general, the methods are found to agree on the order of the model variables although some disagreements arise due to slightly dissimilar approaches to the concept of variable influence. The relative values and the ranking of variables based on the values are discovered to be more valuable than the actual numerical values themselves. Although the most influential variables seem to be plausible. there are some discrepancies between the indicated influences in the model and literature. Thus, improvements are suggested to the network. (C) 2012 Elsevier Ltd. All rights reserved.",""
"One of the most desired aspects for power suppliers is the acquisition/sale of energy for a future demand. However, power consumption forecast is characterized not only by the variables of the power system itself, but also related to social-economic and climatic factors. Hence, it is imperative for the power suppliers to project and correlate these parameters. This paper presents a study of power load forecast for power suppliers, considering the applicability of wavelets, time series analysis methods and artificial neural networks, for both mid and long term forecasts. Both the periods of forecast are of major importance for power suppliers to define the future power consumption of a given region. The paper also studies the establishment of correlations among the variables using Bayesian networks. The results obtained are much more effective when compared to those projected by the power suppliers based on specialist information. The research discussed here is implemented on a decision support system, contributing to the decision making for acquisition/sale of energy at a future demand; also providing them with new ways for inference and analyses with the correlation model presented here. (C) 2011 Elsevier Ltd. All rights reserved.","The research discussed here is implemented on a decision support system, contributing to the decision making for acquisition/sale of energy at a future demand; also providing them with new ways for inference and analyses with the correlation model presented here."
"The aeronautics industry is attempting to implement important changes to its maintenance strategy. The article presents a new framework for making final decision on aeroplane maintenance actions. It emphasizes on the use of prognostics within this global framework to replace corrective and Preventive Maintenance practise for a predictive maintenance to minimize the cost of the maintenance support and to increase aircraft/fleet operability. The main objective of the article is to show the Bayesian network model as a useful technique for prognosis. The specific use case for predicting brake wear on the plane is developed based on this technique. The network allows estimate brake wear from the aircraft operational plan. This model, together with other models to make predictions for various components of the aeroplane (that should be monitored) offers a forward-looking approach of the status of the plane, allowing later the evaluation of different operational plans based on operational risk assessment and economic cost of each one of them depending on the scheduled checks. (C) 2011 Elsevier Ltd. All rights reserved.",""
"Discriminative subclass models can provide good estimates of complex 'continuous to discrete' conditional probabilities for hybrid Bayesian network models. However, the conventional approach of specifying deterministic 'hard' subclasses via unsupervised clustering can lead to inaccurate models. The multimodal softmax (MMS) model is presented as a new probabilistic discriminative subclass model that overcomes this unreliability. By invoking fully probabilistic latent 'soft' subclasses, MMS permits learning via standard statistical methods without requiring explicit clustering/relabeling of data. MMS is also shown to be closely related to the mixture of experts model and the generative Gaussian mixture classifier. Synthetic and benchmark classification results demonstrate the MMS model's correctness and usefulness for hybrid probabilistic modeling. (C) 2011 Elsevier Ltd. All rights reserved.","MMS is also shown to be closely related to the mixture of experts model and the generative Gaussian mixture classifier."
"The growing demand for higher trustworthiness of software poses an unprecedented challenge to the software industry. Risk management is the important part for high quality software development processes. However, under the constraints of project cost and duration, it is very difficult to establish the budget for risk management. To integrate efficient risk management and pure software process is the goal of this paper. We propose a software process model with risk management and cost control modules to help improve software process risk management. Furthermore, based on this process model, a measurement model that includes process risk and software trustworthiness metrics is presented. Through risk management effectiveness calculation methods and risk transfer assumptions, a software process risk optimization model is proposed. This model can be used to derive an optimized risk management scheme for the process of trustworthy software development, with constraints of process cost and duration. Simulation cases are then analyzed by this model framework. The results show that risk management is critical to enhance trustworthiness but risk management is an effective complement, rather than the most fundamental process, to enhance the trustworthiness of software. Software developers should adopt appropriate and optimal strategies about risk management inputs, especially in lower CMMI level companies. (C) 2011 Elsevier Inc. All rights reserved.",""
"There has been a substantial amount of research on the relationship between hippocampal neurogenesis and behaviour over the past 15 years, but the causal role that new neurons have on cognitive and affective behavioural tasks is still far from clear. This is partly due to the difficulty of manipulating levels of neurogenesis without inducing off-target effects, which might also influence behaviour. In addition, the analytical methods typically used do not directly test whether neurogenesis mediates the effect of an intervention on behaviour. Previous studies may have incorrectly attributed changes in behavioural performance to neurogenesis because the role of known (or unknown) neurogenesis-independent mechanisms was not formally taken into consideration during the analysis. Causal models can tease apart complex causal relationships and were used to demonstrate that the effect of exercise on pattern separation is via neurogenesis-independent mechanisms. Many studies in the neurogenesis literature would benefit from the use of statistical methods that can separate neurogenesis-dependent from neurogenesis-independent effects on behaviour.",""
"Peer-to-Peer (P2P) architectures for live video streaming has attracted a significant attention from both academia and industry. P2P design enables end-hosts to relay streams to each other overcoming the scalability issue of centralized architectures. However, these systems struggle to provide a service of comparable quality to that of traditional television. Since end-hosts are controlled by users, their behavior has a strong impact on the performance of P2P streaming systems, leading to potential service disruption and low streaming quality. Thus, considering the user behavior in these systems could bring significant performance improvements. Toward this end, we propose a Bayesian network that captures all the elements making part of the user behavior or related to it. This network is built from the information found in a cross-analysis of numerous large-scale measurement campaigns, analyzing the user behavior in video streaming systems. We validate our model through intensive simulations showing that our model can learn a user behavior and is able to predict several activities helping thus in optimizing these systems for a better performance. We also propose a method based on traces collection of the same user type that accelerates the learning process of this network. Furthermore, we evaluate the performance of this model through exploring its applications and comparison with non-contextual models. (C) 2012 Elsevier B.V. All rights reserved.",""
"In this paper, we propose integration of multimodal features using conditional random fields (CRFs) for the segmentation of broadcast news stories. We study story boundary cues from lexical, audio and video modalities, where lexical features consist of lexical similarity, chain strength and overall cohesiveness; acoustic features involve pause duration, pitch, speaker change and audio event type; and visual features contain shot boundaries, anchor faces and news title captions. These features are extracted in a sequence of boundary candidate positions in the broadcast news. A linear-chain CRF is used to detect each candidate as boundary/non-boundary tags based on the multimodal features. Important interlabel relations and contextual feature information are effectively captured by the sequential learning framework of CRFs. Story segmentation experiments show that the CRF approach outperforms other popular classifiers, including decision trees (DTs), Bayesian networks (BNs), naive Bayesian classifiers (NBs), multilayer perception (MLP), support vector machines (SVMs) and maximum entropy (ME) classifiers.","Story segmentation experiments show that the CRF approach outperforms other popular classifiers, including decision trees (DTs), Bayesian networks (BNs), naive Bayesian classifiers (NBs), multilayer perception (MLP), support vector machines (SVMs) and maximum entropy (ME) classifiers."
"Clustering Web data is one important technique for extracting knowledge from the Web. In this paper, a novel method is presented to facilitate the clustering. The method determines the appropriate number of clusters and provides suitable representatives for each cluster by inference from a Bayesian network. Furthermore, by means of the Bayesian network, the contents of the Web pages are converted into vectors of lower dimensions. The method is also extended for hierarchical clustering, and a useful heuristic is developed to select a good hierarchy. The experimental results show that the clusters produced benefit from high quality.","The method determines the appropriate number of clusters and provides suitable representatives for each cluster by inference from a Bayesian network."
"In this paper, Bayesian-Networks (BN) and Ant Colony Optimization (ACO) techniques are combined to find the best path through a graph representing all available itineraries to acquire a professional competence. The combination of these methods allows us to design a dynamic learning path, useful in a rapidly changing world. One of the most important advances in this work is that the amount of pheromones released is variable. This amount is calculated by taking into account the results acquired in the last completed course in relation to the minimum score required. By using ACO and BN, a fitness function, responsible of automatically selecting the next course in the learning graph, is defined. This is done by generating a path that maximizes the probability of each user's success in the course. Therefore, the path can change to improve learners average performance, taking into account the pedagogical weight of each learning unit and the social behavior of the system. Furthermore, a discrete dynamical system is obtained and its stability is studied. How to wrap an existing Learning Management System is also described in this work. Finally, an experiment compares this approach with the old on-line learning system being used previously.",""
"We address a specific case of joint probability mapping, where the information presented is the probabilistic associations of random variables under a certain condition variable (conditioned associations). Bayesian and dependency networks graphically map the joint probabilities of random variables, though both networks may identify associations that are independent of the condition (background associations). Since the background associations have the same topological features as conditioned associations, it is difficult to discriminate between conditioned and non-conditioned associations, which results in a major increase in the search space. We introduce a modification of the dependency network method, which produces a directed graph, containing only condition-related associations. The graph nodes represent the random variables and the graph edges represent the associations that arise under the condition variable. This method is based on ridge-regression, where one can utilize a numerically robust and computationally efficient algorithm implementation. We illustrate the method's efficiency in the context of a medically relevant process, the emergence of drug-resistant variants of human immunodeficiency virus (HIV) in drug-treated, HIV-infected people. Our mapping was used to discover associations between variants that are conditioned by the initiation of a particular drug treatment regimen. We have demonstrated that our method can recover known associations of such treatment with selected resistance mutations as well as documented associations between different mutations. Moreover, our method revealed novel associations that are statistically significant and biologically plausible.","This method is based on ridge-regression, where one can utilize a numerically robust and computationally efficient algorithm implementation."
"The recent increased interest in information fusion methods for solving complex problem, such as in image analysis, is motivated by the wish to better exploit the multitude of information, available from different sources, to enhance decision-making. In this paper, we propose a novel method, that advances the state of the art of fusing image information from different views, based on a special class of probabilistic graphical models, called causal independence models. The strength of this method is its ability to systematically and naturally capture uncertain domain knowledge, while performing information fusion in a computationally efficient way. We examine the value of the method for mammographic analysis and demonstrate its advantages in terms of explicit knowledge representation and accuracy (increase of at least 6.3% and 5.2% of true positive detection rates at 5% and 10% false positive rates) in comparison with previous single-view and multi-view systems, and benchmark fusion methods such as naive Bayes and logistic regression. (C) 2012 Elsevier B.V. All rights reserved.","% of true positive detection rates at 5% and 10% false positive rates) in comparison with previous single-view and multi-view systems, and benchmark fusion methods such as naive Bayes and logistic regression."
"The leaf area index (LAI) is a biophysical variable related to atmosphere-biosphere exchange of CO2. One way to obtain LAI value is by the Moderate Resolution Imaging Spectroradiometer (MODIS) biophysical products. In this paper, we use this product to improve the physiological principles predicting growth model within a Gaussian Bayesian network (GBN) setup. The MODIS time series, however, contains gaps caused by persistent clouds, cloud contamination, and other technique problems. We used the Expectation Maximization (EM) algorithm to estimate these missing values. During a period of 26 successive months, the EM algorithm is applied to four different cases: successively and not successively missing values during two different winter seasons, successively and not successively missing values during one spring season, and not successively missing values during the full study. Results show that the maximum value of the averaged absolute error between the original values and those estimated equals 0.16. This low value indicates that the estimated values well represent the original values. Moreover, the root mean square error of the GBN output reduces from 1.57 to 1.49 when performing the EM algorithm to estimate the not successively missing values. We conclude that the EM algorithm within a GBN can adequately handle missing MODIS LAI values and improves the estimation of the LAI.",""
"Operational safety is receiving more and more attention in the Norwegian offshore industry. Almost two thirds of all leaks on offshore installations in the period 2001-2005, according to the Risk Level Project by the Petroleum Safety Authority in Norway, resulted from manual operations and interventions, as well as shut-down and start-up. The intention with the Risk OMT (risk modelling - integration of organisational, human and technical factors) program has been to develop more representative models for calculation of leak frequencies as a function of the volume of manual operations and interventions. In the Risk OMT project a generic risk model has been developed and is adapted to use for specific failure scenarios. The model considers the operational barriers in event trees and fault trees, as well as risk influencing factors that determine the basic event probabilities in the fault trees. The full model, which applies Bayesian belief networks, is presented more thoroughly in a separate paper. This paper presents the evaluation of the model. The model has been evaluated through some case studies, and one important aspect is the evaluation of the importance of each risk influencing factor. In addition some risk-reducing measures have been proposed, and the paper presents how the effect of these measures has been evaluated by using the model. Finally, possible applications and recommendations for further work are discussed. (c) 2012 Elsevier Ltd. All rights reserved.",""
"Bayesian networks are employed to model the uncertainty hindering in the overtaking behavior of young drivers in two-lane highways and reveal the traffic related microscopic characteristics that may influence the decision to overtake. Results using data from an experiment conducted on driving simulator show that male drivers, on average, accept smaller gaps for overtaking than female drivers. For both male and female drivers, the spacing with the lead and the opposing vehicle is more influential to the probability to overtake compared to vehicle speed. Moreover, a thorough look at the relationships between the microscopic traffic characteristics and the probability to overtake reveals differences between male and female drivers regarding the road traffic scene appraisal mechanism on the emergence of an opportunity to overtake. (C) 2012 Elsevier Ltd. All rights reserved.",""
"Acute lung injury (ALI) is a devastating complication of acute illness and one of the leading causes of multiple organ failure and mortality in the intensive care unit (ICU). The detection of this syndrome is limited due to the complexity of the disease, insufficient understanding of its development and progression, and the large amount of risk factors and modifiers. In this preliminary study, we present a novel mathematical model for ALI detection. It is constructed based on clinical and research knowledge using three complementary techniques: rule-based fuzzy inference systems, Bayesian networks, and finite state machines. The model is developed in Matlab(A (R))'s Simulink environment and takes as input pre-ICU and ICU data feeds of critically ill patients. Results of the simulation model were validated against actual patient data from an epidemiologic study. By appropriately combining all three techniques the performance attained is in the range of 71.7-92.6% sensitivity and 60.3-78.4% specificity.","It is constructed based on clinical and research knowledge using three complementary techniques: rule-based fuzzy inference systems, Bayesian networks, and finite state machines."
"Objective: Setting up clinical reports within hospital information systems makes it possible to record a variety of clinical presentations. Directed acyclic graphs (Dags) offer a useful way of representing causal relations in clinical problem domains and are at the core of many probabilistic models described in the medical literature, like Bayesian networks. However, medical practitioners are not usually trained to elicit Dag features. Part of the difficulty lies in the application of the concept of direct causality before selecting all the causal variables of interest for a specific patient. We designed an automated interview to tutor medical doctors in the development of Dags to represent their understanding of clinical reports. Methods: Medical notions were analyzed to find patterns in medical reasoning that can be followed by algorithms supporting the elicitation of causal Dags. Clinical relevance was defined to help formulate only relevant questions by driving an expert's attention towards variables causally related to nodes already inserted in the graph. Key procedural features of the proposed interview are described by four algorithms. Results: The automated interview comprises questions on medical notions, phrased in medical terms. The first elicitation session produces questions concerning the patient's chief complaints and the outcomes related to diseases serving as diagnostic hypotheses, their observable manifestations and risk factors. The second session focuses on questions that refine the initial causal paths by considering syndromes, dysfunctions, pathogenic anomalies, biases and effect modifiers. A case study concerning a gastro-enterological problem and one dealing with an infected patient illustrate the output produced by the algorithms, depending on the answers provided by the doctor. Conclusions: The proposed elicitation framework is characterized by strong consistency with medical background and by a progressive introduction of relevant medical topics. Revision and testing of the subjectively elicited Dag is performed by matching the collected answers with the evidence included in accepted sources of biomedical knowledge. (C) 2011 Elsevier B.V. All rights reserved.",""
"Objectives: Prediction of prostate cancer pathological stage is an essential step in a patient's pathway. It determines the treatment that will be applied further. In current practice, urologists use the pathological stage predictions provided in Partin tables to support their decisions. However. Partin tables are based on logistic regression (LR) and built from US data. Our objective is to investigate a range of both predictive methods and of predictive variables for pathological stage prediction and assess them with respect to their predictive quality based on UK data. Methods and material: The latest version of Partin tables was applied to a large scale British dataset in order to measure their performances by mean of concordance index (c-index). The data was collected by the British Association of Urological Surgeons (BAUS) and gathered records from over 1700 patients treated with prostatectomy in 57 centers across UK. The original methodology was replicated using the BAUS dataset and evaluated using concordance index. In addition, a selection of classifiers, including, among others. LR, artificial neural networks and Bayesian networks (BNs) was applied to the same data and compared with each other using the area under the ROC curve (AUC). Subsets of the data were created in order to observe how classifiers perform with the inclusion of extra variables. Finally a local dataset prepared by the Aberdeen Royal Infirmary was used to study the effect on predictive performance of using different variables. Results: Partin tables have low predictive quality (c-index = 0.602) when applied on UK data for comparison on patients with organ confined and extra prostatic extension conditions, patients at the two most frequently observed pathological stages. The use of replicate lookup tables built from British data shows an improvement in the classification, but the overall predictive quality remains low (c-index = 0.610). Comparing a range of classifiers shows that BNs generally outperform other methods. Using the four variables from Partin tables, naive Bayes is the best classifier for the prediction of each class label (AUC = 0.662 for OC). When two additional variables are added, the results of LR (0.675), artificial neural networks (0.656) and BN methods (0.679) are overall improved. BNs show higher AUCs than the other methods when the number of variables raises Conclusion: The predictive quality of Partin tables can be described as low to moderate on UK data. This means that following the predictions generated by Partin tables, many patients would received an inappropriate treatment, generally associated with a deterioration of their quality of life. In addition to demographic differences between UK and the original US population, the methodology and in particular LR present limitations. BN represents a promising alternative to LR from which prostate cancer staging can benefit. Heuristic search for structure learning and the inclusion of more variables are elements that further improve BN models quality. (C) 2011 Elsevier B.V. All rights reserved.","Partin tables are based on logistic regression (LR) and built from US data."
"Objectives: Many classification problems must deal with data that contains missing values. In such cases data imputation is critical. This paper evaluates the performance of several statistical and machine learning imputation methods, including our novel multiple imputation ensemble approach, using different datasets. Materials and methods: Several state-of-the-art approaches are compared using different datasets. Some state-of-the-art classifiers (including support vector machines and input decimated ensembles) are tested with several imputation methods. The novel approach proposed in this work is a multiple imputation method based on random subspace, where each missing value is calculated considering a different cluster of the data. We have used a fuzzy clustering approach for the clustering algorithm. Results: Our experiments have shown that the proposed multiple imputation approach based on clustering and a random subspace classifier outperforms several other state-of-the-art approaches. Using the Wilcoxon signed-rank test (reject the null hypothesis, level of significance 0.05) we have shown that the proposed best approach is outperformed by the classifier trained using the original data (i.e., without missing values) only when >20% of the data are missed. Moreover, we have shown that coupling an imputation method with our cluster based imputation we outperform the base method (level of significance similar to 0.05). Conclusion: Starting from the assumptions that the feature set must be partially redundant and that the redundancy is distributed randomly over the feature set, we have proposed a method that works quite well even when a large percentage of the features is missing (>= 30%). Our best approach is available (MATLAB code) at bias.csr.unibo.it/nanni/MI.rar. (C) 2011 Elsevier B.V. All rights reserved.","Objectives: Many classification problems must deal with data that contains missing values."
"Climate change is likely to affect plants in multiple ways, but predicting the consequences for habitat suitability requires a process-based understanding of the interactions. This is at odds with existing approaches that are mostly phenomenological and largely restricted to predicting the effects of changing temperature and rainfall on species distributions at a coarse spatial scale. We examine the multiple effects of climate change, including predicting the effects of altered flood regimes and land-use change, on the potential distribution of the invasive riparian species lippia (Phyla canescens) across a 26 000 km2 catchment in eastern Australia. We determined habitat suitability for lippia by combining process-understanding of experts and an eco-physiological bioclimatic model within a Bayesian belief network. The bioclimatic model predicted substantial changes in habitat suitability by 2070 under both a wetter (Echam Mark 3) and drier (Hadley Centre Mark 2) climate change scenario, but only the more likely drier scenario reduced suitability in our test region. The area suitable for lippia was predicted to increase at least threefold with increased flooding under a wet climate scenario, although this would be partially negated by land-use change to cultivation. The region would become unsuitable to lippia with reduced flooding under a drier scenario irrespective of land-use changes, although existing populations would persist if grazing persisted. Independent field validation verified model structure and parameterization, and therefore the opinion of experts, but identified site-scale deficiencies in the available environmental data layers. Model predictions suggest that adaptation options for managing lippia will be greatly reduced under a drying scenario, but identify potential restoration opportunities under either scenario. This work highlights the value of predictive models that incorporate process-understanding at sufficiently fine spatial resolution to capture the important processes underpinning habitat suitability.",""
"Pattern recognition using Dynamic Bayesian Networks (DBNs) is currently a growing area of study. The classification performance greatly relies on the choice of a DBN model that will best describe the dependencies in each class of data. In this paper, we present DBN models trained for the classification of handwritten digit. Two approaches to improve the suitability of the models are presented. One uses a fixed DBN structure, and is based on an Evolutionary Algorithm optimisation of the selection and of the layout of the observations for each class of data. The second approach is about learning part of the structure of the models from the training set of each class. Parameter learning is then performed for each DBN. Classification results are presented for the described models, and compared with previously published results. Both approaches were found to improve the recognition rate compared to previous results. (C) 2012 Elsevier B.V. All rights reserved.","The classification performance greatly relies on the choice of a DBN model that will best describe the dependencies in each class of data."
"A novel dynamical model for the study of operational risk in banks and suitable for the calculation of the Value at Risk (VaR) is proposed. The equation of motion takes into account the interactions among different bank's processes, the spontaneous generation of losses via a noise term and the efforts made by the bank to avoid their occurrence. Since the model is very general, it can be tailored on the internal organizational structure of a specific bank by estimating some of its parameters from historical operational losses. The model is exactly solved in the case in which there are no causal loops in the matrix of couplings and it is shown how the solution can be exploited to estimate also the parameters of the noise. The forecasting power of the model is investigated by using a fraction! of simulated data to estimate the parameters, showing that for f = 0.75 the VaR can be forecast with an error similar or equal to 10(-3). (C) 2011 Elsevier B.V. All rights reserved.",""
"We present methods able to predict the presence and strength of conditional and unconditional dependencies (correlations) between two variables Y and Z never jointly measured on the same samples, based on multiple data sets measuring a set of common variables. The algorithms are specializations of prior work on learning causal structures from overlapping variable sets. This problem has also been addressed in the field of statistical matching. The proposed methods are applied to a wide range of domains and are shown to accurately predict the presence of thousands of dependencies. Compared against prototypical statistical matching algorithms and within the scope of our experiments, the proposed algorithms make predictions that are better correlated with the sample estimates of the unknown parameters on test data; this is particularly the case when the number of commonly measured variables is low. The enabling idea behind the methods is to induce one or all causal models that are simultaneously consistent with (fit) all available data sets and prior knowledge and reason with them. This allows constraints stemming from causal assumptions (e.g., Causal Markov Condition, Faithfulness) to propagate. Several methods have been developed based on this idea, for which we propose the unifying name Integrative Causal Analysis (INCA). A contrived example is presented demonstrating the theoretical potential to develop more general methods for co-analyzing heterogeneous data sets. The computational experiments with the novel methods provide evidence that causally-inspired assumptions such as Faithfulness often hold to a good degree of approximation in many real systems and could be exploited for statistical inference. Code, scripts, and data are available at www.mensxmachina.org.","The computational experiments with the novel methods provide evidence that causally-inspired assumptions such as Faithfulness often hold to a good degree of approximation in many real systems and could be exploited for statistical inference."
"Cardiovascular disease (CVD) is the major cause of death globally. More people die of CVDs each year than from any other disease. Over 80% of CVD deaths occur in low and middle income countries and occur almost equally in male and female. In this paper, different computational models based on Bayesian Networks, Multilayer Perceptron, Radial Basis Function and Logistic Regression methods are presented to predict early risk detection of the cardiovascular event. A total of 929 (626 male and 303 female) heart attack data are used to construct the models. The models are tested using combined as well as separate male and female data. Among the models used, it is found that the Multilayer Perceptron model yields the best accuracy result.","In this paper, different computational models based on Bayesian Networks, Multilayer Perceptron, Radial Basis Function and Logistic Regression methods are presented to predict early risk detection of the cardiovascular event."
"Treatment with lamivudine of patients infected with hepatitis B virus (HBV) results in a high rate of drug resistance, which is primarily associated with the rtM204I/V substitution in the HBV reverse transcriptase domain. Here we show that the rtM204I/V substitution, although essential, is insufficient for establishing resistance against lamivudine. The analysis of 639 HBV whole-genome sequences obtained from 11 patients shows that rtM204I/V is independently acquired by more than one intra-host HBV variant, indicating the convergent nature of lamivudine resistance. The differential capacity of HBV variants to develop drug resistance suggests that fitness effects of drug-resistance mutations depend on the genetic structure of the HBV genome. An analysis of Bayesian networks that connect rtM204I/V to many sites of HBV proteins confirms that lamivudine resistance is a complex trait encoded by the entire HBV genome rather than by a single mutation. These findings have implications for public health and offer a more general framework for understanding drug resistance.",""
"The dynamic tree (DT) graphical model is a popular analytical tool for image segmentation and object classification tasks. A DT is a useful model in this context because its hierarchical property enables the user to examine information in multiple scales and its flexible structure can more easily fit complex region boundaries compared to rigid quadtree structures such as tree-structured Bayesian networks. This paper proposes a novel framework for data fusion called a deformable Bayesian network (DFBN) by using a DT model to fuse measurements from multiple sensing platforms into a nonredundant representation. The structural flexibility of the DFBN will be used to fuse common information across different sensor measurements. The appropriate structure update strategies for the DFBN and its parameters for the data fusion application are discussed. A real-world example application using sonar images collected from a survey mission is presented. The fusion results using the presented DFBN framework are shown to outperform state-of-the-art approaches such as the Gaussian mean shift and spectral clustering algorithms. The DFBN's complexity and scalability are discussed to address its potential for a larger data set.","The dynamic tree (DT) graphical model is a popular analytical tool for image segmentation and object classification tasks."
"The learning Bayesian network (BN) structure from data is an NP-hard problem and still one of the most exciting challenges in the machine learning. In this work, a novel algorithm is presented which combines ideas from local learning, constraint-based, and search-and-score techniques in a principled and effective way. It first reconstructs the junction tree of a BN and then performs a K2-scoring greedy search to orientate the local edges in the cliques of junction tree. Theoretical and experimental results show the proposed algorithm is capable of handling networks with a large number of variables. Its comparison with the well-known K2 algorithm is also presented.",""
"The successful application of estimation of distribution algorithms (EDAs) to solve different kinds of problems has reinforced their candidature as promising black-box optimization tools. However, their internal behavior is still not completely understood and therefore it is necessary to work in this direction in order to advance their development. This paper presents a methodology of analysis which provides new information about the behavior of EDAs by quantitatively analyzing the probabilistic models learned during the search. We particularly focus on calculating the probabilities of the optimal solutions, the most probable solution given by the model and the best individual of the population at each step of the algorithm. We carry out the analysis by optimizing functions of different nature such as Trap5, two variants of Ising spin glass and Max-SAT. By using different structures in the probabilistic models, we also analyze the impact of the structural model accuracy in the quantitative behavior of EDAs. In addition, the objective function values of our analyzed key solutions are contrasted with their probability values in order to study the connection between function and probabilistic models. The results not only show information about the internal behavior of EDAs, but also about the quality of the optimization process and setup of the parameters, the relationship between the probabilistic model and the fitness function, and even about the problem itself. Furthermore, the results allow us to discover common patterns of behavior in EDAs and propose new ideas in the development of this type of algorithms.",""
"We present an automatic vehicle detection system for aerial surveillance in this paper. In this system, we escape from the stereotype and existing frameworks of vehicle detection in aerial surveillance, which are either region based or sliding window based. We design a pixelwise classification method for vehicle detection. The novelty lies in the fact that, in spite of performing pixelwise classification, relations among neighboring pixels in a region are preserved in the feature extraction process. We consider features including vehicle colors and local features. For vehicle color extraction, we utilize a color transform to separate vehicle colors and nonvehicle colors effectively. For edge detection, we apply moment preserving to adjust the thresholds of the Canny edge detector automatically, which increases the adaptability and the accuracy for detection in various aerial images. Afterward, a dynamic Bayesian network (DBN) is constructed for the classification purpose. We convert regional local features into quantitative observations that can be referenced when applying pixelwise classification via DBN. Experiments were conducted on a wide variety of aerial videos. The results demonstrate flexibility and good generalization abilities of the proposed method on a challenging data set with aerial surveillance images taken at different heights and under different camera angles.","We design a pixelwise classification method for vehicle detection."
"We present an efficient procedure for factorising probabilistic potentials represented as probability trees. This new procedure is able to detect some regularities that cannot be captured by existing methods. In cases where an exact decomposition is not achievable, we propose a heuristic way to carry out approximate factorisations guided by a parameter called factorisation degree, which is fast to compute. We show how this parameter can be used to control the tradeoff between complexity and accuracy in approximate inference algorithms for Bayesian networks.","We show how this parameter can be used to control the tradeoff between complexity and accuracy in approximate inference algorithms for Bayesian networks."
"In recent years, learner models have emerged from the research laboratory and research classrooms into the wider world. Learner models are now embedded in real world applications which can claim to have thousands, or even hundreds of thousands, of users. Probabilistic models for skill assessment are playing a key role in these advanced learning environments. In this paper, we review the learner models that have played the largest roles in the success of these learning environments, and also the latest advances in the modeling and assessment of learner skills. We conclude by discussing related advancements in modeling other key constructs such as learner motivation, emotional and attentional state, meta-cognition and self-regulated learning, group learning, and the recent movement towards open and shared learner models.",""
"Motivation: Quantification of sequence abundance in RNA-Seq experiments is often conflated by protocol-specific sequence bias. The exact sources of the bias are unknown, but may be influenced by polymerase chain reaction amplification, or differing primer affinities and mixtures, for example. The result is decreased accuracy in many applications, such as de novo gene annotation and transcript quantification. Results: We present a new method to measure and correct for these influences using a simple graphical model. Our model does not rely on existing gene annotations, and model selection is performed automatically making it applicable with few assumptions. We evaluate our method on several datasets, and by multiple criteria, demonstrating that it effectively decreases bias and increases uniformity. Additionally, we provide theoretical and empirical results showing that the method is unlikely to have any effect on unbiased data, suggesting it can be applied with little risk of spurious adjustment.",""
"Understanding the mechanisms of gene regulation during breast cancer is one of the most difficult problems among oncologists because this regulation is likely comprised of complex genetic interactions. Given this complexity, a computational study using the Bayesian network technique has been employed to construct a gene regulatory network from microarray data. Although the Bayesian network has been notified as a prominent method to infer gene regulatory processes, learning the Bayesian network structure is NP hard and computationally intricate. Therefore, we propose a novel inference method based on low-order conditional independence that extends to the case of the Bayesian network to deal with a large number of genes and an insufficient sample size. This method has been evaluated and compared with full-order conditional independence and different prognostic indices on a publicly available breast cancer data set. Our results suggest that the low-order conditional independence method will be able to handle a large number of genes in a small sample size with the least mean square error. In addition, this proposed method performs significantly better than other methods, including the full-order conditional independence and the St. Gallen consensus criteria. The proposed method achieved an area under the ROC curve of 0.79203, whereas the full-order conditional independence and the St. Gallen consensus criteria obtained 0.76438 and 0.73810, respectively. Furthermore, our empirical evaluation using the low-order conditional independence method has demonstrated a promising relationship between six gene regulators and two regulated genes and will be further investigated as potential breast cancer metastasis prognostic markers. Published by Elsevier Inc.","Therefore, we propose a novel inference method based on low-order conditional independence that extends to the case of the Bayesian network to deal with a large number of genes and an insufficient sample size."
"Probabilistic reasoning is an essential feature when dealing with many application domains. Starting with the idea that ontologies are the right way to formalize domain knowledge and that Bayesian networks are the right tool for probabilistic reasoning, we propose an approach for extracting a Bayesian network from a populated ontology and for reasoning over it. The paper presents the theory behind the approach, its design and examples of its use.",""
"The failure of critical components in industrial systems may have negative consequences on the availability, the productivity, the security and the environment. To avoid such situations, the health condition of the physical system, and particularly of its critical components, can be constantly assessed by using the monitoring data to perform on-line system diagnostics and prognostics. The present paper is a contribution on the assessment of the health condition of a computer numerical control (CNC) tool machine and the estimation of its remaining useful life (RUL). The proposed method relies on two main phases: an off-line phase and an on-line phase. During the first phase, the raw data provided by the sensors are processed to extract reliable features. These latter are used as inputs of learning algorithms in order to generate the models that represent the wear's behavior of the cutting tool. Then, in the second phase, which is an assessment one, the constructed models are exploited to identify the tool's current health state, predict its RUL and the associated confidence bounds. The proposed method is applied on a benchmark of condition monitoring data gathered during several cuts of a CNC tool. Simulation results are obtained and discussed at the end of the paper. (C) 2011 Elsevier Ltd. All rights reserved.",""
"Predicting the accurate prognosis of breast cancer from high throughput microarray data is often a challenging task. Although many statistical methods and machine learning techniques were applied to diagnose the prognosis outcome of breast cancer, they are suffered from the low prediction accuracy (usually lower than 70%). In this paper, we propose a better method (genetic algorithm-support vector machine, we called GASVM) to significant improve the prediction accuracy of breast cancer from gene expression profiles. To further improve the classification performance, we also apply GASVM model using combined clinical and microarray data. In this paper, we evaluate the performance of the GASVM model based on data provided by 97 breast cancer patients. Four kinds of gene selection methods are used: all genes (All), 70 correlation-selected genes (C70), 15 medical literature-selected genes (R15), and 50 T-test-selected genes (150). With optimized parameter values identified from GASVM model, the average predictive accuracy of our model approaches 95% for 150 and 90% for C70 or R15 in all four kernel functions using integrated clinical and microarray data. Our model produces results more accurately than the average 70% predictive accuracy of other machine learning methods. The results indicate that the GASVM model has the potential to better assist physicians in the prognosis of breast cancer through the use of both clinical and microarray data. (C) 2011 Elsevier Ltd. All rights reserved.","To further improve the classification performance, we also apply GASVM model using combined clinical and microarray data."
"Ubiquitous decision support systems require more intelligent mechanism in which more timely and accurate decision support is available. However, conventional context-aware systems, which have been popular in the ubiquitous decision support systems field, cannot provide such agile and proactive decision support. To fill this research void, this paper proposes a new concept of context prediction mechanism by which the ubiquitous decision support devices are able to predict users' future contexts in advance, and provide more timely and proactive decision support that users would be satisfied much more. Especially, location prediction is useful because ubiquitous decision support systems could dynamically adapt their decision support contents for a user based on a user's future location. In this sense, as an alternative for the inference engine mechanism to be used in the ubiquitous decision support systems capable of context-prediction, we propose an inductive approach to recognizing a user's location by learning a dynamic Bayesian network model. The dynamic Bayesian network model has been evaluated with a set of contextual data from undergraduate students. The evaluation result suggests that a dynamic Bayesian network model offers significant predictive power in the location prediction. Besides, we found that the dynamic Bayesian network model has a great potential for the future types of ubiquitous decision support systems. (C) 2011 Elsevier Ltd. All rights reserved.","In this sense, as an alternative for the inference engine mechanism to be used in the ubiquitous decision support systems capable of context-prediction, we propose an inductive approach to recognizing a user's location by learning a dynamic Bayesian network model."
"The focus of this work is the analysis of the influence of transformational leadership on organizational factors, and their impacts on the project performance. The factors considered are communication, flexibility, continuous delivery and continuous improvement, overlap of activities, and maturity of the team, in projects with a high degree of innovation. Bayesian networks were chosen as a simulation tool. Results showed that for a moderate level of overlap of activities, the maximum project performance is obtained when the leadership components individual consideration, inspirational motivation, idealized influence and intellectual stimulation, are either at moderate levels. This leads to high levels of team maturity, flexibility and continuous delivery, while continuous improvement and communication tend to be moderate. It is highlighted the characterization of the individual contribution of the variables to the project performance and the empirical application of Bayesian networks, as an alternative to statistical methods commonly employed in leadership and management studies. (C) 2011 Elsevier Ltd. All rights reserved.",""
"In the night of the 26 and 27 October 2005, a fire broke out in the K-Wing of the Schiphol Cell Complex near Amsterdam. Eleven of 43 occupants of this wing died due to smoke inhalation. The Dutch Safety Board analysed the fire and released a report 1 year later. This article presents how a probabilistic model based on Bayesian networks can be used to analyse such a fire. The paper emphasises the usefulness of the model for this analysis. In additional it discusses the applicability for prioritisation of the recommendations such as those posed by the investigation board for the improvements of fire safety in special buildings. The big advantage of the model is that it can be used not only for fire analyses after accidents, but also prior to the accident, for example in the design phase of the building, to estimate the outcome of a possible fire given different possible scenarios. This contribution shows that if such a model was used before the fire occurred the number of fatalities would have not come as a surprise, since the model predicts a larger percentage of people dying than happened in the real fire. (C) 2012 Elsevier Ltd. All rights reserved.",""
"This paper presents a learnable tabu search (TS) guided by estimation of distribution algorithm (EDA), called LTS-EDA, for maximum diversity problem. The LTS-EDA introduces knowledge model and can extract knowledge during the search process of TS, and thus it adopts dual or cooperative evolution/search structure, consisting of probabilistic model space in clustered EDA and solution space searched by TS. The clustered EDA, as a learnable constructive method, is used to create a new starting solution, and the simple TS, as an improvement method, attempts to improve the solution created by the clustered EDA in the LTS-EDA. A distinguishing feature of the LTS-EDA is the usage of the clustered EDA with effective linkage learning to guide TS. In the clustered EDA, different clusters (models) focus on different substructures, and the combination of information from different clusters (models) effectively combines substructures. The LTS-EDA is tested on 50 large size benchmark problems with the size ranging from 2,000 to 5,000. Simulation results show that the LTS-EDA is better than the advanced algorithms proposed recently.",""
"Trustworthy location information is important because it is a critical input to a wide variety of location-based applications. However, the localization infrastructure is vulnerable to physical attacks, and consequently, the localization results are affected. In this paper, we aim to achieve robust localization under infrastructure attacks. We first investigated the impact of infrastructure attacks on localization and showed that the performance of location estimations degraded significantly under the attack. We then derived an attack-resistant scheme that is not algorithm specific and can be integrated with existing localization algorithms. Our attack-resistant scheme exploited the characteristics of the geometric patterns returned by location estimates under the attack; that is, the localization results of a wireless device under the normal situation were clearly clustered together, whereas the localization results were scattered when an attack was present. Thus, our attack-resistant scheme is grounded on K-means clustering analysis of intra-distance of localization results from all possible combinations of any three access points. To evaluate the effectiveness and scalability of our proposed scheme, we used received signal strength for validation and applied our approach to three broad classes of localization algorithms: lateration based, fingerprint matching, and Bayesian networks. We validated our scheme in the ORBIT test bed (North Brunswick, NJ, USA) using an 802.11 (Wi-Fi) network and in a real office building environment using an 802.15.4 (ZigBee) network. The extensive experimental results demonstrated that the application of our scheme could help the broad range of localization algorithms to achieve comparable or even better localization performance when under infrastructure attacks as compared with normal situations without attack, thus, effectively eliminating the effects of infrastructure attacks. Copyright (c) 2011 John Wiley & Sons, Ltd.",""
"Twin studies suggest 45% heritability of trait impulsivity. Results from candidate gene studies to date are contradictory; impulsivity phenotypes were measured by different behavioral and questionnaire methods related either to the dopaminergic or to the serotonergic system. Here we report an association study of both dopaminergic (COMT rs4680, DRD4 48?bp VNTR, DRD2/ANKK1 rs1800497) and serotonergic (HTR1A rs6925, HTR1B rs13212041, SLC6A4 5-HTTLPR) gene polymorphisms and trait impulsivity assessed with the Barratt Impulsiveness Scale (BIS-11) in a sample of 687 Caucasian young adults. Results showed lower impulsivity in the presence of the DRD4 7-repeat (P?=?0.006) and the HTR1B rs13212041 alleles (P?=?0.003). These findings stayed significant after Bonferroni correction. A multivariate analysis using Bayesian networks confirmed independent effects of these two polymorphisms and provided a coherent characterization of the system of dependencies with respect to the impulsivity construct as well as its subscales. These results clearly suggest an additive effect of dopaminergic and serotonergic polymorphisms on trait impulsivity. (c) 2012 Wiley Periodicals, Inc.",""
"Biosurveillance systems designed and deployed in the United States and abroad to allow public health authorities to monitor the health of their communities have significant design limitations despite their wide usage. One limitation is the lack of algorithmic solutions to combine local data sources for regional situation awareness. The objective of the current study is to develop and demonstrate the value of automated information fusion methods applied to the distributed neighboring public health sites. A prototype system consisting of distributed Bayesian models was designed to enable informed regional and local cognitive decision support response. The Intelligent Decision Support Network (IDSN) is composed of Bayesian Information Fusion Models (BIEMs) that target a particular syndrome or disease type. Using local data from county health departments in Northern Virginia for the time period between August 2005 and May 2007, we estimated the probability of a gastrointestinal (GI) outbreak in two ways: First, based on data from the local hospitals only; and second, based on the relative probability of outbreak by combining local hospital data and probabilities of Cl events from the neighboring counties' BIEMs. Preliminary findings showed that the network of distributed models detected events that would be undetected without multi-jurisdictional data. (C) 2011 Elsevier B.V. All rights reserved.",""
"This paper is concerned with the design of a Bayesian network structure that is suitable for operational risk modelling. The model's structure is designed specifically from the perspective of a business unit operational risk manager whose role is to measure, record, predict, communicate, analyse and control operational risk within their unit. The problem domain modelled is a functioning structured finance operations unit within a major Australian bank. The network model design incorporates a number of existing human factor frameworks to account for human error and operational risk events within the domain. The design also supports a modular structure, allowing for the inclusion of many operational loss event types, making it adaptable to different operational risk environments. Journal of the Operational Research Society (2012) 63, 431-444. doi: 10.1057/jors.2011.7 Published online 11 May 2011",""
"In this study, we applied Bayesian networks to prioritize the factors that influence hazardous material (Hazmat) transportation accidents. The Bayesian network structure was built based on expert knowledge using Dempster-Shafer evidence theory, and the structure was modified based on a test for conditional independence. We collected and analyzed 94 cases of Chinese Hazmat transportation accidents to compute the posterior probability of each factor using the expectation-maximization learning algorithm. We found that the three most influential factors in Hazmat transportation accidents were human factors, the transport vehicle and facilities, and packing and loading of the Hazmat. These findings provide an empirically supported theoretical basis for Hazmat transportation corporations to take corrective and preventative measures to reduce the risk of accidents. (C) 2011 Elsevier Ltd. All rights reserved.",""
"Background Mood disorders are highly heritable forms of major mental illness. A major breakthrough in elucidating the genetic architecture of mood disorders was anticipated with the advent of genome-wide association studies (GWAS). However, to date few susceptibility loci have been conclusively identified. The genetic etiology of mood disorders appears to be quite complex, and as a result, alternative approaches for analyzing GWAS data are needed. Recently, a polygenic scoring approach that captures the effects of alleles across multiple loci was successfully applied to the analysis of GWAS data in schizophrenia and bipolar disorder (BP). However, this method may be overly simplistic in its approach to the complexity of genetic effects. Data mining methods are available that may be applied to analyze the high dimensional data generated by GWAS of complex psychiatric disorders. Results We sought to compare the performance of five data mining methods, namely, Bayesian networks, support vector machine, random forest, radial basis function network, and logistic regression, against the polygenic scoring approach in the analysis of GWAS data on BP. The different classification methods were trained on GWAS datasets from the Bipolar Genome Study (2191 cases with BP and 1434 controls) and their ability to accurately classify case/control status was tested on a GWAS dataset from the Wellcome Trust Case Control Consortium. Conclusion The performance of the classifiers in the test dataset was evaluated by comparing area under the receiver operating characteristic curves. Bayesian networks performed the best of all the data mining classifiers, but none of these did significantly better than the polygenic score approach. We further examined a subset of single-nucleotide polymorphisms (SNPs) in genes that are expressed in the brain, under the hypothesis that these might be most relevant to BP susceptibility, but all the classifiers performed worse with this reduced set of SNPs. The discriminative accuracy of all of these methods is unlikely to be of diagnostic or clinical utility at the present time. Further research is needed to develop strategies for selecting sets of SNPs likely to be relevant to disease susceptibility and to determine if other data mining classifiers that utilize other algorithms for inferring relationships among the sets of SNPs may perform better. Psychiatr Genet 22: 55-61 (C) 2012 Wolters Kluwer Health vertical bar Lippincott Williams & Wilkins.","Results We sought to compare the performance of five data mining methods, namely, Bayesian networks, support vector machine, random forest, radial basis function network, and logistic regression, against the polygenic scoring approach in the analysis of GWAS data on BP."
"Programmable Logic Controllers (PLC) are widely used in industry. Reliable PLC systems are vital to many critical applications. The reliability analysis of PLC is special because the hardware component and embedded software are combined and operated in a specific manner. This paper presents a probabilistic modeling for PLC systems, which considers both the hardware and software components. Three analysis strategies are proposed for probabilistic evaluation of reliability characterization. The first method is an input-based analysis which considers the impact of errors from primary inputs. The second one is an action-based analysis. It extends the first method by considering the impact of software-based processing deviations on primary inputs. The third method is an action-traverse analysis. It computes the reliability characterization in a single topological traverse through the primary inputs during the software execution. Experimental results demonstrate the effectiveness of our approaches when compared to fault tree analysis. (C) 2011 Elsevier Ltd. All rights reserved.",""
"Belief revision is the problem of finding the most plausible explanation for an observed set of evidences. It has many applications in various scientific domains like natural language understanding, medical diagnosis and computational biology. Bayesian Networks (BN) is an important probabilistic graphical formalism widely used for belief revision tasks. In BN, belief revision can be achieved by finding the maximum a posteriori (MAP) assignment. Finding MAP is an NP-Hard problem. In previous work, we showed how to find the MAP assignment in BN using High Order Recurrent Neural Networks (HORN) through an intermediate representation of Cost-Based Abduction. This method eliminates the need to explicitly construct the energy function in two steps, objective and constraints. This paper builds on that previous work by providing the theoretical foundation and proving that the resultant HORN used to find MAP is strongly equivalent to the original BN it tries to solve. (C) 2011 Elsevier B.V. All rights reserved.",""
"Modern biology and medicine aim at hunting molecular and cellular causes of biological functions and diseases. Gene regulatory networks (GRN) inferred from gene expression data are considered an important aid for this research by providing a map of molecular interactions. Hence, GRNs have the potential enabling and enhancing basic as well as applied research in the life sciences. In this paper, we introduce a new method called BC3NET for inferring causal gene regulatory networks from large-scale gene expression data. BC3NET is an ensemble method that is based on bagging the C3NET algorithm, which means it corresponds to a Bayesian approach with noninformative priors. In this study we demonstrate for a variety of simulated and biological gene expression data from S. cerevisiae that BC3NET is an important enhancement over other inference methods that is capable of capturing biochemical interactions from transcription regulation and proteinprotein interaction sensibly. An implementation of BC3NET is freely available as an R package from the CRAN repository.","cerevisiae that BC3NET is an important enhancement over other inference methods that is capable of capturing biochemical interactions from transcription regulation and proteinprotein interaction sensibly."
"By probing its functional anatomy, the default mode network (DMN) can be considered consisting of two interacting hub and non-hub subsystems. The hub subsystem includes posterior cingulate cortex (PCC), medial prefrontal cortex (MPFC) and bilateral inferior parietal cortex (IPC). The non-hub subsystem contains inferior temporal cortex (ITC) and (para) hippocampus (HC). In this study, Gaussian Bayesian Network (BN) and Gaussian Dynamic Bayesian Network (DBN) were applied separately to detect the instantaneous and temporal connection relationship within each and between the two DMN subsystems. It was found that the directional instantaneous interactions between the two subsystems were primarily \"from non-hub to hub\". The temporal interactions between hub and non-hub regions, on the other hand, are less presented between the two subsystems. The hub subsystem demonstrated both strong instantaneous and temporal interactions among the hub regions, while the non-hub regions were only strongly inter-connected instantaneously but temporally isolated with each other. In addition, one of the hub regions, PCC, appears to be a confluent node and important in the functional integration within the network. (c) 2012 Elsevier Ireland Ltd. All rights reserved.",""
"Background: Identification of active causal regulators is a crucial problem in understanding mechanism of diseases or finding drug targets. Methods that infer causal regulators directly from primary data have been proposed and successfully validated in some cases. These methods necessarily require very large sample sizes or a mix of different data types. Recent studies have shown that prior biological knowledge can successfully boost a method's ability to find regulators. Results: We present a simple data-driven method, Correlation Set Analysis (CSA), for comprehensively detecting active regulators in disease populations by integrating co-expression analysis and a specific type of literature-derived causal relationships. Instead of investigating the co-expression level between regulators and their regulatees, we focus on coherence of regulatees of a regulator. Using simulated datasets we show that our method performs very well at recovering even weak regulatory relationships with a low false discovery rate. Using three separate real biological datasets we were able to recover well known and as yet undescribed, active regulators for each disease population. The results are represented as a rank-ordered list of regulators, and reveals both single and higher-order regulatory relationships. Conclusions: CSA is an intuitive data-driven way of selecting directed perturbation experiments that are relevant to a disease population of interest and represent a starting point for further investigation. Our findings demonstrate that combining co-expression analysis on regulatee sets with a literature-derived network can successfully identify causal regulators and help develop possible hypothesis to explain disease progression.",""
"Bayesian belief networks (BBNs) are a widespread tool for modelling the effects of management decisions and activities on a variety of environmental and ecological responses. Parameterisation of BBNs is often achieved by elicitation involving multiple experts, and this may result in different conditional probability distribution tables for the nodes in a BBN. Another common use of BBNs is in the comparison of alternative management scenarios. This paper describes and implements the 'belief index' (BI), an empirical measure for evaluating outcomes in BBN modelling that summarises the probabilities (or beliefs) of any one node in a BBN. A set of four species-specific BBNs for managing watering events for wetland fish is outlined and used to statistically assess between-expert and between-species variability in parameter estimates by means of the BI. Different scenarios for management decisions are also compared using the % improvement measure, a derivative of the BI. (C) 2012 Published by Elsevier B.V.",""
"Online path planning (OPP) for unmanned aerial vehicles (UAVs) is a basic issue of intelligent flight and is indeed a dynamic multi-objective optimization problem (DMOP). In this paper, an OPP framework is proposed in the sense of model predictive control (MPC) to continuously update the environmental information for the planner. For solving the DMOP involved in the MPC we propose a dynamic multi-objective evolutionary algorithm based on linkage and prediction (LP-DMOEA). Within this algorithm, the historical Pareto sets are collected and analysed to enhance the performance. For intelligently selecting the best path from the output of the OPP, the Bayesian network and fuzzy logic are used to quantify the bias to each optimization objective. The DMOEA is validated on three benchmark problems characterized by different changing types in decision and objective spaces. Moreover, the simulation results show that the LP-DMOEA overcomes the restart method for OPP. The decision-making method for solution selection can assess the situation in an adversarial environment and accordingly adapt the path planner.",""
"The existing incremental learning algorithms for Bayesian networks have shortcomings such as fallibility, premature convergence to local maximum, and high space complexity. In order to overcome the issues, in this paper we proposed a novel algorithm based on chaotic dual-population evolution strategies, we designed an optimal encoding scheme, fitness function and the evolutionary operators. The algorithm keeps two parallel populations during evolution, and adopts chaotic theory to make the evolution has both strong abilities of global and local search. Furthermore, we discussed the application of the proposed algorithm to nanoelectronics area, and proposed dynamic reliability estimation and error detection of nanoelectronics devices based on the proposed incremental learning algorithm. Theoretical analysis and experimental results demonstrate the proposed algorithm has higher accuracy and lower space complexity than existing state-of-the-art methods.",""
"Possibilistic networks, which are compact representations of possibility distributions, are powerful tools for representing and reasoning with uncertain and incomplete information in the framework of possibility theory. They are like Bayesian networks but lie on possibility theory to deal with uncertainty, imprecision and incompleteness. While classification is a very useful task in many real world applications, possibilistic network-based classification issues are not well investigated in general and possibilistic-based classification inference with uncertain observations in particular. In this paper, we address on one hand the theoretical foundations of inference in possibilistic classifiers under uncertain inputs and propose on the other hand a novel efficient algorithm for the inference in possibilistic network-based classification under uncertain observations. We start by studying and analyzing the counterpart of Jeffrey's rule in the framework of possibility theory. After that, we address the validity of Markov-blanket criterion in the context of possibilistic networks used for classification with uncertain inputs purposes. Finally, we propose a novel algorithm suitable for possibilistic classifiers with uncertain observations without assuming any independence relations between observations. This algorithm guarantees the same results as if classification were performed using the possibilistic counterpart of Jeffrey's rule. Classification is achieved in polynomial time if the target variable is binary. The basic idea of our algorithm is to only search for totally plausible class instances through a series of equivalent and polynomial transformations applied on the possibilistic classifier taking into account the uncertain observations.","While classification is a very useful task in many real world applications, possibilistic network-based classification issues are not well investigated in general and possibilistic-based classification inference with uncertain observations in particular."
"The process by which genes and memes influence behaviour is poorly understood. Genes generally may have a strong influence as predispositions directing individuals towards certain behaviours; whereas memes may have a less direct influence as information inputs to cognitive processes determining behaviour. In certain areas of medical science, knowledge has progressed towards approximate quantification of genetic influences, while social psychology can provide models of mimetic influence as the spread of attitudes. This paper describes a computational model integration of genetic and mimetic influences in a healthcare domain. It models mimetic influences of advertising and health awareness messages in populations with genetic predispositions towards obesity; environmental variables influence both gene expression and mimetic force. Sensitivity analysis using the model with different population network structures is used to investigate the relative force of meme spread and influence.",""
"Tsunami early warning (TEW) is a challenging task as a decision has to be made within few minutes on the basis of incomplete and error-prone data. Deterministic warning systems have difficulties in integrating and quantifying the intrinsic uncertainties. In contrast, probabilistic approaches provide a framework that handles uncertainties in a natural way. Recently, we have proposed a method using Bayesian networks (BNs) that takes into account the uncertainties of seismic source parameter estimates in TEW. In this follow-up study, the method is applied to 10 recent large earthquakes offshore Sumatra and tested for its performance. We have evaluated both the general model performance given the best knowledge we have today about the source parameters of the 10 events and the corresponding response on seismic source information evaluated in real-time. We find that the resulting site-specific warning level probabilities represent well the available tsunami wave measurements and observations. Difficulties occur in the real-time tsunami assessment if the moment magnitude estimate is severely over- or underestimated. In general, the probabilistic analysis reveals a considerably large range of uncertainties in the near-field TEW. By quantifying the uncertainties the BN analysis provides important additional information to a decision maker in a warning centre to deal with the complexity in TEW and to reason under uncertainty.",""
"The use of neural signals for prosthesis control is an emerging frontier of research to restore lost function to amputees and the paralyzed. Electrocorticography (ECoG) brain-machine interfaces (BMI) are an alternative to EEG and neural spiking and local field potential BMI approaches. Conventional ECoG BMIs rely on spectral analysis at specific electrode sites to extract signals for controlling prostheses. We compare traditional features with information about the connectivity of an ECoG electrode network. We use time-varying dynamic Bayesian networks (TV-DBN) to determine connectivity between ECoG channels in humans during a motor task. We show that, on average, TV-DBN connectivity decreases from baseline preceding movement and then becomes negative, indicating an alteration in the phase relationship between electrode pairs. In some subjects, this change occurs preceding and during movement, before changes in low or high frequency power. We tested TV-DBN output in a hand kinematic decoder and obtained an average correlation coefficient (r(2)) between actual and predicted joint angle of 0.40, and as high as 0.66 in one subject. This result compares favorably with spectral feature decoders, for which the average correlation coefficient was 0.13. This work introduces a new feature set based on connectivity and demonstrates its potential to improve ECoG BMI accuracy.",""
"The application of the formal framework of causal Bayesian Networks to childrens causal learning provides the motivation to examine the link between judgments about the causal structure of a system, and the ability to make inferences about interventions on components of the system. Three experiments examined whether children are able to make correct inferences about interventions on different causal structures. The first two experiments examined whether childrens causal structure and intervention judgments were consistent with one another. In Experiment 1, children aged between 4 and 8 years made causal structure judgments on a three-component causal system followed by counterfactual intervention judgments. In Experiment 2, childrens causal structure judgments were followed by intervention judgments phrased as future hypotheticals. In Experiment 3, we explicitly told children what the correct causal structure was and asked them to make intervention judgments. The results of the three experiments suggest that the representations that support causal structure judgments do not easily support simple judgments about interventions in children. We discuss our findings in light of strong interventionist claims that the two types of judgments should be closely linked.","The application of the formal framework of causal Bayesian Networks to childrens causal learning provides the motivation to examine the link between judgments about the causal structure of a system, and the ability to make inferences about interventions on components of the system."
"We describe a framework and an algorithm for approximately solving a class of hybrid influence diagrams (IDs) containing discrete and continuous chance variables, discrete and continuous decision variables, and deterministic conditional distributions for chance variables. A conditional distribution for a chance variable is said to be deterministic if its variances, for each state of its parents, are all zeroes. The solution algorithm is an extension of Shenoy's fusion algorithm for discrete influence diagrams. To mitigate the integration and optimization problems associated with solving hybrid IDs, we propose using mixture of polynomials approximations of conditional probability density and utility functions and piecewise linear approximations of nonlinear deterministic conditional distributions for continuous chance variables. The class of hybrid IDs that can be solved by our framework are those that do not involve divisions. The framework and algorithm are illustrated by solving two small examples of hybrid IDs.",""
"For the computational analysis of biological problems-analyzing data, inferring networks and complex models, and estimating model parameters-it is common to use a range of methods based on probabilistic logic constructions, sometimes collectively called machine learning methods. Probabilistic modeling methods such as Bayesian Networks (BN) fall into this class, as do Hierarchical Bayesian Networks (HBN), Probabilistic Boolean Networks (PBN), Hidden Markov Models (HMM), and Markov Logic Networks (MLN). In this review, we describe the most general of these (MLN), and show how the above-mentioned methods are related to MLN and one another by the imposition of constraints and restrictions. This approach allows us to illustrate a broad landscape of constructions and methods, and describe some of the attendant strengths, weaknesses, and constraints of many of these methods. We then provide some examples of their applications to problems in biology and medicine, with an emphasis on genetics. The key concepts needed to picture this landscape of methods are the ideas of probabilistic graphical models, the structures of the graphs, and the scope of the logical language repertoire used (from First-Order Logic [FOL] to Boolean logic.) These concepts are interlinked and together define the nature of each of the probabilistic logic methods. Finally, we discuss the initial applications of MLN to genetics, show the relationship to less general methods like BN, and then mention several examples where such methods could be effective in new applications to specific biological and medical problems.",""
"We present a new Bayesian network modeling that learns the behavior of an unknown system from real data and can be used for reliability engineering and optimization processes in industrial systems. The suggested approach relies on quantitative criteria for addressing the trade-off between the complexity of a learned model and its prediction accuracy. These criteria are based on measures from Information Theory as they predetermine both the accuracy as well as the complexity of the model. We illustrate the proposed method by a classical example of system reliability engineering. Using computer experiments, we show how in a targeted Bayesian network learning, a tremendous reduction in the model complexity can be accomplished, while maintaining most of the essential information for optimizing the system.",""
"The application of social network theory to understanding the nature and occurrence of multiple transfers of particulate evidence is demonstrated to be a useful means of identifying the prevalence of such transfers. Four experimental scenarios of increasing complexity empirically demonstrate that this trace particulate evidence exhibits behaviour in accordance with that previously identified for hair and fibre evidence. However, the utilisation of social network concepts is demonstrated to add valuable insights into the behaviour of this form of evidence and is shown to be a useful tool in highlighting the complexity of evidence dynamics. The identification of the networks within which trace evidence is transferred has implications for the robust interpretation of particulate evidence during the process of crime reconstruction. (C) 2011 Forensic Science Society. Published by Elsevier Ireland Ltd. All rights reserved.",""
"Employing a probabilistic student model in a scientific inquiry learning environment often presents two challenges. First, what constitute the appropriate variables for modeling scientific inquiry skills in such a learning environment, considering the fact that it practices exploratory learning approach? Following exploratory learning approach, students are granted the freedom to navigate from one GUI to another. Second, do causal dependencies exist between the identified variables, and if they do, how should they be defined? To tackle the challenges, this research work attempted the Bayesian Networks framework. Leveraging on the framework, two student models were constructed to predict the acquisition of scientific inquiry skills for INQPRO, a scientific inquiry learning environment developed in this research work. The student models can be differentiated by the variables they modeled and the causal dependencies they encoded. An on-field evaluation involving 101 students was performed to assess the most appropriate structure of the INQPRO's student model. To ensure fairness in model comparison, the same Dynamic Bayesian Network (DBN) construction approach was employed. Lastly, this paper highlights the properties of the student model that provide optimal results for modeling scientific inquiry skill acquisition in INQPRO.",""
"Formal logical tools are able to provide some amount of reasoning support for information analysis, but are unable to represent uncertainty. Bayesian network tools represent probabilistic and causal information, but in the worst case scale as poorly as some formal logical systems and require specialized expertise to use effectively. We describe a framework for systems that incorporate the advantages of both Bayesian and logical systems. We define a formalism for the conversion of automatically generated natural deduction proof trees into Bayesian networks. We then demonstrate that the merging of such networks with domain-specific causal models forms a consistent Bayesian network with correct values for the formulas derived in the proof. In particular, we show that hard evidential updates in which the premises of a proof are found to be true force the conclusions of the proof to be true with probability one, regardless of any dependencies and prior probability values assumed for the causal model. We provide several examples that demonstrate the generality of the natural deduction system by using inference schemes not supportable directly in Horn clause logic. We compare our approach to other ones, including some that use non-standard logics.","We provide several examples that demonstrate the generality of the natural deduction system by using inference schemes not supportable directly in Horn clause logic."
"Bayesian networks are commonly used for determining the probability of events that are influenced by various variables. Bayesian probabilities encode degrees of belief about certain events, and a dynamic knowledge body is used to strengthen, update, or weaken these assumptions. The creation of Bayesian networks requires at least three challenging tasks: (i) the determination of relevant variables (nodes), (ii) the determination of relationships between the identified variables (links), and (iii) the calculation of the conditional probability tables (CPTs) for each node in the Bayesian network. Based on existing domain ontologies, we propose a method for the ontology-based construction of Bayesian networks. The method supports (i) the construction of the graphical Bayesian network structure (nodes and links), (ii) the construction of CPUs that preserve semantic constraints of the ontology, and (iii) the incorporation of already existing knowledge facts (findings). The developed method enables the efficient construction and modification of Bayesian networks based on existing ontologies. (C) 2011 Elsevier B.V. All rights reserved.",""
"In this paper, a real-time methodology for the detection of stress events while driving is presented. The detection is based on the use of physiological signals, i.e., electrocardiogram, electrodermal activity, and respiration, as well as past observations of driving behavior. Features are calculated over windows of specific length and are introduced in a Bayesian network to detect driver's stress events. The accuracy of the stress event detection based only on physiological features, evaluated on a data set obtained in real driving conditions, resulted in an accuracy of 82%. Enhancement of the stress event detection model with the incorporation of driving event information has reduced false positives, yielding an increased accuracy of 96%. Furthermore, our methodology demonstrates good adaptability due to the application of online learning of the model parameters.",""
"This paper presents a first-in-first-out (FIFO) rule consistent model for the continuous dynamic network loading problem. The model calculates the link travel time functions at a basic finite set of equally spaced times that are used to interpolate a monotone spline for all the other times. The model assumes a nonlinear link travel time function of the link volumes, but some corrections are made to satisfy the FIFO rule at the basic set. Furthermore, the use of monotone cubic splines preserving monotonicity guarantees that the FIFO rule is satisfied at all points. The model consists of five units: 1) a path origin flow wave definition unit; 2) a path wave propagation unit; 3) a congestion analysis unit; 4) a network flow propagation unit; and 5) an inference engine unit. The path flow intensity wave, which is the basic information, is modeled as a linear combination of basic waves. Next, the individual path waves are propagated throughout the paths by using a conservation equation that stretches or enlarges the wave lengths and increases or reduces the wave heights, depending on the degree of congestion at different links. Then, the individual path waves are combined together to generate the link and node waves. Finally, the inference engine unit combines all information items to make them compatible in times and locations using the aforementioned iterative method until convergence. The method is illustrated by some examples. The results seem to reproduce the observed trends closely. The required CPU times oscillated between seconds and a few minutes.","The model consists of five units: 1) a path origin flow wave definition unit; 2) a path wave propagation unit; 3) a congestion analysis unit; 4) a network flow propagation unit; and 5) an inference engine unit."
"In this paper we present the R package gRain for propagation in graphical independence networks (for which Bayesian networks is a special instance). The paper includes a description of the theory behind the computations. The main part of the paper is an illustration of how to use the package. The paper also illustrates how to turn a graphical model and data into an independence network.",""
"Millions of individuals have congenital or acquired neuro-motor conditions that limit control of their muscles, including those that manipulate the vocal tract. These conditions, collectively called dysarthria, result in speech that is very difficult to understand both by human listeners and by traditional automatic speech recognition (ASR), which in some cases can be rendered completely unusable. In this work we first introduce a new method for acoustic-to-articulatory inversion which estimates positions of the vocal tract given acoustics using a nonlinear Hammerstein system. This is accomplished based on the theory of task-dynamics using the TORGO database of dysarthric articulation. Our approach uses adaptive kernel canonical correlation analysis and is found to be significantly more accurate than mixture density networks, at or above the 95% level of confidence for most vocal tract variables. Next, we introduce a new method for ASR in which acoustic-based hypotheses are re-evaluated according to the likelihoods of their articulatory realizations in task-dynamics. This approach incorporates high-level, long-term aspects of speech production and is found to be significantly more accurate than hidden Markov models, dynamic Bayesian networks, and switching Kalman filters. (C) 2011 Elsevier B.V. All rights reserved.",""
"We give a definition of a bounded edge within the causal directed acyclic graph framework. A bounded edge generalizes the notion of a signed edge and is defined in terms of bounds on a ratio of survivor probabilities. We derive rules concerning the propagation of bounds. Bounds on causal effects in the presence of unmeasured confounding are also derived using bounds related to specific edges on a graph. We illustrate the theory developed by an example concerning estimating the effect of antihistamine treatment on asthma in the presence of unmeasured confounding.",""
"We propose a new type of multivariate statistical model that permits non-Gaussian distributions as well as the inclusion of conditional independence assumptions specified by a directed acyclic graph. These models feature a specific factorisation of the likelihood that is based on pair-copula constructions and hence involves only univariate distributions and bivariate copulas, of which some may be conditional. We demonstrate maximum-likelihood estimation of the parameters of such models and compare them to various competing models from the literature. A simulation study investigates the effects of model misspecification and highlights the need for non-Gaussian conditional independence models. The proposed methods are finally applied to modeling financial return data. The Canadian Journal of Statistics 40: 86109; 2012 (C) 2012 Statistical Society of Canada",""
"Two approaches for extending a quantitative Bayesian network (BN) to deal with qualitative information include a qualitative probability network extension and causal independence. Both approaches help developers to remedy the gap between the complicated BN formalism and the actual problem. Lucas utilizes these two methods in establishing a theory of qualitative causal (QC) interaction patterns where qualitative probability influences (QPIs) (considering whether it is better to hold a single cause than not) affect the network model. QC patterns help to offer developers a high-level starting point when developing BNs. However, in real-world applications, usually, we need to know QPIs on multiple causes, namely, whether holding some subset of the causes will be a better choice than holding other subsets. To this end, we introduce a concept called causality probability from which QPIs can be easily induced. We investigate the local optima and global optima of causality probabilities considering multiple causes. For the local optima, we present the qualitative influences on all binary interaction types, while for the global optima, we achieve an upper bound of causality probability and discuss the conditions to reach the upper bound. Our results are useful for BN developers to get an overview of causality relations.",""
"Almost 30 years ago, Bayesian networks (BNs) were developed in the field of artificial intelligence as a framework that should assist researchers and practitioners in applying the theory of probability to inference problems of more substantive size and, thus, to more realistic and practical problems. Since the late 1980s, Bayesian networks have also attracted researchers in forensic science and this tendency has considerably intensified throughout the last decade. This review article provides an overview of the scientific literature that describes research on Bayesian networks as a tool that can be used to study, develop and implement probabilistic procedures for evaluating the probative value of particular items of scientific evidence in forensic science. Primary attention is drawn here to evaluative issues that pertain to forensic DNA profiling evidence because this is one of the main categories of evidence whose assessment has been studied through Bayesian networks. The scope of topics is large and includes almost any aspect that relates to forensic DNA profiling. Typical examples are inference of source (or, 'criminal identification'), relatedness testing, database searching and special trace evidence evaluation (such as mixed DNA stains or stains with low quantities of DNA). The perspective of the review presented here is not exclusively restricted to DNA evidence, but also includes relevant references and discussion on both, the concept of Bayesian networks as well as its general usage in legal sciences as one among several different graphical approaches to evidence evaluation. (C) 2011 Elsevier Ireland Ltd. All rights reserved.","Almost 30 years ago, Bayesian networks (BNs) were developed in the field of artificial intelligence as a framework that should assist researchers and practitioners in applying the theory of probability to inference problems of more substantive size and, thus, to more realistic and practical problems."
"Computational tools for analyzing biochemical phenomena are becoming increasingly important. Recently, high-level formal languages for modeling and simulating biochemical reactions have been proposed. These languages make the formal modeling of complex reactions accessible to domain specialists outside of theoretical computer science. This research explores the use of genetic programming to automate the construction of models written in one such language. Given a description of desired time-course data, the goal is for genetic programming to construct a model that might generate the data. The language investigated is Kahramanogullari's and Cardelli's Programming Interface for Modeling (PIM) language. The PIM syntax is defined in a grammar-guided genetic programming system. All time series generated during simulations are described by statistical feature tests, and the fitness evaluation compares feature proximity between the target and candidate solutions. PIM models of varying complexity were used as target expressions for genetic programming, and were successfully reconstructed in all cases. This shows that the compositional nature of PIM models is amenable to genetic program search.",""
"The metabolic syndrome is a set of risk factors that include abdominal obesity, insulin resistance, dyslipidemia and hypertension. It has affected around 25% of adults in the US and become a serious problem in Asian countries recently due to the change in dietary habit and life style. On the other hand, Bayesian networks that are the models to solve the problems of uncertainty provide a robust and transparent formalism for probabilistic modeling, so they have been used as a method for diagnostic or prognostic model in medical domain. Since the K2 algorithm, a well-known algorithm for Bayesian networks structure learning, is influenced by an input order of the attributes, an optimization of BN attribute ordering has been studied as a research issue. This paper proposes a novel ordering optimization method using a genetic algorithm based on medical expert knowledge in order to solve this problem. For experiments, we use the dataset examined twice in 1993 and 1995 in Yonchon County of Korea. It has 18 attributes of 1193 subjects participated in both surveys. Using this dataset, we make the prognostic model of the metabolic syndrome using Bayesian networks with an optimized ordering by evolutionary approach. Through an ordering optimization, the prognostic model of higher performance is constructed, and the optimized Bayesian network model by the proposed method outperforms the conventional BN model as well as neural networks and k-nearest neighbors. Finally, we present the application program using the prognostic model of the metabolic syndrome in order to show the usefulness of the proposed method. (C) 2011 Elsevier Ltd. All rights reserved.",""
"A typical characteristic of refrigeration compressor performance tests is their long duration. A reduction in the time periods related to this activity can be achieved using unsteady-state data analysis. This paper presents an original approach to predicting compressor performance using Bayesian networks and a hybrid Fuzzy-Bayesian network. All analysis was performed using real test data. (C) 2011 Elsevier Ltd. All rights reserved.",""
"The health risk of noncarcinogenic substances is usually represented by the hazard quotient (HQ) or target organ-specific hazard index (TOSHI). However, three problems arise from these indicators. Firstly, the HQ overestimates the health risk of noncarcinogenic substances for non-critical organs. Secondly, the TOSHI makes inappropriately the additive assumption for multiple hazardous substances affecting the same organ. Thirdly, uncertainty of the TOSHI undermines the accuracy of risk characterization. To address these issues, this article proposes the use of Bayesian belief networks (BBN) for health risk assessment (HRA) and the procedure involved is developed using the example of road constructions. According to epidemiological studies and using actual hospital attendance records, the BBN-HRA can specifically identify the probabilistic relationship between an air pollutant and each of its induced disease, which can overcome the overestimation of the HQ for non-critical organs. A fusion technique of conditional probabilities in the BBN-HRA is devised to avoid the unrealistic additive assumption. The use of the BBN-HRA is easy even for those without HRA knowledge. The input of pollution concentrations into the model will bring more concrete information on the morbidity and mortality rates of all the related diseases rather than a single score, which can reduce the uncertainty of the TOSHI.",""
"In this paper, we investigate the problem of reverse engineering the topology of gene regulatory networks from temporal gene expression data. We adopt a computational intelligence approach comprising swarm intelligence techniques, namely particle swarm optimization (PSO) and ant colony optimization (ACO). In addition, the recurrent neural network (RNN) formalism is employed for modeling the dynamical behavior of gene regulatory systems. More specifically, ACO is used for searching the discrete space of network architectures and PSO for searching the corresponding continuous space of RNN model parameters. We propose a novel solution construction process in the context of ACO for generating biologically plausible candidate architectures. The objective is to concentrate the search effort into areas of the structure space that contain architectures which are feasible in terms of their topological resemblance to real-world networks. The proposed framework is initially applied to the reconstruction of a small artificial network that has previously been studied in the context of gene network reverse engineering. Subsequently, we consider an artificial data set with added noise for reconstructing a subnetwork of the genetic interaction network of S. cerevisiae (yeast). Finally, the framework is applied to a real-world data set for reverse engineering the SOS response system of the bacterium Escherichia coli. Results demonstrate the relative advantage of utilizing problem-specific knowledge regarding biologically plausible structural properties of gene networks over conducting a problem-agnostic search in the vast space of network architectures.",""
"Network inference algorithms can assist life scientists in unraveling gene-regulatory systems on a molecular level. In recent years, great attention has been drawn to the reconstruction of Boolean networks from time series. These need to be binarized, as such networks model genes as binary variables (either \"expressed\" or \"not expressed\"). Common binarization methods often cluster measurements or separate them according to statistical or information theoretic characteristics and may require many data points to determine a robust threshold. Yet, time series measurements frequently comprise only a small number of samples. To overcome this limitation, we propose a binarization that incorporates measurements at multiple resolutions. We introduce two such binarization approaches which determine thresholds based on limited numbers of samples and additionally provide a measure of threshold validity. Thus, network reconstruction and further analysis can be restricted to genes with meaningful thresholds. This reduces the complexity of network inference. The performance of our binarization algorithms was evaluated in network reconstruction experiments using artificial data as well as real-world yeast expression time series. The new approaches yield considerably improved correct network identification rates compared to other binarization techniques by effectively reducing the amount of candidate networks.","Network inference algorithms can assist life scientists in unraveling gene-regulatory systems on a molecular level."
"We present a maximum margin parameter learning algorithm for Bayesian network classifiers using a conjugate gradient (CG) method for optimization. In contrast to previous approaches, we maintain the normalization constraints on the parameters of the Bayesian network during optimization, i.e., the probabilistic interpretation of the model is not lost. This enables us to handle missing features in discriminatively optimized Bayesian networks. In experiments, we compare the classification performance of maximum margin parameter learning to conditional likelihood and maximum likelihood learning approaches. Discriminative parameter learning significantly outperforms generative maximum likelihood estimation for naive Bayes and tree augmented naive Bayes structures on all considered data sets. Furthermore, maximizing the margin dominates the conditional likelihood approach in terms of classification performance in most cases. We provide results for a recently proposed maximum margin optimization approach based on convex relaxation [1]. While the classification results are highly similar, our CG-based optimization is computationally up to orders of magnitude faster. Margin-optimized Bayesian network classifiers achieve classification performance comparable to support vector machines (SVMs) using fewer parameters. Moreover, we show that unanticipated missing feature values during classification can be easily processed by discriminatively optimized Bayesian network classifiers, a case where discriminative classifiers usually require mechanisms to complete unknown feature values in the data first.","We present a maximum margin parameter learning algorithm for Bayesian network classifiers using a conjugate gradient (CG) method for optimization."
"Recently, demand for service robots increases, and, particularly, one for personal service robots, which requires robot intelligence, will be expected to increase more. Accordingly, studies on intelligent robots are spreading all over the world. In this situation, we attempt to realize context-awareness for home robot while previous robot research focused on image processing, control and low-level context recognition. This paper uses probabilistic modeling for service robots to provide users with high-level context-aware services required in home environment, and proposes a systematic modeling approach for modeling a number of Bayesian networks. The proposed approach supplements uncertain sensor input using Bayesian network modeling and enhances the efficiency in modeling and reasoning processes using modular design based on domain knowledge. We verify the proposed method is useful as measuring the performance of context-aware module and conducting subjective test. (C) 2011 Elsevier Ltd. All rights reserved.",""
"Clinicians often experience difficulties in the diagnosis of dementia due to the intrinsic complexity of the process and lack of comprehensive diagnostic tools. Different models have been proposed to provide medical decision support in dementia diagnosis. The aim of this study is to improve on the performance of a recent application of Bayesian belief networks using an alternative approach based on logistic regression. A pool of 14 variables has been evaluated in a sample of 164 patients suspected of dementia. First, a logistic regression model for dementia prediction is developed using all variables included in the previous model; then, a second model is built using a stepwise logistic regression starting with all collected variables and selecting the pool of the relevant ones. A range of performance metrics have been used to evaluate the developed models. The new models have resulted in very good predictive power, demonstrating general performance improvement compared to a state-of-the-art prediction model. Interestingly, the approach based on statistical variables selection outperformed the model which used variables selected by domain experts in the previous study. Further collaborative studies are now required to determine the optimal approach and to overcome existing limitations imposed by the size of the considered sample. (C) 2011 Elsevier Ltd. All rights reserved.","The aim of this study is to improve on the performance of a recent application of Bayesian belief networks using an alternative approach based on logistic regression."
"In this paper we propose a framework, called mixtures of truncated basis functions (MoTBFs), for representing general hybrid Bayesian networks. The proposed framework generalizes both the mixture of truncated exponentials (MTEs) framework and the Mixture of Polynomials (MoPs) framework. Similar to MTEs and MoPs. MoTBFs are defined so that the potentials are closed under combination and marginalization, which ensures that inference in MoTBF networks can be performed efficiently using the Shafer-Shenoy architecture. Based on a generalized Fourier series approximation, we devise a method for efficiently approximating an arbitrary density function using the MoTBF framework. The translation method is more flexible than existing MTE or MoP-based methods, and it supports an online/anytime tradeoff between the accuracy and the complexity of the approximation. Experimental results show that the approximations obtained are either comparable or significantly better than the approximations obtained using existing methods. (C) 2011 Elsevier Inc. All rights reserved.","MoTBFs are defined so that the potentials are closed under combination and marginalization, which ensures that inference in MoTBF networks can be performed efficiently using the Shafer-Shenoy architecture."
"Bayesian networks (BNs) provide a powerful graphical model for encoding the probabilistic relationships among a set of variables, and hence can naturally be used for classification. However, Bayesian network classifiers (BNCs) learned in the common way using likelihood scores usually tend to achieve only mediocre classification accuracy because these scores are less specific to classification, but rather suit a general inference problem. We propose risk minimization by cross validation (RMCV) using the 0/1 loss function, which is a classification-oriented score for unrestricted BNCs. RMCV is an extension of classification-oriented scores commonly used in learning restricted BNCs and non-BN classifiers. Using small real and synthetic problems, allowing for learning all possible graphs, we empirically demonstrate RMCV superiority to marginal and class-conditional likelihood-based scores with respect to classification accuracy. Experiments using twenty-two real-world datasets show that BNCs learned using an RMCV-based algorithm significantly outperform the naive Bayesian classifier (NBC), tree augmented NBC (TAN), and other BNCs learned using marginal or conditional likelihood scores and are on par with non-BN state of the art classifiers, such as support vector machine, neural network, and classification tree. These experiments also show that an optimized version of RMCV is faster than all unrestricted BNCs and comparable with the neural network with respect to run-time. The main conclusion from our experiments is that unrestricted BNCs, when learned properly, can be a good alternative to restricted BNCs and traditional machine-learning classifiers with respect to both accuracy and efficiency. (C) 2011 Elsevier Inc. All rights reserved.","Bayesian networks (BNs) provide a powerful graphical model for encoding the probabilistic relationships among a set of variables, and hence can naturally be used for classification."
"Complex software networks, as a typical kind of man-made complex networks, have attracted more and more attention from various fields of science and engineering over the past ten years. With the dramatic increase of scale and complexity of software systems, it is essential to develop a systematic approach to further investigate the complex software systems by using the theories and methods of complex networks and complex adaptive systems. This paper attempts to briefly review some recent advances in complex software networks and also develop some novel tools to further analyze complex software networks, including modeling, analysis, evolution, measurement, and some potential real-world applications. More precisely, this paper first describes some effective modeling approaches for characterizing various complex software systems. Based on the above theoretical and practical models, this paper introduces some recent advances in analyzing the static and dynamical behaviors of complex software networks. It is then followed by some further discussions on potential real-world applications of complex software networks. Finally, this paper outlooks some future research topics from an engineering point of view.",""
"During the post-World War II era, the Mojave Desert Region of San Bernardino County, California, has experienced rapid levels of population growth. Over the past several decades, growth has accelerated, accompanied by significant shifts in ethnic composition, most notably from predominantly White non-Hispanic to Hispanic. This study explores the impacts of changing ethnicity on future development and the loss of open space by modeling ethnic propensities regarding family size and settlement preferences reflected by U.S. Census Bureau data. Demographic trends and land conversion data were obtained for seven Mojave Desert communities for the period between 1990 and 2001. Using a spatially explicit, logistic regression-based urban growth model, these data and trends were used to project community-specific future growth patterns from 2000 to 2020 under three future settlement scenarios: (1) an \"historic\" scenario reported in earlier research that uses a Mojave-wide average settlement density of 3.76 persons/ha; (2) an \"existing\" scenario based on community-specific settlement densities as of 2001; and (3) a \"demographic futures\" scenario based on community-specific settlement densities that explicitly model the Region's changing ethnicity. Results found that under the demographic futures scenario, by 2020 roughly 53% of within-community open space would remain, under the existing scenario only 40% would remain, and under the historic scenario model the communities would have what amounts to a deficit of open space. Differences in the loss of open space across the scenarios demonstrate the importance of considering demographic trends that are reflective of the residential needs and preferences of projected future populations.","Using a spatially explicit, logistic regression-based urban growth model, these data and trends were used to project community-specific future growth patterns from 2000 to 2020 under three future settlement scenarios: (1) an \"historic\" scenario reported in earlier research that uses a Mojave-wide average settlement density of 3."
"Information diffusion and virus propagation are fundamental processes taking place in networks. While it is often possible to directly observe when nodes become infected with a virus or publish the information, observing individual transmissions (who infects whom, or who influences whom) is typically very difficult. Furthermore, in many applications, the underlying network over which the diffusions and propagations spread is actually unobserved. We tackle these challenges by developing a method for tracing paths of diffusion and influence through networks and inferring the networks over which contagions propagate. Given the times when nodes adopt pieces of information or become infected, we identify the optimal network that best explains the observed infection times. Since the optimization problem is NP-hard to solve exactly, we develop an efficient approximation algorithm that scales to large datasets and finds provably near-optimal networks. We demonstrate the effectiveness of our approach by tracing information diffusion in a set of 170 million blogs and news articles over a one year period to infer how information flows through the online media space. We find that the diffusion network of news for the top 1,000 media sites and blogs tends to have a core-periphery structure with a small set of core media sites that diffuse information to the rest of the Web. These sites tend to have stable circles of influence with more general news media sites acting as connectors between them.",""
"The protozoan parasite Trichomonas vaginalis is the causative agent of trichomoniasis, the most widespread nonviral sexually transmitted disease in humans. It possesses hydrogenosomes-anaerobic mitochondria that generate H-2, CO2, and acetate from pyruvate while converting ADP to ATP via substrate-level phosphorylation. T. vaginalis hydrogenosomes lack a genome and translation machinery; hence, they import all their proteins from the cytosol. To date, however, only 30 imported proteins have been shown to localize to the organelle. A total of 226 nuclear-encoded proteins inferred from the genome sequence harbor a characteristic short N-terminal presequence, reminiscent of mitochondrial targeting peptides, which is thought to mediate hydrogenosomal targeting. Recent studies suggest, however, that the presequences might be less important than previously thought. We sought to identify new hydrogenosomal proteins within the 59,672 annotated open reading frames (ORFs) of T. vaginalis, independent of the N-terminal targeting signal, using a machine learning approach. Our training set included 57 gene and protein features determined for all 30 known hydrogenosomal proteins and 576 nonhydrogenosomal proteins. Several classifiers were trained on this set to yield an import score for all proteins encoded by T. vaginalis ORFs, predicting the likelihood of hydrogenosomal localization. The machine learning results were tested through immunofluorescence assay and immunodetection in isolated cell fractions of 14 protein predictions using hemagglutinin constructs expressed under the homologous SCS alpha promoter in transiently transformed T. vaginalis cells. Localization of 6 of the 10 top predicted hydrogenosome-localized proteins was confirmed, and two of these were found to lack an obvious N-terminal targeting signal.","Several classifiers were trained on this set to yield an import score for all proteins encoded by T."
"Vehicle classification has evolved into a significant subject of study due to its importance in autonomous navigation, traffic analysis, surveillance and security systems, and transportation management. While numerous approaches have been introduced for this purpose, no specific study has been conducted to provide a robust and complete video-based vehicle classification system based on the rear-side view where the camera's field of view is directly behind the vehicle. In this paper, we present a stochastic multiclass vehicle classification system which classifies a vehicle (given its direct rear-side view) into one of four classes: sedan, pickup truck, SUV/minivan, and unknown. A feature set of tail light and vehicle dimensions is extracted which feeds a feature selection algorithm to define a low-dimensional feature vector. The feature vector is then processed by a hybrid dynamic Bayesian network to classify each vehicle. Results are shown on a database of 169 videos for four classes.","Vehicle classification has evolved into a significant subject of study due to its importance in autonomous navigation, traffic analysis, surveillance and security systems, and transportation management."
"To understand the processes of growth and biomass production in plants, we ultimately need to elucidate the structure of the underlying regulatory networks at the molecular level. The advent of high-throughput postgenomic technologies has spurred substantial interest in reverse engineering these networks from data, and several techniques from machine learning and multivariate statistics have recently been proposed. The present article discusses the problem of inferring gene regulatory networks from gene expression time series, and we focus our exposition on the methodology of Bayesian networks. We describe dynamic Bayesian networks and explain their advantages over other statistical methods. We introduce a novel information sharing scheme, which allows us to infer gene regulatory networks from multiple sources of gene expression data more accurately. We illustrate and test this method on a set of synthetic data, using three different measures to quantify the network reconstruction accuracy. The main application of our method is related to the problem of circadian regulation in plants, where we aim to reconstruct the regulatory networks of nine circadian genes in Arabidopsis thaliana from four gene expression time series obtained under different experimental conditions.",""
"Recent work on the interpretation of counterfactual conditionals has paid much attention to the role of causal independencies. One influential idea from the theory of Causal Bayesian Networks is that counterfactual assumptions are made by intervention on variables, leaving all of their causal non-descendants unaffected. But intervention is not applicable across the board. For instance, backtracking counterfactuals, which involve reasoning from effects to causes, cannot proceed by intervention in the strict sense, for otherwise they would be equivalent to their consequents. We discuss these and similar cases, focusing on two factors which play a role in determining whether and which causal parents of the manipulated variable are affected: Speakers' need for an explanation of the hypothesized state of affairs, and differences in the resilience of beliefs that are independent of degrees of certainty. We describe the relevant theoretical notions in some detail and provide experimental evidence that these factors do indeed affect speakers' interpretation of counterfactuals.",""
"Several methods have been presented in the literature that successfully used SIFT features for object identification, as they are reasonably invariant to translation, rotation, scale, illumination and partial occlusion. However, they have poor performance for classification tasks. In this work, SIFT features are used to solve object class recognition problems in images using a two-step process. In its first step, the proposed method performs clustering on the extracted features in order to characterize the appearance of the different classes. Then, in the classification step, it uses a three layer Bayesian network for object class recognition. Experiments show quantitatively that clusters of SIFT features are suitable to represent classes of objects. The main contributions of this paper are the introduction of a Bayesian network approach in the classification step to improve performance in an object class recognition task, and a detailed experimentation that shows robustness to changes in illumination, scale, rotation and partial occlusion. (C) 2011 Elsevier Ltd. All rights reserved.","However, they have poor performance for classification tasks."
"Pervasive computing applications often involve sensor-rich networking environments that capture various types of user contexts such as locations, activities, vital signs, and so on. Such context information is useful in a variety of applications, for example, monitoring health information to promote independent living in \"aging-in-place\" scenarios, or providing safety and security of people and infrastructures. In reality, both sensed and interpreted contexts are often ambiguous, thus leading to potentially dangerous decisions if not properly handled. Therefore, a significant challenge in the design and development of realistic and deployable context-aware services for pervasive computing applications lies in the ability to deal with ambiguous contexts. In this paper, we propose a resource-optimized, quality-assured context mediation framework for sensor networks. The underlying approach is based on efficient context-aware data fusion, information-theoretic reasoning, and selection of sensor parameters, leading to an optimal state estimation. In particular, we apply dynamic Bayesian networks to derive context and deal with context ambiguity or error in a probabilistic manner. Experimental results using SunSPOT sensors demonstrate the promise of this approach.",""
"The concept class C-N induced by a Bayesian network N can be embedded into some Euclidean inner product space. The Vapnik-Chervonenkis (VC)-dimension of the concept class and the minimum dimension of the inner product space are very important indicators for evaluating the classification capability of the Bayesian network. In this paper, we investigate the properties of the concept class C-Ni induced by a multivalued Bayesian network N-k, where each node X-i of N-k is a k-valued variable. We focus on the values of two dimensions: (i) the VC-dimension of the concept class denoted as VCdim(N-k), and (ii) the minimum dimension of the inner product space into which C-Nk can be embedded. We show that the values of these two dimensions are k(n) - 1 for fully connected k-valued Bayesian networks N-F(k) with n variables. For non-fully connected k-valued Bayesian networks N-k without V-structure, we prove that the two dimensional values are (k - 1)Sigma(n)(i-1) k(mi) + 1, where m(i) denotes the number of parents for the i(th) variable. We also derive the upper and lower bounds on the minimum dimension of the inner product space induced by non-fully connected Bayesian networks. (C) 2011 Elsevier Inc. All rights reserved.","The Vapnik-Chervonenkis (VC)-dimension of the concept class and the minimum dimension of the inner product space are very important indicators for evaluating the classification capability of the Bayesian network."
"Background: In System Biology, iterations of wet-lab experiments followed by modelling approaches and model-inspired experiments describe a cyclic workflow. This approach is especially useful for the inference of gene regulatory networks based on high-throughput gene expression data. Experiments can verify or falsify the predicted interactions allowing further refinement of the network model. Aspergillus fumigatus is a major human fungal pathogen. One important virulence trait is its ability to gain sufficient amounts of iron during infection process. Even though some regulatory interactions are known, we are still far from a complete understanding of the way iron homeostasis is regulated. Results: In this study, we make use of a reverse engineering strategy to infer a regulatory network controlling iron homeostasis in A. fumigatus. The inference approach utilizes the temporal change in expression data after a change from iron depleted to iron replete conditions. The modelling strategy is based on a set of linear differential equations and offers the possibility to integrate known regulatory interactions as prior knowledge. Moreover, it makes use of important selection criteria, such as sparseness and robustness. By compiling a list of known regulatory interactions for iron homeostasis in A. fumigatus and softly integrating them during network inference, we are able to predict new interactions between transcription factors and target genes. The proposed activation of the gene expression of hapX by the transcriptional regulator SrbA constitutes a so far unknown way of regulating iron homeostasis based on the amount of metabolically available iron. This interaction has been verified by Northern blots in a recent experimental study. In order to improve the reliability of the predicted network, the results of this experimental study have been added to the set of prior knowledge. The final network includes three SrbA target genes. Based on motif searching within the regulatory regions of these genes, we identify potential DNA-binding sites for SrbA. Our wet-lab experiments demonstrate high-affinity binding capacity of SrbA to the promoters of hapX, hemA and srbA. Conclusions: This study presents an application of the typical Systems Biology circle and is based on cooperation between wet-lab experimentalists and in silico modellers. The results underline that using prior knowledge during network inference helps to predict biologically important interactions. Together with the experimental results, we indicate a novel iron homeostasis regulating system sensing the amount of metabolically available iron and identify the binding site of iron-related SrbA target genes. It will be of high interest to study whether these regulatory interactions are also important for close relatives of A. fumigatus and other pathogenic fungi, such as Candida albicans.","This approach is especially useful for the inference of gene regulatory networks based on high-throughput gene expression data."
"There has been a huge effort in the advancement of analytical techniques for molecular biological data over the past decade. This has led to many novel algorithms that are specialized to deal with data associated with biological phenomena, such as gene expression and protein interactions. In contrast, ecological data analysis has remained focused to some degree on off-the-shelf statistical techniques though this is starting to change with the adoption of state-of-the-art methods, where few assumptions can be made about the data and a more explorative approach is required, for example, through the use of Bayesian networks. In this paper, some novel bioinformatics tools for microarray data are discussed along with their 'crossover potential' with an application to fisheries data. In particular, a focus is made on the development of models that identify functionally equivalent species in different fish communities with the aim of predicting functional collapse.",""
"Background: Bayesian networks (BNs) have been widely used to estimate gene regulatory networks. Many BN methods have been developed to estimate networks from microarray data. However, two serious problems reduce the effectiveness of current BN methods. The first problem is that BN-based methods require huge computational time to estimate large-scale networks. The second is that the estimated network cannot have cyclic structures, even if the actual network has such structures. Results: In this paper, we present a novel BN-based deterministic method with reduced computational time that allows cyclic structures. Our approach generates all the combinational triplets of genes, estimates networks of the triplets by BN, and unites the networks into a single network containing all genes. This method decreases the search space of predicting gene regulatory networks without degrading the solution accuracy compared with the greedy hill climbing (GHC) method. The order of computational time is the cube of number of genes. In addition, the network estimated by our method can include cyclic structures. Conclusions: We verified the effectiveness of the proposed method for all known gene regulatory networks and their expression profiles. The results demonstrate that this approach can predict regulatory networks with reduced computational time without degrading the solution accuracy compared with the GHC method.",""
"The gene regulatory network (GRN) reveals the regulatory relationships among genes and can provide a systematic understanding of molecular mechanisms underlying biological processes. The importance of computer simulations in understanding cellular processes is now widely accepted; a variety of algorithms have been developed to study these biological networks. The goal of this study is to provide a comprehensive evaluation and a practical guide to aid in choosing statistical methods for constructing large scale GRNs. Using both simulation studies and a real application in E. coli data, we compare different methods in terms of sensitivity and specificity in identifying the true connections and the hub genes, the ease of use, and computational speed. Our results show that these algorithms performed reasonably well, and each method has its own advantages: (1) GeneNet, WGCNA (Weighted Correlation Network Analysis), and ARACNE (Algorithm for the Reconstruction of Accurate Cellular Networks) performed well in constructing the global network structure; (2) GeneNet and SPACE (Sparse PArtial Correlation Estimation) performed well in identifying a few connections with high specificity.",""
"Results: PSICOV displays a mean precision substantially better than the best performing normalized mutual information approach and Bayesian networks. For 118 out of 150 targets, the L/5 (i.e. top-L/5 predictions for a protein of length L) precision for long-range contacts (sequence separation > 23) was >= 0.5, which represents an improvement sufficient to be of significant benefit in protein structure prediction or model quality assessment.",""
"In order to monitor dam safety condition better, a Dynamic Bayesian Networks (DBN) model is developed to overcome the shortcomings of the ordinary monitoring methods in this paper. Ordinary methods include comprehensive assessment methods and numerical simulation methods. Comprehensive assessment methods have shortcomings such as weight determination, scale difference, variables correlation, etc. In addition, comprehensive assessment methods cannot describe the multi-scale characteristics of monitoring data and dynamic property of large dam. Numerical simulation methods need complex mathematical theory, mechanics methodologies and high performance computer. DBN is a novel model with the consideration of correlations, delay and multi-scale characteristics of the variables such as deformation, seepage, stress and water load and temperature loads, as well as duration at every state. And the new model is also a simpler method with less experience, computational complexity and fewer experiments comparing with numerical simulation. Case study shows that better effect has been achieved in dam safety condition monitoring because the model can take the specific properties of dam into account. The main novel technical contributions of this paper are as follows: applying DBN to establish a multi-scale dynamic model on large engineering conditions monitoring at the first time; providing a new concept of latent durational-state time and analyzing its meaning of dam in the physical sense; using mechanics simulation analysis to verify the new model.",""
"Offline clinical guidelines are typically designed to integrate a clinical knowledge base, patient data and an inference engine to generate case specific advice. In this regard, offline clinical guidelines are still popular among the healthcare professionals for updating and support of clinical guidelines. Although their current format and development process have several limitations, these could be improved with artificial intelligence approaches such as expert systems/decision support systems. This paper first, presents up to date critical review of existing clinical expert systems namely AAPHelpm, MYCIN, EMYCIN, PIP, GLIF and PROforma. Additionally, an analysis is performed to evaluate all these fundamental clinical expert systems. Finally, this paper presents the proposed research and development of a clinical expert system to help healthcare professionals for treatment. [Saba T, Al-Zaharani S, Rehman A. Expert System for Offline Clinical Guidelines and Treatment Life Sci J 2012;9(4): 2639-2658] (ISSN: 1097-8135). http://www.lifesciencesite.com 393.","Offline clinical guidelines are typically designed to integrate a clinical knowledge base, patient data and an inference engine to generate case specific advice."
"It is shown that for discrete objects, constructed on limited samples, there are examples that confirm both the faithfulness assumption, and examples for which they are not fulfilled. Thus, the properties of separation procedures for the continuous models do not hold for discrete objects.",""
"Many applications naturally involve time series data and the vector autoregression (VAR), and the structural VAR (SVAR) are dominant tools to investigate relations between variables in time series. In the first part of this work, we show that the SVAR method is incapable of identifying contemporaneous causal relations for Gaussian process. In addition, least squares estimators become unreliable when the scales of the problems are large and observations are limited. In the remaining part, we propose an approach to apply Bayesian network learning algorithms to identify SVARs from time series data in order to capture both temporal and contemporaneous causal relations, and avoid high-order statistical tests. The difficulty of applying Bayesian network learning algorithms to time series is that the sizes of the networks corresponding to time series tend to be large, and high-order statistical tests are required by Bayesian network learning algorithms in this case. To overcome the difficulty, we show that the search space of conditioning sets d-separating two vertices should be a subset of the Markov blankets. Based on this fact, we propose an algorithm enabling us to learn Bayesian networks locally, and make the largest order of statistical tests independent of the scales of the problems. Empirical results show that our algorithm outperforms existing methods in terms of both efficiency and accuracy.","Many applications naturally involve time series data and the vector autoregression (VAR), and the structural VAR (SVAR) are dominant tools to investigate relations between variables in time series."
"This paper presents a wireless body area network platform that performs physical activities recognition using accelerometers, biosignals and smartphones. Multiple classifiers and sensor combinations were examined to identify the classifier with the best recognition performance for the static and dynamic activities. The Functional Trees classifier proved to provide the best results among the classifiers evaluated (Naive Bayes, Bayesian Networks, Support Vector Machines and Decision Trees [C4.5, Random Forest]) and was used to train the model which was implemented for the real time activity recognition on the smartphone. The identified patterns of daily physical activities were used to examine conformance with medical advice, regarding physical activity guidelines. An algorithm based on Skip Chain Conditional Random Fields, received as inputs the recognized activities and data retrieved from the GPS receiver of the smartphone to develop dynamic daily patterns that enhance prediction results. The presented platform can be extended to be used in the prevention of short-term complications of metabolic diseases such as diabetes.","Multiple classifiers and sensor combinations were examined to identify the classifier with the best recognition performance for the static and dynamic activities."
"We investigate solution sets of a special kind of linear inequality systems. In particular, we derive characterizations of these sets in terms of minimal solution sets. The studied inequalities emerge as information inequalities in the context of Bayesian networks. This allows to deduce structural properties of Bayesian networks, which is important within causal inference.","This allows to deduce structural properties of Bayesian networks, which is important within causal inference."
"Learning Bayesian network (BN) structure from data is a typical NP-hard problem. But almost existing algorithms have the very high complexity when the number of variables is large. In order to solve this problem(s), we present an algorithm that integrates with a decomposition-based approach and a scoring-function-based approach for learning BN structures. Firstly, the proposed algorithm decomposes the moral graph of BN into its maximal prime subgraphs. Then it orientates the local edges in each subgraph by the K2-scoring greedy searching. The last step is combining directed subgraphs to obtain final BN structure. The theoretical and experimental results show that our algorithm can efficiently and accurately identify complex network structures from small data set.",""
"Comprehensive problem framing that includes different perspectives is essential for holistic understanding of complex problems and as the first step in building models. We involved five stakeholders to frame the management problem of the Central Baltic herring fishery. By using the Bayesian belief networks (BBNs) approach, the views of the stakeholders were built into graphical influence diagrams representing variables and their dependencies. The views of the scientists involved concentrated on biological concerns, whereas the fisher, the manager, and the representative of an environmental nongovernmental organization included markets and fishing industry influences. Management measures were considered to have a relatively small impact on the development of the herring stock; their impact on socioeconomic objectives was greater. Overall, the framings by these stakeholders propose a focus on socioeconomic issues in research and management and explicitly define management objectives, not only in biological but also in social and economic terms. We find the approach an illustrative tool to structure complex issues systematically. Such a tool can be used as a forum for discussion and for decision support that explicitly includes the views of different stakeholder groups. It enables the examination of social and biological factors in one framework and facilitates bridging the gap between social and natural sciences. A benefit of the BBN approach is that the graphical model structures can be transformed into a quantitative form by inserting probabilistic information.",""
"We present a Bayesian network model based on the ecological risk assessment framework to evaluate potential impacts to habitats and resources resulting from wildfire, grazing, forest management activities, and insect outbreaks in a forested landscape in northeastern Oregon. The Bayesian network structure consisted of three tiers of nodes: landscape disturbances, habitats, and the ecological resources or endpoints of interest to land managers. Nodes at each tier were linked to lower nodes if ecological and spatial relationships existed between them. All parameters had four potential discrete states: zero, low, medium, and high. Our model reliably predicted probable risk to habitats and endpoints from natural and anthropogenic disturbances. The disturbances most likely to transform habitats and effect ecological resources were forest management and wildfire. Of the six habitats, moist forest (characterized by Douglas fir and grand fir) was found to be at greatest risk of ecological impacts. The management endpoint with the highest likelihood of impact was historical range of variability (HRV) for salmon habitat, followed by recreation (hunting native ungulates) and HRV wildfire. We found that the Bayesian approach to ecological risk assessment was a useful method to assess potential impacts to ecological resources resulting from forest management and natural disturbances.",""
"In literature there are several studies on the performance of Bayesian network structure learning algorithms. The focus of these studies is almost always the heuristics the learning algorithms are based on, i.e., the maximization algorithms (in score-based algorithms) or the techniques for learning the dependencies of each variable (in constraint-based algorithms). In this article, we investigate how the use of permutation tests instead of parametric ones affects the performance of Bayesian network structure learning from discrete data. Shrinkage tests are also covered to provide a broad overview of the techniques developed in current literature.",""
"Network models have been widely used in many subject areas to characterize the interactions between physical entities. A typical problem is to identify the network for multiple related tasks that share some similarities. In this case, a transfer learning approach that can leverage the knowledge gained during the modeling of one task to help better model another task is highly desirable. This article proposes a transfer learning approach that adopts a Bayesian hierarchical model framework to characterize the relatedness between tasks and additionally uses L-1-regularization to ensure robust learning of the networks with limited sample sizes. A method based on the Expectation-Maximization (EM) algorithm is further developed to learn the networks from data. Simulation studies are performed that demonstrate the superiority of the proposed transfer learning approach over single-task learning that learns the network of each task in isolation. The proposed approach is also applied to identify brain connectivity networks associated with Alzheimer's Disease (AD) from functional magnetic resonance image data. The findings are consistent with the AD literature.",""
"In this paper, a probabilistic model based on a Bayesian Network is presented for the still water bending moment acting on a tanker struck in a collision. The damage is assumed to occur along one side of the ship and is idealised by means of a rectangular hole leading to flooding of the breached compartments. The stochastic characterisation of the position and extension of the hole along the ship is derived according to two different probabilistic models found in literature. The results are not only compared in the light of the different features of the input probabilistic models but are also interpreted in terms of a ranking of the various situations according to their probability of occurrence, the magnitude of the generated load effect and the spatial correspondence between the maximum bending moment and the damaged area. The final goal is to provide information for the selection of accident scenarios and design values for this variable.",""
"An accident occurring at sea, though a rare event, has a huge impact both on the economy and the environment. A better and safer shipping practice always demands new ways to improve marine traffic and this essentially requires learning from past experience/faults. In this regard, probabilistic analysis of accidents and associated consequences can play a very important role in making a better and safer maritime transport system. Bayesian networks represent a class of probabilistic models based on statistics, decision theory and graph theory. This paper introduces the use of data-driven Bayesian modelling in risk analysis and makes a comparison with the different data-driven Bayesian methods available. The data for this study are based on the Lloyds database of accidents from 1997 to 2009. Important influential variables from this database are grouped and a Bayesian network that shows the relationship between the corresponding variables is constructed which in turn provides an insight into probabilistic dependencies existing among the variables in the database and the underlying reasons for these accidents.",""
"An important and challenging problem in systems biology is the inference of gene regulatory networks from short non-stationary time series of transcriptional profiles. A popular approach that has been widely applied to this end is based on dynamic Bayesian networks (DBNs), although traditional homogeneous DBNs fail to model the non-stationarity and time-varying nature of the gene regulatory processes. Various authors have therefore recently proposed combining DBNs with multiple changepoint processes to obtain time varying dynamic Bayesian networks (TV-DBNs). However, TV-DBNs are not without problems. Gene expression time series are typically short, which leaves the model over-flexible, leading to over-fitting or inflated inference uncertainty. In the present paper, we introduce a Bayesian regularization scheme that addresses this difficulty. Our approach is based on the rationale that changes in gene regulatory processes appear gradually during an organism's life cycle or in response to a changing environment, and we have integrated this notion in the prior distribution of the TV-DBN parameters. We have extensively tested our regularized TV-DBN model on synthetic data, in which we have simulated short non-homogeneous time series produced from a system subject to gradual change. We have then applied our method to real-world gene expression time series, measured during the life cycle of Drosophila melanogaster, under artificially generated constant light condition in Arabidopsis thaliana, and from a synthetically designed strain of Saccharomyces cerevisiae exposed to a changing environment.","An important and challenging problem in systems biology is the inference of gene regulatory networks from short non-stationary time series of transcriptional profiles."
"The management of end-of-life systems is becoming a major concern for systems manufacturers, as the negative impact of these systems on the environment is a matter of increasing public awareness, and their appropriate treatment offers economic opportunities. In this context, the disassembly of these systems in order to recycle their components is a possible and sound option that can make it possible to sustain economical progress while respecting environment requirements. The work undertaken in this paper considers modelling and optimising issues of such disassembly activities. An integrated approach is proposed to model and optimise the selection of valuable components of end-of-life systems, their recycling options and the way to obtain them. Because the framework of such problems is highly uncertain, we propose the use of Bayesian networks and their extension in terms of influence diagrams as mathematical tools for structuring and managing uncertainties. With this approach, one can take into account uncertainties rising from different sources on one hand and as a support for optimisation on the other hand.",""
"A positive definite completion problem pertains to determining whether the unspecified positions of a partial (or incomplete) matrix can be completed in a desired subclass of positive definite matrices. In this paper we study an important and new class of positive definite completion problems where the desired subclasses are the spaces of covariance and inverse-covariance matrices of probabilistic models corresponding to Bayesian networks (also known as directed acyclic graph models). We provide fast procedures that determine whether a partial matrix can be completed in either of these spaces and thereafter proceed to construct the completed matrices. We prove an analogue of the positive definite completion result for undirected graphs in the context of directed acyclic graphs, and thus proceed to characterize the class of directed acyclic graphs which can always be completed. We also proceed to give closed form expressions for the inverse and the determinant of a completed matrix as a function of only the elements of the corresponding partial matrix.",""
"Many of the similarity-based virtual screening approaches assume that molecular fragments that are not related to the biological activity carry the same weight as the important ones. This was the reason that led to the use of Bayesian networks as an alternative to existing tools for similarity-based virtual screening. In our recent work, the retrieval performance of the Bayesian inference network (BIN) was observed to improve significantly when molecular fragments were reweighted using the relevance feedback information. In this paper, a set of active reference structures were used to reweight the fragments in the reference structure. In this approach, higher weights were assigned to those fragments that occur more frequently in the set of active reference structures while others were penalized. Simulated virtual screening experiments with MDL Drug Data Report datasets showed that the proposed approach significantly improved the retrieval effectiveness of ligand-based virtual screening, especially when the active molecules being sought had a high degree of structural heterogeneity.","In our recent work, the retrieval performance of the Bayesian inference network (BIN) was observed to improve significantly when molecular fragments were reweighted using the relevance feedback information."
"An important challenge in system biology is the inference of biological networks from postgenomic data. Among these biological networks, a gene transcriptional regulatory network focuses on interactions existing between transcription factors (TFs) and and their corresponding target genes. A large number of reverse engineering algorithms were proposed to infer such networks from gene expression profiles, but most current methods have relatively low predictive performances. In this paper, we introduce the novel TNIFSED method (Transcriptional Network Inference from Functional Similarity and Expression Data), that infers a transcriptional network from the integration of correlations and partial correlations of gene expression profiles and gene functional similarities through a supervised classifier. In the current work, TNIFSED was applied to predict the transcriptional network in Escherichia coli and in Saccharomyces cerevisiae, using datasets of 445 and 170 affymetrix arrays, respectively. Using the area under the curve of the receiver operating characteristics and the F-measure as indicators, we showed the predictive performance of TNIFSED to be better than unsupervised state-of-the-art methods. TNIFSED performed slightly worse than the supervised SIRENE algorithm for the target genes identification of the TF having a wide range of yet identified target genes but better for TF having only few identified target genes. Our results indicate that TNIFSED is complementary to the SIRENE algorithm, and particularly suitable to discover target genes of \"orphan\" TFs.","An important challenge in system biology is the inference of biological networks from postgenomic data."
"Currently real-time control and online quality estimation of the resistance spot welding (RSW) has benefited a lot from monitoring the electrode displacement. Based on these emerging monitoring techniques a new approach is proposed to determine the optimal welding parameters and help to assess the weld quality. Two causal models are built with the offline trained Bayesian Belief Networks (BBN). The first model which is a pattern determination net deals with the optimal control criteria, i.e. an ideal combination of the maximum electrode displacement and electrode travel velocity, to provide more reliable welding process and qualified welds. The second model which is a weld quality assessment net reveals the dependency of the weld quality on the features displayed by the displacement curve, which can be used for overdesigning the safety welds or for online assessing weld quality as the probabilistic forecasting model. The experimental results show that the proposed approach is valid and feasible to determine the controlled parameters and to predict the weld quality in practices.",""
"The development and analysis of increasingly complex systems require the intensive use of models and of sophisticated approaches to systems modeling. This paper focuses on workflows supporting the solution of complex, composed, formal models used to study and/or develop real-world systems. The workflows we deal with orchestrate multiple distributed tools and applications in order to provide the user with a powerful, composed solution environment. The aim is to automate and reproduce analysis and simulation tasks starting from a high level, graph-based description of the model to be solved. This paper thus introduces solution workflows and presents the Solution Process Definition Language (SPDL) for the specification of solution workflows processes. One of the key elements of SPDL is its formal semantics, which allow for unambiguous specification of its constructs and validation of the workflows. A workflow pattern analysis of SPDL is also provided. SPDL and its execution environment, the OsMoSys framework, are then applied to a homeland security scenario. The OsMoSys framework and the SPDL language provide a practical contribution to the applicability of model engineering techniques by enabling the semiautomatic solution of complex models. Note to Practitioners-The usage of formal methods is widely advocated to provide evidence of critical properties of systems for certification and/or assessment purposes. This paper was motivated by the problem of (partially) automating the development and the analysis of complex formal models of real systems. Existing approaches generally require that whoever builds such models has an in-depth knowledge of several fields, besides the domain to study: modeling languages (e.g., Petri Nets, Fault Trees, Process Algebras, Queueing Networks, Bayesian Networks), solution techniques, analysis/simulation tools, as well as the way to combine them in order to perform more powerful experiments. In this paper, we propose a workflow-based solution, in order to enable the automated development and analysis of multiformalism models by composing predefined models and existing tools. Our approach also promotes the reuse of models and experiments, through the construction of libraries of models and workflows and the definition of sophisticated experiments.",""
"Bayesian networks (BNs) are probabilistic expert systems which have emerged over the last few decades as a powerful data mining technique. Also, BNs have become especially popular in biomedical applications where they have been used for diagnosing diseases and studying complex cellular networks, among many other applications. In this study, we built a BN in a fully automated way in order to analyse data regarding injuries due to the inhalation, ingestion and aspiration of foreign bodies (FBs) in children. Then, a sensitivity analysis was carried out to characterize the uncertainty associated with the model. While other studies focused on characteristics such as shape, consistency and dimensions of the FBs which caused injuries, we propose an integrated environment which makes the relationships among the factors underlying the problem clear. The advantage of this approach is that it gives a picture of the influence of critical factors on the injury severity and allows for the comparison of the effect of different FB characteristics (volume, FB type, shape and consistency) and children's features (age and gender) on the risk of experiencing a hospitalization. The rates it consents to calculate provide a more rational basis for promoting care-givers' education of the most influential risk factors regarding the adverse outcomes.",""
"In this paper, a semiautomated system for modeling 3D objects, especially buildings from aerial video, over a semi-urban scene is presented. First, the video frames are preprocessed to minimize the rotational effects of camera motion. The 3D translational coordinates of the sensor are used to stitch the video frames into nadir and stereo mosaics. The features extracted from the stereo mosaics, like elevation, edges and corners, visual entropy, and color information, are employed in a Bayesian framework to identify the 3D objects in the scene, such as buildings and trees. The initial 3D building models are further optimized by projecting them onto individual video frames. A novel method for setting the input parameters of vision algorithms required for feature extraction, using the data-driven probabilistic inference in Bayesian Networks, has been designed. This method automates the 3D object identification process and precludes the need for manual intervention. Improvements that can be used to increase the accuracy of 3D models when Lidar data is fused with aerial video during the object identification process are also discussed. (c) 2012 SPIE and IS&T. [DOI: 10.1117/1.JEI.21.1.013007]","A novel method for setting the input parameters of vision algorithms required for feature extraction, using the data-driven probabilistic inference in Bayesian Networks, has been designed."
"The coastal zone is a complex environment in which a variety of forcing factors interact causing shoreline evolution. Coastal managers seek to predict coastal evolution and to identify regions vulnerable to erosion. Here, a Bayesian network is developed to identify the primary factors influencing decadal-scale shoreline evolution of European coasts and to reproduce the observed evolution trends. Sensitivity tests demonstrate the robustness of the model, showing higher predictive capabilities for stable coasts than for eroding coasts. Finally, the study highlights the need to update and expand large-scale coastal data sets, particularly by including local scale processes and anthropogenic impacts.",""
"Because of the increasing diversity of data sets and measurement techniques in biology, a growing spectrum of modeling methods is being developed. It is generally recognized that it is critical to pick the appropriate method to exploit the amount and type of biological data available for a given system. Here, we describe a method for use in situations where temporal data from a network is collected over multiple time points, and in which little prior information is available about the interactions, mathematical structure, and statistical distribution of the network. Our method results in models that we term Nonparametric exterior derivative estimation Ordinary Differential Equation (NODE) model's. We illustrate the method's utility using spatiotemporal gene expression data from Drosophila melanogaster embryos. We demonstrate that the NODE model's use of the temporal characteristics of the network leads to quantifiable improvements in its predictive ability over nontemporal models that only rely on the spatial characteristics of the data. The NODE model provides exploratory visualizations of network behavior and structure, which can identify features that suggest additional experiments. A new extension is also presented that uses the NODE model to generate a comb diagram, a figure that presents a list of possible network structures ranked by plausibility. By being able to quantify a continuum of interaction likelihoods, this helps to direct future experiments.",""
"This paper is devoted to presenting an updated approach based on the use of Bayesian networks. This methodology aims to update the statistical parameters related to a mechanical model thanks to experimental data. The proposed updating process has been applied to engineering problems. The results show that the proposed approach could be used to estimate the material parameters of a given structure and avoids carrying out destructive tests on site.",""
"Cognitive systems were first introduced by Mitola and in the last decade they have proved to be beneficial in self-management functionalities of future generation networks. The advantages and the way that networks gain benefits from cognitive systems is analysed in this article. Moreover, since such systems are closely related to machine learning, the focus of this article is also placed on machine learning techniques applied both in the network and the user devices side. In particular, celebrating 10 years of cognitive systems, this survey-oriented article presents an extended state-of-the-art of machine learning applied to cognitive systems as coming from the recent research and an overview of three different learning capabilities of both the network and the user device.",""
"It has been acknowledged that natural sciences alone cannot provide an adequate basis for the management of complex environmental problems. The scientific knowledge base has to be expanded in a more holistic direction by incorporating social and economic issues. As well, the multifaceted knowledge has to be summarized in a form that can support science-based decision making. This is, however, difficult. Interdisciplinary skills, practices, and methodologies are needed that enable the integration of knowledge from conceptually different disciplines. Through a focus on our research process, we analyzed how and what kind of interdisciplinarity between natural scientists, environmental economists, and social scientists grew from the need to better understand the complexity and uncertainty inherent to the Baltic salmon fisheries, and how divergent knowledge was integrated in a form that can support science-based decision making. The empirical findings suggest that interdisciplinarity is an extensive learning process that takes place on three levels: between individuals, between disciplines, and between types of knowledge. Such a learning process is facilitated by agreeing to a methodological epoche and by formulating a global question at the outset of a process.",""
"This article proposes a simplified fault-diagnosis system based on Bayesian networks with noisy-OR/AND nodes to estimate the faulty item/section(s) of a large power station and its transmission lines. The proposed method utilizes the final information of protective relays and corresponding circuit breakers to construct the Bayesian fault diagnosis model for each section. The learning algorithm for Bayesian network parameters takes the sum of the mean-squared error between the expected values and the computed values of certain target variables as the minimizing optimization function to adjust the network parameters continuously. By comparing the result beliefs of possible faulty sections, the faulty item/section(s) becomes a candidate. In order to test the validity and feasibility of that method, a computer simulation of the High Dam power station and its 500-kV transmission lines is used. It is shown that the proposed diagnosis method has many merits, such as rapid reasoning, less storage memory and processing time, easy correctness of diagnosing results, flexibility, and application into a large power station and its transmission lines for real-time fault diagnosis. Finally, it assists and supports the operator of the control room to make the right decision, especially in case of communication loss.",""
"Purpose - To counteract the effects of global competition, many organizations have extended their enterprises by forming supply chain networks. However, as organizations increase their dependence on these networks, they become more vulnerable to their suppliers' risk profiles. The purpose of this paper is to present a methodology for modeling and evaluating risk profiles in supply chains via Bayesian networks. Design/methodology/approach - Empirical data from 15 casting suppliers to a major US automotive company are analyzed using Bayesian networks. The networks provide a methodological approach for determining a supplier's external, operational, and network risk probability, and the potential revenue impact a supplier can have on the company. Findings - Bayesian networks can be used to develop supplier risk profiles to determine the risk exposure of a company's revenue stream. The supplier risk profiles can be used to determine those risk events which have the largest potential impact on an organization's revenues, and the highest probability of occurrence. Research limitations/implications - A limitation to the use of Bayesian networks to model supply chain risks is the proper identification of risk events and risk categories that can impact a supply chain. Practical implications - The methodology used in this study can be adopted by managers to formulate supply chain risk management strategies and tactics which mitigate overall supply chain risks. Social implications - The methodology used in this study can be used by organizations to reduce supply chain risks which yield numerous societal benefits. Originality/value - As part of a comprehensive supplier risk management program, organizations along with their suppliers can develop targeted approaches to minimize the occurrence of supply chain risk events.",""
"Most existing research on the job shop scheduling problem has been focused on the minimization of makespan (i.e., the completion time of the last job). However, in the fiercely competitive market nowadays, delivery punctuality is more important for maintaining a high service reputation. So in this paper, we aim at solving job shop scheduling problems with the total weighted tardiness objective. Several dispatching rules are adopted in the Giffler-Thompson algorithm for constructing active schedules. It is noticeable that the rule selections for scheduling consecutive operations are not mutually independent but actually interrelated. Under such circumstances, a probabilistic model-building genetic algorithm (PMBGA) is proposed to optimize the sequence of selected rules. First, we use Bayesian networks to model the distribution characteristics of high-quality solutions in the population. Then, the new generation of individuals is produced by sampling the established Bayesian network. Finally, some elitist individuals are further improved by a special local search module based on parameter perturbation. The superiority of the proposed approach is verified by extensive computational experiments and comparisons.",""
"In a sustainable development context, the stakes of the last stage of system life cycle, the end-of-life stage, have increased over recent years. End-of-life systems have to be de-manufactured in order to be valued so as to respond to environmental concerns. The aim of a disassembly strategy consists in issuing a solution to the whole decision problem raised during the end-of-life stage of systems. Indeed, decision makers have to select valuable components according to technical, economical and environmental criteria and then design and optimise a disassembly support system that will generate these products. The solution obtained is what we refer to in this article as a disassembly trajectory. The work presented in this article is about planning these trajectories on different horizons integrating several arrivals of end-of-life systems. The proposed approach, with Bayesian networks and influence diagrams as the underlying mathematical tools, enables dynamically defined uncertainties to be taken into account.",""
"Community concern at the continual environmental decline of the Snowy River (New South Wales and Victoria, Australia) has resulted in a substantial investment in river rehabilitation works, referred to as the Snowy Rehabilitation Project. Much of the investment in the science component of the project went into developing physical models to better understand the behavior of sediments that have accumulated over time in the river channel. The outcomes of these models were intended to assist river managers in better controlling sediment impacts to restore instream ecological function. To synthesize the learnings from these studies for use by catchment managers, a simple decision support tool, referred to as the Snowy tool, was constructed. The Snowy tool was designed to link outcomes from the models with management activities (or interventions) to outcomes within the river channel. It took the form of a probabilistic model (Bayesian network) that incorporates data from a hydraulic model (HEC-RAS), combined with expert opinion, and riverine response models. This article overviews the Snowy tool, and stresses the importance of the use of Bayesian networks in adaptive management frameworks and in guiding investments in research and on-ground decision-making.",""
"The Ranger uranium mine is surrounded by the World Heritage Kakadu National Park, Australia, and is upstream of the Ramsar-listed wetlands of the Magela Creek floodplain. We present the results of a Quantitative Ecological Risk Assessment (QERA) for the floodplain that combines both point source mining risks and diffuse non-mining landscape-scale risks. A high level of protection for the biodiversity of aquatic ecosystems was used as the assessment endpoint. Mining risks in the surface water pathway were assessed for four key mine-associated solutes (uranium, manganese, magnesium, and sulphate), and non-mining landscape-scale risks were assessed for weeds, feral pig damage, unmanaged dry season fire, and saltwater intrusion from potential sea-level rise due to climate change. Results show that non-mining landscape-scale risks are currently several orders of magnitude greater than risks from mine water contaminants. A weed (Para grass; Urocloa mutica) is the major ecological risk because of its extent, effect, and rapid spread rate. The QERA was incorporated into a Bayesian Belief Network to help evaluate different management strategies. We conclude that non-mining landscape-scale risks to the floodplain should receive the same level of close scrutiny and investment as that applied to uranium mining risks.",""
"As the market competition becomes fiercer, contemporary make-to-order firms are confronted with both due date quotation and production scheduling problems at the same time. On the one hand, in order to attract customers, the firm needs to quote a short lead time; on the other hand, once a due date has been promised, the firm must spare no effort to deliver the goods no later than this date. If due date assignment and shop scheduling are processed separately by two systems, the overall performance is unlikely to be satisfactory because the two tasks are actually interrelated (e.g. a tighter due date setting will increase the chances of tardiness despite its appeal for the incoming customer). Therefore, we consider the problem by integrating due date assignment and shop scheduling into one optimisation model. A double-layered heuristic optimisation algorithm is presented for solving this problem. In the upper-layer genetic algorithm which performs coarse-granularity optimisation, Bayesian networks are used to learn the distribution of optimal due date values. As the second-layer algorithm, a parameter perturbation method is applied for a finer-granularity neighbourhood search. Computational experiments prove the efficacy and efficiency of the proposed algorithm.",""
"The Case-Based Reasoning (CBR) solves problems by using the past problem solving experiences. How to apply these experiences depends on the type of the problem. The method presented in this paper tries to overcome this difficulty in CBR for optimization problems, using Bayesian Optimization Algorithm (BOA). BOA evolves a population of candidate solutions through constructing Bayesian networks and sampling them. After solving the problems through BOA, Bayesian networks describing solutions features are obtained. In our method, these Bayesian networks are stored in a case-base. For solving a new problem, the Bayesian networks of those problems which are similar to the new problem, are retrieved and combined. This compound Bayesian network is used for generating the initial population and constructing the probabilistic models of BOA in solving the new problem. Our method improves CBR in two ways: first, in our method, how to use the knowledge stored in the case-base is disregarding the problem itself and is universally; second, this method stores the probabilistic descriptions of the previous solutions in order to make the stored knowledge more flexible. Experimental results showed that in addition to the mentioned advantages, our method improved the solutions quality.",""
"The increasing complexity and size of digital designs, in conjunction with the lack of a potent verification methodology that can effectively cope with this trend, continue to inspire engineers and academics in seeking ways to further automate design verification. In an effort to increase performance and to decrease engineering effort, research has turned to artificial intelligence (AI) techniques for effective solutions. The generation of tests for simulation-based verification can be guided by machine-learning techniques. In fact, recent advances demonstrate that embedding machine-learning (ML) techniques into a coverage-directed test generation (CDG) framework can effectively automate the test generation process, making it more effective and less error-prone. This article reviews some of the most promising approaches in this field, aiming to evaluate the approaches and to further stimulate more directed research in this area.",""
"Human-induced alteration of the natural flow regime is a major threat to freshwater ecosystems and biodiversity. The effects of hydrological alteration on the structural and functional attributes of riverine communities are expected to be multiple and complex, and they may not be described easily by a single model. Based on existing knowledge of key hydrological and ecological attributes, we explored potential effects of a flow-regulation scenario on macroinvertebrate assemblage composition and diversity in two river systems in Australia's relatively undeveloped wet dry tropics. We used a single Bayesian belief network (BBN) to model potential changes in multiple assemblage attribute's within each river type during dry and wet seasons given two flow scenarios: the current, near-natural flow condition, and flow regulation. We then used multidimensional scaling (MDS) ordination to visually summarize and compare the most probable attributes of assemblages and their environment under the different scenarios. The flow-regulation scenario provided less certainty in the ecological responses of one river type during the dry season, which reduced the ability to make predictions from the BBN outputs directly. However, visualizing the BBN results in an ordination highlighted similarities and differences between the scenarios that may have been otherwise difficult to ascertain. In particular, the MDS showed that flow regulation would reduce the seasonal differentiation in hydrology and assemblage characteristics that is expected under the current low level of development. Our approach may have wider application in understanding ecosystem responses to different river management practices and should be transferred easily to other ecosystems or biotic assemblages to provide researchers, managers, and decision makers an enhanced understanding of ecological responses to potential anthropogenic disturbance.",""
"The root epidermis of Arabidopsis provides an exceptional model for studying the molecular basis of cell fate and differentiation. To obtain a systems-level view of root epidermal cell differentiation, we used a genome-wide transcriptome approach to define and organize a large set of genes into a transcriptional regulatory network. Using cell fate mutants that produce only one of the two epidermal cell types, together with fluorescence-activated cell-sorting to preferentially analyze the root epidermis transcriptome, we identified 1,582 genes differentially expressed in the root-hair or non-hair cell types, including a set of 208 \"core\" root epidermal genes. The organization of the core genes into a network was accomplished by using 17 distinct root epidermis mutants and 2 hormone treatments to perturb the system and assess the effects on each gene's transcript accumulation. In addition, temporal gene expression information from a developmental time series dataset and predicted gene associations derived from a Bayesian modeling approach were used to aid the positioning of genes within the network. Further, a detailed functional analysis of likely bHLH regulatory genes within the network, including MYC1, bHLH54, bHLH66, and bHLH82, showed that three distinct subfamilies of bHLH proteins participate in root epidermis development in a stage-specific manner. The integration of genetic, genomic, and computational analyses provides a new view of the composition, architecture, and logic of the root epidermal transcriptional network, and it demonstrates the utility of a comprehensive systems approach for dissecting a complex regulatory network.",""
"We present a modeling framework that supports semantically informed statistical inference about unobserved true location and positional uncertainty for geographic information spanning multiple sources. We demonstrate the use of a semantic representation of information sources to support construction of a Bayesian belief network that operationalizes the data integration process. Our approach allows for positional data, metadata, and other ancillary and derived information to inform inference regarding unobserved true position. In our application, we use two line feature datasets and a set of GPS data points describing a portion of the street network in Laramie, Wyoming. Using source metadata we inform prior distributions. Additionally, we use feature straightness to illustrate how form and process - gridded streets and the process of the Public Land Survey - can be used to improve inference for true position. The presented modeling framework is suitable for multiple data sources when the best data are not necessarily known and when the information semantics associated with the input data can be described in a systematic way.","We present a modeling framework that supports semantically informed statistical inference about unobserved true location and positional uncertainty for geographic information spanning multiple sources."
"In recent years, Bayesian networks using unsupervised extracted image features have been applied in many remote sensing information mining systems to enable semantic-sensitive image retrieval. However, a simple Bayesian network insufficiently accounts for the spatial information, that is, the relations among image regions, for the semantic inference process. This drawback significantly impacts the retrieval performance, especially if the utilised features contain no or little spatial information. Therefore, this article proposes a context-sensitive Bayesian network, which infers semantic concepts of image regions based on the spectral and textural characteristics of the regions themselves as well as their contexts, that is, the adjacent regions. In order to compare the context-sensitive Bayesian network with the simple Bayesian network, comprehensive experiments were conducted based on high-resolution multispectral IKONOS imagery. The results show that the incorporation of the image regions' spatial relations not only significantly improves the accuracy of the semantic concepts inference, but also allows more flexibility in choosing the type of low-level features.","However, a simple Bayesian network insufficiently accounts for the spatial information, that is, the relations among image regions, for the semantic inference process."
"The establishment of more efficient approaches for developmental neurotoxicity testing (DNT) has been an emerging issue for children's environmental health. Here we describe a systematic approach for DNT using the neuronal differentiation of mouse embryonic stem cells (mESCs) as a model of fetal programming. During embryoid body (EB) formation, mESCs were exposed to 12 chemicals for 24 h and then global gene expression profiling was performed using whole genome microarray analysis. Gene expression signatures for seven kinds of gene sets related to neuronal development and neuronal diseases were selected for further analysis. At the later stages of neuronal cell differentiation from EBs, neuronal phenotypic parameters were determined using a high-content image analyzer. Bayesian network analysis was then performed based on global gene expression and neuronal phenotypic data to generate comprehensive networks with a linkage between early events and later effects. Furthermore, the probability distribution values for the strength of the linkage between parameters in each network was calculated and then used in principal component analysis. The characterization of chemicals according to their neurotoxic potential reveals that the multi-parametric analysis based on phenotype and gene expression profiling during neuronal differentiation of mESCs can provide a useful tool to monitor fetal programming and to predict developmentally neurotoxic compounds.",""
"Real-time sensing in water distribution systems provides a potentially powerful analytical tool for providing water security. Through monitoring surrogate parameters (e.g., pH, turbidity, and residual chlorine) over time, the natural variations of a distribution system's parameters are established, allowing rapid detection of changes in water quality. However, the level of performance that water quality event detection algorithms have exhibited to date is insufficient for real-world utilization. Bayesian belief networks (BBNs) offer a formalized method of reasoning under uncertainty and are well suited to the analysis of multiple sources of information. The application of a BBN to water quality event detection is described. Surrogate parameters (pH, conductivity, and turbidity) were monitored during an experimental E. coli contamination. Difference filtration using a 60-s moving window of observations identified rapid rates of change present in the surrogate parameter signals, demonstrated as responsive to contamination as simulated in bench-scale studies. A BBN was constructed to assimilate the surrogate parameter variation and compute temporal probability distributions about the contamination of an experimental system. The BBN topology, probability distributions and data transformation techniques applied were validated through successful identification of contaminant injections. DOI:10.1061/(ASCE)WR.1943-5452.0000163. (C) 2012 American Society of Civil Engineers.",""
"Constraint-based search methods, which are a major approach to learning Bayesian networks, are expected to be effective in causal discovery tasks. However, such methods often suffer from impracticality of classical hypothesis testing for conditional independence when the sample size is insufficiently large. We present a new conditional independence (CI) testing method that is designed to be effective for small samples. Our method uses the minimum free energy principle, which originates from thermodynamics, with the \"Data Temperature\" assumption recently proposed by us. This CI method incorporates the maximum entropy principle and converges to classical hypothesis tests in asymptotic regions. In our experiments using repository datasets (Alarm/Insurance/Hailfinder/Barley/Mildew), the results show that our method improves the learning performance of the well known PC algorithm in the view of edge-reversed errors in addition to extra/missing errors.",""
"Grid systems are popular today due to their ability to solve large problems in business and science. Job failures which are inherent in any computational environment are more common in grids due to their dynamic and complex nature. Furthermore, traditional methods for job failure recovery have proven costly and thus a need to shift toward proactive and predictive management strategies is necessary in such systems. In this paper, an innovative effort has been made to predict the futurity of jobs in a production grid environment. First of all, we investigated the relationship between workload characteristics and job failures by analyzing workload traces of AuverGrid which is a part of EGEE (Enabling Grids for E-science) project. After the recognition of failure patterns, the success or failure status of jobs during 6 months of AuverGrid activity was predicted with approximately 96% accuracy. The quality of services on the grid can be improved by integrating the result of this work into management services like scheduling and monitoring.",""
"Wireless sensor networks (WSNs) play an important role in forest fire risk monitoring. Various applications are in operation. However, the use of mobile sensors in forest risk monitoring remains largely unexplored. Our research contributes to fill this gap by designing a model which abstracts mobility constraints within different types of contexts for the inference of mobile sensor behaviour. This behaviour is focused on achieving a suitable spatial coverage of the WSN when monitoring forest fire risk. The proposed mobility constraint model makes use of a Bayesian network approach and consists of three components: (1) a context typology describing different contexts in which a WSN monitors a dynamic phenomenon; (2) a context graph encoding probabilistic dependencies among variables of interest; and (3) contextual rules encoding expert knowledge and application requirements needed for the inference of sensor behaviour. As an illustration, the model is used to simulate the behaviour of a mobile WSN to obtain a suitable spatial coverage in low and high fire risk scenarios. It is shown that the implemented Bayesian network within the mobility constraint model can successfully infer behaviour such as sleeping sensors, moving sensors, or deploying more sensors to enhance spatial coverage. Furthermore, the mobility constraint model contributes towards mobile sensing in which the mobile sensor behaviour is driven by constraints on the state of the phenomenon and the sensing system. (C) 2011 Elsevier Ltd. All rights reserved.","Our research contributes to fill this gap by designing a model which abstracts mobility constraints within different types of contexts for the inference of mobile sensor behaviour."
"Objectives: We propose a new graphical framework for extracting the relevant dietary, social and environmental risk factors that are associated with an increased risk of nasopharyngeal carcinoma (NPC) on a case-control epidemiologic study that consists of 1289 subjects and 150 risk factors. Methods: This framework builds on the use of Bayesian networks (BNs) for representing statistical dependencies between the random variables. We discuss a novel constraint-based procedure, called Hybrid Parents and Children (HPC), that builds recursively a local graph that includes all the relevant features statistically associated to the NPC, without having to find the whole BN first. The local graph is afterwards directed by the domain expert according to his knowledge. It provides a statistical profile of the recruited population, and meanwhile helps identify the risk factors associated to NPC. Results: Extensive experiments on synthetic data sampled from known BNs show that the HPC outperforms state-of-the-art algorithms that appeared in the recent literature. From a biological perspective, the present study confirms that chemical products, pesticides and domestic fume intake from incomplete combustion of coal and wood are significantly associated with NPC risk. These results suggest that industrial workers are often exposed to noxious chemicals and poisonous substances that are used in the course of manufacturing. This study also supports previous findings that the consumption of a number of preserved food items, like house made proteins and sheep fat, are a major risk factor for NPC. Conclusion: BNs are valuable data mining tools for the analysis of epidemiologic data. They can explicitly combine both expert knowledge from the field and information inferred from the data. These techniques therefore merit consideration as valuable alternatives to traditional multivariate regression techniques in epidemiologic studies. (C) 2011 Elsevier B.V. All rights reserved.","These techniques therefore merit consideration as valuable alternatives to traditional multivariate regression techniques in epidemiologic studies."
"Wireless sensor networks enable continuous and reliable data acquisition for real-time monitoring in a variety of application areas. Due to the large amount of data collected and the potential complexity of emergent patterns, scalable and distributed reasoning is preferable when compared to centralised inference as this allows network wide decisions to be reached robustly without specific reliance on particular network components. In this paper, we provide an overview of distributed inference for both wearable and ambient sensing with specific focus on graphical modelsillustrating their ability to be mapped to the topology of a physical network. Examples of research conducted by the authors in the use of ambient and wearable sensors are provided, demonstrating the possibility for distributed, real-time activity monitoring within a home healthcare environment. Copyright (C) 2010 John Wiley & Sons, Ltd.","Due to the large amount of data collected and the potential complexity of emergent patterns, scalable and distributed reasoning is preferable when compared to centralised inference as this allows network wide decisions to be reached robustly without specific reliance on particular network components."
"In this paper, we are interested in investigating the causal relationships among futures sugar prices in the Zhengzhou futures exchange market (ZF), the spot sugar prices in Zhengzhou (ZS) and the futures sugar prices in New York futures exchange market (NF). A useful tool called Bayesian network is introduced to analyze the problem. Since there are only three variables in our Bayesian network, the algorithm is straightforward: we display all the 25 possible network structures and adopt certain scoring metrics to evaluate them. We applied five different scoring metrics in total. Firstly, for each metric, we obtained 24 scores, each calculated from one of the 24 possible structures i.e. a Directed Acyclic Graph (DAG). Then we eliminated the network structure which represents the independence of the three variables according to our prior knowledge concerning the futures sugar market. After that, the optimal network structure which implies the causal relationships was selected according to the corresponding scoring metric. Finally, after comparing the results from different scoring metrics, we obtained the relatively affirmative conclusion that ZS causes ZF from both the Bayesian Dirichlet (BD) metric, Bayesian Dirichlet-Akaike Information Criterion (BD-AIC) metric, Bayesian Dirichlet-Bayesian Information Criterion (BD-BIC) metric and Bayesian Information Criterion (BIC) metric. The conclusions that NF causes ZF and ZF causes ZS from the Akaike Information Criterion (AIC) metric and ZF causes ZS from the BIC metric were useful and significant to our investigation.",""
"Probabilistic graphical models have been widely recognized as a powerful formalism in the bioinformatics field, especially in gene expression studies and linkage analysis. Although less well known in association genetics, many successful methods have recently emerged to dissect the genetic architecture of complex diseases. In this review article, we cover the applications of these models to the population association studies' context, such as linkage disequilibrium modeling, fine mapping and candidate gene studies, and genome-scale association studies. Significant breakthroughs of the corresponding methods are highlighted, but emphasis is also given to their current limitations, in particular, to the issue of scalability. Finally, we give promising directions for future research in this field.",""
"The paper describes the development of an integrated multi-agent online dispute resolution environment called IMODRE that was designed to assist parties involved in Australian family law disputes achieve legally fairer negotiated outcomes. The system extends our previous work in developing negotiation support systems Family_Winner and AssetDivider. In this environment one agent uses a Bayesian Belief Network expertly modeled with knowledge of the Australian Family Law domain to advise disputants of their Best Alternatives to Negotiated Agreements. Another agent incorporates the percentage split of marital property into an integrative bargaining process and applies heuristics and game theory to equitably distribute marital property assets and facilitate further trade-offs. We use this system to add greater fairness to Family property law negotiations.",""
"For the electrical sector, minimizing non-technical losses is a very important task because it has a high impact in the company profits. Thus, this paper describes some new advances for the detection of non-technical losses in the customers of one of the most important power utilities of Spain and Latin America: Endesa Company. The study is within the framework of the MIDAS project that is being developed at the Electronic Technology Department of the University of Seville with the funding of this company. The advances presented in this article have an objective of detecting customers with anomalous drops in their consumed energy (the most-frequent symptom of a non-technical loss in a customer) by means of a windowed analysis with the use of the Pearson coefficient. On the other hand, besides Bayesian networks, decision trees have been used for detecting other types of patterns of non-technical loss. The algorithms have been tested with real customers of the database of Endesa Company. Currently, the system is in operation. (C) 2011 Elsevier Ltd. All rights reserved.",""
"In this paper we propose to apply the Information Bottleneck (IB) approach to the sub-class of Statistical Relational Learning (SRL) languages that are reducible to Bayesian networks. When the resulting networks involve hidden variables, learning these languages requires the use of techniques for learning from incomplete data such as the Expectation Maximization (EM) algorithm. Recently, the IB approach was shown to be able to avoid some of the local maxima in which EM can get trapped when learning with hidden variables. Here we present the algorithm Relational Information Bottleneck (RIB) that learns the parameters of SRL languages reducible to Bayesian Networks. In particular, we present the specialization of RIB to a language belonging to the family of languages based on the distribution semantics, Logic Programs with Annotated Disjunction (LPADs). This language is prototypical for such a family and its equivalent Bayesian networks contain hidden variables. RIB is evaluated on the IMDB, Cora and artificial datasets and compared with LeProbLog, EM, Alchemy and PRISM. The experimental results show that RIB has good performances especially when some logical atoms are unobserved. Moreover, it is particularly suitable when learning from interpretations that share the same Herbrand base.",""
"Mobile devices have evolved and experienced an immense popularity over the last few years. This growth however has exposed mobile devices to an increasing number of security threats. Despite the variety of peripheral protection mechanisms described in the literature, authentication and access control cannot provide integral protection against intrusions. Thus, a need for more intelligent and sophisticated security controls such as intrusion detection systems (IDSs) is necessary. Whilst much work has been devoted to mobile device IDSs, research on anomaly-based or behaviour-based IDS for such devices has been limited leaving several problems unsolved. Motivated by this fact, in this paper, we focus on anomaly-based IDS for modern mobile devices. A dataset consisting of iPhone users data logs has been created, and various classification and validation methods have been evaluated to assess their effectiveness in detecting misuses. Specifically, the experimental procedure includes and cross-evaluates four machine learning algorithms (i.e. Bayesian networks, radial basis function, K-nearest neighbours and random Forest), which classify the behaviour of the end-user in terms of telephone calls, SMS and Web browsing history. In order to detect illegitimate use of service by a potential malware or a thief, the experimental procedure examines the aforementioned services independently as well as in combination in a multimodal fashion. The results are very promising showing the ability of at least one classifier to detect intrusions with a high true positive rate of 99.8%. Copyright (C) 2011 John Wiley & Sons, Ltd.","A dataset consisting of iPhone users data logs has been created, and various classification and validation methods have been evaluated to assess their effectiveness in detecting misuses."
"This paper presents a methodology to systematically assess and manage the risks associated with tunnel construction. The methodology consists of combining a geologic prediction model that allows one to predict geology ahead of the tunnel construction, with a construction strategy decision model that allows one to choose amongst different construction strategies the one that leads to minimum risk. This model used tunnel boring machine performance data to relate to and predict geology. Both models are based on Bayesian Networks because of their ability to combine domain knowledge with data, encode dependencies among variables, and their ability to learn causal relationships. The combined geologic prediction-construction strategy decision model was applied to a case, the Porto Metro, in Portugal. The results of the geologic prediction model were in good agreement with the observed geology, and the results of the construction strategy decision support model were in good agreement with the construction methods used. Very significant is the ability of the model to predict changes in geology and consequently required changes in construction strategy. This risk assessment methodology provides a powerful tool with which planners and engineers can systematically assess and mitigate the inherent risks associated with tunnel construction. (C) 2011 Elsevier Ltd. All rights reserved.",""
"Personal mobile devices such as cellular phones, smart phones and PMPs have advanced incredibly in the past decade. The mobile technologies make research on the life log and user-context awareness feasible. In other words, sensors in mobile devices can collect the variety of user's information, and various works have been conducted using that information. Most of works used a user's location information as the most useful clue to recognize the user context. However, the location information in the conventional works usually depends on a GPS receiver that has limited function, because it cannot localize a person in a building and thus lowers the performance of the user-context awareness. This paper develops a system to solve such problems and to infer a user's hidden information more accurately using Bayesian network and indoor-location information. Also, this paper presents a new technique for localization in a building using a decision tree and signals for the Wireless LAN because the decision tree has many advantages which outweigh other localization techniques. (C) 2011 Elsevier Ltd. All rights reserved.",""
"This paper presents a distributed system for the recognition of human actions using views of the scene grabbed by different cameras. 2D frame descriptors are extracted for each available view to capture the variability in human motion. These descriptors are projected into a lower dimensional space and fed into a probabilistic classifier to output a posterior distribution of the action performed according to the descriptor computed at each camera. Classifier fusion algorithms are then used to merge the posterior distributions into a single distribution. The generated single posterior distribution is fed into a sequence classifier to make the final decision on the performed activity. The system can instantiate different algorithms for the different tasks, as the interfaces between modules are clearly defined. Results on the classification of the actions in the IXMAS dataset are reported. The accuracy of the proposed system is similar to state-of-the-art 3D methods, even though it uses only well-known 2D pattern recognition techniques and does not need to project the data into a 3D space or require camera calibration parameters. (C) 2011 Elsevier B.V. All rights reserved.","These descriptors are projected into a lower dimensional space and fed into a probabilistic classifier to output a posterior distribution of the action performed according to the descriptor computed at each camera."
"We present a novel probabilistic framework that fuses information coming from the audio and video modality to perform speaker diarization. The proposed framework is a Dynamic Bayesian Network (DBN) that is an extension of a factorial Hidden Markov Model (fHMM) and models the people appearing in an audiovisual recording as multimodal entities that generate observations in the audio stream, the video stream, and the joint audiovisual space. The framework is very robust to different contexts, makes no assumptions about the location of the recording equipment, and does not require labeled training data as it acquires the model parameters using the Expectation Maximization (EM) algorithm. We apply the proposed model to two meeting videos and a news broadcast video, all of which come from publicly available data sets. The results acquired in speaker diarization are in favor of the proposed multimodal framework, which outperforms the single modality analysis results and improves over the state-of-the-art audio-based speaker diarization.",""
"Security risk assessment and mitigation are two vital processes that need to be executed to maintain a productive IT infrastructure. On one hand, models such as attack graphs and attack trees have been proposed to assess the cause-consequence relationships between various network states, while on the other hand, different decision problems have been explored to identify the minimum-cost hardening measures. However, these risk models do not help reason about the causal dependencies between network states. Further, the optimization formulations ignore the issue of resource availability while analyzing a risk model. In this paper, we propose a risk management framework using Bayesian networks that enable a system administrator to quantify the chances of network compromise at various levels. We show how to use this information to develop a security mitigation and management plan. In contrast to other similar models, this risk model lends itself to dynamic analysis during the deployed phase of the network. A multiobjective optimization platform provides the administrator with all trade-off information required to make decisions in a resource constrained environment.",""
"In this study, we propose an integrated approach that combines Bayesian Networks and Total Cost of Ownership (TCO) to address complexities involved in selecting an international facility for a manufacturing plant. Our goal is to efficiently represent uncertain data and ambiguous information, and to unite them to improve the quality of the decisions. Bayesian Networks provide a framework to elicit information from experts, and provide a structure guide to efficient reasoning, even with incomplete knowledge. Our method is presented in a hierarchical structure so that it can be decomposed into the forms of more manageable units. We consider many tangible and intangible facility location criteria, then these criteria are grouped into few numbers of factors. These factors are then combined to form a cost perspective using the essentials of TCO. (C) 2011 Elsevier Ltd. All rights reserved.",""
"This paper is concerned with developing an online algorithm for detecting and estimating systematic errors (gross errors) in mass and energy balances from measurement data. This method has its application in diagnosing problems in an oil sands process. Conventional techniques for detecting gross errors presently exist for offline application. The proposed online method entitled Dynamic Bayesian Gross Error Detection (DBGED) is a dynamic Bayesian analogue of traditional gross error detection, and can be considered as a type of Switching Kalman Filter. As such, related topics such as Kalman Filtering, observability and Dynamic Bayesian Inference are discussed. In addition to detecting gross errors, the DBGED also estimates detected gross error magnitudes in real time (as an augmented state variable) so that future measurements can be corrected. When the estimate converges to yield satisfactory prediction errors, gross error estimation is stopped and instruments are corrected with a constant gross error correction term. DBGED performance is demonstrated through a simulation example and an example of an industrial application. (C) 2011 Elsevier Ltd. All rights reserved.","As such, related topics such as Kalman Filtering, observability and Dynamic Bayesian Inference are discussed."
"Thanks to the recent progress in the judicial proceedings management, especially related to the introduction of audio/video recording facilities, the challenge of identification of emotional states can be tackled. Discovering affective states embedded into speech signals could help in semantic retrieval of multimedia clips, and therefore in a deep understanding of mechanisms behind courtroom debates and judges/jurors decision making processes. In this paper two main contributions are given: (1) the collection of real-world human emotions coming from courtroom audio recordings; (2) the investigation of a hierarchical classification system, based on a risk minimization method, able to recognize emotional states from speech signatures. The accuracy of the proposed classification approach - named Multilayer Support Vector Machines - has been evaluated by comparing its performance with traditional machine learning approaches, by using both benchmark datasets and real courtroom recordings. Results in recognition obtained by the proposed technique outperform the prediction power achieved by traditional approaches like SVM, k-Nearest Neighbors, Naive Bayes, Decision Trees and Bayesian Networks. (C) 2011 Elsevier B.V. All rights reserved.","In this paper two main contributions are given: (1) the collection of real-world human emotions coming from courtroom audio recordings; (2) the investigation of a hierarchical classification system, based on a risk minimization method, able to recognize emotional states from speech signatures."
"The inference of gene regulatory networks from gene expression data is a difficult problem because the performance of the inference algorithms depends on a multitude of different factors. In this paper we study two of these. First, we investigate the influence of discrete mutual information (MI) estimators on the global and local network inference performance of the C3NET algorithm. More precisely, we study 4 different MI estimators (Empirical, Miller-Madow, Shrink and Schurmann-Grassberger) in combination with 3 discretization methods (equal frequency, equal width and global equal width discretization). We observe the best global and local inference performance of C3NET for the Miller-Madow estimator with an equal width discretization. Second, our numerical analysis can be considered as a systems approach because we simulate gene expression data from an underlying gene regulatory network, instead of making a distributional assumption to sample thereof. We demonstrate that despite the popularity of the latter approach, which is the traditional way of studying MI estimators, this is in fact not supported by simulated and biological expression data because of their heterogeneity. Hence, our study provides guidance for an efficient design of a simulation study in the context of network inference, supporting a systems approach.","The inference of gene regulatory networks from gene expression data is a difficult problem because the performance of the inference algorithms depends on a multitude of different factors."
"Modern technologies and especially next generation sequencing facilities are giving a cheaper access to genotype and genomic data measured on the same sample at once. This creates an ideal situation for multifactorial experiments designed to infer gene regulatory networks. The fifth \"Dialogue for Reverse Engineering Assessments and Methods\" (DREAM5) challenges are aimed at assessing methods and associated algorithms devoted to the inference of biological networks. Challenge 3 on \"Systems Genetics\" proposed to infer causal gene regulatory networks from different genetical genomics data sets. We investigated a wide panel of methods ranging from Bayesian networks to penalised linear regressions to analyse such data, and proposed a simple yet very powerful meta-analysis, which combines these inference methods. We present results of the Challenge as well as more in-depth analysis of predicted networks in terms of structure and reliability. The developed meta-analysis was ranked first among the 16 teams participating in Challenge 3A. It paves the way for future extensions of our inference method and more accurate gene network estimates in the context of genetical genomics.","The fifth \"Dialogue for Reverse Engineering Assessments and Methods\" (DREAM5) challenges are aimed at assessing methods and associated algorithms devoted to the inference of biological networks."
"Background: State Space Model (SSM) is a relatively new approach to inferring gene regulatory networks. It requires less computational time than Dynamic Bayesian Networks (DBN). There are two types of variables in the linear SSM, observed variables and hidden variables. SSM uses an iterative method, namely Expectation-Maximization, to infer regulatory relationships from microarray datasets. The hidden variables cannot be directly observed from experiments. How to determine the number of hidden variables has a significant impact on the accuracy of network inference. In this study, we used SSM to infer Gene regulatory networks (GRNs) from synthetic time series datasets, investigated Bayesian Information Criterion (BIC) and Principle Component Analysis (PCA) approaches to determining the number of hidden variables in SSM, and evaluated the performance of SSM in comparison with DBN. Method: True GRNs and synthetic gene expression datasets were generated using GeneNetWeaver. Both DBN and linear SSM were used to infer GRNs from the synthetic datasets. The inferred networks were compared with the true networks. Results: Our results show that inference precision varied with the number of hidden variables. For some regulatory networks, the inference precision of DBN was higher but SSM performed better in other cases. Although the overall performance of the two approaches is compatible, SSM is much faster and capable of inferring much larger networks than DBN. Conclusion: This study provides useful information in handling the hidden variables and improving the inference precision.","How to determine the number of hidden variables has a significant impact on the accuracy of network inference."
"Inferring the topology of a gene-regulatory network (GRN) from genome-scale time-series measurements of transcriptional change has proved useful for disentangling complex biological processes. To address the challenges associated with this inference, a number of competing approaches have previously been used, including examples from information theory, Bayesian and dynamic Bayesian networks (DBNs), and ordinary differential equation (ODE) or stochastic differential equation. The performance of these competing approaches have previously been assessed using a variety of in silico and in vivo datasets. Here, we revisit this work by assessing the performance of more recent network inference algorithms, including a novel non-parametric learning approach based upon nonlinear dynamical systems. For larger GRNs, containing hundreds of genes, these non-parametric approaches more accurately infer network structures than do traditional approaches, but at significant computational cost. For smaller systems, DBNs are competitive with the non-parametric approaches with respect to computational time and accuracy, and both of these approaches appear to be more accurate than Granger causality-based methods and those using simple ODEs models.","To address the challenges associated with this inference, a number of competing approaches have previously been used, including examples from information theory, Bayesian and dynamic Bayesian networks (DBNs), and ordinary differential equation (ODE) or stochastic differential equation."
"To effectively intervene when cells are trapped in pathological modes of operation it is necessary to build models that capture relevant network structure and include characterization of dynamical changes within the system. The model must be of sufficient detail that it facilitates the selection of intervention points where pathological cell behavior arising from improper regulation can be stopped. What is known about this type of cellular decision-making is consistent with the general expectations associated with any kind of decision-making operation. If the result of a decision at one node is serially transmitted to other nodes, resetting their states, then the process may suffer from mechanistic inefficiencies of transmission or from blockage or activation of transmission through the action of other nodes acting on the same node. A standard signal-processing network model, Bayesian networks, can model these properties. This paper employs a Bayesian tree model to characterize conditional pathway logic and quantify the effects of different branching patterns, signal transmission efficiencies and levels of alternate or redundant inputs. In particular, it characterizes master genes and canalizing genes within the quantitative framework. The model is also used to examine what inferences about the network structure can be made when perturbations are applied to various points in the network.","The model is also used to examine what inferences about the network structure can be made when perturbations are applied to various points in the network."
"Gene regulation is a complicated process. The interaction of many genes and their products forms an intricate biological network. Identification of this dynamic network will help us understand the biological processes in a systematic way. However, the construction of a dynamic network is very challenging for a high-dimensional system. In this article we propose to use a set of ordinary differential equations (ODE), coupled with dimensional reduction by clustering and mixed-effects modeling techniques, to model the dynamic gene regulatory network (GRN). The ODE models allow us to quantify both positive and negative gene regulation as well as feedback effects of genes in a functional module on the dynamic expression changes of genes in another functional module, which results in a directed graph network. A five-step procedure-clustering, smoothing, regulation identification, parameter estimates refining, and function enrichment analysis (CSIEF)-is developed to identify the ODE-based dynamic GRN. In the proposed CSIEF procedure, a series of cutting-edge statistical methods and techniques are employed, that include nonparametric mixed-effects models with a mixture distribution for clustering, nonparametric mixed-effects smoothing-based methods for ODE models, the smoothly clipped absolute deviation (SCAD)-based variable selection, and stochastic approximation EM (SAEM) approach for mixed-effects ODE model parameter estimation. The key step, the SCAD-based variable selection, is justified by investigating its asymptotic properties and validated by Monte Carlo simulations. We apply the proposed method to identify the dynamic GRN for yeast cell cycle progression data. We are able to annotate the identified modules through function enrichment analyses. Some interesting biological findings are discussed. The proposed procedure is a promising tool for constructing a general dynamic GRN and more complicated dynamic networks. This article has supplementary material online.",""
"When a posterior distribution has multiple modes, unconditional expectations, such as the posterior mean, may not offer informative summaries of the distribution. Motivated by this problem, we propose to decompose the sample space of a multimodal distribution into domains of attraction of local modes. Domain-based representations are defined to summarize the probability masses of and conditional expectations on domains of attraction, which are much more informative than the mean and other unconditional expectations. A computational method, the multi-domain sampler, is developed to construct domain-based representations for an arbitrary multimodal distribution. The multi-domain sampler is applied to structural learning of protein-signaling networks from high-throughput single-cell data, where a signaling network is modeled as a causal Bayesian network. Not only does our method provide a detailed landscape of the posterior distribution but also improves the accuracy and the predictive power of estimated networks. This article has supplementary material online.",""
"Background: Predicting an intensive care unit patient's outcome is highly desirable. An end goal is for computational techniques to provide updated, accurate predictions about changing patient condition using a manageable number of physiologic parameters. Methods: Principal component analysis was used to select input parameters for critical care patient outcome models. Vital signs and laboratory values from each patient's hospital stay along with outcomes (\"Discharged\" vs. \"Deceased\") were collected retrospectively at a Level I Trauma-Military Medical Center in the southwest; intensive care unit patients were included if they had been admitted for burn, infection, or hypovolemia during a 5-year period ending October 2007. Principal component analysis was used to determine which of the 24 parameters would serve as inputs in a Bayesian network developed for outcome prediction. Results: Data for 581 patients were collected. Pulse pressure, heart rate, temperature, respiratory rate, sodium, and chloride were found to have statistically significant differences between Discharged and Deceased groups for \"Hypovolemia\" patients. For \"Burn\" patients, pulse pressure, hemoglobin, hematocrit, and potassium were found to have statistically significant differences. For a \"Combined\" group, heart rate, temperature, respiratory rate, sodium, and chloride had statistically significant differences. A Bayesian network based on these results, developed for the Combined group, achieved an accuracy of 75% when predicting patient outcome. Conclusions: Outcome prediction for critical care patients is possible. Future work should explore model development using additional temporal data and should include prospective validation. Such technology could serve as the basis of real-time intelligent monitoring systems for critical patients.",""
"Recently Halpern and Pearl and Hitchcock have presented influential accounts of actual (token) causation using Bayesian networks. These accounts have been deterministic. Here we present a probabilistic extension to these active path analyses of actual causation. The extension uses \"soft\" interventions to set distributions rather than just single values. The resulting account can handle at least as wide a range of examples as the original accounts, without assuming determinism.",""
"Risk assessment and risk management for deep foundation pit engineering are essential for quality and safety in civil engineering owing to the needs of urban construction projects. However, uncertainty and fuzziness continue to challenge studies of the probability and consequences of risks in this area. Therefore, a fuzzy comprehensive evaluation method based on Bayesian networks (BNs) is proposed to assess the risks of deep foundation pit construction. This methodology has five main parts: modeling of BNs, determination of occurrence probabilities of risk events, assessment of consequences, calculations of risk value and membership degree of risk rating, and definitions of risk acceptance criteria. In a case study, primary data analysis of a simple accident database from a deep foundation pit construction project in Shanghai is used to provide the probabilities of basic events and approximate distributions of risk consequences. The probability of every risk event is calculated by using deductive BN techniques. Then the consequence of each event is calculated by using fuzzy analysis (i.e., statistical consequence distributions and weight coefficients of risk events are determined through the database). A fuzzy comprehensive evaluation model with a membership function is also presented, and each risk event in the deep foundation pit construction his rated. In addition, risk precautions and control measures are suggested on the basis of the risk assessment results and are applied to risk management in deep foundation pit construction. DOI: 10.1061/(ASCE)CO.1943-7862.0000391. (C) 2011 American Society of Civil Engineers.",""
"An important open problem of computational neuroscience is the generic organization of computations in networks of neurons in the brain. We show here through rigorous theoretical analysis that inherent stochastic features of spiking neurons, in combination with simple nonlinear computational operations in specific network motifs and dendritic arbors, enable networks of spiking neurons to carry out probabilistic inference through sampling in general graphical models. In particular, it enables them to carry out probabilistic inference in Bayesian networks with converging arrows (\"explaining away\") and with undirected loops, that occur in many real-world tasks. Ubiquitous stochastic features of networks of spiking neurons, such as trial-to-trial variability and spontaneous activity, are necessary ingredients of the underlying computational organization. We demonstrate through computer simulations that this approach can be scaled up to neural emulations of probabilistic inference in fairly large graphical models, yielding some of the most complex computations that have been carried out so far in networks of spiking neurons.","We show here through rigorous theoretical analysis that inherent stochastic features of spiking neurons, in combination with simple nonlinear computational operations in specific network motifs and dendritic arbors, enable networks of spiking neurons to carry out probabilistic inference through sampling in general graphical models."
"Estimation of effective connectivity, a measure of the influence among brain regions, can potentially reveal valuable information about organization of brain networks. Effective connectivity is usually evaluated from the functional data of a single modality. In this paper we show why that may lead to incorrect conclusions about effective connectivity. In this paper we use Bayesian networks to estimate connectivity on two different modalities. We analyze structures of estimated effective connectivity networks using aggregate statistics from the field of complex networks. Our study is conducted on functional MRI and magnetoencephalography data collected from the same subjects under identical paradigms. Results showed some similarities but also revealed some striking differences in the conclusions one would make on the fMRI data compared with the MEG data and are strongly supportive of the use of multiple modalities in order to gain a more complete picture of how the brain is organized given the limited information one modality is able to provide. (C) 2011 Elsevier Ltd. All rights reserved.",""
"Bayesian networks (BNs), also known as Bayesian belief networks or Bayes nets, are a kind of probabilistic graphical model that has become very popular to practitioners mainly due to the powerful probability theory involved, which makes them able to deal with a wide range of problems. The goal of this review is to show how BNs are being used in environmental modelling. We are interested in the application of BNs, from January 1990 to December 2010, in the areas of the ISI Web of Knowledge related to Environmental Sciences. It is noted that only the 4.2% of the papers have been published under this item. The different steps that configure modelling via BNs have been revised: aim of the model, data pre-processing, model learning, validation and software. Our literature review indicates that BNs have barely been used for Environmental Science and their potential is, as yet, largely unexploited. (C) 2011 Elsevier Ltd. All rights reserved.",""
"This study uses hospital administrative data to ascertain the differences in the patient characteristics, process and outcomes of care between the Emergency Department (ED) triage categories of patients admitted from an ED presentation into a large metropolitan teaching hospital with a Stroke Care Unit. Bayesian Networks (BNs) derived from the administrative data were used to provide the descriptive models. Nearly half the patients in each stroke subtype were triaged as 'Urgent' (to be seen within 30 minutes). With a decrease in the urgency of triage categories, the proportion admitted within 8 hours decreased dramatically and the proportion of formal discharge increased. Notably, 45% of transient ischaemic attacks (TIAs) were categorized as 'Semi-urgent' (to be attended within 60 minutes), indicating an opportunity to improve emergency assessment of TIAs. The results illustrate the utility of hospital administrative data and the applicability of BNs for review of the current triage practices and subsequent impact.",""
"In daily life, language is an important tool of communication between people. Besides language, facial action can also provide a great amount of information. Therefore, facial action recognition has become a popular research topic in the field of human-computer interaction (HCI). However, facial action recognition is quite a challenging task due to its complexity. In a literal sense, there are thousands of facial muscular movements, many of which have very subtle differences. Moreover, muscular movements always occur simultaneously when the pose is changed. To address this problem, we first build a fully automatic facial points detection system based on a local Gabor filter bank and principal component analysis. Then, novel dynamic Bayesian networks are proposed to perform facial action recognition using the junction tree algorithm over a limited number of feature points. In order to evaluate the proposed method, we have used the Korean face database for model training. For testing, we used the CUbiC FacePix, facial expressions and emotion database, Japanese female facial expression database, and our own database. Our experimental results clearly demonstrate the feasibility of the proposed approach. (C) 2011 Society of Photo-Optical Instrumentation Engineers (SPIE). [DOI: 10.1117/1.3662426]",""
"Peat is an important carbon sink in the context of climate change. However, well-documented examples suggest that risk of peat erosion is widespread and significant. Our understanding of peat vulnerability to erosion is commonly constrained by the complexity of drivers, and their interactions, in this process. However, the key constraints are: limited, consistent and comprehensive quantitative data relating to this process and, more significantly, the explicit relationships between the occurrence of peat erosion and its causes and drivers. Bayesian belief networks (BBNs) provide a methodology for integrating qualitative and quantitative knowledge. BBNs can capture and structure available knowledge and rationalize complex interactions, where empirical data are limited or poorly compatible and processes are complex or uncertain. In this article we explore the BBN potential to advance our understanding and to identify gaps in current knowledge. BBN has been demonstrated to be a useful tool in structuring and utilizing currently available knowledge, often with limited evidence, of peat's actual exposure to erosive forces. Despite considerable research into peat erosion processes and understanding the inherent vulnerability of peat, results presented indicate clear gaps in knowledge regarding the role of land management, spatially explicit data related to land management as well as limited evidence of the relevant relationships between many of the variables. The attention of further research will focus on these gaps. The BBN approach provides a framework in which different scenarios of biophysical, climatic and land management (social and economic) conditions can examine and assess the probability of erosion.",""
"One of the key computational problems in Bayesian networks is computing the maximal posterior probability of a set of variables in the network, given an observation of the values of another set of variables. In its most simple form, this problem is known as the MPE-problem. In this paper, we give an overview of the computational complexity of many problem variants, including enumeration variants, parameterized problems, and approximation strategies to the MPE-problem with and without additional (neither observed nor explained) variables. Many of these complexity results appear elsewhere in the literature; other results have not been published yet. The paper aims to provide a fairly exhaustive overview of both the known and new results. (C) 2011 Elsevier Inc. All rights reserved.",""
NA,""
"Neuron morphology is crucial for neuronal connectivity and brain information processing. Computational models are important tools for studying dendritic morphology and its role in brain function. We applied a class of probabilistic graphical models called Bayesian networks to generate virtual dendrites from layer III pyramidal neurons from three different regions of the neocortex of the mouse. A set of 41 morphological variables were measured from the 3D reconstructions of real dendrites and their probability distributions used in a machine learning algorithm to induce the model from the data. A simulation algorithm is also proposed to obtain new dendrites by sampling values from Bayesian networks. The main advantage of this approach is that it takes into account and automatically locates the relationships between variables in the data instead of using predefined dependencies. Therefore, the methodology can be applied to any neuronal class while at the same time exploiting class-specific properties. Also, a Bayesian network was defined for each part of the dendrite, allowing the relationships to change in the different sections and to model heterogeneous developmental factors or spatial influences. Several univariate statistical tests and a novel multivariate test based on Kullback-Leibler divergence estimation confirmed that virtual dendrites were similar to real ones. The analyses of the models showed relationships that conform to current neuroanatomical knowledge and support model correctness. At the same time, studying the relationships in the models can help to identify new interactions between variables related to dendritic morphology.",""
"It is known that from DNA microarray experiments, a huge amount of data about gene expression of different cell populations can be generated. To deal with effectively large datasets in which instances are described by many features and reduce high data dimensionality, feature selection emerges as a useful approach in the flow of data analysis. In this paper, we propose a correlational Bayesian network for feature selection and analysis. The proposed algorithm has capability of effectively identifying and manipulating correlational individuals. It improves performance and provides high accurate results than other Bayesian networks in learning and automatic control of the population size. The use of Bayesian framework to infer the weights and weight decay terms as well as to perform model selection makes it possible to find neural models with high generalization capability and less complexity. Backpropagation network is used to classify cancer types. The experiment using the correlational Bayesian network strategy has been carried out and a comparison study with the unmodified Bayesian network approach is also completed.",""
"This paper proposes a novel online two-level multitarget tracking and detection (MTTD) algorithm. The algorithm focuses on multitarget detection and tracking for the case of multiple measurements per target and for an unknown and varying number of targets. Information is continuously exchanged in both directions between the two levels. Using the high level target position and shape information, the low level clusters the measurements. Furthermore, the low level features automatic relevance detection (ARD), as it automatically determines the optimal number of clusters from the measurements taking into account the expected target shapes. The high level's data association allows for a varying number of targets. A joint probabilistic data association algorithm looks for associations between clusters of measurements and targets. These associations are used to update the target trackers and the target shapes with the individual measurements. No information is lost in the two-level approach since the measurement information is not summarized into features. The target trackers are based on an underlying motion model, while the high level is supplemented with a filter estimating the number of targets. The algorithm is verified using both simulations and experiments using two sensor modalities, video and laser scanner, for detection and tracking of people and ants.",""
"Studies show that application of the prior knowledge in biasing the Estimation of Distribution Algorithms (EDAs), such as Bayesian Optimization Algorithm (BOA), increases the efficiency of these algorithms significantly. One of the main advantages of the EDAs over other optimization algorithms is that the former provides a trail of probabilistic models of candidate solutions with increasing quality. Some recent studies have applied these probabilistic models, obtained from previously solved problems in biasing the BOA algorithm, to solve the future problems. In this paper, in order to improve the previous works and reduce their disadvantages, a method based on Case Based Reasoning (CBR) is proposed for biasing the BOA algorithm. Herein, after running BOA for solving optimization problems, each problem, the corresponding solution, as well as the last Bayesian network obtained from the BOA algorithm, will be stored as an entry in the case-base. Upon introducing a new problem, similar problems from the case-base are retrieved and the last Bayesian networks of these solved problems are combined according to the degree of their similarity with the new problem; hence, a compound Bayesian network is constructed. The compound Bayesian network is sampled and the initial population for the BOA algorithm is generated. This network will be applied efficiently for biasing future probabilistic models during the runs of BOA for the new problem. The proposed method is tested on three well-known combinatorial benchmark problems. Experimental results show significant improvements in algorithm execution time and quality of solutions, compared to previous methods. (C) 2011 Elsevier B.V. All rights reserved.",""
"In this paper, we consider how to recover the structure of a Bayesian network from a moral graph. We present a more accurate characterization of moral edges, based on which a complete subset (i.e., a separator) contained in the neighbor set of one vertex of the putative moral edge in some prime block of the moral graph can be chosen. This results in a set of separators needing to be searched generally smaller than the sets required by some existing algorithms. A so-called structure-finder algorithm is proposed for structural learning. The complexity analysis of the proposed algorithm is discussed and compared with those for several existing algorithms. We also demonstrate how to construct the moral graph locally from, separately, the Markov blanket, domain knowledge and d-separation trees. Simulation studies are used to evaluate the performances of various strategies for structural learning. We also analyze a gene expression data set by using the structure-finder algorithm. (C) 2011 Elsevier B.V. All rights reserved.",""
"Background: While contemporary methods of microarray analysis are excellent tools for studying individual microarray datasets, they have a tendency to produce different results from different datasets of the same disease. We aim to solve this reproducibility problem by introducing a technique (SNet). SNet provides both quantitative and descriptive analysis of microarray datasets by identifying specific connected portions of pathways that are significant. We term such portions within pathways as \"subnetworks\". Results: We tested SNet on independent datasets of several diseases, including childhood ALL, DMD and lung cancer. For each of these diseases, we obtained two independent microarray datasets produced by distinct labs on distinct platforms. In each case, our technique consistently produced almost the same list of significant nontrivial subnetworks from two independent sets of microarray data. The gene-level agreement of these significant subnetworks was between 51.18% to 93.01%. In contrast, when the same pairs of microarray datasets were analysed using GSEA, t-test and SAM, this percentage fell between 2.38% to 28.90% for GSEA, 49.60% tp 73.01% for t-test, and 49.96% to 81.25% for SAM. Furthermore, the genes selected using these existing methods did not form subnetworks of substantial size. Thus it is more probable that the subnetworks selected by our technique can provide the researcher with more descriptive information on the portions of the pathway actually affected by the disease. Conclusions: These results clearly demonstrate that our technique generates significant subnetworks and genes that are more consistent and reproducible across datasets compared to the other popular methods available (GSEA, t-test and SAM). The large size of subnetworks which we generate indicates that they are generally more biologically significant (less likely to be spurious). In addition, we have chosen two sample subnetworks and validated them with references from biological literature. This shows that our algorithm is capable of generating descriptive biologically conclusions.",""
"Data fusion methods are powerful tools for integrating the different views of an organism provided by various types of experimental data. We describe various methodologies for integrating and drawing inferences from a collection of biomedical data, primarily focusing on protein and gene expression data. Computational experiments performed using biomedical data, including known protein-protein interactions, hydropathy profiles, gene expression data and amino acid sequences, demonstrate the utility of this approach. Overall, studies agree in that methodologies using carefully selected data of various types to predict particular classes, groups and interactions, perform better than when applied to a single type of data. (C) 2011 Elsevier B.V. All rights reserved.","We describe various methodologies for integrating and drawing inferences from a collection of biomedical data, primarily focusing on protein and gene expression data."
"Dam safety has drawn increasing attention from the public. To ensure dam safety, it is essential to diagnose any dam distresses and their causes properly. The main objective of this paper is to develop a robust probability-based tool using Bayesian networks for the diagnosis of embankment dam distresses at the global level based on past dam distress data. A database of 993 distressed in-service embankment dams in China has been compiled, including general information on the dams, distresses, and causes. Based on the database, general characteristics of embankment dam distresses are studied using Bayesian networks, which can tackle not only the multiplicity of dam distresses and causes, but also the complex interrelations among them. Common patterns and causes of distresses are identified. The interrelations among the dam distresses and their causes are quantified using conditional probabilities determined based on the historical frequencies from the dam distress database. A sensitivity analysis is also conducted to identify and rank the most important factors that cause the distresses. With the prior information of common characteristics extracted from the database, Bayesian networks are further used to diagnose a specific distressed dam at the local level by combining global-level performance records and project-specific evidence in a systematic structure, which is presented in a companion paper.",""
"Based on prior information on common characteristics of dam distresses extracted from the dam distress database described in a companion paper, this paper attempts to extend the technique of Bayesian networks to the diagnosis of a specific distressed dam. The diagnosis is conducted by combining two sources of information, i.e., global-level knowledge from the database and project-specific evidence. Based on results of the diagnosis, key distress factors for a specific dam can be identified and suitable remedial measures can be suggested. Further, the Bayesian network analysis is conducted to evaluate the effectiveness of the adopted remedial measures. A case study on the diagnosis of a distressed embankment dam, Chenbihe Dam, with seepage problems is presented to illustrate the methodology. In this case study, the observed leakage rates, seepage exit locations, and boundary conditions of the embankment are used as project-specific evidence.",""
"The standard Bayesian Information Criterion (BIC) is derived under regularity conditions which are not always satisfied in the case of graphical models with hidden variables. In this paper we derive the BIC for the binary graphical tree models where all the inner nodes of a tree represent binary hidden variables. This provides an extension of a similar formula given by Rusakov and Geiger for naive Bayes models. The main tool used in this paper is the connection between the growth behavior of marginal likelihood integrals and the real log-canonical threshold.",""
"In this study, we analyze the supplier selection process by combining Bayesian Networks (BN) and Total Cost of Ownership (TCO) methods. The proposed approach aims to efficiently incorporate and exploit the buyer's domain-specific information when the buyer has both limited and uncertain information regarding the supplier. This study examines uncertainty from a total cost perspective, with regards to causes of supplier performance and capability on buyer's organization. The proposed approach is assessed and tested in automotive industry for tier-1 supplier for selecting its own suppliers. To efficiently facilitate expert opinions, we form factors to represent and explain various supplier selection criteria and to reduce complexity. The case study in automotive industry shows several advantages of the proposed method. A BN approach facilitates a more insightful evaluation and selection of alternatives given its semantics for decision making. The buyer can also make an accurate cost estimation that are specifically linked with suppliers' performance. Both buyer and supplier have clear vision to reduce costs and to improve the relations. (C) 2011 Elsevier Ltd. All rights reserved.",""
"Gene regulatory networks give important insights into the mechanisms underlying physiology and pathophysiology. The derivation of gene regulatory networks from high-throughput expression data via machine learning strategies is problematic as the reliability of these models is often compromised by limited and highly variable samples, heterogeneity in transcript isoforms, noise, and other artifacts. Here, we develop a novel algorithm, dubbed Dandelion, in which we construct and train intraspecies Bayesian networks that are translated and assessed on independent test sets from other species in a reiterative procedure. The interspecies disease networks are subjected to multi-layers of analysis and evaluation, leading to the identification of the most consistent relationships within the network structure. In this study, we demonstrate the performance of our algorithms on datasets from animal models of oculopharyngeal muscular dystrophy (OPMD) and patient materials. We show that the interspecies network of genes coding for the proteasome provide highly accurate predictions on gene expression levels and disease phenotype. Moreover, the cross-species translation increases the stability and robustness of these networks. Unlike existing modeling approaches, our algorithms do not require assumptions on notoriously difficult one-to-one mapping of protein orthologues or alternative transcripts and can deal with missing data. We show that the identified key components of the OPMD disease network can be confirmed in an unseen and independent disease model. This study presents a state-of-the-art strategy in constructing interspecies disease networks that provide crucial information on regulatory relationships among genes, leading to better understanding of the disease molecular mechanisms.",""
"Cancer evolves through the accumulation of mutations, but the order in which mutations occur is poorly understood. Inference of a temporal ordering on the level of genes is challenging because clinically and histologically identical tumors often have few mutated genes in common. This heterogeneity may at least in part be due to mutations in different genes having similar phenotypic effects by acting in the same functional pathway. We estimate the constraints on the order in which alterations accumulate during cancer progression from cross-sectional mutation data using a probabilistic graphical model termed Hidden Conjunctive Bayesian Network (H-CBN). The possible orders are analyzed on the level of genes and, after mapping genes to functional pathways, also on the pathway level. We find stronger evidence for pathway order constraints than for gene order constraints, indicating that temporal ordering results from selective pressure acting at the pathway level. The accumulation of changes in core pathways differs among cancer types, yet a common feature is that progression appears to begin with mutations in genes that regulate apoptosis pathways and to conclude with mutations in genes involved in invasion pathways. H-CBN models provide a quantitative and intuitive model of tumorigenesis showing that the genetic events can be linked to the phenotypic progression on the level of pathways.","Inference of a temporal ordering on the level of genes is challenging because clinically and histologically identical tumors often have few mutated genes in common."
"Biotin is an essential water-soluble vitamin crucial for maintaining normal body functions. The importance of biotin for human health has been under-appreciated but there is plenty of opportunity for future research with great importance for human health. Currently, carrying out predictions of biotin metabolism involves tedious manual manipulations. In this paper, we report the development of BiotinNet, an internet based program that uses Bayesian networks to integrate published data on various aspects of biotin metabolism. Users can provide a combination of values on the levels of biotin related metabolites to obtain the predictions on other metabolites that are not specified. As an inherent feature of Bayesian networks, the uncertainty of the prediction is also quantified and reported to the user. This program enables convenient in silica experiments regarding biotin metabolism, which can help researchers design future experiments while new data can be continuously incorporated. (C) 2011 Elsevier Ireland Ltd. All rights reserved.",""
"Inferring gene networks from longitudinal gene expression microarrays is a crucial step towards the study of gene regulatory mechanisms. A decade ago, expensive microarray technology restricted the number of samples undergoing gene expression profiling in single studies, leading the inference algorithms that assume stationary gene networks to the best solution. Thanks to decreasing cost of modern microarray technologies, more gene expression profiles can be assessed in single studies. With more samples available, we can relax the stationarity assumption and develop a method to infer dynamic gene networks, which can reflect more realistic biology where genes adaptively orchestrate each other. This paper applied the framework of dynamic Bayesian networks to infer adaptive gene interactions by identifying individual transition networks between pairs of consecutive times. Due to high computational burden of inferring the interconnection patterns among all genes over time, we designed a parallelizable inference algorithm to make feasible the task. We validated our approach by two clinical studies: yellow fever vaccination and mechanical periodontal therapy. The inferred dynamic networks achieved more than 90% predictive accuracy, a significant improvement when compared to stationary models (p<0.05). The adaptive models can help explain the induction of innate immunology in greater details after yellow fever vaccination and interpret the anti-inflammatory effect of mechanical periodontal therapy.","A decade ago, expensive microarray technology restricted the number of samples undergoing gene expression profiling in single studies, leading the inference algorithms that assume stationary gene networks to the best solution."
"Objectives: Bayesian networks (BNs) are rapidly becoming a leading technology in applied Artificial Intelligence, with many applications in medicine. Both automated learning of BNs and expert elicitation have been Used to build these networks, but the potentially more useful combination of these two methods remains underexplored. In this paper we examine a number of approaches to their combination when learning structure and present new techniques for assessing their results. Methods and materials: Using public-domain medical data, we run an automated causal discovery system, CaMML, which allows the incorporation of multiple kinds of prior expert knowledge into its search, to test and compare unbiased discovery with discovery biased with different kinds of expert opinion. We use adjacency matrices enhanced with numerical and colour labels to assist with the interpretation of the results. We present an algorithm for generating a single BN from a set of learned BNs that incorporates user Preferences regarding complexity vs completeness. These techniques are presented as part of the first detailed workflow for hybrid structure learning within the broader knowledge engineering process. Results: The detailed knowledge engineering workflow is shown to be useful for structuring a complex iterative BN development process. The adjacency matrices make it clear that for our medical case study using the IOWA dataset, the simplest kind of prior information (partially sorting variables into tiers) was more effective in aiding model discovery than either using no prior information or using more sophisticated and detailed expert priors. The method for generating a single BN captures relationships that would be overlooked by other approaches in the literature. Conclusion: Hybrid causal learning of BNs is an important emerging technology. We present methods for incorporating it into the knowledge engineering process, including visualisation and analysis of the learned networks. (C) 2011 Elsevier B.V. All rights reserved.",""
"As they are used to evaluate the importance of research at different levels by funding agencies and promotion committees, bibliometric indices have received a lot of attention from the scientific community over the last few years. Many bibliometric indices have been developed in order to take into account aspects not previously covered. The result is that, nowadays, the scientific community faces the challenge of selecting which of this pool of indices meets the required quality standards. In view of the vast number of bibliometric indices, it is necessary to analyze how they relate to each other (irrelevant, dependent and so on). Our main purpose is to learn a Bayesian network model from data to analyze the relationships among bibliometric indices. The induced Bayesian network is then used to discover probabilistic conditional (in) dependencies among the indices and, also for probabilistic reasoning. We also run a case study of 14 well-known bibliometric indices on computer science and artificial intelligence journals.",""
"An approach for the integration of Object-Oriented Bayesian Networks (OOBNs) and Evolutionary Multiobjective Optimization (EMO) is proposed for integrated water resource management and decision support. Bayesian Networks (BNs) offer a novel and powerful tool for modelling complex water systems, facilitating the use of hierarchical modelling by improving the efficiency and communication between the different parts of a model. EMO offers a range of non-dominated optimal management solutions on a Pareto front that facilitate the identification of tradeoffs among conflicting criteria regarding stakeholder's preferences. The integrated tool provides new possibilities for undertaking an integrated analysis where stakeholder participation can play an important role. It is used for simultaneously analysing the whole water system, characterising uncertainty as probabilities and evaluating different management options. The tool is applied to an overexploited water system located in Southern Spain that is supplied totally by groundwater. In this study, a complex model based on BNs is designed and used as the core of the study. The transition to Evolutionary Bayesian networks (EOBNs) allows stakeholder involvement to be utilized more effectively for designing and evaluating the model's consistency, and taking into account their conflicting interests.",""
"Three-dimensional gait analysis (3DGA) generates a wealth of highly variable data. Gait classifications help to reduce, simplify and interpret this vast amount of 3DGA data and thereby assist and facilitate clinical decision making in the treatment of CP. CP gait is often a mix of several clinically accepted distinct gait patterns. Therefore, there is a need for a classification which characterizes each CP gait by different degrees of membership for several gait patterns, which are considered by clinical experts to be highly relevant. In this respect, this paper introduces Bayesian networks (BN) as a new approach for classification of 3DGA data of the ankle and knee in children with CP. A BN is a probabilistic graphical model that represents a set of random variables and their conditional dependencies via a directed acyclic graph. Furthermore, they provide an explicit way of introducing clinical expertise as prior knowledge to guide the BN in its analysis of the data and the underlying clinically relevant relationships. BNs also enable to classify gait on a continuum of patterns, as their outcome consists of a set of probabilistic membership values for different clinically accepted patterns. A group of 139 patients with CP was recruited and divided into a training- (n = 80% of all patients) and a validation-dataset (n = 20% of all patients). An average classification accuracy of 88.4% was reached. The BN of this study achieved promising accuracy rates and was found to be successful for classifying ankle and knee joint motion on a continuum of different clinically relevant gait patterns. (C) 2011 Elsevier Ltd. All rights reserved.","Gait classifications help to reduce, simplify and interpret this vast amount of 3DGA data and thereby assist and facilitate clinical decision making in the treatment of CP."
"For those systems that can be modeled with perfect nodes and unreliable arcs such as communication systems, a reliability graph with general gates (RGGG) was developed. Similarly, for more efficient system reliability analysis of those systems that can be modeled with unreliable nodes and perfect arcs, a reliability block diagram with general gates (RBDGG) was developed as an intuitive and easy-to-use method for system reliability analysis. One of the unique characteristics of RBDGG is to allow node connection relations of general gates such as the AND gate and the k-out-of-n gate other than the OR gate connection relation of the conventional reliability block diagram (RBD). Mathematical formulations for RBDGG and a method of mapping a RBDGG model into an equivalent Bayesian network model without losing the one-to-one matching characteristic of the RBDGG for quantitative analysis is also provided. With the application to two example systems, the usefulness of the proposed RBDGG is demonstrated. (C) 2011 Elsevier Ltd. All rights reserved.",""
"Mining temporal network models from discrete event streams is an important problem with applications in computational neuroscience, physical plant diagnostics, and human-computer interaction modeling. In this paper, we introduce the notion of excitatory networks which are essentially temporal models where all connections are stimulative, rather than inhibitive. The emphasis on excitatory connections facilitates learning of network models by creating bridges to frequent episode mining. Specifically, we show that frequent episodes help identify nodes with high mutual information relationships and that such relationships can be summarized into a dynamic Bayesian network (DBN). This leads to an algorithm that is significantly faster than state-of-the-art methods for inferring DBNs, while simultaneously providing theoretical guarantees on network optimality. We demonstrate the advantages of our approach through an application in neuroscience, where we show how strong excitatory networks can be efficiently inferred from both mathematical models of spiking neurons and several real neuroscience datasets.",""
"Background: When designing pharmaceutical products, the relationships between causal factors and pharmaceutical responses are intricate. A Bayesian network (BN) was used to clarify the latent structure underlying the causal factors and pharmaceutical responses of a tablet containing solid dispersion (SD) of indomethacin (IMC). Method: IMC, a poorly water-soluble drug, was tested with polyvinylpyrrolidone as the carrier polymer. Tablets containing a SD or a physical mixture of IMC, different quantities of magnesium stearate, microcrystalline cellulose, and low-substituted hydroxypropyl cellulose, and subjected to different compression force were selected as the causal factors. The pharmaceutical responses were the dissolution properties and tensile strength before and after the accelerated test and a similarity factor, which was used as an index of the storage stability. Result: BN models were constructed based on three measurement criteria for the appropriateness of the graph structure. Of these, the BN model based on Akaike's information criterion was similar to the results for the analysis of variance. To quantitatively estimate the causal relationships underlying the latent structure in this system, conditional probability distributions were inferred from the BN model. The responses were accurately predicted using the BN model, as reflected in the high correlation coefficients in a leave-one-out cross-validation procedure. Conclusion: The BN technique provides a better understanding of the latent structure underlying causal factors and responses.",""
"We explore a newly proposed system architecture, called tower of knowledge (ToK), in the context of labelling components of building scenes. The ToK architecture allows the incorporation of statistical feature distributions and logic rules concerning the definition of a component, within a probabilistic framework. The maximum likelihood method of label assignment is modified by being multiplied with a function, called utility function, that expresses the information coming from the logic rules programmed to the system. The logic rules are designed to define an object/component by answering the questions \"why\" and \"how\", referring to the actions in which a particular object may be observed to participate and the characteristics it should have in order to be able to participate in these actions. Two sets of measurements are assumed to be available: those made initially for all components routinely, and which supply the initial statistically based inference of possible labels of each component, and those that are made in order to confirm or deny a particular characteristic of the component that would allow it to participate in a specific action. A recursive version of the architecture is also proposed, in which the distributions of the former types of measurement may be learnt in the process, having no training data at all. Multi-view images are used as input to the system, which uses standard techniques to build the 3D models of the buildings. The system is tested on labelling the components of 10 3D models of buildings. The components are identified either manually, or fully automatically. The results are compared with those obtained by expandable Bayesian networks. The recursive version of ToK proves to be able to cope very well even without any training data, where it learns the characteristics of the various components by simply applying the pre-programmed logic rules that connect labels, actions and attributes. (C) 2011 Elsevier Inc. All rights reserved.","Two sets of measurements are assumed to be available: those made initially for all components routinely, and which supply the initial statistically based inference of possible labels of each component, and those that are made in order to confirm or deny a particular characteristic of the component that would allow it to participate in a specific action."
"Despite its usefulness, design knowledge is not often captured or documented, and is therefore lost or damaged after a product design is completed. As a way to address this issue, two major formalisms can be used for modeling, representing, and reasoning about causal design knowledge: fuzzy cognitive map (FCM) and Bayesian belief network (BBN). Although FCM has been used extensively in knowledge engineering, few methodologies exist for systematically constructing it. In this paper, we present a methodology and application-FCM Constructor-to systematically acquire design knowledge from domain experts, and to construct a corresponding BBN. To show the system's usability, we use three realistic product design cases to compare BBNs that are directly generated by domain experts, with BBNs that are generated using the FCM Constructor. We find that the BBN constructed through the FCM Constructor is similar, based on reasoning results, to the BBN constructed directly by specifying conditional probability tables of BBNs. (C) 2011 Elsevier Ltd. All rights reserved.",""
"This paper presents a novel approach to simulation metamodeling using dynamic Bayesian networks (DBNs) in the context of discrete event simulation. A DBN is a probabilistic model that represents the joint distribution of a sequence of random variables and enables the efficient calculation of their marginal and conditional distributions. In this paper, the construction of a DBN based on simulation data and its utilization in simulation analyses are presented. The DBN metamodel allows the study of the time evolution of simulation by tracking the probability distribution of the simulation state over the duration of the simulation. This feature is unprecedented among existing simulation metamodels. The DBN metamodel also enables effective what-if analysis which reveals the conditional evolution of the simulation. In such an analysis, the simulation state at a given time is fixed and the probability distributions representing the state at other time instants are updated. Simulation parameters can be included in the DBN metamodel as external random variables. Then, the DBN offers a way to study the effects of parameter values and their uncertainty on the evolution of the simulation. The accuracy of the analyses allowed by DBNs is studied by constructing appropriate confidence intervals. These analyses could be conducted based on raw simulation data but the use of DBNs reduces the duration of repetitive analyses and is expedited by available Bayesian network software. The construction and analysis capabilities of DBN metamodels are illustrated with two example simulation studies. (C) 2011 Elsevier B.V. All rights reserved.",""
"Background: Understanding gene interactions in complex living systems can be seen as the ultimate goal of the systems biology revolution. Hence, to elucidate disease ontology fully and to reduce the cost of drug development, gene regulatory networks (GRNs) have to be constructed. During the last decade, many GRN inference algorithms based on genome-wide data have been developed to unravel the complexity of gene regulation. Time series transcriptomic data measured by genome-wide DNA microarrays are traditionally used for GRN modelling. One of the major problems with microarrays is that a dataset consists of relatively few time points with respect to the large number of genes. Dimensionality is one of the interesting problems in GRN modelling. Results: In this paper, we develop a biclustering function enrichment analysis toolbox (BicAT-plus) to study the effect of biclustering in reducing data dimensions. The network generated from our system was validated via available interaction databases and was compared with previous methods. The results revealed the performance of our proposed method. Conclusions: Because of the sparse nature of GRNs, the results of biclustering techniques differ significantly from those of previous methods.","During the last decade, many GRN inference algorithms based on genome-wide data have been developed to unravel the complexity of gene regulation."
"This paper analyses and discusses arguments that emerge from a recent discussion about the proper assessment of the evidential value of correspondences observed between the characteristics of a crime stain and those of a sample from a suspect when (i) this latter individual is found as a result of a database search and (ii) remaining database members are excluded as potential sources (because of different analytical characteristics). Using a graphical probability approach (i.e., Bayesian networks), the paper here intends to clarify that there is no need to (i) introduce a correction factor equal to the size of the searched database (i.e., to reduce a likelihood ratio), nor to (ii) adopt a propositional level not directly related to the suspect matching the crime stain (i.e., a proposition of the kind 'some person in (outside) the database is the source of the crime stain' rather than 'the suspect (some other person) is the source of the crime stain'). The present research thus confirms existing literature on the topic that has repeatedly demonstrated that the latter two requirements (i) and (ii) should not be a cause of concern. (C) 2011 Elsevier Ireland Ltd. All rights reserved.",""
"Interval data are widely used in real applications to represent the values of quantities in uncertain situations. However, the implied probabilistic causal relationships among interval-valued variables with interval data cannot be represented and inferred by general Bayesian networks with point-based probability parameters. Thus, it is desired to extend the general Bayesian network with effective mechanisms of representation, learning and inference of probabilistic causal relationship simplied in interval data. In this paper, we define the interval probabilities, the bound-limited weak conditional interval probabilities and the probabilistic description, as well as the multiplication rules. Furthermore, we propose the method for learning the Bayesian network structure from interval data and the algorithm for corresponding approximate inferences. Experimental results show that our methods are feasible, and we conclude that the Bayesian network with interval probability parameters is the expansion of the general Bayesian network","Thus, it is desired to extend the general Bayesian network with effective mechanisms of representation, learning and inference of probabilistic causal relationship simplied in interval data."
"Introduction: This study describes a method for reducing the number of variables frequently considered in modeling the severity of traffic accidents. The method's efficiency is assessed by constructing Bayesian networks (BN). Method: It is based on a two stage selection process. Several variable selection algorithms, commonly used in data mining, are applied in order to select subsets of variables. BNs are built using the selected subsets and their performance is compared with the original BN (with all the variables) using five indicators. The BNs that improve the indicators' values are further analyzed for identifying the most significant variables (accident type, age, atmospheric factors, gender, lighting, number of injured, and occupant involved). A new BN is built using these variables, where the results of the indicators indicate, in most of the cases, a statistically significant improvement with respect to the original BN. Conclusions: It is possible to reduce the number of variables used to model traffic accidents injury severity through BNs without reducing the performance of the model. Impact on Industry: The study provides the safety analysts a methodology that could be used to minimize the number of variables used in order to determine efficiently the injury severity of traffic accidents without reducing the performance of the model. (C) 2011 National Safety Council and Elsevier Ltd. All rights reserved.",""
"In this paper, our system is a Markovien system which we can see it like a Dynamic Bayesian Networks. One of the major interests of these systems resides in the complete training of the models (topology and parameters) starting from training data. The representation of knowledge bases on description, by graphs, relations of causality existing between the variables defining the field of study. The theory of Dynamic Bayesian Networks is a generalization of the Bayesians Networks to the dynamic processes. Our objective amounts finding the better structure which represents the relationships (dependencies) between the variables of a dynamic bayesian network. In applications in pattern recognition, one will carry out the fixing of the structure which obliges us to admit some strong assumptions (for example independence between some variables).",""
"Functional integration in the brain refers to distributed interactions among functionally segregated regions. Investigation of effective connectivity in brain networks, i.e, the directed causal influence that one brain region exerts over another region, is being increasingly recognized as an important tool for understanding brain function in neuroimaging studies. Methods for identifying intrinsic relationships among elements in a network are increasingly in demand. Over the last few decades several techniques such as Bayesian networks, Granger causality, and dynamic causal models have been developed to identify causal relations in dynamic systems. At the same time, established techniques such as structural equation modeling (SEM) are being modified and extended in order to reveal underlying interactions in imaging data. In the R package FIAR, which stands for Functional Integration Analysis in R, we have implemented many of the latest techniques for analyzing brain networks based on functional magnetic resonance imaging (fMRI) data. The package can be used to analyze experimental data, but also to simulate data under certain models.",""
"In this paper, we put forth the first join tree propagation algorithm that selectively applies either arc reversal (AR) or variable elimination (VE) to build the propagated messages. Our approach utilizes a recent method for identifying the propagated join tree messages a priori. When it is determined that a join tree node will construct a single distribution to be sent to a neighbouring node, VE is utilized as it builds a single distribution in the most direct fashion; otherwise, AR is applied as it maintains a factorization of distributions allowing for barren variables to be exploited during propagation later on in the join tree. Experimental results, involving evidence processing in four benchmark Bayesian networks, empirically demonstrate that selectively applying VE and AR is faster than applying one of these methods exclusively on the entire network. (C) 2010 Elsevier Inc. All rights reserved.",""
"Time series are found widely in engineering and science. We study forecasting of stochastic, dynamic systems based on observations from multivariate time series. We model the domain as a dynamic multiply sectioned Bayesian network (DMSBN) and populate the domain by a set of proprietary, cooperative agents. We propose an algorithm suite that allows the agents to perform one-step forecasts with distributed probabilistic inference. We show that as long as the DMSBN is structural time-invariant (possibly parametric time-variant), the forecast is exact and its time complexity is exponentially more efficient than using dynamic Bayesian networks (DBNs). in comparison with independent DBN-based agents, multiagent DMSBNs produce more accurate forecasts. The effectiveness of the framework is demonstrated through experiments on a supply chain testbed. (C) 2010 Elsevier Inc. All rights reserved.","We propose an algorithm suite that allows the agents to perform one-step forecasts with distributed probabilistic inference."
"Gamma-D-glutamyl-L-tryptophan (SCV-07) demonstrated an overall efficacy signal in ameliorating oral mucositis (OM) in a clinical trial of head and neck cancer patients. However, not all SCV-07-treated subjects responded positively. Here we determined if specific gene clusters could discriminate between subjects who responded to SCV-07 and those who did not. Microarrays were done using peripheral blood RNA obtained at screening and on the last day of radiation from 28 subjects enrolled in the SCV-07 trial. An analytical technique was applied that relied on learned Bayesian networks to identify gene clusters which discriminated between individuals who received SCV-07 and those who received placebo, and which differentiated subjects for whom SCV-07 was an effective OM intervention from those for whom it was not. We identified 107 genes that discriminated SCV-07 responders from non-responders using four models and applied Akaike Information Criteria (AIC) and Bayes Factor (BF) analysis to evaluate predictive accuracy. AIC were superior to BF: the accuracy of predicting placebo vs. treatment was 78% using BF, but 91% using the AIC score. Our ability to differentiate responders from non-responders using the AIC score was dramatic and ranged from 93% to 100% depending on the dataset that was evaluated. Predictive Bayesian networks were identified and functional cluster analyses were performed. A specific 10 gene cluster was a critical contributor to the predictability of the dataset. Our results demonstrate proof of concept in which the application of a genomics-based analytical paradigm was capable of discriminating responders and non-responders for an OM intervention. (C) 2011 Elsevier Ltd. All rights reserved.",""
"Structural learning of Bayesian networks (BNs) is an NP-hard problem which is generally addressed by means of heuristic search algorithms. Despite the fact that earlier proposals for dealing with this task were based on searching the space of Directed Acyclic Graphs (DAGs), there are some alternative approaches. One of these approaches for structural learning consists of searching the space of orderings, as given a certain topological order among the problem variables, it is relatively easy to build (and evaluate) a BN compatible with it. In practice, the latter methods make it possible to obtain good results, but they are still costly in terms of computation. In this article, we prove the correctness of the method used to evaluate each ordering, and we propose some efficient learning algorithms based on it. Our first proposal is based on the Hill-Climbing algorithm, and uses an improved neighbourhood definition. The second algorithm is an extension of the first one, and is based on the well-known Variable Neighbourhood Search metaheuristic. Finally, iterative versions of both algorithms are also proposed. The algorithms have been tested over a set of different domains, and have been compared with other methods such as Hill-Climbing in the space of DAGs or Greedy Equivalent Search, in order to study their behaviour in practice.",""
"We clarify the status of the so-called causal minimality condition in the theory of causal Bayesian networks, which has received much attention in the recent literature on the epistemology of causation. In doing so, we argue that the condition is well motivated in the interventionist (or manipulability) account of causation, assuming the causal Markov condition which is essential to the semantics of causal Bayesian networks. Our argument has two parts. First, we show that the causal minimality condition, rather than an add-on methodological assumption of simplicity, necessarily follows from the substantive interventionist theses, provided that the actual probability distribution is strictly positive. Second, we demonstrate that the causal minimality condition can fail when the actual probability distribution is not positive, as is the case in the presence of deterministic relationships. But we argue that the interventionist account still entails a pragmatic justification of the causal minimality condition. Our argument in the second part exemplifies a general perspective that we think commendable: when evaluating methods for inferring causal structures and their underlying assumptions, it is relevant to consider how the inferred causal structure will be subsequently used for counterfactual reasoning.",""
"A major problem for Canadian health organizations is finding best evidence for evidence-based best practice recommendations. Medications are not always effectively used and misuse may harm patients. Drugs are the fastest-growing element of Canadian health care spending, second only to hospital spending. Three hundred million prescriptions are filled annually. Prescription drugs accounted for 5.8% of total health care spending in 1980 and close to 18% today. A primary long-term goal of this research is to develop a decision support system for evidence-based management, quality control and best practice recommendations for medical prescriptions. Our results will improve accessibility and management of information by: (1) building an prototype for adaptive information extraction, text and data mining from (online) documents to find evidence on which to base best practices; and (2) employing multiply sectioned Bayesian networks (MSBNs) to infer a probabilistic interpretation to validate evidence for recommendations; MSBNs provide this structure. Best practices to improve drug-related health outcomes; patients' quality of life; and cost-effective use of medications by changing knowledge and behavior. This research will support next generation eHealth decision support systems, which routinely find and verify evidence from multiple sources, leading to cost-effective use of drugs, improve patients' quality of life and optimize drug-related health outcomes.",""
"The analysis of the relationships between people and nature is complex, because it involves bringing together insights from a range of disciplines, and, when stakeholders are involved, the perspectives and values of different interest groups. Although it has been suggested that analytical-deliberate approaches may be useful in dealing with some of this complexity, the development of methods is still at an early stage. This is particularly so in relation to debates around the concept of ecosystem services where biophysical, social and economic insights need to be integrated in ways that can be accessed by decision-makers. The paper draws on case studies to examine the use of Bayesian Belief Networks (BBNs) as a means of implementing analytical-deliberative approaches in relation to mapping ecosystem services and modelling scenario outcomes. It also explores their use as a tool for representing individual and group values. It is argued that when linked with GIS techniques BBNs allow mapping and modelling approaches rapidly to be developed and tested in an efficient and transparent way, and that they are a valuable scenario-building tool. The case-study materials also show that BBNs can support multicriteria forms of deliberative analysis that can be used to capture stakeholder opinions so that different perspectives can be compared and shared social values identified.",""
"Motivation: Dynamic Bayesian networks (DBN) are widely applied in modeling various biological networks including the gene regulatory network (GRN). Due to the NP-hard nature of learning static Bayesian network structure, most methods for learning DBN also employ either local search such as hill climbing, or a meta stochastic global optimization framework such as genetic algorithm or simulated annealing. Results: This article presents GlobalMIT, a toolbox for learning the globally optimal DBN structure from gene expression data. We propose using a recently introduced information theoretic-based scoring metric named mutual information test (MIT). With MIT, the task of learning the globally optimal DBN is efficiently achieved in polynomial time.",""
"Participatory methods provide an increasingly accepted path to integrated water assessment. This research describes an interdisciplinary exercise of scenario design and modelling, which provides a methodology to couple hard science numerical modelling approaches with the involvement of key water actors and socioeconomic issues. A decision support system based on probabilistic methods (Bayesian networks) is the tool chosen for dealing with the interdisciplinary issues involved in this aquifer. Given the long-standing conflicts in the area, modelling work largely focuses on carrying out an impact assessment produced by different scenarios established under the light of the mandatory objectives established by the European Union Water Framework Directive. This methodology is applied to a semi-arid aquifer located in SE Spain (Serral-Salinas) that represents an extreme case of intensive groundwater use. Irrigation has been a catalyst for welfare in the area for the past 40 years, despite the resulting large groundwater drawdown and continuous rise of groundwater pumping energy costs. Modelling results suggest that only a drastic change in the abstraction regime can produce a solution to long term sustainability of the aquifers. In addition, the impact assessment advises that such objectives are unlikely to be met due to the high economic costs of this action.",""
"Many statistical methods have been proposed to estimate causal models in classical situations with fewer variables than observations. However, modern datasets including gene expression data increase the needs of high-dimensional causal modeling in challenging situations with orders of magnitude more variables than observations. In this paper, we propose a method to find exogenous variables in a linear non-Gaussian causal model, which requires much smaller sample sizes than conventional methods and works even under orders of magnitude more variables than observations. Exogenous variables work as triggers that activate causal chains in the model, and their identification leads to more efficient experimental designs and better understanding of the causal mechanism. We present experiments with artificial data and real-world gene expression data to evaluate the method. (C) 2011 Elsevier Ltd. All rights reserved.",""
"Computer vision techniques have made considerable progress in recognizing object categories by learning models that normally rely on a set of discriminative features. However, in contrast to human perception that makes extensive use of logic-based rules, these models fail to benefit from knowledge that is explicitly provided. In this paper, we propose a framework that can perform knowledge-assisted analysis of visual content. We use ontologies to model the domain knowledge and a set of conditional probabilities to model the application context. Then, a Bayesian network is used for integrating statistical and explicit knowledge and performing hypothesis testing using evidence-driven probabilistic inference. In addition, we propose the use of a focus-of-attention (FoA) mechanism that is based on the mutual information between concepts. This mechanism selects the most prominent hypotheses to be verified/tested by the BN, hence removing the need to exhaustively test all possible combinations of the hypotheses set. We experimentally evaluate our framework using content from three domains and for the following three tasks: 1) image categorization; 2) localized region labeling; and 3) weak annotation of video shot keyframes. The results obtained demonstrate the improvement in performance compared to a set of baseline concept classifiers that are not aware of any context or domain knowledge. Finally, we also demonstrate the ability of the proposed FoA mechanism to significantly reduce the computational cost of visual inference while obtaining results comparable to the exhaustive case.","Then, a Bayesian network is used for integrating statistical and explicit knowledge and performing hypothesis testing using evidence-driven probabilistic inference."
"Automatic learning of Bayesian networks from data is a challenging task, particularly when the data are scarce and the problem domain contains a high number of random variables. The introduction of expert knowledge is recognized as an excellent solution for reducing the inherent uncertainty of the models retrieved by automatic learning methods. Previous approaches to this problem based on Bayesian statistics introduce the expert knowledge by the elicitation of informative prior probability distributions of the graph structures. In this paper, we present a new methodology for integrating expert knowledge, based on Monte Carlo simulations and which avoids the costly elicitation of these prior distributions and only requests from the expert information about those direct probabilistic relationships between variables which cannot be reliably discerned with the help of the data.",""
"Available water quality indices have some limitations such as incorporating a limited number of water quality variables and providing deterministic outputs. This paper presents a hybrid probabilistic water quality index by utilizing fuzzy inference systems (FIS), Bayesian networks (BNs), and probabilistic neural networks (PNNs). The outputs of two traditional water quality indices, namely the indices proposed by the National Sanitation Foundation and the Canadian Council of Ministers of the Environment, are selected as inputs of the FIS. The FIS is trained based on the opinions of several water quality experts. Then the trained FIS is used in a Monte Carlo analysis to provide the required input-output data for training both the BN and PNN. The trained BN and PNN can be used for probabilistic water quality assessment using water quality monitoring data. The efficiency and applicability of the proposed methodology is evaluated using water quality data obtained from water quality monitoring system of the Jajrood River in Iran.","This paper presents a hybrid probabilistic water quality index by utilizing fuzzy inference systems (FIS), Bayesian networks (BNs), and probabilistic neural networks (PNNs)."
"In the highly competitive marketplace, consumer acceptability has become an important factor in the product design process. However, manufacturers and designers often misunderstand what consumers really want. Thus, acceptability evaluation and prediction is important in product development. This study developed an intelligent model to solve consumer acceptability problem in an attempt to evaluate consumer acceptability with better performance. The model adopted three well-known feature ranking methods to rank features of importance. In addition, it employed the Bayesian Network (BN), Radial Basis Function (RBF) Networks, Support Vector Machine-Sequential Minimal Optimization (SVM-SMO) and their ensembles to build prediction models. In this study, we also focus on the use of non- parametric statistical test for the comparison algorithms performance in classification. To demonstrate applicability of the proposed model, we adopted a real case, car evaluation, to show that the consumer acceptability problem can be easily evaluated and predicted using the proposed model. The results show that the model can improve the performance of consumer acceptability problems and can be easily extended to other industries. (C) 2011 Elsevier Ltd. All rights reserved.","In this study, we also focus on the use of non- parametric statistical test for the comparison algorithms performance in classification."
"Bayesian Belief Networks (BBN) are conceptually sensible models for aviation risk assessment. The aim here is to examine the ability of BBN-based techniques to make accurate aviation risk predictions. BBNs consist of a framework of causal factors linked by conditional probabilities. BBN conditional probabilities are elicited from aviation experts. The issue is that experts are not being asked about their expertise but about others' failure rates. A simple model of expertise, which incorporates the main features proposed by researchers, implies that a best-expert's estimates of failure rates are based on accessible quantitative data on accidents, incidents, etc. Best-expert estimates will use the best available and accessible data. Depending on the frequency of occurrence, this will be data on similar events, on similar types of event, or general mental rules about event frequencies. These considerations, plus the need to be cautious about statistical fluctuations, limit the accuracy of conditional probability estimates. The BBN framework assumes what is known as the Causal Markov Condition. In the present context, this assumes that there are no hidden common causes for sequences of failure events. Examples are given from safety regulation comparisons and serious accident investigations to indicate that common causes may be frequent occurrences in aviation. This is because some States/airlines have safety cultures that do not meet 'best practice'. BBN accuracy might be improved by using data from controlled experiments. Aviation risk assessment is now very difficult, so further work on resilience engineering could be a better way of achieving safety improvements. (C) 2011 Elsevier Ltd. All rights reserved.",""
"Artificial neural networks (ANNs) and Bayesian belief networks (BBNs) utilizing select environmental variables were developed and evaluated, with the intent to model net ecosystem metabolism (a proxy for system trophic state) within a freshwater wetland. Network modeling was completed independently for distinct data subsets, representing periods of low' and 'high' water levels throughout in the wetland. ANNs and BBNs were 'benchmarked' against traditional parametric analyses, with network architectures outperforming regression models. ANNs delivered the greatest predictive accuracy for NEM and did not require expert knowledge about system variables for their development. BBNs provided users with an interactive diagram depicting predictor interaction and the qualitative/quantitative effects of variable dynamics upon NEM, thereby affording better information extraction. Importantly, BBNs accommodated the imbalanced nature of the dataset and appeared less affected (than ANNs) with variable auto-correlation traits that are typically observed within large and 'noisy' environmental datasets. (C) 2011 Elsevier Ltd. All rights reserved.","ANNs and BBNs were 'benchmarked' against traditional parametric analyses, with network architectures outperforming regression models."
"For pathogens that must be treated with combinations of antibiotics and acquire resistance through genetic mutation, knowledge of the order in which drug-resistance mutations occur may be important for determining treatment policies. Diagnostic specimens collected from patients are often available; this makes it possible to determine the presence of individual drug resistance-conferring mutations and combinations of these mutations. In most cases, these specimens are only available from a patient at a single point in time; it is very rare to have access to multiple specimens from a single patient collected over time as resistance accumulates to multiple drugs. Statistical methods that use branching trees have been successfully applied to such cross-sectional data to make inference on the ordering of events that occurred prior to sampling. Here, we propose a Bayesian approach to fitting branching tree models that has several advantages, including the ability to accommodate prior information regarding measurement error or cross resistance and the natural way it permits the characterization of uncertainty. Our methods are applied to a data set for drug-resistant TB in Peru; the goal of the analysis was to determine the order with which patients develop resistance to the drugs commonly used for treating TB in this setting. Copyright (C) 2011 John Wiley & Sons, Ltd.","Statistical methods that use branching trees have been successfully applied to such cross-sectional data to make inference on the ordering of events that occurred prior to sampling."
"Decision science tools can be used in evaluating response options and making inferences on risks to ecosystem services (ES) from ecological disasters. Influence diagrams (IDs) are probabilistic networks that explicitly represent the decisions related to a problem and their influence on desired or undesired outcomes. To examine how IDs might be useful in probabilistic risk management for spill response efforts, an ID was constructed to display the potential interactions between exposure events and the trade-offs between costs and ES impacts from spilled oil and response decisions in the DWH spill event. Quantitative knowledge was not formally incorporated but an ID platform for doing this was examined. Probabilities were assigned for conditional relationships in the ID and scenarios examining the impact of different response actions on components of spilled oil were investigated in hypothetical scenarios. Given the structure of the ID, potential knowledge gaps included understanding of the movement of oil, the ecological risk of different spill-related stressors to key receptors (e.g., endangered species, fisheries), and the need for stakeholder valuation of the ES benefits that could be impacted by a spill. Framing the Deepwater Horizon problem domain in an ID conceptualized important variables and relationships that could be optimally accounted for in preparing and managing responses in future spills. These features of the developed IDs may assist in better investigating the uncertainty, costs, and the tradeoffs if large-scale, deep ocean spills were to occur again.","Decision science tools can be used in evaluating response options and making inferences on risks to ecosystem services (ES) from ecological disasters."
"Expressing knowledge as expert experience and discovering knowledge implied in data are two important ways for knowledge acquisition. Consistent combination of these two kinds of knowledge has attracted much attention due to the potential applications to knowledge fusion and wide requirements of decision support. In this paper, we focus on the probabilistic modeling of expert experience represented as logical predicate formulas, aiming at the effective fusion of logical and probabilistic knowledge. Taking qualitative probabilistic network (QPN) as the underlying framework of probabilistic knowledge implied in data as well as the abstraction of general Bayesian networks (BNs), we are to construct the probabilistic graphical model for both the given predicate formulas and the ultimate result of knowledge fusion. We first propose the concept and the construction algorithm of predicate graph (PG) to describe the dependence relations among predicate formulas, and discuss PG's probabilistic semantics correspondingly. We then prove that PG is a probability dependency model and has the same semantics with a general probabilistic graphical model. Consequently, we give the method for fusing PG and QPN. Experimental results show the effectiveness of our methods. (C) 2011 Elsevier Inc. All rights reserved.",""
"In this work, we evaluate the sensitivity of Gaussian Bayesian networks to perturbations or uncertainties in the regression coefficients of the network arcs and the conditional distributions of the variables. The Kullback-Leibler divergence measure is used to compare the original network to its perturbation. By setting the regression coefficients to zero or non-zero values, the proposed method can remove or add arcs, making it possible to compare different network structures. The methodology is implemented with some case studies. (C) 2011 Elsevier Ltd. All rights reserved.","In this work, we evaluate the sensitivity of Gaussian Bayesian networks to perturbations or uncertainties in the regression coefficients of the network arcs and the conditional distributions of the variables."
"In this paper, a hybrid causal logic (HCL) model is improved by mapping a fuzzy fault tree (FFT) into a Bayesian network (BN). The first step is to substitute an FFT for the traditional FT. The FFT is based on the Takagi-Sugeno model and the translation rules needed to convert the FFT into a BN are derived. The proposed model is demonstrated in a study of a fire hazard on an offshore oil production facility. It is clearly shown that the FFT can be directly converted into a BN and that the parameters of the FFT can be estimated more accurately using the basic inference techniques of a BN. The improved HCL approach is able to both accurately determine how failures cause an undesired problem using FFT and also model non-deterministic cause-effect relationships among system elements using the BN.","It is clearly shown that the FFT can be directly converted into a BN and that the parameters of the FFT can be estimated more accurately using the basic inference techniques of a BN."
"The present paper aims to assess the usefulness of the integration of a qualitative risk-based inspection (RBI) procedure with the modelling of Bayesian belief networks (BBNs). In such a way, qualitative RBI, usually performed following standard procedures, might still be applied, even with the lack of some necessary data. Another benefit of the proposed method is the capability to include new factors in the evaluation, overtaking the rigid structure of the procedure. The qualitative RBI was chosen for its simple and clear structure, while the choice of the BBN formalism was made because of its flexibility and power to represent discontinuous variables. Bayesian formalism, moreover, allows a quick and easy extension of the model with additional variables. As a result, a BBN modelling a qualitative RBI was constructed. In the network, all of the RBI variables were included, as well as new external variables. An application for a chemical plant was tested in order to prove that BBNs can profitably model qualitative RBIs and can go beyond its rigid structure with the addition of new variables.",""
"This paper considers the issue of designing a framework to efficiently manage the risk due to some adverse events an organization or a system may face. Risk comes from human being's incapacity to predict the consequences or outcomes of some external events and/or their own actions, or to express precisely their knowledge about things. Thus, risk is linked to uncertainties that are inherent to almost all activities of human being. Designing an effective risk management decision making framework necessitate to correctly address these uncertainties in terms of appropriate mathematical tools along with procedures to identify variables (risk factors, state of the system, consequences, objectives or stakes, possible actions, etc.) impacting decision process and relationships linking them and finally aggregating approaches to present high level managers with concise information. In this paper we will use a meta-matrix analysis to identify relationships between previously determined variables, Bayesian networks and influence diagrams, graphical tools that permit easy representation of probabilistic relationships (independence, causality, correlation, etc.) between variables to quantify these relationships, and Choquet integral as an aggregation tool.",""
"Recent advances in experimental techniques have made it possible to generate an enormous amount of 'raw' biological data, with cancer biology being no exception. The main challenge faced by cancer biologists now is the generation of plausible hypotheses that can be evaluated against available data and/or validated through further experimentation. For persons trained in control theory, there is now a significant opportunity to work with biologists to create a virtuous cycle of hypothesis generation and experimental validation. Given the large number of uncertain factors in any biological experiment, probabilistic methods are natural in this setting. In this paper; we discuss four specific problems in cancer biology that are amenable to study using probabilistic methods, namely: reverse engineering gene regulatory networks, constructing context-specific gene regulatory networks, analyzing the significance of expression levels for collections of genes, and discriminating between drivers (mutations that cause cancer) and passengers (mutations that are caused by cancer or have no impact). Some research problems that merit the attention of the controls community are also suggested.",""
"Many organizations continue to emphasize the need for increased creativity and innovation among their employees and within markets. However, most firms try diverse strategies to enhance creativity, restructuring the workplace or selecting creativity-generating units. As such, the purpose of this paper is to differentiate team creativity from individual creativity. We investigate three important antecedents of creativity - shared leadership, individual knowledge, and knowledge sharing - and then examine the relationship and differences between individual creativity and team creativity. We found that individual knowledge does not influence knowledge sharing or team creativity, but has a significant impact on individual creativity. Individual knowledge indirectly, rather than directly, influences team creativity via individuals. Shared leadership and knowledge sharing have a more significant influence on team creativity than individual creativity.",""
"As exhibition becomes large-scale, it becomes important to recommend booths to each customer according to his/her preference. Existing recommender systems in an exhibition are usually based on the pre-recorded preference information on exhibition and are voiced about invasion of privacy. In this paper, we propose a booth recommender system based on Bayesian network reflecting customer's dynamic preference without invasion of privacy in offline exhibition environment. The proposed methodology is evaluated with real transaction data of a Korean exhibition and compared with collaborative filtering methodology. From the experimental results, we can see that considering the visiting sequence of booths and customer's dynamic preference result a better performance. We expect that the proposed methodology can be extended into other areas such as department stores, outlets, museums, and other large-scale exhibitions.",""
"In recent years, many companies have adopted outsourcing for their IS system maintenance. Outsourcing can be expensive, so many companies need to know the optimal effort of IT maintenance. Also, IT maintenance service providers try to come up with optimal man-months prediction and provide reliable service to clients, but still remain profitable. Problems with the recent trend of IT outsourcing are that clients become sensitive about the quality of IT outsourcing services and worry about the cost. Therefore, outsourcing service providers cannot be successful as service providers in the IS outsourcing market without convincing the client of the objectivity and transparency of IT outsourcing-related costs. This study investigates how to effectively determine proper man-months for the IT maintenance service to clients. To that end, this study derived various types of variables from one of the companies that outsources its IT maintenance service. To predict and verify the proper size of man-month, we employed Bayesian network (BN) classifier as a way to analyze methods. A selected BN structure-leaning algorithm and Markov blanket in this empirical experiment show that our method adapted well to man-month prediction study in the IS maintenance outsourcing domain. This paper also discusses some of limitations and future research in this field.","To predict and verify the proper size of man-month, we employed Bayesian network (BN) classifier as a way to analyze methods."
"In the field of corporate credit grading, researchers have recently been interested in machine learning techniques rather than typical statistical ones in order to find better prediction performance. However, little research has been done to find a model for both describing causal relations and better prediction performance at the same time. This research intends to propose an enhanced prediction model for corporate credit grading to find the relationship among the influencing factors. For the sake of this purpose, we applied the Bayesian Network which is a brand new approach in this area. To validate our research, we analyzed 1,019 records collected by a Korean corporate credit rating agency based on the Bayesian Network, and other famous machine learning techniques such as the Decision Tree and Neural Network are also applied for benchmarking tests. In addition, we applied several algorithms of Bayesian Networks, which have been frequently adapted in prior research. The results show that the TAN algorithm of the Bayesian network and the J48 algorithm in the Decision Tree are the best prediction models, performing slightly better than other benchmarking techniques. However, the K2 algorithm could be a recommended model for its richness of interpretation capability and practical application: Firstly, it can explain relationships among factors clearly, which is critical to the stakeholders of a corporation. Secondly the performance is not much behind that of the best prediction model. Lastly, its simulation capability, such as in the what-if analysis, provides rich information for the researcher as well as industry experts.",""
"Pharmacogenetics aims to elucidate the genetic factors underlying the individual's response to pharmacotherapy. Coupled with the recent (and ongoing) progress in high-throughput genotyping, sequencing and other genomic technologies, pharmacogenetics is rapidly transforming into pharmacogenomics, while pursuing the primary goals of identifying and studying the genetic contribution to drug therapy response and adverse effects, and existing drug characterization and new drug discovery. Accomplishment of both of these goals hinges on gaining a better understanding of the underlying biological systems; however, reverse-engineering biological system models from the massive datasets generated by the large-scale genetic epidemiology studies presents a formidable data analysis challenge. In this article, we review the recent progress made in developing such data analysis methodology within the paradigm of systems biology research that broadly aims to gain a 'holistic', or 'mechanistic' understanding of biological systems by attempting to capture the entirety of interactions between the components (genetic and otherwise) of the system.",""
"Characterizing associations among multiple single-nucleotide polymorphisms (SNPs) within and across genes, and measures of disease progression or disease status will potentially offer new insight into disease etiology and disease progression. However, this presents a significant analytic challenge due to the existence of multiple potentially informative genetic loci, as well as environmental and demographic factors, and the generally uncharacterized and complex relationships among them. Latent variable modeling approaches offer a natural framework for analysis of data arising from these population-based genetic association investigations of complex diseases as they are well-suited to uncover simultaneous effects of multiple markers. In this manuscript we describe application and performance of two such latent variable methods, namely structural equation models (SEMs) and mixed effects models (MEMs), and highlight their theoretical overlap. The relative advantages of each paradigm are investigated through simulation studies and, finally, an application to data arising from a study of antiretroviral-associated dyslipidemia in HIV-infected individuals is provided for illustration.",""
NA,""
"In counterterrorism risk management decisions, the analyst can choose to represent terrorist decisions as defender uncertainties or as attacker decisions. We perform a comparative analysis of probabilistic risk analysis (PRA) methods including event trees, influence diagrams, Bayesian networks, decision trees, game theory, and combined methods on the same illustrative examples (container screening for radiological materials) to get insights into the significant differences in assumptions and results. A key tenent of PRA and decision analysis is the use of subjective probability to assess the likelihood of possible outcomes. For each technique, we compare the assumptions, probability assessment requirements, risk levels, and potential insights for risk managers. We find that assessing the distribution of potential attacker decisions is a complex judgment task, particularly considering the adaptation of the attacker to defender decisions. Intelligent adversary risk analysis and adversarial risk analysis are extensions of decision analysis and sequential game theory that help to decompose such judgments. These techniques explicitly show the adaptation of the attacker and the resulting shift in risk based on defender decisions.",""
"In the last three decades, predictive models have been developed and applied worldwide for freshwater bioassessment. They consist of statistical tools that follow the concept of the Reference Condition Approach. Composed of several sequential steps, these assessment tools assess the deviation of given site assemblages from the expected biological condition in the absence of human disturbance. The most common approaches (RIVPACS/AUSRIVAS and BEAST) are based on a posteriori classifications that use the biological composition of a community to classify reference sites in groups, and afterwards to establish which environmental features best discriminate the biological groups obtained. Here, we review the predictive modeling procedures used in freshwaters bioassessment (RIVPACS/AUSRIVAS, BEAST, ANNA, Artificial Neural Networks, Bayesian Belief Networks and others) as well as the biological elements to which they have been applied. We also review the Spanish and Portuguese experiences in the development and application of predictive models, with particular attention to regional environmental conditions, the different modeling approaches, and the available implementation tools. Moreover, and considering the natural continuity within the Iberian Peninsula (which include several transnational rivers), we discuss the possibilities of the development of common predictive models across the region, considering all factors that may influence their performance, such as the target scale used to develop the models (regional or peninsular); common reference criteria; sampling and sorting procedures; the taxonomic resolution used in the models; the temporal variability (mainly in the Iberian Mediterranean region); and the biological elements to consider. We concluded that there are good technical conditions for the implementations of a common predictive approach throughout the Iberian Peninsula, which should allow a global biological assessment of streams with different biological elements and seasons that could be used by water managers in the context of the Water Framework Directive.","The most common approaches (RIVPACS/AUSRIVAS and BEAST) are based on a posteriori classifications that use the biological composition of a community to classify reference sites in groups, and afterwards to establish which environmental features best discriminate the biological groups obtained."
"Recent advances in integrated land use and transport modeling have included a shift from aggregate-level modeling to disaggregate, household-level modeling. One potential advantage of this shift is that interdependencies of changes that influence household decisions can be more systematically modeled. However, existing models have not embraced this opportunity fully. Especially in the context of long-term mobility decisions (relocation/car ownership), decisions made on the basis of various dimensions are modeled as independent and cross sectional, whereas in reality they are strongly interlinked. To address these shortcomings, this paper proposes a conceptual framework that offers a more general approach to modeling the dynamics and interdependences across different time horizons of a household's lifecycle and mobility decisions. The framework incorporates the concept of stress, defined as a discrepancy between a household's present situation and its aspiration level, which in turn depends, among other things, on the household's social network. Bayesian belief networks are used to represent the complex direct and indirect dependencies between life-cycle events and long- and short-term mobility decisions. DOI: 10.1061/(ASCE)UP.1943-5444.0000066. (C) 2011 American Society of Civil Engineers.",""
"Histone modification is an important subject of epigenetics that plays an intrinsic role in transcriptional regulation. It has been suggested that multiple histone modifications act in a combinatorial fashion to form a 'histone code'. In this study, the combinatorial patterns of histone modifications were studied by using a Bayesian network at the level of individual nucleosomes in S. cerevisiae. Our results indicated that there were 23 combinatorial patterns for 12 histone modifications investigated when a general Bayesian network was constructed. Meanwhile, different networks were also constructed for the genes with high transcript levels (H-network) and low transcript levels (L-network), respectively. Comparison among the general network, H-network and L-network illustrated four conserved combinations: H2BK16Ac -> H3K4me3; H3K14Ac -> H3K4me3; H2AK7Ac -> H3K14Ac; and H4K12Ac -> H3K18Ac. The detailed analysis for some combinations demonstrated that the combinations were ascribed to some histone-modifying enzymes. Copyright (C) 2011 John Wiley & Sons, Ltd.",""
"Scaling complementary metal oxide semiconductor (CMOS) devices has been a method used very successfully over the last four decades to improve the performance and the functionality of very large scale integrated (VLSI) designs. Still, scaling is heading towards several fundamental limits as the feature size is being decreased towards 10 nm and less. One of the challenges associated with scaling is the expected increase of static and dynamic parameter fluctuations and variations, as well as intrinsic and extrinsic noises, with significant effects on reliability. Therefore, there is a clear, growing need for electronic design automation (EDA) tools that can predict the reliability of future massive nano-scaled designs with very high accuracy. Such tools are essential to help VLSI designers optimize the conflicting trade-offs between area-power-delay and reliability requirements. In this paper, we introduce an EDA tool that quickly and accurately estimates the reliability of any CMOS gate. The tool improves the accuracy of the reliability calculation at the gate level by taking into consideration the gate's topology, the reliability of the individual devices, the applied input vector, as well as the noise margins. It can also be used to estimate the effect on different types of faults and defects, and to estimate the effects of enhancing the reliability of individual devices on the gate's overall reliability.",""
"This paper deals with the error recovery problem when scanned license plate data are used to predict traffic flows. The aim is to reduce the effects of errors owing to lost plates or mistaken transcription, to improve estimation results. To this end, a method is given and discussed for traffic flow prediction using plate scanning data and taking into account possible errors in plate number recognition. The proposed method uses Bayesian networks because this is an efficient tool for introducing the plate scan error flow as a variable in the model and mending the mistakes in the scan pattern. Several examples are used to illustrate the proposed model. Finally, some conclusions are included. DOI: 10.1061/(ASCE)TE.1943-5436.0000249. (C) 2011 American Society of Civil Engineers.",""
"Modern mobile devices are increasingly capable of simultaneously connecting to multiple access networks with different characteristics. Restricted coverage combined with user mobility will vary the availability of networks for a mobile device. Most proposed solutions for such an environment are reactive in nature, such as performing a vertical handover to the network that offers the highest bandwidth. But the cost of the handover may not be justified if that network is only available for a short time. Knowledge of future network availability and their capabilities are the basis for proactive schemes which will improve network selection and utilization. We have previously proposed a prediction model that can use any available context such as GSM Location Area, WLAN presence or even whether the power cable is plugged in, to predict network availability. As it may not be possible to sense all of the context variables that influence future network availability, in this paper we introduce a generic, new model incorporating a hidden variable to account for this. Specifically, we propose a Dynamic Bayesian Network based context prediction model to predict network availability. Predictions performed for WLAN availability with the real user data collected in our experiments show 20% or more improvement compared to both of our earlier proposals of order 1 and 2 semi-Markov models. (C) 2011 Elsevier B.V. All rights reserved.",""
"Chain graph (CG) is a hybrid probabilistic graphical model (PGM) capable of modeling heterogeneous relationships among random variables. So far, however, its application in image and video analysis is very limited due to lack of principled learning and inference methods for a CG of general topology. To overcome this limitation, we introduce methods to extend the conventional chain-like CG model to CG model with more general topology and the associated methods for learning and inference in such a general CG model. Specifically, we propose techniques to systematically construct a generally structured CG, to parameterize this model, to derive its joint probability distribution, to perform joint parameter learning, and to perform probabilistic inference in this model. To demonstrate the utility of such an extended CG, we apply it to two challenging image and video analysis problems: human activity recognition and image segmentation. The experimental results show improved performance of the extended CG model over the conventional directed or undirected PGMs. This study demonstrates the promise of the extended CG for effective modeling and inference of complex real-world problems.","So far, however, its application in image and video analysis is very limited due to lack of principled learning and inference methods for a CG of general topology."
"The patients' clinical and healthcare data should virtually be available everywhere, both to provide a more efficient and effective medical approach to their pathologies, as well as to make public healthcare decision makers able to verify the efficacy and efficiency of the adopted healthcare processes. Unfortunately, customised solutions adopted by many local Health Information Systems in Italy make it difficult to share the stored data outside their own environment. In the last years, worldwide initiatives have aimed to overcome such sharing limitation. An important issue during the passage towards standardised, integrated information systems is the possible loss of previously collected data. The herein presented project realises a suitable architecture able to guarantee reliable, automatic, user-transparent storing and retrieval of information from both modern and legacy systems. The technical and management solutions provided by the project avoid data loss and overlapping, and allow data integration and organisation suitable for data-mining and data-warehousing analysis.",""
"Quantification of margins and uncertainties (QMU) was originally introduced as a framework for assessing confidence in nuclear weapons, and has since been extended to more general complex systems. We show that when uncertainties are strictly bounded, QMU is equivalent to a graphical model, provided confidence is identified with reliability one. In the more realistic case that uncertainties have long tails, we find that QMU confidence is not always a good proxy for reliability, as computed from the graphical model. We explore the possibility of defining QMU in terms of the graphical model, rather than through the original procedures. The new formalism, which we call probabilistic QMU, or pQMU, is fully probabilistic and mathematically consistent, and shows how QMU may be interpreted within the framework of system reliability theory. (C) 2011 Elsevier Ltd. All rights reserved.",""
"Performance assessment of complex systems is ideally done through full system-level testing which is seldom available for high consequence systems. Further, a reality of engineering practice is that some features of system behavior are not known from experimental data, but from expert assessment, only. On the other hand, individual component data, which are part of the full system are more readily available. The lack of system level data and the complexity of the system lead to a need to build computational models of a system in a hierarchical or building block approach (from simple components to the full system). The models are then used for performance prediction in lieu of experiments, to estimate the confidence in the performance of these systems. Central to this are the need to quantify the uncertainties present in the system and to compare the system response to an expected performance measure. This is the basic idea behind Quantification of Margins and Uncertainties (QMU). QMU is applied in decision making-there are many uncertainties caused by inherent variability (aleatoric) in materials, configurations, environments, etc., and lack of information (epistemic) in models for deterministic and random variables that influence system behavior and performance. This paper proposes a methodology to quantify margins and uncertainty in the presence of both aleatoric and epistemic uncertainty. It presents a framework based on Bayes networks to use available data at multiple levels of complexity (i.e. components, subsystem, etc.) and demonstrates a method to incorporate epistemic uncertainty given in terms of intervals on a model parameter. Published by Elsevier Ltd.",""
"Modeling of biological networks is a difficult endeavor, but exploration of this problem is essential for understanding the systems behavior of biological processes. In this contribution, developed for sparse data, we present a new continuous Bayesian graphical learning algorithm to cotemporally model proteins in signaling networks and genes in transcriptional regulatory networks. In this continuous Bayesian algorithm, the correlation matrix is singular because the number of time points is less than the number of biological entities (genes or proteins). A suitable restriction on the degree of the graph's vertices is applied and a Metropolis-Hastings algorithm is guided by a BIC-based posterior probability score. Ten independent and diverse runs of the algorithm are conducted, so that the probability space is properly well-explored. Diagnostics to test the applicability of the algorithm to the specific data sets are developed; this is a major benefit of the methodology. This novel algorithm is applied to two time course experimental data sets: 1) protein modification data identifying a potential signaling network in chondrocytes, and 2) gene expression data identifying the transcriptional regulatory network underlying dendritic cell maturation. This method gives high estimated posterior probabilities to many of the proteins' directed edges that are predicted by the literature; for the gene study, the method gives high posterior probabilities to many of the literature-predicted sibling edges. In simulations, the method gives substantially higher estimated posterior probabilities for true edges and true subnetworks than for their false counterparts.",""
"Nutrient exports from agriculture contribute to eutrophication of rivers and lakes. In many jurisdictions \"Best Management Practices\" (BMP's) are the cornerstone of mitigation efforts. In this paper we examine the use of Monte-Carlo simulations to combine fertiliser distribution, grazing and runoff data, and regression equations developed from field-scale monitoring, to estimate the maximal effect of fertiliser BMP's on phosphorus (P) exports. The simulation data are then compared with a Bayesian Network that can be used to quickly evaluate the effects of different management scenarios on P exports and communicate those results to landholders. Both techniques demonstrate that for systems similar to those for which the regression equations were derived, improved fertiliser management is unlikely to have a major impact on Total P (TP) exports (i.e. <10%). While the contribution of fertiliser to TP exports in a general sense is relatively small this study suggests that aberrant behaviour (i.e. fertiliser application immediately preceding rainfall runoff) can dramatically increase P exports. The major factor affecting TP exports appears to be the systematic or background P which includes native P and P from previously applied amendments. For communicating the effects of different management scenarios to landholders, Bayesian Networks are shown to be generally superior to Monte-Carlo techniques. However, the study suggests care is needed in selecting the states for the Bayesian Networks and demonstrates that at the extremes, the discretisation required by Bayesian Network software can produce misleading results. (C) 2011 Elsevier Ltd. All rights reserved.","In this paper we examine the use of Monte-Carlo simulations to combine fertiliser distribution, grazing and runoff data, and regression equations developed from field-scale monitoring, to estimate the maximal effect of fertiliser BMP's on phosphorus (P) exports."
"Despite their fame and capability in detecting out-of-control conditions, control charts are not effective tools for fault diagnosis. There are other techniques in the literature mainly based on process information and control charts patterns to help control charts for root cause analysis. However these methods are limited in practice due to their dependency on the expertise of practitioners. In this study, we develop a network for capturing the cause and effect relationship among chart patterns, process information and possible root causes/assignable causes. This network is then trained under the framework of Bayesian networks and a suggested data structure using process information and chart patterns. The proposed method provides a real time identification of single and multiple assignable causes of failures as well as false alarms while improving itself performance by learning from mistakes. It also has an acceptable performance on missing data. This is demonstrated by comparing the performance of the proposed method with methods like neural nets and K-Nearest Neighbor under extensive simulation studies. (C) 2011 Elsevier Ltd. All rights reserved.",""
"Multi-dimensional classification aims at finding a function that assigns a vector of class values to a given vector of features. In this paper, this problem is tackled by a general family of models, called multi-dimensional Bayesian network classifiers (MBCs). This probabilistic graphical model organizes class and feature variables as three different subgraphs: class subgraph, feature subgraph, and bridge (from class to features) subgraph. Under the standard 0-1 loss function, the most probable explanation (MPE) must be computed, for which we provide theoretical results in both general MBCs and in MBCs decomposable into maximal connected components. Moreover, when computing the MPE, the vector of class values is covered by following a special ordering (gray code). Under other loss functions defined in accordance with a decomposable structure, we derive theoretical results on how to minimize the expected loss. Besides these inference issues, the paper presents flexible algorithms for learning MBC structures from data based on filter, wrapper and hybrid approaches. The cardinality of the search space is also given. New performance evaluation metrics adapted from the single-class setting are introduced. Experimental results with three benchmark data sets are encouraging, and they outperform state-of-the-art algorithms for multi-label classification. (C) 2011 Elsevier Inc. All rights reserved.","Multi-dimensional classification aims at finding a function that assigns a vector of class values to a given vector of features."
"The main goal of this paper is to describe an architecture for solving large general hybrid Bayesian networks (BNs) with deterministic conditionals for continuous variables using local computation. In the presence of deterministic conditionals for continuous variables, we have to deal with the non-existence of the joint density function for the continuous variables. We represent deterministic conditional distributions for continuous variables using Dirac delta functions. Using the properties of Dirac delta functions, we can deal with a large class of deterministic functions. The architecture we develop is an extension of the Shenoy-Shafer architecture for discrete BNs. We extend the definitions of potentials to include conditional probability density functions and deterministic conditionals for continuous variables. We keep track of the units of continuous potentials. Inference in hybrid BNs is then done in the same way as in discrete BNs but by using discrete and continuous potentials and the extended definitions of combination and marginalization. We describe several small examples to illustrate our architecture. In addition, we solve exactly an extended version of the crop problem that includes non-conditional linear Gaussian distributions and non-linear deterministic functions. Published by Elsevier Inc.","Inference in hybrid BNs is then done in the same way as in discrete BNs but by using discrete and continuous potentials and the extended definitions of combination and marginalization."
"In this work, we focused on understanding the required performance levels throughout each section of rowing races to finish in certain positions. We conducted our analysis in terms of each 500-m sector of those races for which historical performance data existed. We considered the ranking and time taken in each sector by a given boat as two important predictor factors/attributes that can be taken into account for strategic pacing planning in standard rowing races. We developed a novel hybrid data mining approach based on probabilistic modelling and combinatorial optimization to find the optimal permutation of split measures (times and rankings) that can maximize the chance of a boat winning certain medals in a standard 2000-m rowing race. We further extended our probabilistic model to analyse rowing data from other perspectives. In this research, we considered race type (fast, medium, slow) as well as country profiles. The latter analysis could be used for strategic planning in terms of combating opposing countries' strategies by understanding their racing patterns.",""
"Waterflooding is among the oldest and perhaps most economical of oil-recovery processes to extend field life and increase ultimate oil recovery from naturally depleting reservoirs. Today, organizations have to strive for lean and efficient technologies and processes to maximize profits when looking deeper into their reservoir portfolios in order to identify additional waterflooding opportunities. Time and information constraints can limit the depth and rigor of such a screening evaluation. Time is reflected by the effort of screening a vast number of reservoirs for the applicability of implementing a waterflood, whereas information is reflected by the availability and quality of data (consistency of measured and modeled data with the inherent rules of a petroleum system) from which to extract significant knowledge necessary to make good development decisions. A new approach to screening a large number of reservoirs uses a wide variety of input information and satisfies a number of constraints such as physical, financial, geopolitical, and human constraints. In a fully stochastic workflow that includes stochastic back population of incomplete data sets, stochastic proxy models over time series, and stochastic ranking methods using Bayesian belief networks (BBNs), more than 1,500 reservoirs were screened for additional recovery potential with waterflooding operations. The objective of the screening process was to reduce the number of reservoirs by one order of magnitude to approximately 100 potential candidates that are suitable for a more detailed evaluation. Numerical models were used to create response surfaces as surrogate reservoir models that capture the sensitivity and uncertainty of the influencing input parameters on the output. Reservoir uncertainties were combined with expert knowledge and environmental variables and were used as proxy model states in the formulation of objective functions. The input parameters were initiated and processed in a stochastic manner throughout the presented work. The output is represented by a ranking of potential waterflood candidates. The benefit of this approach is in the inclusion of a wide range of influencing parameters while at the same time speeding up the screening process without jeopardizing the quality of the results.",""
"Few problems have created the combined interest of so many unrelated areas as the evolution of cooperation. As a result, several mechanisms have been identified to work as catalyzers of cooperative behavior. Yet, these studies, mostly grounded on evolutionary dynamics and game theory, have neglected the important role played by intention recognition in behavioral evolution. Here we address explicitly this issue, characterizing the dynamics emerging from a population of intention recognizers. We derive a Bayesian network model for intention recognition in the context of repeated social dilemmas and evolutionary game theory, by assessing the internal dynamics of trust between intention recognizers and their opponents. Intention recognizers are then able to predict the next move of their opponents based on past direct interactions, which, in turn, enables them to prevail over the most famous strategies of repeated dilemmas of cooperation, even in presence of noise. Overall, our framework offers new insights on the complexity and beauty of behavioral evolution driven by elementary forms of cognition.",""
"This paper presents a systems modelling approach to evaluating the success of an agroforestry extension program in Leyte, the Philippines. During the program, variables which are intrinsic to farmers' socio-economic and farming systems were found to have influenced the uptake and acceptance of extension advice. Evaluation of the program therefore depended on identifying the variables and their interdependencies and assessing their relative influence on program outputs. For this purpose, a systems approach which encourages breaking systems into component variables, but also acknowledges the context of problems, assisted construction of models. Using both empirical data collected during program activities and input from stakeholders, Bayesian Belief Network software was used to predict critical success factors for four aspects of the overall extension system, namely recruitment, use of written extension materials, farmers' self-efficacy and retention of participating farmers throughout the program. A key predicted constraint to program recruitment is farmers' perception of harvest security and while this variable can be partly addressed through dissemination of information on harvesting legislation, title security cannot. Differing levels of farmers' education result in differences in predicted reading ability, comprehension of extension literature and possible misconstrual of information. The variable most critical to the development of farmers' self-efficacy is extended on-farm technical assistance and support.",""
"We propose a flexible framework for evaluating prospect dependencies in oil and gas exploration and for solving decision-making problems in this context. The model uses a Bayesian network (BN) for encoding the dependencies in a geologic system at source, reservoir, and trap levels. We discuss different evaluation criteria that allow us to formulate specific decision problems and solve these within the BN framework. The BN model offers a realistic graphic model for capturing the underlying causal geologic process and allows fast statistical computations of marginal and conditional probabilities. We illustrate the use of our BN model by considering two situations. In the first situation, we wish to gain information about an area where hydrocarbons have been discovered, and use the value of perfect information to determine which locations are the best to drill. In the second situation, we consider the problem of abandoning an area when only dry wells are drilled. For this latter, we use an abandoned revenue criterion to determine the drilling locations. The application is from the North Sea. Our main focus is the description, visualization, and interpretation of the results for relating the statistical modeling to the local understanding of the geology.",""
"Determination of the effective distance of emergency evacuation signs (EESs) is a basic requirement for optimising and improving emergency sign systems. This paper proposes a computational model for calculating the effective distance of EESs, taking into consideration an inertia effect as well as other factors that influence evacuation processes. The model is then incorporated into an evacuation model by extending the movement rules of cellular automata (CA). Finally, evacuation simulation experiments are conducted to confirm the necessity and rationality of computing the effective distance of EESs. The effective distance computational model can mirror the interaction between the evacuees and EESs, making the evacuation model even closer to the complexity and heterogeneity of a real evacuation. (C) 2011 Elsevier Ltd. All rights reserved.",""
"Traditionally, in speech recognition, the hidden Markov model state emission probability distributions are usually associated to continuous random variables, by using Gaussian mixtures. Thus, complex multimodal inter-feature dependencies are not accurately modeled by Gaussian models, since they are unimodal distributions and mixtures of Gaussians are needed in these complex cases, but this is done in a loose and inefficient way. Graphical models provide a precise and simple mechanism to model the dependencies among two or more variables. This paper proposes the use of discrete random variables as observations and graphical models to extract the internal dependence structure in the feature vectors. Therefore, speech features are quantized to a small number of levels, in order to obtain a tractable model. These quantized speech features provide a mechanism to increase the robustness against noise uncertainty. In addition, discrete random variables allow the learning of joint statistics of the observation densities. A method to estimate a graphical model with a constrained number of dependencies is shown in this paper, being a special kind of Bayesian network. Experimental results show that by using this modeling, better performance can be obtained compared to standard baseline systems.",""
"One of the first steps in building a spoken language understanding (SLU) module for dialogue systems is the extraction of flat concepts out of a given word sequence, usually provided by an automatic speech recognition (ASR) system. In this paper, six different modeling approaches are investigated to tackle the task of concept tagging. These methods include classical, well-known generative and discriminative methods like Finite State Transducers (FSTs), Statistical Machine Translation (SMT), Maximum Entropy Markov Models (MEMMs), or Support Vector Machines (SVMs) as well as techniques recently applied to natural language processing such as Conditional Random Fields (CRFs) or Dynamic Bayesian Networks (DBNs). Following a detailed description of the models, experimental and comparative results are presented on three corpora in different languages and with different complexity. The French MEDIA corpus has already been exploited during an evaluation campaign and so a direct comparison with existing benchmarks is possible. Recently collected Italian and Polish corpora are used to test the robustness and portability of the modeling approaches. For all tasks, manual transcriptions as well as ASR inputs are considered. Additionally to single systems, methods for system combination are investigated. The best performing model on all tasks is based on conditional random fields. On the MEDIA evaluation corpus, a concept error rate of 12.6% could be achieved. Here, additionally to attribute names, attribute values have been extracted using a combination of a rule-based and a statistical approach. Applying system combination using weighted ROVER with all six systems, the concept error rate (CER) drops to 12.0%.",""
"Feedback control is an important regulatory process in biological systems, which confers robustness against external and internal disturbances. Genes involved in feedback structures are therefore likely to have a major role in regulating cellular processes. Here we rely on a dynamic Bayesian network approach to identify feedback loops in cell cycle regulation. We analyzed the transcriptional profile of the cell cycle in HeLa cancer cells and identified a feedback loop structure composed of 10 genes. In silico analyses showed that these genes hold important roles in system's dynamics. The results of published experimental assays confirmed the central role of 8 of the identified feedback loop genes in cell cycle regulation. In conclusion, we provide a novel approach to identify critical genes for the dynamics of biological processes. This may lead to the identification of therapeutic targets in diseases that involve perturbations of these dynamics. (C) 2011 Elsevier Inc. All rights reserved.",""
"Operationalising the holistic approach implicit in an ecosystem services assessment is a challenge, incorporating social and economic considerations alongside the physical, chemical and biological function of ecosystems. The paper considers the role of statistics within a range of frameworks proposed for the analysis of ecosystem services. The use of different statistical techniques within the component parts of an ecosystem services assessment framework are discussed, including (1) data availability and sampling strategies, (2) statistical data analysis, (3) geography and spatial models, (4) meta-analysis, (5) environmental models, (6) societal models, (7) feedbacks and loop analysis, and (8) graphical models including Bayesian belief networks. Issues of value and the potential for a statistical contribution to multivariate non-monetary valuation are considered. We argue that statistics has an underpinning role by providing tools to link together the component elements along with their uncertainties for a thorough ecosystem services assessment, and should be an integral part of this developing inter-disciplinary research area. Copyright (C) 2011 John Wiley & Sons, Ltd.",""
"Priors gained from expert opinion can be the key to effective decision-making. Yet, there is continuing controversy with its use because of its subjective and potentially biased nature. I examine the use of expert opinion through four environmental case studies in which one or more experts participated in an elicitation exercise to provide inference on ecological problems. In each case study, I examine how expert opinion informed the model and the potential pitfalls that could result, especially in data-limited situations. I discuss two opposing schools of thought: (1) experts provide a valuable source of information that can offer useful insights into a model and (2) expert priors are most times biased, leading to incorrect results and bad decisions. I show that expert opinion has a place in ecological analyses if carefully structured in a model. In situations in which data are limited or simply not available, steps can be taken to ensure its proper use and interpretation from models so decisions can be made urgently and updated when new data becomes available. Copyright (C) 2011 John Wiley & Sons, Ltd.","I examine the use of expert opinion through four environmental case studies in which one or more experts participated in an elicitation exercise to provide inference on ecological problems."
"Prediction of overall visual quality based on instrumental measurements is a challenging task. Despite the several proposed models and methods, there exists a gap between the instrumental measurements of print and human visual assessment of natural images. In this work, a computational model for representing and quantifying the overall visual quality of prints is proposed. The computed overall quality should correspond to the human visual quality perception when viewing the printed images. The proposed model is a Bayesian network which connects the objective instrumental measurements to the subjective opinion distribution of human observers. This relationship can be used to score printed images, and additionally, to computationally study the connections of the attributes. A novel graphical learning approach using an iterative evolve-estimate-simulate loop learning the quality model based on psychometric data and instrumental measurements is suggested. The network structure is optimised by applying evolutionary computation (evolve). The estimation of the Bayesian network parameters is within the evolutionary loop. In this loop, the maximum likelihood approach is used (estimate). The stochastic learning process is guided by priors devised from the psychometric subjective experiments (performance through simulation). The model reveals and represents the explanatory factors between its elements providing insight to the psychophysical phenomenon of how observers perceive visual quality and which measurable entities affect the quality perception. By using true data, the design choices are demonstrated. It is also shown that the best-performing network establishes a clear and intuitively correct structure between the objective measurements and psychometric data. (C) 2011 Elsevier B.V. All rights reserved.",""
"To analyze the key path of Bayesian network in complex systems, this study proposes to analyze the sensitivity of causal chains of Bayesian networks using the Petri net structural analysis approach to obtain the key chain through which the cause influences the consequence. First, the Bayesian network is transformed into Petri net, the structural analysis approach of which is employed to analyze structural nature of the Bayesian network, ensuring correctness of the constructed Bayesian network structure. Then based on the above fact that the structure is correct, S-invariants of a Petri net is used to search for simple causal chains of the Bayesian network. Finally, the causal effect is defined and sensitivity analysis is made on the causal chains. The said method is applied to MDS causal chain analysis. Results show that the proposed method is direct viewing and practical. This method has some reference value for decision making in complex systems. (C) 2011 Wiley Periodicals, Inc.",""
"Chain event graphs are graphical models that while retaining most of the structural advantages of Bayesian networks for model interrogation, propagation and learning, more naturally encode asymmetric state spaces and the order in which events happen than Bayesian networks do. In addition, the class of models that can be represented by chain event graphs for a finite set of discrete variables is a strict superset of the class that can be described by Bayesian networks. In this paper we demonstrate how with complete sampling, conjugate closed form model selection based on product Dirichlet priors is possible, and prove that suitable homogeneity assumptions characterise the product Dirichlet prior on this class of models. We demonstrate our techniques using two educational examples. (C) 2011 Elsevier Inc. All rights reserved.",""
"Disaster victim identification (DVI) can be aided by DNA-evidence, by comparing the DNA-profiles of unidentified individuals with those of surviving relatives. The DNA-evidence is used optimally when such a comparison is done by calculating the appropriate likelihood ratios. Though conceptually simple, the calculations can be quite involved, especially with large pedigrees, precise mutation models etc. In this article we describe a series of test cases designed to check if software designed to calculate such likelihood ratios computes them correctly. The cases include both simple and more complicated pedigrees, among which inbred ones. We show how to calculate the likelihood ratio numerically and algebraically, including a general mutation model and possibility of allelic dropout. In Appendix A we show how to derive such algebraic expressions mathematically. We have set up these cases to validate new software, called Bonaparte, which performs pedigree likelihood ratio calculations in a DVI context. Bonaparte has been developed by SNN Nijmegen (The Netherlands) for the Netherlands Forensic Institute (NFI). It is available free of charge for non-commercial purposes (see www.dnadvi.nl for details). Commercial licenses can also be obtained. The software uses Bayesian networks and the junction tree algorithm to perform its calculations. (C) 2010 Elsevier Ireland Ltd. All rights reserved.",""
"The construct of \"feature fatigue\" represents the phenomenon of customer's inconsistent satisfaction: customers prefer to choose products with more features and capacities initially, but once actually worked with a product they will find the complex ones are too hard to use. Clearly, customer's dissatisfaction after use will have a negative effect on company's long-term revenue, and the inconsistence is a big challenge for firm's product development. Researchers have proposed some methods to \"defeat\" feature fatigue, however, most recent research just analyzes features one by one and ignore the relationships among them. Another problem is that the uncertain nature of customer preferences has not been paid enough attention. To solve these problems, a probability based methodology for feature fatigue analysis is proposed, in which Bayesian network techniques are used to represent the uncertain customer preferences for capacity and usability. And in this method, sensitivity analysis is implemented to identify the key features that affect feature fatigue most, and the relationships among features are analyzed using Bayesian network inference. An example is given to illustrate the usage of the proposed method in product development process. (C) 2011 Elsevier Ltd. All rights reserved.","And in this method, sensitivity analysis is implemented to identify the key features that affect feature fatigue most, and the relationships among features are analyzed using Bayesian network inference."
"Background: Detecting epistatic interactions plays a significant role in improving pathogenesis, prevention, diagnosis and treatment of complex human diseases. A recent study in automatic detection of epistatic interactions shows that Markov Blanket-based methods are capable of finding genetic variants strongly associated with common diseases and reducing false positives when the number of instances is large. Unfortunately, a typical dataset from genome-wide association studies consists of very limited number of examples, where current methods including Markov Blanket-based method may perform poorly. Results: To address small sample problems, we propose a Bayesian network-based approach (bNEAT) to detect epistatic interactions. The proposed method also employs a Branch-and-Bound technique for learning. We apply the proposed method to simulated datasets based on four disease models and a real dataset. Experimental results show that our method outperforms Markov Blanket-based methods and other commonly-used methods, especially when the number of samples is small. Conclusions: Our results show bNEAT can obtain a strong power regardless of the number of samples and is especially suitable for detecting epistatic interactions with slight or no marginal effects. The merits of the proposed approach lie in two aspects: a suitable score for Bayesian network structure learning that can reflect higher-order epistatic interactions and a heuristic Bayesian network structure learning method.",""
"The paper presents a semantic annotation framework that is capable of extracting relevant information from unstructured, ungrammatical and incoherent data sources. The framework, named BNOSA, uses ontology to conceptualize a problem domain and to extract data from the given corpora, and Bayesian networks to resolve conflicts and to predict missing data. The framework is extensible as it is capable of dynamically extracting data from any problem domain given a pre-defined ontology and a corresponding Bayesian network. Experiments have been conducted to analyze the performance of BNOSA on several problem domains. The sets of corpora used in the experiments belong to selling-purchasing websites where product information is entered by ordinary web users in a structure-free format. The results show that BNOSA performs reasonably well to find location of the data of interest using context keywords provided as part of the domain ontology. In case of more than one value being extracted for an attribute or if the value is missing, Bayesian networks identify the most appropriate value for that attribute. (C) 2011 Elsevier B. V. All rights reserved.",""
"We propose an efficient and parameter-free scoring criterion, the factorized conditional log-likelihood (fCLL), for learning Bayesian network classifiers. The proposed score is an approximation of the conditional log-likelihood criterion. The approximation is devised in order to guarantee decomposability over the network structure, as well as efficient estimation of the optimal parameters, achieving the same time and space complexity as the traditional log-likelihood scoring criterion. The resulting criterion has an information-theoretic interpretation based on interaction information, which exhibits its discriminative nature. To evaluate the performance of the proposed criterion, we present an empirical comparison with state-of-the-art classifiers. Results on a large suite of benchmark data sets from the UCI repository show that fCLL-trained classifiers achieve at least as good accuracy as the best compared classifiers, using significantly less computational resources.","We propose an efficient and parameter-free scoring criterion, the factorized conditional log-likelihood (fCLL), for learning Bayesian network classifiers."
"We present a parallel algorithm for the score-based optimal structure search of Bayesian networks. This algorithm is based on a dynamic programming (DP) algorithm having O(n . 2(n)) time and space complexity, which is known to be the fastest algorithm for the optimal structure search of networks with n nodes. The bottleneck of the problem is the memory requirement, and therefore, the algorithm is currently applicable for up to a few tens of nodes. While the recently proposed algorithm overcomes this limitation by a space-time trade-off, our proposed algorithm realizes direct parallelization of the original DP algorithm with O(n(sigma)) time and space overhead calculations, where sigma > 0 controls the communication-space trade-off. The overall time and space complexity is O(n(sigma+1)2(n)). This algorithm splits the search space so that the required communication between independent calculations is minimal. Because of this advantage, our algorithm can run on distributed memory supercomputers. Through computational experiments, we confirmed that our algorithm can run in parallel using up to 256 processors with a parallelization efficiency of 0.74, compared to the original DP algorithm with a single processor. We also demonstrate optimal structure search for a 32-node network without any constraints, which is the largest network search presented in literature.",""
"We propose a novel method of analyzing human interactions based on the walking trajectories of human subjects, which provide elementary and necessary components for understanding and interpretation of complex human interactions in visual surveillance tasks. Our principal assumption is that an interaction episode is composed of meaningful small unit interactions, which we call \"sub-interactions.\" We model each sub-interaction by a dynamic probabilistic model and propose a modified factorial hidden Markov model (HMM) with factored observations. The complete interaction is represented with a network of dynamic probabilistic models (DPMs) by an ordered concatenation of sub-interaction models. The rationale for this approach is that it is more effective in utilizing common components, i.e., sub-interaction models, to describe complex interaction patterns. By assembling these sub-interaction models in a network, possibly with a mixture of different types of DPMs, such as standard HMMs, variants of HMMs, dynamic Bayesian networks, and so on, we can design a robust model for the analysis of human interactions. We show the feasibility and effectiveness of the proposed method by analyzing the structure of network of DPMs and its success on four different databases: a self-collected dataset, Tsinghua University's dataset, the public domain CAVIAR dataset, and the Edinburgh Informatics Forum Pedestrian dataset.",""
"The European anchovy (Engraulis encrasicolus) is a short-lived pelagic species distributed in Atlantic European waters, with the Bay of Biscay being one of the main centres of abundance. Because it is a short-lived species, the state of the stock is determined largely by incoming recruitment. Recruitment is highly variable and depends on a variety of factors, such as the size of the spawning stock and environmental conditions in the area. The use of a coupled model that could serve to predict the evolution of the anchovy stock in the short, medium, and long term under several fishing-pressure scenarios and given climate scenarios is demonstrated. This coupled model consists of a Gadget (Globally Applicable Disaggregated General Ecosystem Toolbox) model that was used to analyse the status of the Bay of Biscay anchovy population and to simulate future scenarios based on the estimated recruitment levels, combined with a probabilistic Bayesian network model for recruitment estimation based on machine-learning methods and using climatic indices as potential forecasting factors. The results indicate that certain combinations of medium to high fishing pressure and adverse environmental conditions could force the stock outside its biological reference boundaries.",""
"Aquatic ecosystems are continuously threatened by a growing number of human induced changes. Macroinvertebrate biomonitoring is particularly efficient in pinpointing the cause-effect structure between slow and subtle changes and their detrimental consequences in aquatic ecosystems. The greatest obstacle to implementing efficient biomonitoring is currently the cost-intensive human expert taxonomic identification of samples. While there is evidence that automated recognition techniques can match human taxa identification accuracy at greatly reduced costs, so far the development of automated identification techniques for aquatic organisms has been minimal. In this paper, we focus on advancing classification and data retrieval that are instrumental when processing large macroinvertebrate image datasets. To accomplish this for routine biomonitoring, in this paper we shall investigate the feasibility of automated river macroinvertebrate classification and retrieval with high precision. Besides the state-of-the-art classifiers such as Support Vector Machines (SVMs) and Bayesian Classifiers (BCs), the focus is particularly drawn on feed-forward artificial neural networks (ANNs), namely multilayer perceptrons (MLPs) and radial basis function networks (RBFNs). Since both ANN types have been proclaimed superior by different investigations even for the same benchmark problems, we shall first show that the main reason for this ambiguity lies in the static and rather poor comparison methodologies applied in most earlier works. Especially the most common drawback occurs due to the limited evaluation of the ANN performances over just one or few network architecture(s). Therefore, in this study, an extensive evaluation of each classifier performance over an ANN architecture space is performed. The best classifier among all, which is trained over a dataset of river macroinvertebrate specimens, is then used in the MUVIS framework for the efficient search and retrieval of particular macroinvertebrate peculiars. Classification and retrieval results present high accuracy and can match an experts' ability for taxonomic identification. (C) 2011 Elsevier Ltd. All rights reserved.","In this paper, we focus on advancing classification and data retrieval that are instrumental when processing large macroinvertebrate image datasets."
"This paper proposes a proactive management system for the events that occur across multiple personal user devices, including desktop PCs, laptops, and smart phones. We implemented the Personal Event Management Service using Dynamic Bayesian Networks (PEMS-DBN) system that proactively executes appropriate tasks across multiple devices without explicit user requests by recognizing the user's device reuse intention, based on the observed actions of the user for specific devices. The client module of PEMS-DBN installed on each device monitors the user actions and recognizes user intention by using dynamic Bayesian networks. The server provides data sharing and maintenance for the clients. A series of experiments were performed to evaluate user satisfaction and system accuracy, and also the amounts of resource consumption during intention recognition and proactive execution are measured to ensure the system efficiency. The experimental results showed that the PEMS-DBN system can proactively provide appropriate, personalized services with a high degree of satisfaction to the user in an effective and efficient manner.",""
"Object-oriented Bayesian networks (OOBNs) have recently been introduced to model water systems that can be represented as repetitive patterns. This paper shows the way in which OOBNs can be used as a groundwater management decision support system in two Spanish case studies. The two areas, in the southern and eastern parts of inland Spain, are characterized by a semiarid climate, water scarcity, and frequent droughts; consequently, the agrarian economy in both cases depends on the provision of irrigation from groundwater sources. Both case studies are illustrative examples of conflict among various water actors, complexity, and uncertainty about the consequences of water management actions. Each study is approached from a different viewpoint: one from an agroeconomic and the other from a hydrogeological perspective. The sites display different degrees of aquifer overexploitation and agrarian profitability. This indicates that, in each case, the effects generated by water management interventions and the time needed for recovery to natural conditions are different. The processes governing both systems can be represented as a series of repeating patterns, which makes them good candidates for an OOBN analysis. The OOBNs developed have been constructed with the participation of stakeholders to help minimize conflicts and make management decisions more understandable and acceptable for all users. The innovative nature of this research lies with the implementation of OOBNs for groundwater management. Results of the study demonstrate that the OOBN tool is a powerful decision support system that can help managers make decisions in cases for which the consequences of alternative interventions are unknown. It provides the probabilities of obtaining certain outcomes from alternative management actions for the economy and for the state of the environment. OOBNs meet the requirements of the European Water Framework Directive as a simple, participative, and integrative tool. Finally, this research represents a starting point for additional applications to support the integrated groundwater management of other complex water resources systems. DOI: 10.1061/(ASCE)WR.1943-5452.0000116. (C) 2011 American Society of Civil Engineers.",""
"Motivation: Condition-specific networks capture system-wide behavior under varying conditions such as environmental stresses, cell types or tissues. These networks frequently comprise parts that are unique to each condition, and parts that are shared among related conditions. Existing approaches for learning condition-specific networks typically identify either only differences or only similarities across conditions. Most of these approaches first learn networks per condition independently, and then identify similarities and differences in a post-learning step. Such approaches do not exploit the shared information across conditions during network learning. Results: We describe an approach for learning condition-specific networks that identifies the shared and unique subgraphs during network learning simultaneously, rather than as a post-processing step. Our approach learns networks across condition sets, shares data from different conditions and produces high-quality networks that capture biologically meaningful information. On simulated data, our approach outperformed an existing approach that learns networks independently for each condition, especially for small training datasets. On microarray data of hundreds of deletion mutants in two, yeast stationary-phase cell populations, the inferred network structure identified several common and population-specific effects of these deletion mutants and several high-confidence cases of double-deletion pairs, which can be experimentally tested. Our results are consistent with and extend the existing knowledge base of differentiated cell populations in yeast stationary phase.",""
"This paper presents a new control approach for nonlinear network-induced time delay systems by combining online reset control, neural networks, and dynamic Bayesian networks. We use feedback linearization to construct a nominal control for the system then use reset control and a neural network to compensate for errors due to the time delay. Finally, we obtain a stochastic model of the Networked Control System (NCS) using a Dynamic Bayesian Network (DBN) and use it to design a predictive control. We apply our control methodology to a nonlinear inverted pendulum and evaluate its performance through numerical simulations. We also test our approach with real-time experiments on a dc motor-load NCS with wireless communication implemented using a Ubiquitous Sensor Network (USN). Both the simulation and experimental results demonstrate the efficacy of our control methodology.",""
"This paper presents new variational Bayes (VB) approximations for learning probabilistic discriminative models with latent softmax variables, such as subclass-based multimodal softmax and mixture of experts models. The VB approximations derived here lead to closed-form approximate parameter posteriors and suitable metrics for model selection. Unlike other Bayesian methods for this challenging class of models, the proposed VB methods require neither restrictive structural assumptions nor sampling approximations to cope with the problematic softmax function. As such, the proposed VB methods are also easily extendable to more complex softmax-based hierarchical discriminative models and regression models (for continuous outputs). The proposed VB methods are evaluated on benchmark classification data and a decision modeling application, demonstrating good results.","As such, the proposed VB methods are also easily extendable to more complex softmax-based hierarchical discriminative models and regression models (for continuous outputs)."
"Probabilistic Relational Models (PRMs) are a framework for compactly representing uncertainties (actually probabilities). They result from the combination of Bayesian Networks (BNs), Object-Oriented languages, and relational models. They are specifically designed for their efficient construction, maintenance and exploitation for very large scale problems, where BNs are known to perform poorly. Actually, in large-scale problems, it is often the case that BNs result from the combination of patterns (small BN fragments) repeated many times. PRMs exploit this feature by defining these patterns only once (the so-called PRM's classes) and using them through multiple instances, as prescribed by the Object-Oriented paradigm. This design induces low construction and maintenance costs. In addition, by exploiting the classes' structures, PRM's state-of-the-art inference algorithm \"Structured Variable Elimination\" (SVE) significantly outperforms BN's classical inference algorithms (e.g., Variable Elimination, VE; Local Conditioning, LC). SVE is actually an extension of VE that simply exploits classes to avoid redundant computations. In this article, we show that SVE can be enhanced using LC. Although LC is often thought as being outperformed by VE-like algorithms in BNs, we do think that it should play an important role for PRMs because its features are very well suited for best exploiting PRM classes. In this article, relying on FaA and Jaffray's works, we show how LC can be used in conjunction with VE and deduce an extension of SVE that outperforms it for large-scale problems. Numerical experiments highlight the practical efficiency of our algorithm.","In addition, by exploiting the classes' structures, PRM's state-of-the-art inference algorithm \"Structured Variable Elimination\" (SVE) significantly outperforms BN's classical inference algorithms (e."
"We review the scholarly career of our colleague, Marco Ramoni, who died unexpectedly in the summer of 2010. His work mainly explored the development and application of Bayesian techniques to model clinical, public health, and bioinformatics questions. His contributions have led to improvements in our ability to model behavior that evolves in time, to explore systematic relationships among large sets of covariates, and to tease out the meaning of data on the role of genetic variation in the genesis of important diseases.",""
"Monitoring process is an important part in a high safety digital main control room of nuclear power plant (NPP), it is the source extracted information and found abnormal information in time. As the human factors events arisen from monitoring process recently take place more and more frequent, the authors propose a reliability Markov model to effectively decrease these abnormal events. The model mainly analyzes next monitoring object probability in terms of current information and plant state. The authors divide digital human-machine interface into two parts that are referred as logical homogeneous Markov and logical heterogeneous Markov. For the former, a series of methods of probability evaluation are proposed, such as, Markov transition probability with condition, probability distributed function with human factors, system state and alarm; for the latter, the authors propose the calculation of probability of correlation degree between last time and next time and probability calculation methods with multi-father nodes. The methods can effectively estimate the transition probability from a monitoring component to next monitoring component at time t, can effectively analyze which information is more important in next monitoring process and effectively find more useful information in time t + 1, so that the human factors events in monitoring process can greatly be decreased. (C) 2011 Elsevier Ltd. All rights reserved.",""
"In this paper, Bayesian network (BN) and ant colony optimization (ACO) techniques are combined in order to find the best path through a graph representing all available itineraries to acquire a professional competence. The combination of these methods allows us to design a dynamic learning path, useful in a rapidly changing world. One of the most important advances in this work, apart from the variable amount of pheromones, is the automatic processing of the learning graph. This processing is carried out by the learning management system and helps towards understanding the learning process as a competence-oriented itinerary instead of a stand-alone course. The amount of pheromones is calculated by taking into account the results acquired in the last completed course in relation to the minimum score required and by feeding this into the learning tree in order to obtain a relative impact on the path taken by the student. A BN is used to predict the probability of success, by taking historical data and student profiles into account. Usually, these profiles are defined beforehand; however, in our approach, some characteristics of these profiles, such as the level of knowledge, are classified automatically through supervised and/or unsupervised learning. By using ACO and BN, a fitness function, responsible for automatically selecting the next course in the learning graph, is defined. This is done by generating a path which maximizes the probability of each user's success on the course. Therefore, the path can change in order to adapt itself to learners' preferences and needs, by taking into account the pedagogical weight of each learning unit and the social behaviour of the system.","Usually, these profiles are defined beforehand; however, in our approach, some characteristics of these profiles, such as the level of knowledge, are classified automatically through supervised and/or unsupervised learning."
"Evolutionary algorithms (EAs) are particularly suited to solve problems for which there is not much information available. From this standpoint, estimation of distribution algorithms (EDAs), which guide the search by using probabilistic models of the population, have brought a new view to evolutionary computation. While solving a given problem with an EDA, the user has access to a set of models that reveal probabilistic dependencies between variables, an important source of information about the problem. However, as the complexity of the used models increases, the chance of overfitting and consequently reducing model interpretability, increases as well. This paper investigates the relationship between the probabilistic models learned by the Bayesian optimization algorithm (BOA) and the underlying problem structure. The purpose of the paper is threefold. First, model building in BOA is analyzed to understand how the problem structure is learned. Second, it is shown how the selection operator can lead to model overfitting in Bayesian EDAs. Third, the scoring metric that guides the search for an adequate model structure is modified to take into account the non-uniform distribution of the mating pool generated by tournament selection. Overall, this paper makes a contribution towards understanding and improving model accuracy in BOA, providing more interpretable models to assist efficiency enhancement techniques and human researchers.",""
"This paper presents a comparative study of two machine learning techniques for recognizing handwritten Arabic words, where hidden Markov models (HMMs) and dynamic Bayesian networks (DBNs) were evaluated. The work proposed is divided into three stages, namely preprocessing, feature extraction and classification. Preprocessing includes baseline estimation and normalization as well as segmentation. in the second stage, features are extracted from each of the normalized words, where a set of new features for handwritten Arabic words is proposed, based on a sliding window approach moving across the mirrored word image. The third stage is for classification and recognition, where machine learning is applied using HMMs and DBNs. In order to validate the techniques, extensive experiments were conducted using the IFN/ENIT database which contains 32,492 Arabic words. Experimental results and quantitative evaluations showed that HMM outperforms DBN in terms of higher recognition rate and lower complexity. (C) 2011 Elsevier B.V. All rights reserved.","The work proposed is divided into three stages, namely preprocessing, feature extraction and classification."
"Current research into workplace risk is mainly conducted using conventional descriptive statistics, which, however, fail to properly identify cause-effect relationships and are unable to construct models that could predict accidents. The authors of the present study modelled incidents and accidents in two companies in the mining and construction sectors in order to identify the most important causes of accidents and develop predictive models. Data-mining techniques (decision rules, Bayesian networks, support vector machines and classification trees) were used to model accident and incident data compiled from the mining and construction sectors and obtained in interviews conducted soon after an incident/accident occurred. The results were compared with those for a classical statistical techniques (logistic regression), revealing the superiority of decision rules, classification trees and Bayesian networks in predicting and identifying the factors underlying accidents/incidents. (C) 2011 Elsevier Ltd. All rights reserved.","Data-mining techniques (decision rules, Bayesian networks, support vector machines and classification trees) were used to model accident and incident data compiled from the mining and construction sectors and obtained in interviews conducted soon after an incident/accident occurred."
"The main goal of this paper is to describe inference in hybrid Bayesian networks (BNs) using mixture of polynomials (MOP) approximations of probability density functions (PDFs). Hybrid BNs contain a mix of discrete, continuous, and conditionally deterministic random variables. The conditionals for continuous variables are typically described by conditional PDFs. A major hurdle in making inference in hybrid BNs is marginalization of continuous variables, which involves integrating combinations of conditional PDFs. In this paper, we suggest the use of MOP approximations of PDFs, which are similar in spirit to using mixtures of truncated exponentials (MTEs) approximations. MOP functions can be easily integrated, and are closed under combination and marginalization. This enables us to propagate MOP potentials in the extended Shenoy-Shafer architecture for inference in hybrid BNs that can include deterministic variables. MOP approximations have several advantages over MTE approximations of PDFs. They are easier to find, even for multidimensional conditional PDFs, and are applicable for a larger class of deterministic functions in hybrid BNs.(C) 2010 Elsevier Inc. All rights reserved.","The main goal of this paper is to describe inference in hybrid Bayesian networks (BNs) using mixture of polynomials (MOP) approximations of probability density functions (PDFs)."
"Correlation among neocortical neurons is thought to play an indispensable role in mediating sensory processing of external stimuli. The role of temporal precision in this correlation has been hypothesized to enhance information flow along sensory pathways. Its role in mediating the integration of information at the output of these pathways, however, remains poorly understood. Here, we examined spike timing correlation between simultaneously recorded layer V neurons within and across columns of the primary somatosensory cortex of anesthetized rats during unilateral whisker stimulation. We used Bayesian statistics and information theory to quantify the causal influence between the recorded cells with millisecond precision. For each stimulated whisker, we inferred stable, whisker-specific, dynamic Bayesian networks over many repeated trials, with network similarity of 83.3 +/- 6% within whisker, compared to only 50.3 +/- 18% across whiskers. These networks further provided information about whisker identity that was approximately 6 times higher than what was provided by the latency to first spike and 13 times higher than what was provided by the spike count of individual neurons examined separately. Furthermore, prediction of individual neurons' precise firing conditioned on knowledge of putative pre-synaptic cell firing was 3 times higher than predictions conditioned on stimulus onset alone. Taken together, these results suggest the presence of a temporally precise network coding mechanism that integrates information across neighboring columns within layer V about vibrissa position and whisking kinetics to mediate whisker movement by motor areas innervated by layer V.",""
"This study analyzes multiobjective d-dimensional knapsack problems (MOd-KP) within a comparative analysis of three multiobjective evolutionary algorithms (MOEAs): the epsilon-nondominated sorted genetic algorithm II (epsilon-NSGAII), the strength Pareto evolutionary algorithm 2 (SPEA2) and the e-nondominated hierarchical Bayesian optimization algorithm (epsilon-hBOA). This study contributes new insights into the challenges posed by correlated instances of the MOd-KP that better capture the decision interdependencies often present in real world applications. A statistical performance analysis of the algorithms uses the unary epsilon-indicator, the hypervolume indicator and success rate plots to demonstrate their relative effectiveness, efficiency, and reliability for the MOd-KP instances analyzed. Our results indicate that the epsilon-hBOA achieves superior performance relative to epsilon-NSGAII and SPEA2 with increasing number of objectives, number of decisions, and correlative linkages between the two. Performance of the epsilon-hBOA suggests that probabilistic model building evolutionary algorithms have significant promise for expanding the size and scope of challenging multiobjective problems that can be explored. (C) 2011 Elsevier B.V. All rights reserved.",""
"Motivation: Most current approaches to high-throughput biological data (HTBD) analysis either perform individual gene/protein analysis or, gene/protein set enrichment analysis for a list of biologically relevant molecules. Bayesian Networks (BNs) capture linear and nonlinear interactions, handle stochastic events accounting for noise, and focus on local interactions, which can be related to causal inference. Here, we describe for the first time an algorithm that models biological pathways as BNs and identifies pathways that best explain given HTBD by scoring fitness of each network. Results: Proposed method takes into account the connectivity and relatedness between nodes of the pathway through factoring pathway topology in its model. Our simulations using synthetic data demonstrated robustness of our approach. We tested proposed method, Bayesian Pathway Analysis (BPA), on human microarray data regarding renal cell carcinoma (RCC) and compared our results with gene set enrichment analysis. BPA was able to find broader and more specific pathways related to RCC.","Bayesian Networks (BNs) capture linear and nonlinear interactions, handle stochastic events accounting for noise, and focus on local interactions, which can be related to causal inference."
"Analysing animal health data can be a complex task as the health status of individuals or groups of animals, might depend on many inter-related variables. The objective is to differentiate variables that are directly associated with health status and therefore promising targets for intervention, from variables that are indirectly associated with health status and can therefore at best only affect this indirectly through association with other variables. Bayesian network (BN) modelling is a machine learning technique for empirically identifying associations in complex and high dimensional data, so-called \"structure discovery\". An introduction to structure discovery using BN modelling is presented, comprising the key assumptions required by the methodology, along with a discussion of advantages and limitations. To demonstrate the various steps required to apply BN structure discovery to animal health data, illustrative analyses of data collected during a previously published study concerned with exposure to bovine viral diarrhoea virus in beef cow-calf herds in Scotland are presented. (C) 2011 Elsevier B.V. All rights reserved.",""
"Patient-specific analysis of molecular networks is a promising strategy for making individual risk predictions and treatment decisions in cancer therapy. Although systems biology allows the gene network of a cell to be reconstructed from clinical gene expression data, traditional methods, such as Bayesian networks, only provide an averaged network for all samples. Therefore, these methods cannot reveal patient-specific differences in molecular networks during cancer progression. In this study, we developed a novel statistical method called NetworkProfiler, which infers patient-specific gene regulatory networks for a specific clinical characteristic, such as cancer progression, from gene expression data of cancer patients. We applied NetworkProfiler to microarray gene expression data from 762 cancer cell lines and extracted the system changes that were related to the epithelial-mesenchymal transition (EMT). Out of 1732 possible regulators of E-cadherin, a cell adhesion molecule that modulates the EMT, NetworkProfiler, identified 25 candidate regulators, of which about half have been experimentally verified in the literature. In addition, we used NetworkProfiler to predict EMT-dependent master regulators that enhanced cell adhesion, migration, invasion, and metastasis. In order to further evaluate the performance of NetworkProfiler, we selected Krueppel-like factor 5 (KLF5) from a list of the remaining candidate regulators of E-cadherin and conducted in vitro validation experiments. As a result, we found that knockdown of KLF5 by siRNA significantly decreased E-cadherin expression and induced morphological changes characteristic of EMT. In addition, in vitro experiments of a novel candidate EMT-related microRNA, miR-100, confirmed the involvement of miR-100 in several EMT-related aspects, which was consistent with the predictions obtained by NetworkProfiler.",""
"Personal credit scoring on credit cards has been a critical issue in the banking industry. The bank with the most accurate estimation of its customer credit quality will be the most profitable. The study aims to compare quality prediction models from data mining methods, and improve traditional models by using boosting and genetic algorithms (GA). The predicting models used are instant-based classifiers (such as k-nearest neighbors), Bayesian networks, decision trees, decision tables, logistic regressions, radial basis function neural networks, and support vector machines. Three boosting (or ensemble) algorithms used for performance enhancement are AdaBoost, LogitBoost, and MultiBoost. The mentioned algorithms are optimized by GA for input features. Empirical results indicated that GA substantially improves the performance of underlying classifiers. Considering robustness and reliability, combining GA with ensemble classifiers is better than traditional models. Especially, integrating GA with LogitBoost (C4.5) is the most effective and compact model for credit quality evaluations.","The predicting models used are instant-based classifiers (such as k-nearest neighbors), Bayesian networks, decision trees, decision tables, logistic regressions, radial basis function neural networks, and support vector machines."
"The research questions that motivate transportation safety studies are causal in nature. Safety researchers typically use observational data to answer such questions, but often without appropriate causal inference methodology. The field of causal inference presents several modeling frameworks for probing empirical data to assess causal relations. This paper focuses on exploring the applicability of two such modeling frameworks-Causal Diagrams and Potential Outcomes-for a specific transportation safety problem. The causal effects of pavement marking retroreflectivity on safety of a road segment were estimated. More specifically, the results based on three different implementations of these frameworks on a real data set were compared: Inverse Propensity Score Weighting with regression adjustment and Propensity Score Matching with regression adjustment versus Causal Bayesian Network. The effect of increased pavement marking retroreflectivity was generally found to reduce the probability of target nighttime crashes. However, we found that the magnitude of the causal effects estimated are sensitive to the method used and to the assumptions being violated.","Safety researchers typically use observational data to answer such questions, but often without appropriate causal inference methodology."
"In the preset report, for the first time, support vector machine (SVM), artificial neural network (ANN), Bayesian networks (BNs), k-nearest neighbor (k-NN) are applied and compared on two \"in-house\" datasets to describe the tyrosinase inhibitory activity from the molecular structure. The data set Data I is used for the identification of tyrosinase inhibitors (TIs) including 701 active and 728 inactive compounds. Data II consists of active chemicals for potency estimation of TIs. The 2D TOMOCOMD-CARDD atom-based quadratic indices are used as molecular descriptors. The derived models show rather encouraging results with the areas under the Receiver Operating Characteristic (AURC) curve in the test set above 0.943 and 0.846 for the Data I and Data II, respectively. Multiple comparison tests are carried out to compare the performance of the models and reveal the improvement of machine learning (ML) techniques with respect to statistical ones (see Chemometr. Intell. Lab. Syst. 2010, 104, 249). In some cases, these ameliorations are statistically significant. The tests also demostrate that k-NN, despite being a rather simple approach, presents the best behavior in both data. The obtained results suggest that the ML-based models could help to improve the virtual screening procedures and the confluence of these different techniques can increase the practicality of data mining procedures of chemical databases for the discovery of novel TIs as possible depigmenting agents.",""
"Recently, a Bayesian network model for inferring non-stationary regulatory processes from gene expression time series has been proposed. The Bayesian Gaussian Mixture (BGM) Bayesian network model divides the data into disjunct compartments (data subsets) by a free allocation model, and infers network structures, which are kept fixed for all compartments. Fixing the network structure allows for some information sharing among compartments, and each compartment is modelled separately and independently with the Gaussian BGe scoring metric for Bayesian networks. The BGM model can equally be applied to both static (steady-state) and dynamic (time series) gene expression data. However, it is this flexibility that renders its application to time series data suboptimal. To improve the performance of the BGM model on time series data we propose a revised approach in which the free allocation of data points is replaced by a changepoint process so as to take the temporal structure into account. The practical inference follows the Bayesian paradigm and approximately samples the network, the number of compartments and the changepoint locations from the posterior distribution with Markov chainMonte Carlo (MCMC). Our empirical results show that the proposed modification leads to a more efficient inference tool for analysing gene expression time series.","The practical inference follows the Bayesian paradigm and approximately samples the network, the number of compartments and the changepoint locations from the posterior distribution with Markov chainMonte Carlo (MCMC)."
"The aim of this review is twofold. Firstly, we present the state of the art in dynamic modelling and model-based design, optimisation and control of food systems. The need for nonlinear, dynamic, multi-physics and multi-scale representations of food systems is established. Current difficulties in building such models are reviewed: incomplete, piecewise available knowledge, spread out among different disciplines (physics, chemistry, biology and consumer science) and contributors (scientists, experts, process operators, process managers), scarcity, uncertainty and high cost of measured data, complexity of phenomena and intricacy of time and space scales. Secondly, we concentrate on the opportunities offered by the complex systems science to cope with the difficulties faced by food science and engineering. Newly developed techniques such as model-based viability analysis, optimisation, dynamic Bayesian networks etc. are shown to be relevant and promising for design and optimisation of foods and food processes based on consumer needs and expectations.",""
"Macro programming a spatial computer is the ability to specify application tasks at a global level while relying on compiler-like software to translate the global tasks into the individual component activities. Bayesian networks can be regarded as a powerful tool for macro programming a spatial computer, such as a dense sensor network, in a variety of data analysis applications. In this article we present our architecture to program a spatial computer by means of a distributed Bayesian network and present some applications we developed over a sensor network testing both inference and anomaly-detection analysis.","In this article we present our architecture to program a spatial computer by means of a distributed Bayesian network and present some applications we developed over a sensor network testing both inference and anomaly-detection analysis."
"Recognizing and understanding surgical high-level tasks from sensor readings is important for surgical workflow analysis. Surgical high-level task recognition is also a challenging task in ubiquitous computing because of the inherent uncertainty of sensor data and the complexity of the operating room environment. In this paper, we present a framework for recognizing high-level tasks from low-level noisy sensor data. Specifically, we present a Markov-based approach for inferring high-level tasks from a set of low-level sensor data. We also propose to clean the noisy sensor data using a Bayesian approach. Preliminary results on a noise-free dataset of ten surgical procedures show that it is possible to recognize surgical high-level tasks with detection accuracies up to 90%. Introducing missed and ghost errors to the sensor data results in a significant decrease of the recognition accuracy. This supports our claim to use a cleaning algorithm before the training step. Finally, we highlight exciting research directions in this area. (C) 2010 Elsevier Inc. All rights reserved.",""
"Lane-keeping assistance systems for vehicles may be more acceptable to users if the assistance was adaptive to the driver's state. To adapt systems in this way, a method for detection of driver distraction is needed. Thus, we propose a novel technique for online detection of driver's distraction, modeling the long-range temporal context of driving and head tracking data. We show that long short-term memory (LSTM) recurrent neural networks enable a reliable subject-independent detection of inattention with an accuracy of up to 96.6%. Thereby, our LSTM framework significantly outperforms conventional approaches such as support vector machines (SVMs).",""
"Even though several sow replacement models have been published, integrating information about the health status of sows has not yet been handled satisfactorily. This paper presents a framework for integrating a Weak Sow Index (WSI) into an existing sow replacement model. The WSI, which is developed as part of this study, quantifies various clinical signs into one numerical value representing the risk of a sow to be involuntarily culled. The objective of the study is to investigate the effect of observing clinical signs of sows on the optimal replacement policy. A second objective is to estimate the economic value of observing clinical signs of individual sows. Bayesian networks are used to develop the WSI models for lactating and pregnant sows, respectively. The optimization of the replacement policy is done in a multi-level hierarchical Markov decision process. To illustrate the behaviour of the model, the effect of the WSI on the replacement policy, and the economic benefit of observing clinical signs of individual sows are investigated in two fictitious herds with a high and a low risk of involuntary culling of sows. In general, the value of the WSI has a high influence on the optimal replacement policy, allowing for a better economic classification of sows when taking information about the health status into account. It is shown that the economic value of the WSI is higher in a high risk herd compared to a low risk herd. Among the individual clinical signs, \"unwillingness to stand\" made the lowest contribution to the economic value of the WSI. The highest contribution was made by the clinical sign \"vulva bite\" in pregnant sows. (C) 2011 Elsevier B.V. All rights reserved.","In general, the value of the WSI has a high influence on the optimal replacement policy, allowing for a better economic classification of sows when taking information about the health status into account."
"This article combines Bayes' theorem with flows of probabilities, flows of evidences (likelihoods), and fundamental concepts for learning Bayesian networks as biological models from data. There is a huge amount of biological applications of Bayesian networks. For example in the fields of protein modeling, pathway modeling, gene expression analysis, DNA sequence analysis, protein-protein interaction, or protein-DNA interaction. Usually, the Bayesian networks have to be learned (statistically constructed) from array data. Then they are considered as an executable and analyzable model of the data source. To improve that, this work introduces a Petri net representation for the propagation of probabilities and likelihoods in Bayesian networks. The reason for doing so is to exploit the structural and dynamic properties of Petri nets for increasing the transparency of propagation processes. Consequently the novel Petri nets are called \"probability propagation nets\". By means of examples it is shown that the understanding of the Bayesian propagation algorithm is improved. This is of particular importance for an exact visualization of biological systems by Bayesian networks.",""
"In this paper, two experiments are reported investigating the nature of the cognitive representations underlying causal conditional reasoning performance. The predictions of causal and logical interpretations of the conditional diverge sharply when inferences involving pairs of conditionals-such as if P(1) then Q and if P(2) then Q-are considered. From a causal perspective, the causal direction of these conditionals is critical: are the P(i) causes of 1:2; or symptoms caused by Q. The rich variety of inference patterns can naturally be modelled by Bayesian networks. A pair of causal conditionals where Q is an effect corresponds to a \"collider\" structure where the two causes (P(i)) converge on a common effect. In contrast, a pair of causal conditionals where Q is a cause corresponds to a network where two effects (P(i)) diverge from a common cause. Very different predictions are made by fully explicit or initial mental models interpretations. These predictions were tested in two experiments, each of which yielded data most consistent with causal model theory, rather than with mental models. (C) 2011 Elsevier B.V. All rights reserved.","The predictions of causal and logical interpretations of the conditional diverge sharply when inferences involving pairs of conditionals-such as if P(1) then Q and if P(2) then Q-are considered."
"P>The various uncertainties in the earthquake-triggered tsunami threat assessment are difficult to quantify and/or integrate into the tsunami early warning process. Uncertainties in the (seismic) input parameters and the lack of knowledge about the earthquake slip distribution contribute most to the total uncertainty in real-time evaluated tsunami assessment. We present a method how to integrate and quantify these uncertainties in the warning process by evaluating a tsunami warning level probability distribution with a Bayesian network (BN) approach. As soon as an earthquake is detected, the seismic source parameter estimates are evaluated and a probabilistic overview on different tsunami warning levels is provided, feasible to support a decision maker at a warning center with important additional data. A BN system has been developed exemplarily for the region Sumatra. In this paper, we describe the method of BN generation by ancestral sampling, we critically analyse the assumptions made and weight the pro and cons of the BN approach. A case study demonstrates the workflow of the BN system in real-time and reveals the promising power of a BN analysis in the framework of early warning.",""
"Recently, inferring or sharing of mobile contexts has been actively investigated as cell phones have become more than a communication device. However, most of them focused on utilizing the contexts on social network services, while the means in mining or managing the human network itself were barely considered. In this paper, the SmartPhonebook, which mines users' social connections to manage their relationships by reasoning social and personal contexts, is presented. It works like an artificial assistant which recommends the candidate callees whom the users probably would like to contact in a certain situation. Moreover, it visualizes their social contexts like closeness and relationship with others in order to let the users know their social situations. The proposed method infers the social contexts based on the contact patterns, while it extracts the personal contexts such as the users' emotional states and behaviors from the mobile logs. Here, Bayesian networks are exploited to handle the uncertainties in the mobile environment. The proposed system has been implemented with the MS Windows Mobile 2003 SE Platform on Samsung SPH-M4650 smartphone and has been tested on real-world data. The experimental results showed that the system provides an efficient and informative way for mobile social networking.",""
"Object and situation assessment are crucial components of many advanced driver assistance systems (ADASs). Current object assessment algorithms usually provide a probabilistic representation of relevant entities in the vehicle's environment. Similarly, probabilistic approaches for situation assessment have been proposed. However, the interface between both stages is still an unsolved issue. In this paper, a direct link between Bayes filters and Bayesian networks is proposed which is based on adaptive likelihood nodes. The method is illustrated on the example of a system which automatically determines lane change maneuver recommendations. The presented results using both simulated and real data show that the proposed approach provides a unified approach for handling uncertainties in ADAS applications.",""
"The European organization for the Safety of Air Navigation is developing and maintaining BADA for use in trajectory simulation within the field of Air Traffic Management (ATM). There has been continuous research on using BADA for the estimation of fuel consumption and aviation global emission. In contrast, little attention has been paid to utilize BADA as an on-board navigation support system. In this research, we propose a framework using BADA's operation data to check if an aircraft is operating within a recommended speed, rate of climb or descent (ROCD), or fuel flow. Various simulated flight scenarios were introduced in a controlled deterministic environment, and the results accurately verified the concept of operation.",""
"In this paper, we propose a methodology for relevance analysis of performance indicators in higher education based on the use of Bayesian networks. These graphical models provide, at first glance, a snapshot of the relevant relationships among the variables under consideration. We analyse the behaviour of the proposed methodology in a practical case, showing that it is a useful tool to help decision making when elaborating policies based on performance indicators. The methodology has been implemented in a software that interacts with the Elvira package for graphical models, and that is available to the administration board at the University of Almeria (Spain) through a web interface. The software also implements a new method for constructing composite indicators by using a Bayesian network regression model.","The software also implements a new method for constructing composite indicators by using a Bayesian network regression model."
"Bayesian networks have become a widely used method in the modelling of uncertain knowledge. Owing to the difficulty domain experts have in specifying them, techniques that learn Bayesian networks from data have become indispensable. Recently, however, there have been many important new developments in this field. This work takes a broad look at the literature on learning Bayesian networks-in particular their structure-from data. Specific topics are not focused on in detail, but it is hoped that all the major fields in the area are covered. This article is not intended to be a tutorial-for this, there are many books on the topic, which will be presented. However, an effort has been made to locate all the relevant publications, so that this paper can be used as a ready reference to find the works on particular sub-topics.",""
"Within the framework of Bayesian networks (BNs), most classifiers assume that the variables involved are of a discrete nature, but this assumption rarely holds in real problems. Despite the loss of information discretization entails, it is a direct easy-to-use mechanism that can offer some benefits: sometimes discretization improves the run time for certain algorithms; it provides a reduction in the value set and then a reduction in the noise which might be present in the data; in other cases, there are some Bayesian methods that can only deal with discrete variables. Hence, even though there are many ways to deal with continuous variables other than discretization, it is still commonly used. This paper presents a study of the impact of using different discretization strategies on a set of representative BN classifiers, with a significant sample consisting of 26 datasets. For this comparison, we have chosen Naive Bayes (NB) together with several other semi-Naive Bayes classifiers: Tree-Augmented Naive Bayes (TAN), k-Dependence Bayesian (KDB), Aggregating One-Dependence Estimators (AODE) and Hybrid AODE (HAODE). Also, we have included an augmented Bayesian network created by using a hill climbing algorithm (BNHC). With this comparison we analyse to what extent the type of discretization method affects classifier performance in terms of accuracy and bias-variance discretization. Our main conclusion is that even if a discretization method produces different results for a particular dataset, it does not really have an effect when classifiers are being compared. That is, given a set of datasets, accuracy values might vary but the classifier ranking is generally maintained. This is a very useful outcome, assuming that the type of discretization applied is not decisive future experiments can be d times faster, d being the number of discretization methods considered.","Within the framework of Bayesian networks (BNs), most classifiers assume that the variables involved are of a discrete nature, but this assumption rarely holds in real problems."
"Longitudinal consumer behavior has been modeled by sequence analysis. A popular application involves Acquisition Pattern Analysis exploiting typical acquisition patterns to predict a customer's next purchase. Typically, the acquisition process is represented by an extensional, unidimensional sequence taking values from a symbolic alphabet. Given complex product structures, the extensional state representation rapidly evokes the state-space explosion problem. Consequently, most authors simplify the decision problem to the prediction of acquisitions for selected products or within product categories. This paper advocates the use of intensional state definitions representing the state by a set of variables thereby exploiting structure and allowing to model complex, possibly coupled sequential phenomena. The advantages of this intensional state space representation are demonstrated on a financial-services cross-sell application. A Dynamic Bayesian Network (DBN) models longitudinal customer behavior as represented by acquisition, product ownership and covariate variables. The DBN provides insight in the longitudinal interaction between a household's portfolio maintenance behavior and acquisition behavior. Moreover, it exhibits adequate predictive performance to support the financial-services provider's cross-sell strategy comparable to decision trees but superior to MulltiLayer Perceptron neural networks.",""
"Classical dynamic Bayesian networks (DBNs) are based on the homogeneous Markov assumption and cannot deal with non-homogeneous temporal processes. Various approaches to relax the homogeneity assumption have recently been proposed. The present paper presents a combination of a Bayesian network with conditional probabilities in the linear Gaussian family, and a Bayesian multiple changepoint process, where the number and location of the changepoints are sampled from the posterior distribution with MCMC. Our work improves four aspects of an earlier conference paper: it contains a comprehensive and self-contained exposition of the methodology; it discusses the problem of spurious feedback loops in network reconstruction; it contains a comprehensive comparative evaluation of the network reconstruction accuracy on a set of synthetic and real-world benchmark problems, based on a novel discrete changepoint process; and it suggests new and improved MCMC schemes for sampling both the network structures and the changepoint configurations from the posterior distribution. The latter study compares RJMCMC, based on changepoint birth and death moves, with two dynamic programming schemes that were originally devised for Bayesian mixture models. We demonstrate the modifications that have to be made to allow for changing network structures, and the critical impact that the prior distribution on changepoint configurations has on the overall computational complexity.",""
"Efficient task scheduling, as a crucial step to achieve high performance for multiprocessor platforms, remains one of the challenge problems despite of numerous studies. This paper presents a novel scheduling algorithm based on the Bayesian optimization algorithm (BOA) for heterogeneous computing environments. In the proposed algorithm, scheduling is divided into two phases. First, according to the task graph of multiprocessor scheduling problems, Bayesian networks are initialized and learned to capture the dependencies between different tasks. And the promising solutions assigning tasks to different processors are generated by sampling the Bayesian network. Second, the execution sequence of tasks on the same processor is set by the heuristic-based priority used in the list scheduling approach. The proposed algorithm is evaluated and compared with the related approaches by means of the empirical studies on random task graphs and benchmark applications. The experimental results show that the proposed algorithm is able to deliver more efficient schedules. Further experiments indicate that the proposed algorithm maintains almost the same performance with different parameter settings. (C) 2010 Elsevier B.V. All rights reserved.",""
"As a powerful formalism, Bayesian networks play an increasingly important role in the Uncertainty Field. This paper proposes a hybrid method to discover the knowledge represented in Bayesian networks. The hybrid method combines dependency analysis, ant colony optimization (ACO), and the simulated annealing strategy. Firstly, the new method uses order-0 independence tests with a self-adjusting threshold value to reduce the size of the search space, so that the search process takes less time to find the near-optimal solution. Secondly, better Bayesian network models are generated by using an improved ACO algorithm, where a new heuristic function is introduced to further enhance the search effectiveness and efficiency. Finally, an optimization scheme based on simulated annealing is employed to improve the optimization efficiency in the stochastic search process of ants. In a number of experiments and comparisons, the hybrid method outperforms the original ACO-B which uses ACO and some other network learning algorithms. (C) 2011 Elsevier B.V. All rights reserved.",""
"The tremendous variability in physical conditions of forest enterprises as well as attitudinal aspects of their managers is seen as a major impediment to the understanding and optimization of forest management. For this reason, former studies using several methodological approaches-including meta analysis of econometric studies, binary choice models and stochastic frontier models-in many cases remained on a qualitative and more holistic level. This paper assesses the applicability of Bayesian Belief Networks (BBN) for the analysis of net income based on detailed 2006 economic data from the German federal accountancy network of forest enterprises larger than 200 ha. A network with one dependent (target) and 30 independent (explaining) variables was designed. The BBN has proven helpful for qualitative and to some extent quantitative analysis of economic data. It has become obvious that the completeness of populating the BBN model must be seen as a constraint. The speed of the calculations and the use of dependent probabilities can be seen as benefits of the BBN approach that reduce the risk of misinterpretation in comparison with traditional analysis methods such as the comparison of different strata. The visibility and presentability of the BBN approach facilitates its use in controlling and optimizing processes.",""
"This paper presents a coherent probabilistic framework for taking account of allelic dropout, stutter bands and silent alleles when interpreting STR DNA profiles from a mixture sample using peak size information arising from a PCR analysis. This information can be exploited for evaluating the evidential strength for a hypothesis that DNA from a particular person is present in the mixture. It extends an earlier Bayesian network approach that ignored such artifacts. We illustrate the use of the extended network on a published casework example. (C) 2010 Elsevier Ireland Ltd. All rights reserved.",""
"Early detection in water evaporative installations is one of the keys to fighting against the bacterium Legionella, the main cause of Legionnaire's disease. This paper discusses the general structure, elements and operation of a probabilistic expert system capable of predicting the risk of Legionella in real time from remote information relating to the quality of the water in evaporative installations. The expert system has a master-slave architecture. The slave is a control panel in the installation at risk containing multi-sensors which continuously provide measurements of chemical and physical variables continuously. The master is a net server which is responsible for communicating with the control panel and is in charge of storing the information received, processing the data through the environment R and publishing the results in a web server. The inference engine of the expert system is constructed through Bayesian networks, which are very useful and powerful models that put together probabilistic reasoning and graphical modelling. Bayesian reasoning and Markov Chain Monte Carlo algorithms are applied in order to study the relevant unknown quantities involved in the parametric learning and propagation of evidence phases. (C) 2010 Elsevier Ltd. All rights reserved.","The inference engine of the expert system is constructed through Bayesian networks, which are very useful and powerful models that put together probabilistic reasoning and graphical modelling."
"In telecommunication industry, for many organizations, it is really important to take place in the market. As competition increases between companies, customer churn becomes a great issue to deal with by the telecommunication providers. For an effective churn management, companies try to retain their existing customers, instead of acquiring new ones. Previous researches focus on predicting the customers with a propensity to churn in telecommunication industry. In this study, a model is constructed by Bayesian Belief Network to identify the behaviors of customers with a propensity to churn. The data used are collected from one of the telecommunication providers in Turkey. First, as only discrete variables are used in Bayesian Belief Networks, CHAID (Chi-squared Automatic Interaction Detector) algorithm is applied to discretize continuous variables. Then, a causal map as a base of Bayesian Belief Network is brought out via the results of correlation analysis, multicollinearity test and experts' opinions. According to the results of Bayesian Belief Network, average minutes of calls, average billing amount, the frequency of calls to people from different providers and tariff type are the most important variables that explain customer churn. At the end of the study, three different scenarios that examine the characteristics of the churners are analyzed and promotions are suggested to reduce the churn rate. (C) 2010 Elsevier Ltd. All rights reserved.",""
"Since their introduction in the mid 1970s, influence diagrams have become a de facto standard for representing Bayesian decision problems. The need to represent complex problems has led to extensions of the influence diagram methodology designed to increase the ability to represent complex problems. In this paper, we review the representation issues and modeling challenges associated with influence diagrams. In particular, we look at the representation of asymmetric decision problems including conditional distribution trees, sequential decision diagrams, and sequential valuation networks. We also examine the issue of representing the sequence of decision and chance variables, and how it is done in unconstrained influence diagrams, sequential valuation networks, and sequential influence diagrams. We also discuss the use of continuous chance and decision variables, including continuous conditionally deterministic variables. Finally, we discuss some of the modeling challenges faced in representing decision problems in practice and some software that is currently available. (C) 2010 Elsevier Ltd. All rights reserved.",""
"Experts with different land use interests often use differing definitions of land suitability that can result in competing land use decisions. We use Bayesian belief networks linked to GIS data layers to integrate empirical data and expert knowledge from two different land use interests (development and conservation) in Maine's Lower Penobscot River Watershed. Using ground locations and digital orthoquads, we determined the overall accuracy of the resulting development and conservation suitability maps to be 82% and 89%, respectively. Overlay of the two maps show large areas of land suitable for both conservation protection and economic development and provide multiple options for mitigating potential conflict among these competing land users. The modeling process can be adapted to help prioritize and choose among different alternatives as new information becomes available, or as land use and land-use policies change. The current model structure provides a maximal coverage strategy that allows decision makers to target and prioritize several areas for protection or development and to set specific strategies in the face of changing ecological, social, or economic processes. Having multiple options can generate new hypotheses and decisions at more local scales or for more specific conservation purposes not yet identified by stakeholders and decision makers in the region. Subsequently, new models can be developed using the same process, but with higher resolution data, thereby helping a community evaluate the impacts of alternative land uses between different prioritized areas at finer scales. (C) 2011 Elsevier B.V. All rights reserved.",""
"Background: Protein secondary structure prediction provides insight into protein function and is a valuable preliminary step for predicting the 3D structure of a protein. Dynamic Bayesian networks (DBNs) and support vector machines (SVMs) have been shown to provide state-of-the-art performance in secondary structure prediction. As the size of the protein database grows, it becomes feasible to use a richer model in an effort to capture subtle correlations among the amino acids and the predicted labels. In this context, it is beneficial to derive sparse models that discourage over-fitting and provide biological insight. Results: In this paper, we first show that we are able to obtain accurate secondary structure predictions. Our per-residue accuracy on a well established and difficult benchmark (CB513) is 80.3%, which is comparable to the state-of-the-art evaluated on this dataset. We then introduce an algorithm for sparsifying the parameters of a DBN. Using this algorithm, we can automatically remove up to 70-95% of the parameters of a DBN while maintaining the same level of predictive accuracy on the SD576 set. At 90% sparsity, we are able to compute predictions three times faster than a fully dense model evaluated on the SD576 set. We also demonstrate, using simulated data, that the algorithm is able to recover true sparse structures with high accuracy, and using real data, that the sparse model identifies known correlation structure (local and non-local) related to different classes of secondary structure elements. Conclusions: We present a secondary structure prediction method that employs dynamic Bayesian networks and support vector machines. We also introduce an algorithm for sparsifying the parameters of the dynamic Bayesian network. The sparsification approach yields a significant speed-up in generating predictions, and we demonstrate that the amino acid correlations identified by the algorithm correspond to several known features of protein secondary structure. Datasets and source code used in this study are available at http://noble.gs.washington.edu/proj/pssp.",""
"Bio-chemical networks are often modeled as systems of ordinary differential equations (ODEs). Such systems will not admit closed form solutions and hence numerical simulations will have to be used to perform analyses. However, the number of simulations required to carry out tasks such as parameter estimation can become very large. To get around this, we propose a discrete probabilistic approximation of the ODEs dynamics. We do so by discretizing the value and the time domain and assuming a distribution of initial states w.r.t. the discretization. Then we sample a representative set of initial states according to the assumed initial distribution and generate a corresponding set of trajectories through numerical simulations. Finally, using the structure of the signaling pathway we encode these trajectories compactly as a dynamic Bayesian network. This approximation of the signaling pathway dynamics has several advantages. First, the discretized nature of the approximation helps to bridge the gap between the accuracy of the results obtained by ODE simulation and the limited precision of experimental data used for model construction and verification. Second and more importantly, many interesting pathway properties can be analyzed efficiently through standard Bayesian inference techniques instead of resorting to a large number of ODE simulations. We have tested our method on ODE models of the EGF-NGF signaling pathway [1] and the segmentation clock pathway [2]. The results are very promising in terms of accuracy and efficiency. (C) 2011 Elsevier B.V. All rights reserved.","Second and more importantly, many interesting pathway properties can be analyzed efficiently through standard Bayesian inference techniques instead of resorting to a large number of ODE simulations."
"The probability distribution of duration is a critical input for predicting the potential impact of traffic incidents. Most of the previous duration prediction models are discrete, which divide duration into several intervals. However, sometimes the continuous probability distribution is needed. Therefore a continuous model based on latent Gaussian naive Bayesian (LGNB) classifier is developed in this paper, assuming duration fits a lognormal distribution. The model is calibrated and tested by incident records from the Georgia Department of Transportation. The results show that LGNB can describe the continuous probability distribution of duration well. According to the evidence sensitivity analysis of LGNB, the four classes of incidents classified by LGNB can be interpreted by the level of severity and complexity.","Therefore a continuous model based on latent Gaussian naive Bayesian (LGNB) classifier is developed in this paper, assuming duration fits a lognormal distribution."
"Background: In spite of the high prevalence of suicide behaviours and the magnitude of the resultant burden, little is known about why individuals reattempt. We aim to investigate the relationships between clinical risk factors and the repetition of suicidal attempts. Methods: 1349 suicide attempters were consecutively recruited in the Emergency Room (ER) of two academic hospitals in France and Spain. Patients were extensively assessed and demographic and clinical data obtained. Data mining was used to determine the minimal number of variables that blinded the rest in relation to the number of suicide attempts. Using this set, a probabilistic graph ranking relationships with the target variable was constructed. Results: The most common diagnoses among suicide attempters were affective disorders, followed by anxiety disorders. Risk of frequent suicide attempt was highest among middle-aged subjects, and diminished progressively with advancing age of onset at first attempt. Anxiety disorders significantly increased the risk of presenting frequent suicide attempts. Pathway analysis also indicated that frequent suicide attempts were linked to greater odds for alcohol and substance abuse disorders and more intensive treatment. Conclusions: Novel statistical methods found several clinical features that were associated with a history of frequent suicide attempts. The identified pathways may promote new hypothesis-driven studies of suicide attempts and preventive strategies. (C) 2010 Elsevier Ltd. All rights reserved.",""
"The replacement of defective organs with healthy ones is an old problem, but only a few years ago was this issue put into practice. Improvements in the whole transplantation process have been increasingly important in clinical practice. In this context are clinical decision support systems (CDSSs), which have reflected a significant amount of work to use mathematical and intelligent techniques. The aim of this article was to present consideration of intelligent techniques used in recent years (2009 and 2010) to analyze organ transplant databases. To this end, we performed a search of the PubMed and Institute for Scientific Information (ISI) Web of Knowledge databases to find articles published in 2009 and 2010 about intelligent techniques applied to transplantation databases. Among 69 retrieved articles, we chose according to inclusion and exclusion criteria. The main techniques were: Artificial Neural Networks (ANN), Logistic Regression (LR), Decision Trees (DT), Markov Models (MM), and Bayesian Networks (BN). Most articles used ANN. Some publications described comparisons between techniques or the use of various techniques together. The use of intelligent techniques to extract knowledge from databases of healthcare is increasingly common. Although authors preferred to use ANN, statistical techniques were equally effective for this enterprise.","The main techniques were: Artificial Neural Networks (ANN), Logistic Regression (LR), Decision Trees (DT), Markov Models (MM), and Bayesian Networks (BN)."
"Under the sociological theory of homophily, people who are similar to one another are more likely to interact with one another. Marketers often have access to data on interactions among customers from which, with homophily as a guiding principle, inferences could be made about the underlying similarities. However, larger networks face a quadratic explosion in the number of potential interactions that need to be modeled. This scalability problem renders probability models of social interactions computationally infeasible for all but the smallest networks. In this paper, we develop a probabilistic framework for modeling customer interactions that is both grounded in the theory of homophily and is flexible enough to account for random variation in who interacts with whom. In particular, we present a novel Bayesian nonparametric approach, using Dirichlet processes, to moderate the scalability problems that marketing researchers encounter when working with networked data. We find that this framework is a powerful way to draw insights into latent similarities of customers, and we discuss how marketers can apply these insights to segmentation and targeting activities.","Marketers often have access to data on interactions among customers from which, with homophily as a guiding principle, inferences could be made about the underlying similarities."
"Intelligible and accurate risk-based decision-making requires a complex balance of information from different sources, appropriate statistical analysis of this information and consequent intelligent inference and decisions made on the basis of these analyses. Importantly, this requires an explicit acknowledgement of uncertainty in the inputs and outputs of the statistical model. The aim of this paper is to progress a discussion of these issues in the context of several motivating problems related to the wider scope of agricultural production. These problems include biosecurity surveillance design, pest incursion, environmental monitoring and import risk assessment. The information to be integrated includes observational and experimental data, remotely sensed data and expert information. We describe our efforts in addressing these problems using Bayesian models and Bayesian networks. These approaches provide a coherent and transparent framework for modelling complex systems, combining the different information sources, and allowing for uncertainty in inputs and outputs. While the theory underlying Bayesian modelling has a long and well established history, its application is only now becoming more possible for complex problems, due to increased availability of methodological and computational tools. Of course, there are still hurdles and constraints, which we also address through sharing our endeavours and experiences.","Intelligible and accurate risk-based decision-making requires a complex balance of information from different sources, appropriate statistical analysis of this information and consequent intelligent inference and decisions made on the basis of these analyses."
"In this article, we address the apparent discrepancy between causal Bayes net theories of cognition, which posit that judgments of uncertainty are generated from causal beliefs in a way that respects the norms of probability, and evidence that probability judgments based on causal beliefs are systematically in error. One purported source of bias is the ease of reasoning forward from cause to effect (predictive reasoning) versus backward from effect to cause (diagnostic reasoning). Using causal Bayes nets, we developed a normative formulation of how predictive and diagnostic probability judgments should vary with the strength of alternative causes, causal power, and prior probability. This model was tested through two experiments that elicited predictive and diagnostic judgments as well as judgments of the causal parameters for a variety of scenarios that were designed to differ in strength of alternatives. Model predictions fit the diagnostic judgments closely, but predictive judgments displayed systematic neglect of alternative causes, yielding a relatively poor fit. Three additional experiments provided more evidence of the neglect of alternative causes in predictive reasoning and ruled out pragmatic explanations. We conclude that people use causal structure to generate probability judgments in a sophisticated but not entirely veridical way.",""
"Gene regulatory network models are a major area of study in systems and computational biology and the construction of network models is among the most important problems in these disciplines. The critical epistemological issue concerns validation. Validity can be approached from two different perspectives (i) given a hypothesized network model, its scientific validity relates to the ability to make predictions from the model that can be checked against experimental observations; and (ii) the validity of a network inference procedure must be evaluated relative to its ability to infer a network from sample points generated by the network. This article examines both perspectives in the framework of a distance function between two networks. It considers some of the obstacles to validation and provides examples of both validation paradigms.","Validity can be approached from two different perspectives (i) given a hypothesized network model, its scientific validity relates to the ability to make predictions from the model that can be checked against experimental observations; and (ii) the validity of a network inference procedure must be evaluated relative to its ability to infer a network from sample points generated by the network."
"P>Aim. This paper is a report of a pilot study to examine the relationship of nursing intensity, work environment intensity and nursing resources to nurse job satisfaction. Background. There is an ever increasing amount of information in hospital information systems; however, still very little of it is actually used in nursing management and leadership. Methods. The combination of a retrospective time series and cross-sectional survey data was used. The time series patient data of 9704 in/outpatients and nurse data of 110 nurses were collected from six inpatient units in a medical clinic of a university hospital in Finland in 2006. A unit-level measure of nurse job satisfaction was collected with a survey (n = 98 nurses) in the autumn of 2006. Bayesian networks were applied to examine a model that explains nurse job satisfaction. Results. In a hospital data system, 18 usable nurse staffing indicators were identified. There were four nurse staffing indicators: patient acuity from nursing intensity subgroup, diagnosis-related group volume from work environment subgroup, and skill mix and nurse turnover from nursing resources subgroup that explained the likelihood of nurse job satisfaction in the final model. The Bayesian networks also revealed the elusive non-linear relationship between nurse job satisfaction and patient acuity. Conclusion. Survey-based information on nurse job satisfaction can be modelled with data-based nurse staffing indicators. Nurse researchers could use the Bayesian approach to obtain information about the effects of nurse staffing on nursing outcomes.",""
"Background: As quality-adjusted life years have become the standard metric in health economic evaluations, mapping health-profile or disease-specific measures onto preference-based measures to obtain quality-adjusted life years has become a solution when health utilities are not directly available. However, current mapping methods are limited due to their predictive validity, reliability, and/or other methodological issues. Objectives: We employ probability theory together with a graphical model, called a Bayesian network, to convert health-profile measures into preference-based measures and to compare the results to those estimated with current mapping methods. Methods: A sample of 19,678 adults who completed both the 12-item Short Form Health Survey (SF-12v2) and EuroQoL 5D (EQ-5D) questionnaires from the 2003 Medical Expenditure Panel Survey was split into training and validation sets. Bayesian networks were constructed to explore the probabilistic relationships between each EQ-5D domain and 12 items of the SF-12v2. The EQ-5D utility scores were estimated on the basis of the predicted probability of each response level of the 5 EQ-5D domains obtained from the Bayesian inference process using the following methods: Monte Carlo simulation, expected utility, and most-likely probability. Results were then compared with current mapping methods including multinomial logistic regression, ordinary least squares, and censored least absolute deviations. Results: The Bayesian networks consistently outperformed other mapping models in the overall sample (mean absolute error = 0.077, mean square error = 0.013, and R(2) overall = 0.802), in different age groups, number of chronic conditions, and ranges of the EQ-5D index. Conclusion: Bayesian networks provide a new robust and natural approach to map health status responses into health utility measures for health economic evaluations.","The EQ-5D utility scores were estimated on the basis of the predicted probability of each response level of the 5 EQ-5D domains obtained from the Bayesian inference process using the following methods: Monte Carlo simulation, expected utility, and most-likely probability."
"Drilling process is one of the most important operations in aeronautic industry. It is performed on the wings of the aeroplanes and its main problem lies with the burr generation. At present moment, there is a visual inspection and manual burr elimination task subsequent to the drilling and previous to the riveting to ensure the quality of the product. These operations increase the cost and the resources required during the process. The article shows the use of data mining techniques to obtain a reliable model to detect the generation of burr during high speed drilling in dry conditions on aluminium Al 7075-T6. It makes possible to eliminate the unproductive operations in order to optimize the process and reduce economic cost. Furthermore, this model should be able to be implemented later in a monitoring system to detect automatically and on-line when the generated burr is out of tolerance limits or not. The article explains the whole process of data analysis from the data preparation to the evaluation and selection of the final model. (C) 2011 Elsevier Ltd. All rights reserved.",""
"Protein signaling networks play a central role in transcriptional regulation and the etiology of many diseases. Statistical methods, particularly Bayesian networks, have been widely used to model cell signaling, mostly for model organisms and with focus on uncovering connectivity rather than inferring aberrations. Extensions to mammalian systems have not yielded compelling results, due likely to greatly increased complexity and limited proteomic measurements in vivo. In this study, we propose a comprehensive statistical model that is anchored to a predefined core topology, has a limited complexity due to parameter sharing and uses micorarray data of mRNA transcripts as the only observable components of signaling. Specifically, we account for cell heterogeneity and a multilevel process, representing signaling as a Bayesian network at the cell level, modeling measurements as ensemble averages at the tissue level, and incorporating patient-to-patient differences at the population level. Motivated by the goal of identifying individual protein abnormalities as potential therapeutical targets, we applied our method to the RAS-RAF network using a breast cancer study with 118 patients. We demonstrated rigorous statistical inference, established reproducibility through simulations and the ability to recover receptor status from available microarray data.","We demonstrated rigorous statistical inference, established reproducibility through simulations and the ability to recover receptor status from available microarray data."
"We present a novel algorithm to estimate genome-wide gene networks consisting of more than 20,000 genes from gene expression data using nonparametric Bayesian networks. Due to the difficulty of learning Bayesian network structures, existing algorithms cannot be applied to more than a few thousand genes. Our algorithm overcomes this limitation by repeatedly estimating subnetworks in parallel for genes selected by neighbor node sampling. Through numerical simulation, we confirmed that our algorithm outperformed a heuristic algorithm in a shorter time. We applied our algorithm to microarray data from human umbilical vein endothelial cells (HUVECs) treated with siRNAs, to construct a human genome-wide gene network, which we compared to a small gene network estimated for the genes extracted using a traditional bioinformatics method. The results showed that our genome-wide gene network contains many features of the small network, as well as others that could not be captured during the small network estimation. The results also revealed master-regulator genes that are not in the small network but that control many of the genes in the small network. These analyses were impossible to realize without our proposed algorithm.",""
"To identify the product failure rate grade under diverse configuration and operation conditions, a new conditional Bayesian networks (CBN) model is brought forward. By indicating the conditional independence relationship between attribute variables given the target variable, this model could provide an effective approach to classify the grade of failure rate. Furthermore, on the basis of the CBN model, the procedure of building product failure rate grade classifier is elaborated with modeling and application. At last, a case study is carried out and the results show that, with comparison to other Bayesian networks classifiers and traditional decision tree C4.5, the CBN model not only increases the total classification accuracy, but also reduces the complexity of network structure. (C) 2010 Elsevier Ltd. All rights reserved.","Furthermore, on the basis of the CBN model, the procedure of building product failure rate grade classifier is elaborated with modeling and application."
"There is agreement in that strengthening the sets of neurobiological data would reinforce the diagnostic objectivity of many psychiatric entities. This article attempts to use this approach in borderline personality disorder (BPD). Assuming that most of the biological findings in BPD reflect common underlying pathophysiological processes we hypothesized that most of the data involved in the findings would be statistically interconnected and interdependent, indicating biological consistency for this diagnosis. Prospectively obtained data on scalp and sleep electroencephalography (EEG), clinical neurologic soft signs, the dexamethasone suppression and thyrotropin-releasing hormone stimulation tests of 20 consecutive BPD patients were used to generate a Bayesian network model, an artificial intelligence paradigm that visually illustrates eventual associations (or inter-dependencies) between otherwise seemingly unrelated variables. The Bayesian network model identified relationships among most of the variables. EEG and TSH were the variables that influence most of the others, especially sleep parameters. Neurological soft signs were linked with EEG, TSH, and sleep parameters. The results suggest the possibility of using objective neurobiological variables to strengthen the validity of future diagnostic criteria and nosological characterization of BPD. (C) 2010 Elsevier Ireland Ltd. All rights reserved.",""
"Coordination of neocortical oscillations has been hypothesized to underlie the \"binding\" essential to cognitive function. However, the mechanisms that generate neocortical oscillations in physiological frequency bands remain unknown. We hypothesized that interlaminar relations in neocortex would provide multiple intermediate loops that would play particular roles in generating oscillations, adding different dynamics to the network. We simulated networks from sensory neocortex using nine columns of event-driven rule-based neurons wired according to anatomical data and driven with random white-noise synaptic inputs. We tuned the network to achieve realistic cell firing rates and to avoid population spikes. A physiological frequency spectrum appeared as an emergent property, displaying dominant frequencies that were not present in the inputs or in the intrinsic or activated frequencies of any of the cell groups. We monitored spectral changes while using minimal dynamical perturbation as a methodology through gradual introduction of hubs into individual layers. We found that hubs in layer 2/3 excitatory cells had the greatest influence on overall network activity, suggesting that this subpopulation was a primary generator of theta/beta strength in the network. Similarly, layer 2/3 interneurons appeared largely responsible for gamma activation through preferential attenuation of the rest of the spectrum. The network showed evidence of frequency homeostasis: increased activation of supragranular layers increased firing rates in the network without altering the spectral profile, and alteration in synaptic delays did not significantly shift spectral peaks. Direct comparison of the power spectra with experimentally recorded local field potentials from prefrontal cortex of awake rat showed substantial similarities, including comparable patterns of cross-frequency coupling.",""
"IL-6, IGF-II and IGFBP-2 concentrations in placental lysates were previously shown to be associated with foetal growth. This study aimed to apply a Bayesian Network (BN) model in order to investigate complex dependencies among biochemical and clinical factors and fetal growth outcome. Twenty-one Intra-Uterine Growth Restricted (IUGR) and 25 Appropriate for Gestational Age (AGA) pregnancies were followed throughout pregnancy. Information was collected on maternal and gestational age, neonatal gender, previous gynaecological history. Total protein content, IGF-II, IGFBP-1, IGFBP-2, IL-6, and TNF-alpha concentrations in placental lysates were measured, and IGF-I, IGF-II, IGFBP-1, IGFBP-2 and IL-6 relative gene expression in placenta assessed. A BN and a hybrid forecasting system were implemented: BN revealed a key role of maternal age and TNF-alpha on IUGR and confirmed a close relationship among IGF-II, IL-6 and foetal growth. A relationship between duration of gestation, appropriateness for gestational age, and placental IL-6 concentration was also confirmed. Compared with other techniques, BN showed a better accuracy. Findings confirmed a major role of maternal age in addition to IGF-11, IL-6 and TNF-alpha in IUGR. A direct role of IGFBP-2 was not shown. BN confirmed to be useful in understanding the system's biology and graphically representing variable relationships and hierarchy, particularly where, as in IUGR, many interactions among predictors exist.",""
"This paper presents a novel time varying dynamic Bayesian network (TVDBN) model for the analysis of nonstationary sequences which are of interest in many fields. The changing network structure and parameter in TVDBN are treated as random processes whose values at each time epoch determine a stationary DBN model; this DBN model is then used to specify the distribution of data sequence at the time epoch. Under such a hierarchical formulation, the changing state of network can be incorporated into the Bayesian framework straightforwardly. The network state is assumed to transit smoothly in the joint space of numerical parameter and graphical topology so that we can achieve robust online network learning even without abundant observations. Particle filtering is employed to dynamically update current network state as well as infer hidden data values. We implement our time varying model for data sequences of multinomial and Gaussian distributions, while the general model framework can be used for any other distribution. Simulations on synthetic data and evaluations on video sequences both demonstrate that the proposed TVDBN is effective in modeling nonstationary sequences. Comprehensive comparisons have been made against existing nonstationary models, and our proposed model is shown to be the top performer.",""
"This study uses Bayesian networks (BNs) to simulate the spatial distribution of southern African biomes and bioregions using bioclimatic variables. Two Tree-Augmented Naive (TAN) BN models were parameterized from 23 bioclimatic variables using the expectation-maximization (EM) algorithm. Using sensitivity analyses, the relative influence of each variable was determined using the mutual information from which six bioclimatic variables were selected for the final models. Precipitation of the warmest quarter and extra-terrestrial solar radiation was found to be the most influential variables on both bioregion and biome distributions. Isothermality was the least influential bioclimatic variable at both bioregion and biome levels. Overall correspondence was very high at 93.8 and 87.1% for biomes and bioregions, respectively, whereas classification errors were obtained in transition areas indicating the uncertainties associated with vegetation mapping around margins. The findings indicate that southern African bioregions and biomes can be classified and mapped according to key bioclimatic variables. Spatio-temporal, in particular, monthly and quarterly variations in both precipitation and temperature are found to be ecologically significant in determining the spatial distribution of biomes and bioregions. The findings also reflect the hierarchical relationship of biomes and bioregions as a function of local bioclimatic gradients and interactions. The results indicate the ecological significance of bioclimatic conditions in ecosystem science and offer the opportunity to utilize the models for predicting future responses and sensitivities to climatic changes.","% for biomes and bioregions, respectively, whereas classification errors were obtained in transition areas indicating the uncertainties associated with vegetation mapping around margins."
"In a complex crime scene with many possible suspects and conflicting evidence,crime investigation requires scientific and logical steps to narrow down the suspects. Since human investigators have difficulty in fully handling all reasoning in this highly comp lex hypothesis space, we propose a decision support system to aid the investigation process. The system integrates a rule-based reasoner, a Bayesian network for criminal profiling, and an assumption-based truth maintenance system for evidential reasoning to determine the most plausible suspect. The reasoning process reasons about a profiling classification and an alibi credibility measure. It proved effective in a realistic simulation.","The reasoning process reasons about a profiling classification and an alibi credibility measure."
"Structural equation models and Bayesian networks have been widely used to analyze causal relations between continuous variables. In such frameworks, linear acyclic models are typically used to model the data-generating process of variables. Recently, it was shown that use of non-Gaussianity identifies the full structure of a linear acyclic model, that is, a causal ordering of variables and their connection strengths, without using any prior knowledge on the network structure, which is not the case with conventional methods. However, existing estimation methods are based on iterative search algorithms and may not converge to a correct solution in a finite number of steps. In this paper, we propose a new direct method to estimate a causal ordering and connection strengths based on non-Gaussianity. In contrast to the previous methods, our algorithm requires no algorithmic parameters and is guaranteed to converge to the right solution within a small fixed number of steps if the data strictly follows the model, that is, if all the model assumptions are met and the sample size is infinite.",""
"The relative contributions of fat-free mass (FFM) and fat mass (FM) to body weight are key indicators for several major public health issues. Predictive models could offer new insights into body composition analysis. A non-parametric equation derived from a probabilistic Bayesian network (BN) was established by including sex, age, body weight and height. We hypothesised that it would be possible to assess the body composition of any subject from easily accessible covariables by selecting an adjusted FFM value within a reference dual-energy X-ray absorptiometry (DXA) measurement database (1999-2004 National Health and Nutrition Examination Survey (NHANES), n 10 402). FM was directly calculated as body weight minus FFM. A French DXA database (n 1140) was used (1) to adjust the model parameters (n 380) and (2) to cross-validate the model responses (n 760). French subjects were significantly different from American NHANES subjects with respect to age, weight and FM. Despite this different population context, BN prediction was highly reliable. Correlations between BN predictions and DXA measurements were significant for FFM (R(2) 0.94, P<0.001, standard error of prediction (SEP) 2.82 kg) and the percentage of FM (FM%) (R(2) 0.81, P<0.001, SEP 3.73%). Two previously published linear models were applied to the subjects of the French database and compared with BN predictions. BN predictions were more accurate for both FFM and FM than those obtained from linear models. In addition, BN prediction generated stochastic variability in the FM% expressed in terms of BMI. The use of such predictions in large populations could be of interest for many public health issues.",""
"When a Bayesian network (BN) is modified, for example adding or deleting a node, or changing the probability distributions, we usually will need a total recompilation of the model, despite feeling that a partial (re)compilation could have been enough. Especially when considering dynamic models, in which variables are added and removed very frequently, these recompilations are quite resource consuming. But even further, for the task of building a model, which is in many occasions an iterative process, there is a clear lack of flexibility. When we use the term Incremental Compilation or IC we refer to the possibility of modifying a network and avoiding a complete recompilation to obtain I he new (and different) join tree (JT) The main point we intend to study in this work is JT-based inference in Bayesian networks. Apart from undertaking the triangulation problem itself, we have achieved a great improvement for the compilation in BNs. We do not develop a new architecture for BNs inference, but taking some already existing framework JT-based for probability propagation such as Hugin or Shenoy and Shafer, we have designed a method that can be successfully applied to get better performance, as the experimental evaluation will show.","When we use the term Incremental Compilation or IC we refer to the possibility of modifying a network and avoiding a complete recompilation to obtain I he new (and different) join tree (JT) The main point we intend to study in this work is JT-based inference in Bayesian networks."
"A modular process risk model has been constructed that describes the manufacture of dairy dessert products and hazards that arise from non-proteolytic Clostridium botulinum. The model describes batch manufacture and consumer storage of a family size generic dairy dessert but includes a realistic quantification that could apply to a specific food product. The dairy dessert sector is an expanding part of the UK market. The model includes modules that describe spore loads in raw materials, spore inactivation during thermal processing, volume partition and the population kinetics for non-proteolytic C. botulinum during sequential isothermal storage regimes. Where possible elements of uncertainty and variability are identified explicitly. The model is constructed as a belief network from published data and expert opinions. The model provides marginal probabilities, and associated sensitivities, for a range of endpoint measures centred on the toxicity of a single retail unit after an extended period of storage. The decimal reduction time for non-proteolytic C. botulinum spore populations at the highest (hold) temperature of the primary thermal process and the highest temperature experienced during poorly controlled (consumer) storage are dominant factors determining risks. Priorities for additional information to support risk assessments have been identified. Crown Copyright (C) 2010 Published by Elsevier Ltd. All rights reserved.",""
"Students who exploit properties of an instructional system to make progress while avoiding learning are said to be \"gaming\" the system. In order to investigate what causes gaming and how it impacts students, we analyzed log data from two Intelligent Tutoring Systems (ITS). The primary analyses focused on six college physics classes using the Andes ITS for homework and test preparation, starting with the research question: What is a better predictor of gaming, problem or student? To address this question, we developed a computational gaming detector for automatically labeling the Andes data, and applied several data mining techniques, including machine learning of Bayesian network parameters. Contrary to some prior findings, the analyses indicated that student was a better predictor of gaming than problem. This result was surprising, so we tested and confirmed it with log data from a second ITS (the Algebra Cognitive Tutor) and population (high school students). Given that student was more predictive of gaming than problem, subsequent analyses focused on how students gamed and in turn benefited (or not) from instructional features of the environment, as well as how gaming in general influenced problem solving and learning outcomes.",""
"Adaptive Management (AM) is widely advocated as an approach to dealing with uncertainty in natural resource management as it provides an explicit framework for motivating, designing and interpreting the results of monitoring. One of the major factors impeding implementation is the failure to use appropriate process models; a core element of AM. Process models represent beliefs about the properties and dynamics of an ecological system and ecosystem responses to management. Quantitative models of ecosystem response help resolve ambiguity about the efficacy of management and facilitate iterative updating of knowledge using monitoring data. We report on the use of a state-and-transition model (STM) in the Adaptive Management of native woodland vegetation in south-eastern Australia. The STM is implemented as a Bayesian network, making it simple to communicate and update with new data as they arise. Application of the model is demonstrated using case-study and simulation data. We show how the model may be used to predict the probability of achieving desirable state transitions at restoration sites and how monitoring of those sites can be used to update the model (learn) and adapt (review restoration strategies). After just one monitoring/learning cycle, 7 years after the first investments, we found that updated models predict markedly different transition probabilities compared with initial models based on expert opinion. This has strong implications for the apparent cost-efficiency of restoration strategies. The STM approach provides a sound theoretical basis for restoration decisions, while the Bayesian network implementation provides a workable framework for using the STM adaptively. (C) 2010 Elsevier Ltd. All rights reserved.",""
"Genotype-specific sensitivity of the hepatitis C virus (HCV) to interferon-ribavirin (IFN-RBV) combination therapy and reduced HCV response to IFN-RBV as infection progresses from acute to chronic infection suggest that HCV genetic factors and intrahost HCV evolution play important roles in therapy outcomes. HCV polyprotein sequences (n = 40) from 10 patients with unsustainable response (UR) (breakthrough and relapse) and 10 patients with no response (NR) following therapy were identified through the Virahep-C study. Bayesian networks (BNs) were constructed to relate interrelationships among HCV polymorphic sites to UR/NR outcomes. All models showed an extensive interdependence of HCV sites and strong connections (P <= 0.003) to therapy response. Although all HCV proteins contributed to the networks, the topological properties of sites differed among proteins. E2 and NS5A together contributed similar to 40% of all sites and similar to 62% of all links to the polyprotein BN. The NS5A BN and E2 BN predicted UR/NR outcomes with 85% and 97.5% accuracy, respectively, in 10-fold cross-validation experiments. The NS5A model constructed using physicochemical properties of only five sites was shown to predict the UR/NR outcomes with 83.3% accuracy for 6 UR and 12 NR cases of the HALT-C study. Thus, HCV adaptation to IFN-RBV is a complex trait encoded in the interrelationships among many sites along the entire HCV polyprotein. E2 and NS5A generate broad epistatic connectivity across the HCV polyprotein and essentially shape intrahost HCV evolution toward the IFN-RBV resistance. Both proteins can be used to accurately predict the outcomes of IFN-RBV therapy.",""
"This paper addresses the dynamic recognition of basic facial expressions in videos using feature subset selection. Feature selection has been already used by some static classifiers where the facial expression is recognized from one single image. Past work on dynamic facial expression recognition has emphasized the issues of feature extraction and classification, however, less attention has been given to the critical issue of feature selection in the dynamic scenario. The main contributions of the paper are as follows. First, we show that dynamic facial expression recognition can be casted into a classical classification problem. Second, we combine a facial dynamics extractor algorithm with a feature selection scheme for generic classifiers. We show that the paradigm of feature subset selection with a wrapper technique can improve the dynamic recognition of facial expressions. We provide evaluations of performance on real video sequences using five standard machine learning approaches: Support Vector Machines, K Nearest Neighbor, Naive Bayes, Bayesian Networks, and Classification Trees. (C) 2010 Elsevier B.V. All rights reserved.","Feature selection has been already used by some static classifiers where the facial expression is recognized from one single image."
"The presence of antipatterns can have a negative impact on the quality of a program. Consequently, their efficient detection has drawn the attention of both researchers and practitioners. However, most aspects of antipatterns are loosely specified because quality assessment is ultimately a human-centric process that requires contextual data. Consequently, there is always a degree of uncertainty on whether a class in a program is an antipattern or not. None of the existing automatic detection approaches handle the inherent uncertainty of the detection process. First, we present BDTEX (Bayesian Detection Expert), a Goal Question Metric (GQM) based approach to build Bayesian Belief Networks (BBNs) from the definitions of antipatterns. We discuss the advantages of BBNs over rule-based models and illustrate BDTEX on the Blob antipattern. Second, we validate BDTEX with three antipatterns: Blob, Functional Decomposition, and Spaghetti code, and two open-source programs: GanttProject v1.10.2 and Xerces v2.7.0. We also compare the results of BDTEX with those of another approach, DECOR, in terms of precision, recall, and utility. Finally, we also show the applicability of our approach in an industrial context using Eclipse JDT and JHotDraw and introduce a novel classification of antipatterns depending on the effort needed to map their definitions to automatic detection approaches. (C) 2010 Elsevier Inc. All rights reserved.","Finally, we also show the applicability of our approach in an industrial context using Eclipse JDT and JHotDraw and introduce a novel classification of antipatterns depending on the effort needed to map their definitions to automatic detection approaches."
"In this paper, we proposed an improved two-level dynamic Bayesian network layered time series model (LTSM), which aims to solve the limitations hindering the application of available dynamic Bayesian networks, the hidden Markov model (HMM) and the dynamic texture (DT) model to gait recognition. In the first level, a gait silhouette or feature cycle is divided into several temporally adjacent clusters. Each cluster is modeled by a DT or logistic DT (LDT). In the second level, HMM is built to describe the relationship among the DTs/LDTs. Besides LTSM, LDT is also an improved dynamic Bayesian network presented in this paper to describe the binary image sequence, which introduces the logistic principle component analysis (PCA) to learning its parameters. We demonstrated the validity of LTSM with experiments on both the CMU Mobo gait database and CASIA gait database (dataset B), and that of LDT on the CMU Mobo gait database. Experimental results showed the superiority of the improved dynamic Bayesian networks. (C) 2010 Elsevier Ltd. All rights reserved.",""
"Part I of this series of articles focused on the construction of graphical probabilistic inference procedures, at various levels of detail, for assessing the evidential value of gunshot residue (GSR) particle evidence. The proposed models - in the form of Bayesian networks - address the issues of background presence of GSR particles, analytical performance (i.e., the efficiency of evidence searching and analysis procedures) and contamination. The use and practical implementation of Bayesian networks for case pre-assessment is also discussed. This paper, Part II, concentrates on Bayesian parameter estimation. This topic complements Part I in that it offers means for producing estimates useable for the numerical specification of the proposed probabilistic graphical models. Bayesian estimation procedures are given a primary focus of attention because they allow the scientist to combine (his/her) prior knowledge about the problem of interest with newly acquired experimental data. The present paper also considers further topics such as the sensitivity of the likelihood ratio due to uncertainty in parameters and the study of likelihood ratio values obtained for members of particular populations (e. g., individuals with or without exposure to GSR). (C) 2010 Elsevier Ireland Ltd. All rights reserved.","Part I of this series of articles focused on the construction of graphical probabilistic inference procedures, at various levels of detail, for assessing the evidential value of gunshot residue (GSR) particle evidence."
"Most Relevant Explanation (MRE) is the problem of finding a partial instantiation of a set of target variables that maximizes the generalized Bayes factor as the explanation for given evidence in a Bayesian network. MRE has a huge solution space and is extremely difficult to solve in large Bayesian networks. In this paper, we first prove that MRE is at least NP-hard. We then define a subproblem of MRE called MRE (k) that finds the most relevant k-ary explanation and prove that the decision problem of MRE (k) is NP(PP)-complete. Since MRE needs to find the best solution by MRE(k) over all k, and we can also show that MRE is in NPPP, we conjecture that a decision problem of MRE is NP(PP)-complete as well. Furthermore, we show that MRE remains in NPPPeven if we restrict the number of target variables to be within a log factor of the number of all unobserved variables. These complexity results prompt us to develop a suite of approximation algorithms for solving MRE, One algorithm finds an MRE solution by integrating reversible-jump MCMC and simulated annealing in simulating a non-homogeneous Markov chain that eventually concentrates its mass on the mode of a distribution of the GBF scores of all solutions. The other algorithms are all instances of local search methods, including forward search, backward search, and tabu search. We tested these algorithms on a set of benchmark diagnostic Bayesian networks. Our empirical results show that these methods could find optimal MRE solutions for most of the test cases in our experiments efficiently.",""
"To make robots coexist and share the environments with humans, robots should understand the behaviors or the intentions of humans and further predict their motions. In this paper, an A*-based predictive motion planner is represented for navigation tasks. A generalized pedestrian motion model is proposed and trained by the statistical learning method. To deal with the uncertainty, a localization, tracking and prediction framework is also introduced. The corresponding recursive Bayesian formula represented as DBNs (Dynamic Bayesian Networks) is derived for real time operation. Finally, the simulations and experiments are shown to validate the idea of this paper.",""
"We propose a dynamical model for the estimation of operational risk in banking institutions. Operational risk is the risk that a financial loss occurs as the result of failed processes. Examples of operational losses are losses generated by internal fraud, human error and failed transactions. In order to encompass the most heterogeneous set of processes, in our approach the losses of each process are generated by the interplay among random noise, interactions with other processes and the efforts the bank makes to avoid losses. We show how some relevant parameters of the model can be estimated from a database of historical operational losses, validate the estimation procedure and test the forecasting power of the model. Some advantages of our approach over the traditional statistical techniques are that it allows us to follow the whole time evolution of the losses and to take into account different-time correlations among the processes.",""
"Requirement engineering is a key issue in the development of a software project. Like any other development activity it is not without risks. This work is about the empirical study of risks of requirements by applying machine learning techniques, specifically Bayesian networks classifiers. We have defined several models to predict the risk level for a given requirement using three dataset that collect metrics taken from the requirement specifications of different projects. The classification accuracy of the Bayesian models obtained is evaluated and compared using several classification performance measures. The results of the experiments show that the Bayesians networks allow obtaining valid predictors. Specifically, a tree augmented network structure shows a competitive experimental performance in all datasets. Besides, the relations established between the variables collected to determine the level of risk in a requirement, match with those set by requirement engineers. We show that Bayesian networks are valid tools for the automation of risks assessment in requirement engineering.","This work is about the empirical study of risks of requirements by applying machine learning techniques, specifically Bayesian networks classifiers."
"Existing methods for the semantic analysis of multimedia, although effective for single-medium scenarios, are inherently flawed in cases where knowledge is spread over different media types. In this work we implement a cross media analysis scheme that takes advantage of both visual and textual information for detecting high-level concepts. The novel aspect of this scheme is the definition and use of a conceptual space where information originating from heterogeneous media types can be meaningfully combined and facilitate analysis decisions. More specifically, our contribution is on proposing a modeling approach for Bayesian Networks that defines this conceptual space and allows evidence originating from the domain knowledge, the application context and different content modalities to support or disproof a certain hypothesis. Using this scheme we have performed experiments on a set of 162 compound documents taken from the domain of car manufacturing industry and 118 581 video shots taken from the TRECVID2010 competition. The obtained results have shown that the proposed modeling approach exploits the complementary effect of evidence extracted across different media and delivers performance improvements compared to the single-medium cases. Moreover, by comparing the performance of the proposed approach with an approach using Support Vector Machines (SVM), we have verified that in a cross media setting the use of generative rather than discriminative models are more suited, mainly due to their ability to smoothly incorporate explicit knowledge and learn from a few examples. (C) 2011 Elsevier B.V. All rights reserved.",""
"One kind of functional noncoding RNAs, microRNAs (miRNAs), form a class of endogenous RNAs that can have important regulatory roles in animals and plants by targeting transcripts for cleavage or translation repression. Researches on both experimental and computational approaches have shown that miRNAs indeed involve in the human cancer development and progression. However, the miRNAs that contribute more information to the distinction between the normal and tumor samples (tissues) are still undetermined. Recently, the high-throughput microarray technology was used as a powerful technique to measure the expression level of miRNAs in cells. Analyzing this expression data can allow us to determine the functional roles of miRNAs in the living cells. In this paper, we present a computational method to ( 1) predicting the tumor tissues using high-throughput miRNA expression profiles; (2) finding the informative miRNAs that show strong distinction of expression level in tumor tissues. To this end, we perform a support vector machine (SVM) based method to deeply examine one recent miRNA expression dataset. The experimental results show that SVM-based method outperforms other supervised learning methods such as decision trees, Bayesian networks, and backpropagation neural networks. Furthermore, by using the miRNA-target information and Gene Ontology annotations, we showed that the informative miRNAs have strong evidences related to some types of human cancer including breast, lung, and colon cancer.",""
"Abnormal kinase activity is a frequent cause of diseases, which makes kinases a promising pharmacological target. Thus, it is critical to identify the characteristics of protein kinases regulation by studying the activation and inhibition of kinase sub-units in response to varied stimuli. Bayesian network (BN) is a formalism for probabilistic reasoning that has been widely used for learning dependency models. However, for high-dimensional discrete random vectors the set of plausible models becomes large and a full comparison of all the posterior probabilities related to the competing models becomes infeasible. A solution to this problem is based on the Markov Chain Monte Carlo (MCMC) method. This paper proposes a BN-based framework to discover the dependency correlations of kinase regulation. Our approach is to apply the MCMC method to generate a sequence of samples from a probability distribution, by which to approximate the distribution. The frequent connections (edges) are identified from the obtained sampling graphical models. Our results point to a number of novel candidate regulation patterns that are interesting in biology and include inferred associations that were unknown.",""
"This paper addresses the problem of learning Bayesian network structures from data based on score functions that are decomposable. It describes properties that strongly reduce the time and memory costs of many known methods without losing global optimality guarantees. These properties are derived for different score criteria such as Minimum Description Length (or Bayesian Information Criterion), Akaike Information Criterion and Bayesian Dirichlet Criterion. Then a branch-and-bound algorithm is presented that integrates structural constraints with data in a way to guarantee global optimality. As an example, structural constraints are used to map the problem of structure learning in Dynamic Bayesian networks into a corresponding augmented Bayesian network. Finally, we show empirically the benefits of using the properties with state-of-the-art methods and with the new algorithm, which is able to handle larger data sets than before.",""
"In this paper we consider sparse and identifiable linear latent variable (factor) and linear Bayesian network models for parsimonious analysis of multivariate data. We propose a computationally efficient method for joint parameter and model inference, and model comparison. It consists of a fully Bayesian hierarchy for sparse models using slab and spike priors (two-component delta-function and continuous mixtures), non-Gaussian latent factors and a stochastic search over the ordering of the variables. The framework, which we call SLIM (Sparse Linear Identifiable Multivariate modeling), is validated and bench-marked on artificial and real biological data sets. SLIM is closest in spirit to LiNGAM (Shimizu et al., 2006), but differs substantially in inference, Bayesian network structure learning and model comparison. Experimentally, SLIM performs equally well or better than LiNGAM with comparable computational complexity. We attribute this mainly to the stochastic search strategy used, and to parsimony (sparsity and identifiability), which is an explicit part of the model. We propose two extensions to the basic i.i.d. linear framework: non-linear dependence on observed variables, called SNIM (Sparse Non-linear Identifiable Multivariate modeling) and allowing for correlations between latent variables, called CSLIM (Correlated SLIM), for the temporal and/or spatial data. The source code and scripts are available from http://cogsys.imm.dtu.dk/slim/.","We propose a computationally efficient method for joint parameter and model inference, and model comparison."
"The correct diagnosis of cardiovascular disease is a key factor to reduce social and economic costs. In this context, cardiovascular disease risk assessment tools are of fundamental importance. This work addresses two major drawbacks of the currert cardiovascular risk score systems: reduced number of risk factors considered by each individual tool and the inability of these tools to deal with incomplete information. To achieve these goals a two phase strategy was followed. In the first phase, a common representation procedure, based on a Naive-Bayes classifier methodology, was applied to a set of current risk assessment tools. Classifiers' individual parameters and conditional probabilities were initially evaluated through a frequency estimation method. In a second phase, a combination scheme was proposed exploiting the particular features of Bayes probabilistic reasoning, followed by conditional probabilities optimization based on a genetic algorithm approach. This strategy was applied to describe and combine ASSIGN and Framingham models. Validation results were obtained based on individual models, assuming their statistical correctness. The achieved results are very promising, showing the potential of the strategy to accomplish the desired goals. (C) 2010 Elsevier Ireland Ltd. All rights reserved.","In the first phase, a common representation procedure, based on a Naive-Bayes classifier methodology, was applied to a set of current risk assessment tools."
"Various scientific theories stand in a reductive relation to each other. In a recent article, we have argued that a generalized version of the Nagel-Schaffner model (GNS) is the right account of this relation. In this article, we present a Bayesian analysis of how GNS impacts on confirmation. We formalize the relation between the reducing and the reduced theory before and after the reduction using Bayesian networks, and thereby show that, post-reduction, the two theories are confirmatory of each other. We then ask when a purported reduction should be accepted on epistemic grounds. To do so, we compare the prior and posterior probabilities of the conjunction of both theories before and after the reduction and ask how well each is confirmed by the available evidence.",""
"This paper develops a novel method to model and predict the spatial distribution of vegetation types in Swaziland using physiographic and bioclimatic variables. The method uses a data mining approach implemented within a probabilistic graphical model to match two observed hierarchical levels of vegetation. The classification uses Bayesian networks (BN) and the parameterization is based on the expectation-maximization (EM) algorithm. The model is tested on a random sample of mapped vegetation types in Swaziland and allowed for the identification of the key environmental variables that are most important for capturing the vegetation spatial distribution. We show that while elevation and geology are the most important variables explaining the spatial distribution patterns of vegetation for both models, the influence of the climatic and other variables on the vegetation at the two levels differ. The overall distribution of the predicted vegetation classes was very similar to their distribution on the observed vegetation map. Overall the error rate was found to be 9.35% for a model of 16 vegetation classes and 4.9% for the one with 5 classes, indicating the excellent classification accuracy of the approach despite the complex landscape of the study area. Possible sources of error and some limitations are discussed and conclusions are drawn including suggestions for further investigation. (C) 2011 Elsevier B.V. All rights reserved.","The classification uses Bayesian networks (BN) and the parameterization is based on the expectation-maximization (EM) algorithm."
"This paper describes the development of a participatory decision support system for water management in the Upper Guadiana basin in central Spain where there has long been competition for groundwater resources between the agricultural sector and the environment. In the last few decades the rapid development of irrigation has led to the over-exploitation of the Mancha Occidental aquifer, the main water source in the area; this in turn has led to the loss of ecologically important wetlands. Against this background the River Basin Authority (RBA) has designed a new water management plan aimed at reducing water consumption. The objective of this paper is to evaluate the impact of these measures on both the environment and the agricultural sector. To this end stakeholders have been invited to actively participate in the development of a decision support system (DSS) based on the combination of an agro-economic model and an object-oriented Bayesian network. This DSS has been used to evaluate the trade-off between agriculture and the environment for different management options at different scales. Results indicate that achieving even a partial recovery of the aquifer water levels will require strict enforcement by the RBA of water restrictions on farmers combined with a high offer price for the purchase of water rights. However, compliance with water restrictions inevitably leads to losses in farm income, especially in small vineyard farms, unless additional measures are taken to compensate for those potential losses. The purchase of water rights alone is insufficient to ensure the recovery of water levels; accompanying measures included in the new regional management plan will also need to be undertaken.",""
"Objectives: To detect errors in blood laboratory results using a Bayesian network (BN), to compare results with an established method for detecting errors based on frequency patterns (LabRespond) and logistic regression model. Methods: In Experiment 1 and 2 using a sample of 5,800 observations from the National Health and Nutrition Examination Survey dataset, large, medium and small errors were randomly generated and introduced to liver enzymes (ALT, AST, and LDH) of the dataset. Experiment 1 examined systematic errors, while Experiment 2 investigated random errors. The outcome of interest was the correct detection of liver enzymes as \"error\" or \"not error.\" With the BN, the outcome was predicted by exploiting probabilistic relationships among AST, ALT, LDH, and gender. In addition to AST, ALT, LDH, and gender, LabRespond required more information on related analytes to achieve optimal prediction. We assessed performance by examining the area under the receiver operating characteristics curves using a 10-fold cross validation method, as well as risk stratification tables. Results: In Experiment 1, the BN significantly outperformed both LabRespond and logistic regression in detecting large (both at p < 0.001), medium (p = 0.01 and p < 0.001, respectively), and small (p = 0.03 and, p = 0.05, respectively) systematic errors. In Experiment 2, the BN performed significantly better than LabRespond and multinomial logistic regression in detecting large (p = 0.04 and p < 0.001, respectively) and medium (p = 0.05 and p < 0.001, respectively) random errors. Conclusion: A Bayesian network is better at detection and can detect errors with less information than existing automated models, suggesting that Bayesian model may be an effective means for reducing medical costs and improving patient safety.","Objectives: To detect errors in blood laboratory results using a Bayesian network (BN), to compare results with an established method for detecting errors based on frequency patterns (LabRespond) and logistic regression model."
"Method: Dynamic Bayesian networks (DBNs) have been applied widely to reconstruct the structure of regulatory processes from time series data, and they have established themselves as a standard modelling tool in computational systems biology. The conventional approach is based on the assumption of a homogeneous Markov chain, and many recent research efforts have focused on relaxing this restriction. An approach that enjoys particular popularity is based on a combination of a DBN with a multiple changepoint process, and the application of a Bayesian inference scheme via reversible jump Markov chain Monte Carlo (RJMCMC). In the present article, we expand this approach in two ways. First, we show that a dynamic programming scheme allows the changepoints to be sampled from the correct conditional distribution, which results in improved convergence over RJMCMC. Second, we introduce a novel Bayesian clustering and information sharing scheme among nodes, which provides a mechanism for automatic model complexity tuning. Results: We evaluate the dynamic programming scheme on expression time series for Arabidopsis thaliana genes involved in circadian regulation. In a simulation study we demonstrate that the regularization scheme improves the network reconstruction accuracy over that obtained with recently proposed inhomogeneous DBNs. For gene expression profiles from a synthetically designed Saccharomyces cerevisiae strain under switching carbon metabolism we show that the combination of both: dynamic programming and regularization yields an inference procedure that outperforms two alternative established network reconstruction methods from the biology literature.","An approach that enjoys particular popularity is based on a combination of a DBN with a multiple changepoint process, and the application of a Bayesian inference scheme via reversible jump Markov chain Monte Carlo (RJMCMC)."
"The primary goal of this article is to infer genetic interactions based on gene expression data. A new method for multiorganism Bayesian gene network estimation is presented based on multitask learning. When the input datasets are sparse, as is the case in microarray gene expression data, it becomes difficult to separate random correlations from true correlations that would lead to actual edges when modeling the gene interactions as a Bayesian network. Multitask learning takes advantage of the similarity between related tasks, in order to construct a more accurate model of the underlying relationships represented by the Bayesian networks. The proposed method is tested on synthetic data to illustrate its validity. Then it is iteratively applied on real gene expression data to learn the genetic regulatory networks of two organisms with homologous genes. (C) 2010 Elsevier Ireland Ltd. All rights reserved.",""
"The challenges associated with evaluating the effectiveness of environmental decision support systems (EDSS) based on the perceptions of only a small sample of end-users are well understood. Although methods adopted from Management Information Systems (MISS) evaluation research have benefited from relatively large (100+) sample sizes, permitting the use of multi-criteria analysis of users perceptions, there are few examples of methods for quantifying effectiveness based on smaller groups of end-users. Use of environmental decision support systems in Integrated Water Resources Management (IWRM) has become increasingly prevalent over the passed twenty years, where their potential for facilitating the participatory process has been recognised; however, few quantitative assessments have been reported. This paper reports the application of a quantitative approach to evaluating environmental decision support systems with small groups of end-users in two case studies where the objective was to facilitate the participatory decision-making process in water management projects. The first case study involved nine end-users applying and evaluating a Bayesian network-based tool to facilitate water demand management implementation in Sofia, the capital city of Bulgaria. The second involved eleven end-users applying and evaluating an integrated tool the Integrated Solution Support System (I3S) - during a water stress mitigation project in a European context. End-users' perceptions of effectiveness were elicited and compared using statistical analysis. The results of the two case studies suggest that end-user's employment influences their perceptions of EDSS effectiveness. We also show how the applied evaluation method is flexible enough to assess different EDSS types from a range of dimensions. (C) 2010 Published by Elsevier Ltd.",""
"The Cutty Sark is undergoing major conservation to slow down the deterioration of the original Victorian fabric of the ship. While the conservation work being carried out is \"state of the art,\" there is no evidence at present of the effectiveness of the conservation work over the next fifty years. A prognostics framework is being developed to monitor the \"health\" of the ship's iron structures to help ensure a 50 year life once conservation is completed, with only minor deterioration taking place over time. This paper presents the prognostics framework being developed, which encompasses four approaches: 1-Canary and Parrot devices, 2-Physics-of-Failure (PoF) models, 3-Precursor Monitoring and Data Trend Analysis, and 4-Bayesian Networks. \"Canary\" and \"Parrot\" devices have been designed to mimic the actual mechanisms that would lead to failure of the iron structures, with canary devices failing faster to act as an indicator of forthcoming failures, while parrot devices fail at the same rate as the structure under consideration. A PoF model based on a decrease of the corrosion rate over time is used to predict the remaining life of an iron structure. Mahalanobis Distance (MD) is used as a precursor monitoring technique to obtain a single comparison metric from multiple sensor data to represent anomalies detected in the system. Bayesian Network models are then used as a fusion technique, integrating remaining life predictions from PoF models with information of possible anomalies from MD analysis to provide a new prediction of remaining life. This paper describes why, and how the four approaches are used for diagnostic and prognostics purposes, and how they are integrated into the prognostics framework.",""
"High availability communication networks with very low failure rates are often designed by using physical diversity, i.e., the traffic between a given pair of nodes is routed by using several physically disjoint paths. The selection of the pair of routes that maximizes the connectivity of a node is not an easy problem, because such connectivity cannot be expressed as an additive function of the availability of links and nodes in the path pairs. Previous algorithms for searching the optimal route use additive costs, which can be a loose assumption either when high failure rates can be locally present, or when fully disjoints paths do not exist. In this paper, we construct a Bayesian network to encode the probabilistic relation between the connectivity of a node and the availability of nodes and links. The problem of selecting an optimal pair of routes becomes equivalent to optimizing the structure of the Bayesian network. By introducing appropriate approximations on the double route availability equations, we propose a new algorithm, which outperforms other classical methods when there are sporadic high error rate elements in the network. Simulations and an application example in an electric transport telecontrol network show the performance of the method when compared to standard search.",""
"There is a growing need to evaluate fisheries management plans in a comprehensive interdisciplinary context involving stakeholders. The use of a probabilistic management model to evaluate potential management plans for Baltic salmon fisheries is demonstrated. The analysis draws on several scientific studies: a biological stock assessment with integrated economic analysis of the commercial fisheries, an evaluation of recreational fisheries, and a sociological study aimed at understanding stakeholder perspectives and potential commitment to alternative management plans. A Bayesian belief network is used to synthesize the findings from these separate studies and to evaluate the robustness of management decisions to different priorities and various sources of uncertainty. In particular, the importance of sociological studies in quantifying uncertainty about the commitment of fishers to management plans is highlighted by modelling the link between commitment and implementation success. Such analyses, relying on prior knowledge, can forewarn of the consequences of management choices before they are implemented.",""
"A proof-of-concept demonstration is presented using a novel method for estimating vertical distributions of chlorophyll a (Chl a) from archives of data from ships, combined with remotely sensed data of sea surface temperature, surface Chl a, and wind (U and V vectors) from satellites. Our study area has contrasting hydrographic regimes that include the dynamic southern Benguela upwelling system and the stratified waters of the Agulhas Bank. Cluster analysis is used to identify \"typical\" Chl a profiles from an archive of profiles recorded in 2002-2008. Bayesian networks were then used to relate characteristic profiles to remotely sensed surface features, sub-regions, seasons, and depths. The proposed method could be used to predict daily Chl a profiles for each pixel of a satellite image to estimate biomass and subsurface light fields, and these combined with a light algorithm to model primary production for the Benguela large marine ecosystem.",""
"Current spacecraft health monitoring and fault-diagnosis practices involve around-the-clock limit-checking and trend analysis on large amount of telemetry data. They do not scale well for future multiplatform space missions due the size of the telemetry data and an increasing need to make the long-duration missions cost-effective by limiting the operations team personnel. The need for efficient utilization of telemetry data achieved by employing machine learning and reasoning algorithms has been pointed out in the literature for enhancing diagnostic performance and assisting the less-experienced personnel in performing monitoring and diagnosis tasks. In this paper, we develop a systematic and transparent fault-diagnosis methodology within a hierarchical fault-diagnosis framework for a satellites formation flight. We present our proposed hierarchical decomposition framework through a novel Bayesian network, whose structure is developed from the knowledge of component health-state dependencies. We have developed a methodology for specifying the network parameters that utilizes both node fault-diagnosis performance data and domain experts' beliefs. Our proposed model development procedure reduces the demand for expert's time in eliciting probabilities significantly. Our proposed approach provides the ground personnel with an ability to perform diagnostic reasoning across a number of subsystems and components coherently. Due to the unavailability of real formation flight data, we demonstrate the effectiveness of our proposed methodology by using synthetic data of a leader-follower formation flight architecture. Although our proposed approach is developed from the satellite fault-diagnosis perspective, it is generic and is targeted toward other types of cooperative fleet vehicle diagnosis problems.",""
"In a spatially explicit climate change impact assessment, a Bayesian network (BN) model was implemented to probabilistically simulate future response of the four major vegetation types in Swaziland. Two emission scenarios (A2 and B2) from an ensemble of three statistically downscaled coupled atmosphere-ocean global circulation models (CSIRO-Mk3, CCCma-CGCM3 and UKMO-HadCM3) were used to simulate possible changes in BN-based environmental envelopes of major vegetation communities. Both physiographic and climatic data were used as predictors representing the 2020s, 2050s and the 2080s periods. A comparison of simulated vegetation distribution and the expert vegetation map under baseline conditions showed an overall correspondence of 97.7% and a Kappa coefficient of 0.966. Although the ensemble projections showed comparable trends during the 2020s, the results from the A2 storyline were more drastic indicating that grassland and the Lebombo bushveld will be impacted negatively as early as the 2020s with about 1 degrees C temperature increase. The bioclimatically suitable areas of all but one vegetation type decline drastically after about 2 degrees C warming, more so under the more severe A2 scenario and in particular during the 2080s. The sour bushveld is the only vegetation type that initially responds positively to warming by possibly encroaching to the highly vulnerable grassland areas. Vulnerability of vegetation is increased by the limited ability to migrate into suitable climates due to close affinity to certain geological formations and the fragmentation of the landscape by agriculture and other land uses. This is expected to have serious impacts on biodiversity in the country. Under warmer climates, the likely vegetation types to emerge are uncertain due to future novel combinations of climate and bedrock lithology. The strengths and limitations of the BN approach are also discussed.",""
"In this paper, we review the role of probabilistic graphical models in artificial intelligence. We start by giving an account of the early years when there was important controversy about the suitability of probability for intelligent systems. We then discuss the main milestones for the foundations of graphical models starting with Pearl's pioneering work. Some of the main techniques for problem solving (abduction, classification, and decision-making) are briefly explained. Finally, we propose some important challenges for future research and highlight relevant applications (forensic reasoning, genomics and the use of graphical models as a general optimization tool). (C) 2008 Elsevier B. V. All rights reserved.","Some of the main techniques for problem solving (abduction, classification, and decision-making) are briefly explained."
"This paper demonstrates the use of qualitative probabilistic networks (QPNs) to aid Dynamic Bayesian Networks (DBNs) in the process of learning the structure of gene regulatory networks from microarray gene expression data. We present a study which shows that QPNs define monotonic relations that are capable of identifying regulatory interactions in a manner that is less susceptible to the many sources of uncertainty that surround gene expression data. Moreover, we construct a model that maps the regulatory interactions of genetic networks to QPN constructs and show its capability in providing a set of candidate regulators for target genes, which is subsequently used to establish a prior structure that the DBN learning algorithm can use and which 1) distinguishes spurious correlations from true regulations, 2) enables the discovery of sets of coregulators of target genes, and 3) results in a more efficient construction of gene regulatory networks. The model is compared to the existing literature using the known gene regulatory interactions of Drosophila Melanogaster.",""
"Several gene regulatory network models containing concepts of directionality at the edges have been proposed. However, only a few reports have an interpretable definition of directionality. Here, differently from the standard causality concept defined by Pearl, we introduce the concept of contagion in order to infer directionality at the edges, i.e., asymmetries in gene expression dependences of regulatory networks. Moreover, we present a bootstrap algorithm in order to test the contagion concept. This technique was applied in simulated data and, also, in an actual large sample of biological data. Literature review has confirmed some genes identified by contagion as actually belonging to the TP53 pathway.",""
"In complex industrial system, most of single faults have multiple propagation paths, so any local slight deviation is able to propagate, spread, accumulate and increase through system fault causal chains. It will finally result in unplanned outages and even catastrophic accidents, which lead to huge economic losses, environmental contamination, or human injuries. In order to ensure system intrinsic safety and increase operational performance and reliability in a long period, this study proposes an integrated safety prognosis model (ISPM) considering the randomness, complexity and uncertainty of fault propagation. ISPM is developed based on dynamic Bayesian networks to model the propagation of faults in a complex system, integrating the priori knowledge of the interactions and dependencies among subsystems, components, and the environment of the system, as well as the relationships between fault causes and effects. So the current safety state and potential risk of system can be assessed by locating potential hazard origins and deducing corresponding possible consequences. Furthermore, ISPM is also developed to predict the future degradation trend in terms of future reliability or performance of system, and provide proper proactive maintenance plans. Ant colony algorithm is introduced in ISPM by comprehensively considering two factors as probability and severity of faults, to perform the quantitative risk estimation of the underlining system. The feasibility and benefits of ISPM are investigated with a field case study of gas turbine compressor system. According to the outputs given by ISPM in the application, proactive maintenance, safety-related actions and contingency plans are further discussed and then made to keep the system in a high reliability and safety level in the long term. (C) 2010 Elsevier Ltd. All rights reserved.",""
"Analyzing composite behaviors involving objects from multiple categories in surveillance videos is a challenging task due to the complicated relationships among human and objects. This paper presents a novel behavior analysis framework using a hierarchical dynamic Bayesian network (DBN) for video surveillance systems. The model is built for extracting objects' behaviors and their relationships by representing behaviors using spatial-temporal characteristics. The recognition of object behaviors is processed by the DBN at multiple levels: features of objects at low level, objects and their relationships at middle level, and event at high level, where event refers to behaviors of a single type object as well as behaviors consisting of several types of objects such as \"a person getting in a car.\" Furthermore, to reduce the complexity, a simple model selection criterion is addressed, by which the appropriated model is picked out from a pool of candidate models. Experiments are shown to demonstrate that the proposed framework could efficiently recognize and semantically describe composite object and human activities in surveillance videos. (C) 2011 Society of Photo-Optical Instrumentation Engineers (SPIE). [DOI: 10.1117/1.3554372]",""
"Tumor necrosis factor alpha (TNF-alpha) is a key regulator of inflammation and rheumatoid arthritis (RA). TNF-alpha blocker therapies can be very effective for a substantial number of patients, but fail to work in one third of patients who show no or minimal response. It is therefore necessary to discover new molecular intervention points involved in TNF-alpha blocker treatment of rheumatoid arthritis patients. We describe a data analysis strategy for predicting gene expression measures that are critical for rheumatoid arthritis using a combination of comprehensive genotyping, whole blood gene expression profiles and the component clinical measures of the arthritis Disease Activity Score 28 (DAS28) score. Two separate network ensembles, each comprised of 1024 networks, were built from molecular measures from subjects before and 14 weeks after treatment with TNF-alpha blocker. The network ensemble built from pre-treated data captures TNF-alpha dependent mechanistic information, while the ensemble built from data collected under TNF-alpha blocker treatment captures TNF-alpha independent mechanisms. In silico simulations of targeted, personalized perturbations of gene expression measures from both network ensembles identify transcripts in three broad categories. Firstly, 22 transcripts are identified to have new roles in modulating the DAS28 score; secondly, there are 6 transcripts that could be alternative targets to TNF-alpha blocker therapies, including CD86 - a component of the signaling axis targeted by Abatacept (CTLA4-Ig), and finally, 59 transcripts that are predicted to modulate the count of tender or swollen joints but not sufficiently enough to have a significant impact on DAS28.",""
"An essential component of criminal investigation involves the interrogation of large databases of information held by police and other criminal justice agencies. Data mining and decision support systems have an important role to play in assisting human inference in this forensic domain that creates one of the most challenging decision-making environments. Technologies range widely and include social network analysis, geographical information systems, and data mining technologies for clustering crimes, finding links between crime and profiling offenders, identifying criminal networks, matching crimes, generating suspects, and predicting criminal activity. This paper does not intend to cover the gamut of techniques available to the investigator of crime as this has been presented elsewhere (Oatley GC, Ewart BW, Zeleznikow J. Decision support systems for police: lessons from the application of data mining techniques to 'soft' forensic evidence. Artif Intell Law 2006, 14: 35-100). Rather, the objective is to highlight issues of implementation and interpretation of the techniques available to the crime analyst. To this end, the authors draw from their experiences of working with real-world crime databases (Oatley GC, Belem B, Fernandes K, Hoggarth E, Holland B, Lewis C, Meier P, Morgan K, Santhanam J, et al. The gang gun-crime problem-solutions from social network theory, epidemiology, cellular automata, Bayesian networks and spatial statistics. Accepted: book chapter for IEEE publication Computational Forensics; 2008; Oatley GC, McGarry K, Ewart BW. Offender network metrics. WSEAS Trans Inf Sci Appl 2006, 3: 2440-2448; Oatley GC, Ewart BW. Crimes analysis software: pins in maps, clustering and Bayes net prediction. Expert Syst Appl 2003, 25: 569-588), involving gun and gang crime, fraud, terrorism, burglary, and retail crime. (C) 2011 John Wiley & Sons, Inc. WIREs Data Mining Knowl Discov 2011 1 147-153 DOI: 10.1002/widm.6","Data mining and decision support systems have an important role to play in assisting human inference in this forensic domain that creates one of the most challenging decision-making environments."
"This paper describes the problem of public health monitoring for waterborne disease outbreaks using disparate evidence from health surveillance data streams and environmental sensors. We present a combined monitoring approach along with examples from a recent project at the Johns Hopkins University Applied Physics Laboratory in collaboration with the U. S. Environmental Protection Agency. The project objective was to build a module for the Electronic Surveillance System for the Early Notification of Community-based Epidemics (ESSENCE) to include water quality data with health indicator data for the early detection of waterborne disease outbreaks. The basic question in the fused surveillance application is 'What is the likelihood of the public health threat of interest given recent information from available sources of evidence?' For a scientific perspective, we formulate this question in terms of the estimation of positive predictive value customary in classical epidemiology, and we present a solution framework using Bayesian Networks (BN). An overview of the BN approach presents advantages, disadvantages, and required adaptations needed for a fused surveillance capability that is scalable and robust relative to the practical data environment. In the BN project, we built a top-level health/water-quality fusion BN informed by separate waterborne-disease-related networks for the detection of water contamination and human health effects. Elements of the art of developing networks appropriate to this environment are discussed with examples. Results of applying these networks to a simulated contamination scenario are presented. Copyright (C) 2011 John Wiley & Sons, Ltd.",""
"Bashari et al. (2009) propose combining state and transition models (STMs) with Bayesian networks for decision support tools where the focus is on modelling the system dynamics. There is already an extension of Bayesian networks - so-called dynamic Bayesian networks (DBNs) - for explicitly modelling systems that change over time, that has also been applied in ecological modelling. In this paper we propose a combination of STMs and DBNs that overcome some of the limitations of Bashari et al.'s approach including providing an explicit representation of the next state, while retaining its advantages, such an the explicit representation of transitions. We then show that the new model can be applied iteratively to predict into the future consistently with different time frames. We use Bashari et al.'s rangeland management problem as an illustrative case study. We present a comparative complexity analysis of the different approaches, based on the structure inherent in the problem being modelled. This analysis showed that any models that explicitly represent all the transitions only remain tractable when there are natural constraints in the domain. Thus we recommend modellers should analyse these aspects of their problem before deciding whether to use the framework. (C) 2010 Elsevier B.V. All rights reserved.",""
"This paper presents a novel student model intended to automate word-list-based reading assessments in a classroom setting, specifically for a student population that includes both native and nonnative speakers of English. As a Bayesian Network, the model is meant to conceive of student reading skills as a conscientious teacher would, incorporating cues based on expert knowledge of pronunciation variants and their cognitive or phonological sources, as well as prior knowledge of the student and the test itself. Alongside a hypothesized structure of conditional dependencies, we also propose an automatic method for refining the Bayes Net to eliminate unnecessary arcs. Reading assessment baselines that use strict pronunciation scoring alone (without other prior knowledge) achieve 0.7 correlation of their automatic scores with human assessments on the TBALL dataset. Our proposed structure significantly outperforms this baseline, and a simpler data-driven structure achieves 0.87 correlation through the use of novel features, surpassing the lower range of inter-annotator agreement. Scores estimated by this new model are also shown to exhibit the same biases along demographic lines as human listeners. Though used here for reading assessment, this model paradigm could be used in other pedagogical applications like foreign language instruction, or for inferring abstract cognitive states like categorical emotions.",""
"We consider Service-Oriented Computing (SOC) environments. Such environments are populated with services that stand proxy for a variety of information resources. A fundamental challenge in SOC is to select and compose services, to support specified user needs directly or by providing additional services. Existing approaches for service selection either fail to capture the dynamic relationships between services or assume that the environment is fully observable. In practical situations, however, consumers are often not aware of how the services are implemented. We propose two distributed trust-aware service selection approaches: one based on Bayesian networks and the other on a beta-mixture model. We experimentally validate our approach through a simulation study. Our results show that both approaches accurately punish and reward services in terms of the qualities they offer, and further that the approaches are effective despite incomplete observations regarding the services under consideration.",""
"A new structure learning approach for Bayesian networks based on asexual reproduction optimization (ARO) is proposed in this paper. ARO can be considered an evolutionary-based algorithm that mathematically models the budding mechanism of asexual reproduction. In ARO, a parent produces a bud through a reproduction operator; thereafter, the parent and its bud compete to survive according to a performance index obtained from the underlying objective function of the optimization problem: This leads to the fitter individual. The convergence measure of ARO is analyzed. The proposed method is applied to real-world and benchmark applications, while its effectiveness is demonstrated through computer simulations. Results of simulations show that ARO outperforms genetic algorithm (GA) because ARO results in a good structure and fast convergence rate in comparison with GA.",""
"In this paper, we address the problem of learning discrete Bayesian networks from noisy data. A graphical model based on a mixture of Gaussian distributions with categorical mixing structure coming from a discrete Bayesian network is considered. The network learning is formulated as a maximum likelihood estimation problem and performed by employing an EM algorithm. The proposed approach is relevant to a variety of statistical problems for which Bayesian network models are suitable from simple regression analysis to learning gene/protein regulatory networks from microarray data. (C) 2010 Elsevier B.V. All rights reserved.","The proposed approach is relevant to a variety of statistical problems for which Bayesian network models are suitable from simple regression analysis to learning gene/protein regulatory networks from microarray data."
"Multimodality therapy in selected patients with peritoneal carcinomatosis is gaining acceptance. Treatment-directing decision support tools are needed to individualize care and select patients best suited for cytoreductive surgery +/- hyperthermic intraperitoneal chemotherapy (CRS +/- HEPEC). The purpose of this study is to develop a predictive model that could support surgical decisions in patients with colon carcinomatosis. Fifty-three patients were enrolled in a prospective study collecting 31 clinical-pathological, treatment-related, and outcome data. The population was characterized by disease presentation, performance status, extent of peritoneal cancer (Peritoneal Cancer Index, PCI), primary tumor histology, and nodal staging. These preoperative parameters were analyzed using step-wise machine-learned Bayesian Belief Networks (BBN) to develop a predictive model for overall survival (OS) in patients considered for CRS +/- HIPEC. Area-under-the-curve from receiver-operating-characteristics curves of OS predictions was calculated to determine the model's positive and negative predictive value. Model structure defined three predictors of OS: severity of symptoms (performance status), PCI, and ability to undergo CRS +/- HPEC. Patients with PCI < 10, resectable disease, and excellent performance status who underwent CRS HIPEC had 89 per cent probability of survival compared with 4 per cent for those with poor performance status, PCI > 20, who were not considered surgical candidates. Cross validation of the BBN model robustly classified OS (area-under-the-curve = 0.71). The model's positive predictive value and negative predictive value are 63.3 per cent and 68.3 per cent, respectively. This exploratory study supports the utility of Bayesian classification for developing decision support tools, which assess case-specific relative risk for a given patient for oncological outcomes based on clinically relevant classifiers of survival. Further prospective studies to validate the BBN model-derived prognostic assessment tool are warranted.","Cross validation of the BBN model robustly classified OS (area-under-the-curve = 0."
"While identifying the intention of an utterance has played a major role in natural language understanding, this work is the first to extend intention recognition to the domain of information graphics. A tenet of this work is the belief that information graphics are a form of language. This is supported by the observation that the overwhelming majority of information graphics from popular media sources appear to have some underlying goal or intended message. As Clark noted, language is more than just words. It is any \"signal\" (or lack of signal when one is expected), where a signal is a deliberate action that is intended to convey a message (Clark, 1996 [15]). As a form of language, information graphics contain communicative signals that can be used in a computational system to identify the message that the graphic conveys. We identify the communicative signals that appear in simple bar charts, and present an implemented Bayesian network methodology for reasoning about these signals and hypothesizing a bar chart's intended message. Once the message conveyed by an information graphic has been inferred, it can then be used to facilitate access to this information resource for a variety of users, including 1) users of digital libraries, 2) visually impaired users, and 3) users of devices where graphics are impractical or inaccessible. (C) 2010 Elsevier B.V. All rights reserved.",""
"The paper focuses on developing effective importance sampling algorithms for mixed probabilistic and deterministic graphical models. The use of importance sampling in such graphical models is problematic because it generates many useless zero weight samples which are rejected yielding an inefficient sampling process. To address this rejection problem, we propose the SampleSearch scheme that augments sampling with systematic constraint-based backtracking search. We characterize the bias introduced by the combination of search with sampling, and derive a weighting scheme which yields an unbiased estimate of the desired statistics (e.g., probability of evidence). When computing the weights exactly is too complex, we propose an approximation which has a weaker guarantee of asymptotic unbiasedness. We present results of an extensive empirical evaluation demonstrating that SampleSearch outperforms other schemes in presence of significant amount of determinism. (C) 2010 Elsevier B.V. All rights reserved.",""
"Due to the interannual variability of wind speed a feasibility analysis for the installation of a Wind Energy Conversion System at a particular site requires estimation of the long-term mean wind turbine energy output. A method is proposed in this paper which, based on probabilistic Bayesian networks (BNs), enables estimation of the long-term mean wind speed histogram for a site where few measurements of the wind resource are available. For this purpose, the proposed method allows the use of multiple reference stations with a long history of wind speed and wind direction measurements. That is to say, the model that is proposed in this paper is able to involve and make use of regional information about the wind resource. With the estimated long-term wind speed histogram and the power curve of a wind turbine it is possible to use the method of bins to determine the long-term mean energy output for that wind turbine. The intelligent system employed, the knowledgebase of which is a joint probability function of all the model variables, uses efficient calculation techniques for conditional probabilities to perform the reasoning. This enables automatic model learning and inference to be performed efficiently based on the available evidence. The proposed model is applied in this paper to wind speeds and wind directions recorded at four weather stations located in the Canary Islands (Spain). Ten years of mean hourly wind speed and direction data are available for these stations. One of the conclusions reached is that the BN with three reference stations gave fewer errors between the real and estimated long-term mean wind turbine energy output than when using two measure-correlate-predict algorithms which were evaluated and which use a linear regression between the candidate station and one reference station. (C) 2010 Elsevier Ltd. All rights reserved.","This enables automatic model learning and inference to be performed efficiently based on the available evidence."
"In this paper, we apply an immune-inspired approach to design ensembles of heterogeneous neural networks for classification problems. Our proposal, called Bayesian artificial immune system, is an estimation of distribution algorithm that replaces the traditional mutation and cloning operators with a probabilistic model, more specifically a Bayesian network, representing the joint distribution of promising solutions. Among the additional attributes provided by the Bayesian framework inserted into an immune-inspired search algorithm are the automatic control of the population size along the search and the inherent ability to promote and preserve diversity among the candidate solutions. Both are attributes generally absent from alternative estimation of distribution algorithms, and both were shown to be useful attributes when implementing the generation and selection of components of the ensemble, thus leading to high-performance classifiers. Several aspects of the design are illustrated in practical applications, including a comparative analysis with other attempts to synthesize ensembles.","In this paper, we apply an immune-inspired approach to design ensembles of heterogeneous neural networks for classification problems."
"Recently, various personal information in daily life is stored in mobile devices with sensors. This information reflects heterogeneous aspects of personal life history. People have a tendency to record precious memories from the information in their life. However, it is difficult to extract and summarize the memories from the information. There are many useful traditional ways, such as photograph, video and diary, to record important memories. Especially, writing a diary is beloved as an effective method for a long time because of its effectiveness, remembrance, and empathy of storytelling. This paper proposes a Petri-net based method that organizes mobile contexts to an understandable and interesting story in cartoons. Petri-net based storytelling approach reduces the uncertainty in mobile environment and increases the diversity and causality of a story. A generated story from mobile contexts is compared with personal life history for confirming the usefulness. Also, it is compared with the other method in previous work.",""
"Bayesian networks are a powerful and increasingly popular tool for reasoning under uncertainty, offering intuitive insight into (probabilistic) data-generating processes. They have been successfully applied to many different fields, including bioinformatics. In this paper, Bayesian networks are used to model the joint-probability distribution of selected earthquake, site, and ground-motion parameters. This provides a probabilistic representation of the independencies and dependencies between these variables. In particular, contrary to classical regression, Bayesian networks do not distinguish between target and predictors, treating each variable as random variable. The capability of Bayesian networks to model the ground-motion domain in probabilistic seismic hazard analysis is shown for a generic situation. A Bayesian network is learned based on a subset of the Next Generation Attenuation (NGA) dataset, using 3342 records from 154 earthquakes. Because no prior assumptions about dependencies between particular parameters are made, the learned network displays the most probable model given the data. The learned network shows that the ground-motion parameter (horizontal peak ground acceleration, PGA) is directly connected only to the moment magnitude, Joyner-Boore distance, fault mechanism, source-to-site azimuth, and depth to a shear-wave horizon of 2: 5 km/s (Z2.5). In particular, the effect of V-S30 is mediated by Z2.5. Comparisons of the PGA distributions based on the Bayesian networks with the NGA model of Boore and Atkinson (2008) show a reasonable agreement in ranges of good data coverage.","In particular, contrary to classical regression, Bayesian networks do not distinguish between target and predictors, treating each variable as random variable."
"A fraudulent financial statement involves the intentional furnishing and/or publishing of false information in it and this has become a severe economic and social problem. We consider Data Mining (DM) based financial fraud detection techniques (such as regression, decision tree, neural networks and Bayesian networks) that help identify fraud. The effectiveness of these DM methods (and their limitations) is examined, especially when new schemes of financial statement fraud adapt to the detection techniques. We then explore a self-adaptive framework (based on a response surface model) with domain knowledge to detect financial statement fraud. We conclude by suggesting that, in an era with evolutionary financial frauds, computer assisted automated fraud detection mechanisms will be more effective and efficient with specialized domain knowledge. (C) 2010 Elsevier B.V. All rights reserved.","We consider Data Mining (DM) based financial fraud detection techniques (such as regression, decision tree, neural networks and Bayesian networks) that help identify fraud."
"Aim A relative excess of fat in the upper body region has been proven to be associated with increased coronary artery disease (CAD) risk. Dual-energy X-ray absorptiometry (DXA) is probably the most accurate and precise method available to study fat regional distribution and to directly measure total body fat and lean soft tissue mass. However, while several studies have investigated the abilities of obesity anthropometric measures in predicting CAD, only few studies have evaluated DXA as CAD predictor; particularly, a comparison between a model including information coming from anthropometric measurements and a model in which fat is precisely measured by DXA, is still lacking. In order to verify if CAD severity, as measured by Gensini score, is better predicted when a prognostic model includes DXA measurements rather than anthropometric measures, we compared performance obtained by two Bayesian Networks (BNs) including standard anthropometric measures and DXA, respectively. Methods Data come from 58 consecutive patients, 79% of them having suspected and 21% known CAD. Two BNs were implemented: input variables include anamnestic information, biochemical data and obesity measures. In the first model (BN1) obesity was measured by body mass index and waist-to-hip ratio, while in the second one (BN2) it is quantified by DXA-derived parameters. Results Network graphs and results coming from sensitivity analysis show that in both models lipoproteins and biomarkers of inflammation act as proximal node, while obesity (independently of the chosen measure) appears to be a distal node acting by the intermediation of other variables. Both models show high predictive abilities, the mean percentage classification errors being, respectively, 14.13 and 18.87. Conclusions In our study, the BN predictive ability is slightly superior when obesity is measured using anthropometric data instead of DXA measurements. The reason probably relies on the fact that in the BN the obesity role in predicting CAD is mediated by the action of other factors that appear to be more directly influencing the outcome. Thus, the necessity to dispose of a perfect measure becomes less compulsory and the huge effort to precisely estimate body composition with complex methods as DXA could be avoided when using expert system such as BN as predictive tool.","Both models show high predictive abilities, the mean percentage classification errors being, respectively, 14."
"Complex artifacts are designed today from well specified and well modeled components. But most often, the models of these components cannot be composed into a global functional model of the artifact. A significant observation, modeling and identification effort is required to get such a global model, which is needed in order to better understand, control and improve the designed artifact. Robotics provides a good illustration of this need. Autonomous robots are able to achieve more and more complex tasks, relying on more advanced sensor-motor functions. To better understand their behavior and improve their performance, it becomes necessary but more difficult to characterize and to model, at the global level, how robots behave in a given environment. Low-level models of sensors, actuators and controllers cannot be easily combined into a behavior model. Sometimes high level models operators used for planning are also available, but generally they are too coarse to represent the actual robot behavior. We propose here a general framework for learning from observation data the behavior model of a robot when performing a given task. The behavior is modeled as a Dynamic Bayesian Network, a convenient stochastic structured representations. We show how such a probabilistic model can be learned and how it can be used to improve, on line, the robot behavior with respect to a specific environment and user preferences. Framework and algorithms are detailed; they are substantiated by experimental results for autonomous navigation tasks.",""
"We develop a Bayesian network (BN) model that describes estuarine chlorophyll dynamics in the upper section of the Neuse River Estuary in North Carolina, using automated constraint based structure learning algorithms. We examine the functionality and usefulness of the structure learning algorithms in building model topology with real-time data under different scenarios. Generated BN models are evaluated and a final model is selected. Model results indicate that although the effect of water temperature and river flow on chlorophyll dynamics has remained unchanged following the implementation of the nitrogen Total Maximum Daily Load (TMDL) program; the response of chlorophyll levels to nutrient concentrations has been altered. The results stress the importance of incorporating expert defined constraints and links in conjunction with the automated structure learning algorithms to generate more plausible structures and minimize the sensitivity of the learning algorithms. This hybrid approach towards structure learning allows for the incorporation of existing knowledge while limiting the scope of the learning algorithms to defining the links between environmental variables for which the expert has little or no information. (C) 2010 Elsevier Ltd. All rights reserved.",""
"Background: Cytogenetic abnormalities occur at an early stage of bladder urothelial carcinomas (BUC), and their frequency increases as the cancer becomes more advanced. Objective: To assess the diagnostic performance of a test based on cytogenetic abnormalities to diagnose, stage, and grade BUC from the urine. Design, setting, and participants: We used a 341 bacterial artificial chromosome (BAC) comparative genomic hybridisation (CGH)-array chip (BCA-1) designed to include loci affected in BUC. The chip was first used on 32 frozen BUC biopsies to design staging (BN0) and grading(BN1 and BN2) prediction models based on Bayesian networks analysis. The models were then validated on external data obtained from 98 tumour samples using a 2464 BAC CGH-array chip. The performance of the test was finally assessed on 44 urine pellets collected, including 22 patients who had BUC and 22 controls. Measurements: We measured sensitivity and specificity to diagnose BUC stage and grade from urine pellets. Results and limitations: In the urine, BCA-1 test sensitivity was 95%, specificity was 86%, and accuracy was 91%. The BN0 staging model identified T1-4 tumours in the urine with a sensitivity of 90%, a specificity of 83%, and an accuracy of 87%. The BN1 and BN2 grading models detected high-grade disease with a sensitivity, specificity, and accuracy of 86%, 88%, and 87%, respectively, using BN1 and 100%, 63%, and 82%, respectively, using BN2. BN models performed with similar sensitivity but reduced specificity using the external data. BCA-1 failed to produce results for eight additional samples (failure rate: 9%). The test needed high quantities and quality of DNA, and external validation in larger, prospective, and better-designed studies is necessary to confirm feasibility and performance. Conclusions: The BCA-1 mini-CGH-array chip detected BUC in urine with a high diagnostic performance. It could also accurately discriminate low-grade from high-grade tumours and, to a lesser extent, lamina propria-invasive tumours from pTa tumours. (C) 2010 European Association of Urology. Published by Elsevier B. V. All rights reserved.",""
"Estimating the contribution of the forests to carbon sequestration is commonly done by applying forest growth models. Such models inherently use field observations such as leaf area index (LAI), whereas a relevant information is also available from remotely sensed images. This paper aims to improve the LAI estimated from the forest growth model [physiological principals predicting growth (3-PG)] by combining these values with the LAI derived from the Moderate Resolution Imaging Spectrora-diometer (MODIS) satellite imagery. A Bayesian networks (BNs) approach addresses the bias in the 3-PG model and the noise of the MODIS images. A novel inference strategy within the BN has been developed in this paper to take care of the different structures of the inaccuracies in the two data sources. The BN is applied to the Speulderbos forest in The Netherlands, where the detailed data were available. This paper shows that the outputs obtained with the BN were more accurate than either the 3-PG or the MODIS estimate. It was also found that the BN is more sensitive to the variation of the LAI derived from MODIS than to the variation of the LAI 3-PG values. In this paper, we conclude that the BNs can improve the estimation of the LAI values by combining a forest growth model with satellite imagery.","A novel inference strategy within the BN has been developed in this paper to take care of the different structures of the inaccuracies in the two data sources."
"For hard computational problems, stochastic local search has proven to be a competitive approach to finding optimal or approximately optimal problem solutions. Two key research questions for stochastic local search algorithms are: Which algorithms are effective for initialization? When should the search process be restarted? In the present work, we investigate these research questions in the context of approximate computation of most probable explanations (MPEs) in Bayesian networks (BNs). We introduce a novel approach, based on the Viterbi algorithm, to explanation initialization in BNs. While the Viterbi algorithm works on sequences and trees, our approach works on BNs with arbitrary topologies. We also give a novel formalization of stochastic local search, with focus on initialization and restart, using probability theory and mixture models. Experimentally, we apply our methods to the problem of MPE computation, using a stochastic local search algorithm known as Stochastic Greedy Search. By carefully optimizing both initialization and restart, we reduce the MPE search time for application BNs by several orders of magnitude compared to using uniform at random initialization without restart. On several BNs from applications, the performance of Stochastic Greedy Search is competitive with clique tree clustering, a state-of-the-art exact algorithm used for MPE computation in BNs.",""
"Fault trees and event trees have for decades been the most commonly applied modelling tools in both risk analysis in general and the risk analysis of hydrogen applications including infrastructure in particular. It is sometimes found challenging to make traditional Quantitative Risk Analyses sufficiently transparent and it is frequently challenging for outsiders to verify the probabilistic modelling. Bayesian Networks (BN) are a graphical representation of uncertain quantities and decisions that explicitly reveal the probabilistic dependence between the variables and the related information flow. It has been suggested that BN represent a modelling tool that is superior to both fault trees and event trees with respect to the structuring and modelling of large complex systems. This paper gives an introduction to BN and utilises a case study as a basis for discussing and demonstrating the suitability of EN for modelling the risks associated with the introduction of hydrogen as an energy carrier. In this study we explore the benefits of modelling a hydrogen refuelling station using BN. The study takes its point of departure in input from a traditional detailed Quantitative Risk Analysis conducted by DNV during the HyApproval project. We compare and discuss the two analyses with respect to their advantages and disadvantages. We especially focus on a comparison of transparency and the results that may be extracted from the two alternative procedures. (C) 2010 Professor T. Nejat Veziroglu. Published by Elsevier Ltd. All rights reserved.",""
"Wave Height (WH) is one of the most important factors in design and operation of maritime projects. Different methods such as semi-empirical, numerical and soft computing-based approaches have been developed for WH forecasting. The soft computing-based methods have the ability to approximate nonlinear wind-wave and wave-wave interactions without a prior knowledge about them. In the present study, several soft computing-based models, namely Support Vector Machines (SVMs), Bayesian Networks (BNs), Artificial Neural Networks (ANNs) and Adaptive Neuro-Fuzzy Inference System (ANFIS) are used for mapping wind data to wave height. The data set used for training and testing the simulation models comprises the WH and wind data gathered by National Data Buoy Center (NDBC) in Lake Superior, USA. Several statistical indices are used to evaluate the efficacy of the aforementioned methods. The results show that the ANN, ANFIS and SVM can provide acceptable predictions for wave heights, while the BNs results are unreliable. (C) 2010 Elsevier Ltd. All rights reserved.","In the present study, several soft computing-based models, namely Support Vector Machines (SVMs), Bayesian Networks (BNs), Artificial Neural Networks (ANNs) and Adaptive Neuro-Fuzzy Inference System (ANFIS) are used for mapping wind data to wave height."
"This paper presents and discusses the use of Bayesian procedures - introduced through the use of Bayesian networks in Part I of this series of papers - for 'learning' probabilities from data. The discussion will relate to a set of real data on characteristics of black toners commonly used in printing and copying devices. Particular attention is drawn to the incorporation of the proposed procedures as an integral part in probabilistic inference schemes (notably in the form of Bayesian networks) that are intended to address uncertainties related to particular propositions of interest (e. g., whether or not a sample originates from a particular source). The conceptual tenets of the proposed methodologies are presented along with aspects of their practical implementation using currently available Bayesian network software. (C) 2010 Elsevier Ireland Ltd. All rights reserved.","Particular attention is drawn to the incorporation of the proposed procedures as an integral part in probabilistic inference schemes (notably in the form of Bayesian networks) that are intended to address uncertainties related to particular propositions of interest (e."
"Accurately predicting the localization of proteins is of paramount importance in the quest to determine their respective functions within the cellular compartment. Because of the continuous and rapid progress in the fields of genomics and proteomics, more data are available now than ever before. Coincidentally, data mining methods been developed and refined in order to handle this experimental windfall, thus allowing the scientific community to quantitatively address long-standing questions such as that of protein localization. Here, we develop a frequent pattern tree (FPT) approach to generate a minimum set of rules (mFPT) for predicting protein localization. We acquire a series of rules according to the features of yeast genomic data. The mFPT prediction accuracy is benchmarked against other commonly used methods such as Bayesian networks and logistic regression under various statistical measures. Our results show that mFPT gave better performance than other approaches in predicting protein localization. Meanwhile, setting 0.65 as the minimum hit-rate, we obtained 138 proteins that mFPT predicted differently than the simple naive bayesian method (SNB). In our analysis of these 138 proteins, we present novel predictions for the location for 17 proteins, which currently do not have any defined localization. These predictions can serve as putative annotations and should provide preliminary clues for experimentalists. We also compared our predictions against the eukaryotic subcellular localization database and related predictions by others on protein localization. Our method is quite generalized and can thus be applied to discover the underlying rules for protein-protein interactions, genomic interactions, and structure-function relationships, as well as those of other fields of research.","The mFPT prediction accuracy is benchmarked against other commonly used methods such as Bayesian networks and logistic regression under various statistical measures."
"Motivation: Targeted interventions in combination with the measurement of secondary effects can be used to computationally reverse engineer features of upstream non-transcriptional signaling cascades. Nested effect models (NEMs) have been introduced as a statistical approach to estimate the upstream signal flow from downstream nested subset structure of perturbation effects. The method was substantially extended later on by several authors and successfully applied to various datasets. The connection of NEMs to Bayesian Networks and factor graph models has been highlighted. Results: Here, we introduce a computationally attractive extension of NEMs that enables the analysis of perturbation time series data, hence allowing to discriminate between direct and indirect signaling and to resolve feedback loops.",""
"Maritime traffic poses a major threat to marine ecosystems in the form of oil spills. The Gulf of Finland, the easternmost part of the Baltic Sea, has witnessed a rapid increase in oil transportation during the last 15 years. Should a spill occur, the negative ecological impacts may be reduced by oil combating, the effectiveness of which is, however, strongly dependent on prevailing environmental conditions and available technical resources. This poses increased uncertainty related to ecological consequences of future spills. We developed a probabilistic Bayesian network model that can be used to assess the effectiveness of different oil combating strategies in minimizing the negative effects of oil on six species living in the Gulf of Finland. The model can be used for creating different accident scenarios and assessing the performance of various oil combating actions under uncertainty, which enables its use as a supportive tool in decision-making. While the model is confined to the western Gulf of Finland, the methodology is adaptable to other marine areas facing similar risks and challenges related to oil spills. (C) 2010 Elsevier B.V. All rights reserved.",""
"Background: The combination of genotypic and genome-wide expression data arising from segregating populations offers an unprecedented opportunity to model and dissect complex phenotypes. The immense potential offered by these data derives from the fact that genotypic variation is the sole source of perturbation and can therefore be used to reconcile changes in gene expression programs with the parental genotypes. To date, several methodologies have been developed for modeling eQTL data. These methods generally leverage genotypic data to resolve causal relationships among gene pairs implicated as associates in the expression data. In particular, leading studies have augmented Bayesian networks with genotypic data, providing a powerful framework for learning and modeling causal relationships. While these initial efforts have provided promising results, one major drawback associated with these methods is that they are generally limited to resolving causal orderings for transcripts most proximal to the genomic loci. In this manuscript, we present a probabilistic method capable of learning the causal relationships between transcripts at all levels in the network. We use the information provided by our method as a prior for Bayesian network structure learning, resulting in enhanced performance for gene network reconstruction. Results: Using established protocols to synthesize eQTL networks and corresponding data, we show that our method achieves improved performance over existing leading methods. For the goal of gene network reconstruction, our method achieves improvements in recall ranging from 20% to 90% across a broad range of precision levels and for datasets of varying sample sizes. Additionally, we show that the learned networks can be utilized for expression quantitative trait loci mapping, resulting in upwards of 10-fold increases in recall over traditional univariate mapping. Conclusions: Using the information from our method as a prior for Bayesian network structure learning yields large improvements in accuracy for the tasks of gene network reconstruction and expression quantitative trait loci mapping. In particular, our method is effective for establishing causal relationships between transcripts located both proximally and distally from genomic loci.",""
"One of the fundamental requirements for visual surveillance with Visual Sensor Networks (VSN) is the correct association of camera's observations with the tracks of objects under tracking. In this paper, we model the data association in VSN as an inference problem on dynamic Bayesian networks (DBN) and investigate the key problems for efficient data association in case of missing detection. Firstly, to deal with the problem of missing detection, we introduce a set of random variables, namely routine variables, into the DBN model to describe the uncertainty in the path taken by the moving objects and propose the high-order spatio-temporal model based inference algorithm. Secondly, for the problem of computational intractability of exact inference, we derive two approximate inference algorithms by factorizing the belief state based on the marginal and conditional independence assumptions. Thirdly, we incorporate the inference algorithm into EM framework to make the algorithm suitable for the case when object appearance parameters are unknown. Simulation and experimental results demonstrate the effect of the proposed methods.","In this paper, we model the data association in VSN as an inference problem on dynamic Bayesian networks (DBN) and investigate the key problems for efficient data association in case of missing detection."
"With the recent proliferation of smart phones, they become useful tools to implement high-confidence cyber-physical systems. Among many applications, context sharing systems in mobile environment attract attention with the popularization of social media. Mobile context sharing systems can share more information than web-based social network services because they can use a variety of information from mobile sensors. To share high-level contexts such as activity, emotion, and user relationship, a user had to annotate them manually in previous works. This paper proposes a mobile context sharing system that can recognize high-level contexts automatically by using Bayesian networks based on mobile logs. We have developed a ContextViewer application which consists of a phonebook and amap browser to show the feasibility of the system. Experiments of evaluating Bayesian networks and performing the SUS test confirm that the proposed system is useful.",""
"A major inference task in Bayesian networks is explaining why some variables are observed in their particular states using a set of target variables. Existing methods for solving this problem often generate explanations that are either too simple (underspecified) or too complex (overspecified). In this paper, we introduce a method called Most Relevant Explanation (MRE) which finds a partial instantiation of the target variables that maximizes the generalized Bayes factor (GBF) as the best explanation for the given evidence. Our study shows that GBF has several theoretical properties that enable MRE to automatically identify the most relevant target variables in forming its explanation. In particular, conditional Bayes factor (CBF), defined as the GBF of a new explanation conditioned on an existing explanation, provides a soft measure on the degree of relevance of the variables in the new explanation in explaining the evidence given the existing explanation. As a result, MRE is able to automatically prune less relevant variables from its explanation. We also show that CBF is able to capture well the explaining-away phenomenon that is often represented in Bayesian networks. Moreover, we define two dominance relations between the candidate solutions and use the relations to generalize MRE to find a set of top explanations that is both diverse and representative. Case studies on several benchmark diagnostic Bayesian networks show that MRE is often able to find explanatory hypotheses that are not only precise but also concise.","A major inference task in Bayesian networks is explaining why some variables are observed in their particular states using a set of target variables."
"Current design of improved product generations does not exploit use information from previous products systematically. The emerging shift of manufacturing companies from selling products to providing product service systems, the miniaturization of product-embedded sensors, as well as advances in information technology facilitate a product providers access to operation information of current products, which can be used to improve the development and the quality of follower product generations. The paper presents a framework for the acquisition, aggregation and analysis of product use information as well as for the generation and provision of knowledge for the development of improved product generations. The described approach employs knowledge discovery methods like Bayesian Networks and is supported by an IT prototype of a design assistant system. This prototype has been validated in a use case considering the improvement of a rotary spindle for micro machining. (C) 2011 CIRP.",""
"This paper aims to propose a novel network model, probabilistic Boolean networks (PBN), for supply chain reasoning. We demonstrate how relationships between the variables can be learned from their behaviors and discuss the equivalence between dynamic Bayesian networks (DBN). Compared with the findings of previous investigations, this work emphasizes the advantages of PBN and its roles in leaning DBN in complex temporal settings. Significance: This work proposes a novel network model PBN for supply chain reasoning. This paper demonstrate the advantages of PBN and its roles in leaning DBN in complex temporal settings.",""
"Abductive reasoning (or abduction) is the process of inferring hypotheses from observed data using a certain 'knowledge' encoded in the form of inference rules (or causal relations). Many important kinds of intellectual tasks, including medical diagnosis, fault diagnosis, scientific discovery, legal reasoning, and natural language understanding have been characterised as abduction. Unfortunately, abduction is NP-hard. Genetic algorithms and biologically motivated computational paradigms inspired by the natural evolution turned out to be efficient in solving many hard problems while other existing approaches failed to solve in general. In this article, we present a genetic algorithm called HAKIM, for solving abduction problems. We encode an explanation in a chromosome-like structure, where every gene models a possible single hypothesis. Thereafter, we develop a fitness function that characterises the overall 'quality' of a chromosome representing an explanation; and then use standard genetic operators to compute a set of hypotheses that best explains the observed data. Simulation results on large-scale medical problems reveal the good performance of our model HAKIM.","Abductive reasoning (or abduction) is the process of inferring hypotheses from observed data using a certain 'knowledge' encoded in the form of inference rules (or causal relations)."
"Bayesian networks are a popular model for reasoning under uncertainty. We study the problem of efficient probabilistic inference with these models when some of the conditional probability tables represent deterministic or noisy l-out-of-k functions. These tables appear naturally in real-world applications when we observe a state of a variable that depends on its parents via an addition or noisy addition relation. We provide a lower bound of the rank and an upper bound for the symmetric border rank of tensors representing l-out-of-k functions. We propose an approximation of tensors representing noisy l-out-of-k functions by. a sum of r tensors of rank one, where r is an upper bound of the symmetric border rank of the approximated tensor. We applied the suggested approximation to probabilistic inference in probabilistic graphical models. Numerical experiments reveal that we can get a gain in the order of two magnitudes but at the expense of a certain loss of precision.","We study the problem of efficient probabilistic inference with these models when some of the conditional probability tables represent deterministic or noisy l-out-of-k functions."
"Graphical Markov models, most of all Bayesian networks, have become a very popular way for multidimensional probability distribution representation and processing. What makes representation of a very-high-dimensional probability distribution possible is its independence structure, i.e. a system of conditional independence relations valid for the distribution in question. The fact that some of independence systems can be successfully represented with the help of graphs is reflected in the general title: graphical modelling. However, graphical representation of independence structures is also associated with some disadvantages: only a small part of different independence structures can be faithfully represented by graphs; and still one structure is usually equally well represented by several graphs. These reasons, among others, initiated development of an alternative approach, called here theory of compositional models, which enables us to represent exactly the same class of distributions as Bayesian networks. This paper is a survey of the most important basic concepts and results concerning compositional models necessary for reading advanced papers on computational procedures and other aspects connected with this (relatively new) approach for multidimensional distribution representation.",""
"Learning Markov boundaries from data without having to learn a Bayesian network first can be viewed as a feature subset selection problem and has received much attention due to its significance in the wide applications of AI techniques. Popular constraint based methods suffer from high computational complexity and are usually unstable in spaces of high dimensionality. We propose a new perspective from matroid theory towards the discovery of Markov boundaries of random variable in the domain, and develop a learning algorithm which guarantees to recover the true Markov boundaries by a greedy learning algorithm. Then we use the precision matrix of the original distribution as a measure of independence to make our algorithm feasible in large scale problems, which is essentially an approximation of the probabilistic relations with Gaussians and can find possible variables in Markov boundaries with low computational complexity. Experimental results on standard Bayesian networks show that our analysis and approximation can efficiently and accurately identify Markov boundaries in complex networks from data.",""
"This study explored children's participation in recreational (physical) activities and the extent to which this participation was influenced by individual and household socio-demographics and characteristics of the social and physical environment. Travel and activity diaries were used to collect data on out-of-home recreational activities for a random sample of 4,293 children in primary schools in the Netherlands. These data were investigated in relation to measures describing the social and physical living environment. Specifically, a Bayesian belief network was proposed because it derives and represents simultaneously all direct and indirect relationships between the selected variables. Results indicated that participation in various types of recreational activities was directly related to the socio-economic status of the household, the perceived safety of the neighborhood, the size of agricultural area in the neighborhood, travel distance, and day of the week. Planners and designers are recommended to find a good land use mix, and specifically make sure that they focus their attention on safety issues, as these factors stimulate children's participation in recreational physical activities.",""
"Previous studies have demonstrated that encoding a Bayesian network into a SAT formula and then performing weighted model counting using a backtracking search algorithm can be an effective method for exact inference. In this paper, we present techniques for improving this approach for Bayesian networks with noisy-OR and noisy-MAX relations-two relations that are widely used in practice as they can dramatically reduce the number of probabilities one needs to specify. In particular, we present two SAT encodings for noisy-OR and two encodings for noisy-MAX that exploit the structure or semantics of the relations to improve both time and space efficiency, and we prove the correctness of the encodings. We experimentally evaluated our techniques on large-scale real and randomly generated Bayesian networks. On these benchmarks, our techniques gave speedups of up to two orders of magnitude over the best previous approaches for networks with noisy-OR/MAX relations and scaled up to larger networks. As well, our techniques extend the weighted model counting approach for exact inference to networks that were previously intractable for the approach.","Previous studies have demonstrated that encoding a Bayesian network into a SAT formula and then performing weighted model counting using a backtracking search algorithm can be an effective method for exact inference."
"Within a universal agent-world interaction framework, based on Information Theory and Causal Bayesian Networks, we demonstrate how every agent that needs to acquire relevant information in regard to its strategy selection will automatically inject part of this information back into the environment. We introduce the concept of 'Digested Information' which both quantifies, and explains this phenomenon. Based on the properties of digested information, especially the high density of relevant information in other agents actions, we outline how this could motivate the development of low level social interaction mechanisms, such as the ability to detect other agents.",""
"Learning Bayesian networks is known to be an NP-hard problem and that is the reason why the application of a heuristic search has proven advantageous in many domains. This learning approach is computationally efficient and, even though it does not guarantee an optimal result, many previous studies have shown that it obtains very good solutions. Hill climbing algorithms are particularly popular because of their good trade-off between computational demands and the quality of the models learned. In spite of this efficiency, when it comes to dealing with high-dimensional datasets, these algorithms can be improved upon, and this is the goal of this paper. Thus, we present an approach to improve hill climbing algorithms based on dynamically restricting the candidate solutions to be evaluated during the search process. This proposal, dynamic restriction, is new because other studies available in the literature about restricted search in the literature are based on two stages rather than only one as it is presented here. In addition to the aforementioned advantages of hill climbing algorithms, we show that under certain conditions the model they return is a minimal I-map of the joint probability distribution underlying the training data, which is a nice theoretical property with practical implications. In this paper we provided theoretical results that guarantee that, under these same conditions, the proposed algorithms also output a minimal I-map. Furthermore, we experimentally test the proposed algorithms over a set of different domains, some of them quite large (up to 800 variables), in order to study their behavior in practice.",""
"To obtain location information, localization methods employing Received Signal Strength (RSS) are attractive since it can reuse the existing wireless infrastructure. Among the large class of localization schemes, RSS-based lateration methods have the advantage of providing closed-form solutions for mathematical analysis. However, the localization accuracy of lateration methods employing RSS is significantly affected by the unpredictable setup in indoors. To improve the applicability of RSS-based lateration methods in indoors, we develop two schemes: regression-based and correlation-based. The regression-based approach uses linear regression to discover a better fit of signal propagation model between RSS and the distance, while the correlation-based approach utilizes the correlation among RSS in local area to obtain more accurate signal propagation. Our results using both simulation as well as real experiments demonstrate that our improved methods outperform the original RSS-based lateration methods significantly, and can achieve comparable or even better accuracy than heuristic-based algorithms such as RADAR and Bayesian Networks, which are prevalent in indoors.","To improve the applicability of RSS-based lateration methods in indoors, we develop two schemes: regression-based and correlation-based."
"Portfolio methods support the combination of different algorithms and heuristics, including stochastic local search (SLS) heuristics, and have been identified as a promising approach to solve computationally hard problems. While successful in experiments, theoretical foundations and analytical results for portfolio-based SLS heuristics are less developed. This article aims to improve the understanding of the role of portfolios of heuristics in SLS. We emphasize the problem of computing most probable explanations (MPEs) in Bayesian networks (BNs). Algorithmically, we discuss a portfolio-based SLS algorithm for MPE computation, Stochastic Greedy Search (SGS). SGS supports the integration of different initialization operators (or initialization heuristics) and different search operators (greedy and noisy heuristics), thereby enabling new analytical and experimental results. Analytically, we introduce a novel Markov chain model tailored to portfolio-based SLS algorithms including SGS, thereby enabling us to analytically form expected hitting time results that explain empirical run time results. For a specific BN, we show the benefit of using a homogenous initialization portfolio. To further illustrate the portfolio approach, we consider novel additive search heuristics for handling determinism in the form of zero entries in conditional probability tables in BNs. Our additive approach adds rather than multiplies probabilities when computing the utility of an explanation. We motivate the additive measure by studying the dramatic impact of zero entries in conditional probability tables on the number of zero-probability explanations, which again complicates the search process. We consider the relationship between MAXSAT and MPE, and show that additive utility (or gain) is a generalization, to the probabilistic setting, of MAXSAT utility (or gain) used in the celebrated GSAT and WalkSAT algorithms and their descendants. Utilizing our Markov chain framework, we show that expected hitting time is a rational function-i.e. a ratio of two polynomials of the probability of applying an additive search operator. Experimentally, we report on synthetically generated BNs as well as BNs from applications, and compare SGS's performance to that of Hugin, which performs BN inference by compilation to and propagation in clique trees. On synthetic networks, SGS speeds up computation by approximately two orders of magnitude compared to Hugin. In application networks, our approach is highly competitive in Bayesian networks with a high degree of determinism. In addition to showing that stochastic local search can be competitive with clique tree clustering, our empirical results provide an improved understanding of the circumstances under which portfolio-based SLS outperforms clique tree clustering and vice versa.","Experimentally, we report on synthetically generated BNs as well as BNs from applications, and compare SGS's performance to that of Hugin, which performs BN inference by compilation to and propagation in clique trees."
"Bayesian networks have been widely used as knowledge models in business, engineering, biomedicine, and so on. When a network is learned with incomplete knowledge, the numerical model based on probability theory needs to be extended. This study presents a robust approach for diagnosis from Bayesian networks with fuzzy parameters. A simulation algorithm is designed to answer queries from the graphical models. The formulation of piecewise linear possibility distribution functions maintain the scalability in exact approaches.",""
"Evolutionary escape of pathogens from the selective pressure of immune responses and from medical interventions is driven by the accumulation of mutations. We introduce a statistical model for jointly estimating the dynamics and dependencies among genetic alterations and the associated phenotypic changes. The model integrates conjunctive Bayesian networks, which define a partial order on the occurrences of genetic events, with isotonic regression. The resulting genotype-phenotype map is non-decreasing in the lattice of genotypes. It describes evolutionary escape as a directed process following a phenotypic gradient, such as a monotonic fitness landscape. We present efficient algorithms for parameter estimation and model selection. The model is validated using simulated data and applied to HIV drug resistance data. We find that the effect of many resistance mutations is non-linear and depends on the genetic background in which they occur.","The model integrates conjunctive Bayesian networks, which define a partial order on the occurrences of genetic events, with isotonic regression."
"Diagnosis of misconceptions or \"bugs\" in procedural skills is difficult because of their unstable nature. This study addresses this problem by proposing and evaluating a probability-based approach to the diagnosis of bugs in children's multicolumn subtraction performance using Bayesian networks. This approach assumes a causal network relating hypothesized subtraction bugs to the observed test items. Two research questions are tested within this framework. First, it is investigated whether more reliable assessment of latent subtraction bugs can be achieved by hypothesizing and using subskill nodes in the Bayesian network as causal factors affecting bugs. Second, network performance is evaluated using two types of testing situations, one using binary data (items scored as correct or incorrect) and the other simulating a multiple-choice test format with diagnostic use of specific wrong answers. The resulting four types of Bayesian networks are evaluated for their effectiveness in bug diagnosis. All four networks show good performance, with even the simplest network (bug nodes only, binary data) giving overall bug diagnosis rates of at least 85%. Prediction is best with the most complex network (bug and subskill nodes, diagnostic use of specific wrong answers), for which the correct diagnosis rate reaches 99%. These results suggest that stable and reliable bug diagnosis can be achieved using a Bayesian network framework, but that the stability and effectiveness of diagnosis is increased when the network includes latent subskills in addition to bugs as causal factors, and when specific wrong answers are used for diagnostic purposes.",""
"The development of learner models takes an active part in upcoming adaptive learning environments. The purpose of learner models is to drive personalization based on learner and learning characteristics that are considered as important for the learning process, such as cognitive, affective and behavioral variables. Despite the huge amount of theoretical propositions of learner characteristics considered as relevant for learner models, practical payoffs are rather sparse. This study aims to overview the empirical research on the mere value of learner models in the development of adaptive learning environments. The results show that a lot of high-quality studies are situated in a rather shattered research field, building few bridges from theory to practice. We conclude with the call for a theory or framework integrating current and past research results that is able to guide theory-based and systematic empirical research having concrete hypotheses on the merits of learner characteristics in adaptive learning environments. (C) 2010 Elsevier Ltd. All rights reserved.",""
"Gait recognition algorithms often perform poorly because of low resolution video sequences, subjective human motion and challenging outdoor scenarios. Despite these challenges, gait recognition research is gaining momentum due to increasing demand and more possibilities for deployment by the surveillance industry. Therefore every research contribution which significantly improves this new biometric is a milestone. We propose a probabilistic sub-gait interpretation model to recognize gaits. A sub-gait is defined by us as part of the silhouette of a moving body. Binary silhouettes of gait video sequences form the basic input of our approach. A novel modular training scheme has been introduced in this research to efficiently learn subtle sub-gait characteristics from the gait domain. For a given gait sequence, we get useful information from the sub-gaits by identifying and exploiting intrinsic relationships using Bayesian networks. Finally, by incorporating efficient inference strategies, robust decisions are made for recognizing gaits. Our results show that the proposed model tackles well the uncertainties imposed by typical covariate factors and shows significant recognition performance.","Finally, by incorporating efficient inference strategies, robust decisions are made for recognizing gaits."
"Several different factors contribute to injury severity in traffic accidents, such as driver characteristics, highway characteristics, vehicle characteristics, accidents characteristics, and atmospheric factors. This paper shows the possibility of using Bayesian Networks (BNs) to classify traffic accidents according to their injury severity. BNs are capable of making predictions without the need for pre assumptions and are used to make graphic representations of complex systems with interrelated components. This paper presents an analysis of 1536 accidents on rural highways in Spain, where 18 variables representing the aforementioned contributing factors were used to build 3 different BNs that classified the severity of accidents into slightly injured and killed or severely injured. The variables that best identify the factors that are associated with a killed or seriously injured accident (accident type, driver age, lighting and number of injuries) were identified by inference. (C) 2010 Elsevier Ltd. All rights reserved.","This paper presents an analysis of 1536 accidents on rural highways in Spain, where 18 variables representing the aforementioned contributing factors were used to build 3 different BNs that classified the severity of accidents into slightly injured and killed or severely injured."
"Systems Biology has taken advantage of computational tools and high-throughput experimental data to model several biological processes. These include signaling, gene regulatory, and metabolic networks. However, most of these models are specific to each kind of network. Their interconnection demands a whole-cell modeling framework for a complete understanding of cellular systems. We describe the features required by an integrated framework for modeling, analyzing and simulating biological processes, and review several modeling formalisms that have been used in Systems Biology including Boolean networks, Bayesian networks, Petri nets, process algebras, constraint-based models, differential equations, rule-based models, interacting state machines, cellular automata, and agent-based models. We compare the features provided by different formalisms, and discuss recent approaches in the integration of these formalisms, as well as possible directions for the future.",""
"Tumour cells employ a variety of mechanisms to invade their environment and to form metastases. An important property is the ability of tumour cells to transition between individual cell invasive mode and collective mode. The switch from collective to individual cell invasion in the breast was shown recently to determine site of subsequent metastasis. Previous studies have suggested a range of invasion modes from single cells to large clusters. Here, we use a novel image analysis method to quantify and categorise invasion. We have developed a process using automated imaging for data collection, unsupervised morphological examination of breast cancer invasion using cognition network technology (CNT) to determine how many patterns of invasion can be reliably discriminated. We used Bayesian network analysis to probabilistically connect morphological variables and therefore determine that two categories of invasion are clearly distinct from one another. The Bayesian network separated individual and collective invading cell groups based on the morphological measurements, with the level of cell-cell contact the most discriminating morphological feature. Smaller invading groups were typified by smoother cellular surfaces than those invading collectively in larger groups. Interestingly, elongation was evident in all invading cell groups and was not a specific feature of single cell invasion as a surrogate of epithelial-mesenchymal transition. In conclusion, the combination of cognition network technology and Bayesian network analysis provides an insight into morphological variables associated with transition of cancer cells between invasion modes. We show that only two morphologically distinct modes of invasion exist.",""
"Model-based fault diagnosis using artificial intelligence techniques often deals with uncertain knowledge and incomplete information. Probability reasoning is a method to deal with uncertain or incomplete information, and Bayesian network is a tool that brings it into the real world application. A novel approach for constructing the Bayesian network structure on the basis of a bond graph model is proposed. Specification of prior and conditional probability distributions (CPDs) for the Bayesian network can be completed by expert knowledge and learning from historical data. The resulting Bayesian network is then applied for diagnosing faulty components from physical systems. The performance of the proposed fault diagnosis scheme based on bond graph derived Bayesian network is demonstrated through simulation studies. (C) 2010 Elsevier B.V. All rights reserved.",""
"Feedback mechanisms are important in the analysis of vulnerability and resilience of social-ecological systems, as well as in the analysis of livelihoods, but how to evaluate systems with direct feedbacks has been a great challenge. We applied fuzzy cognitive mapping, a tool that allows analysis of both direct and indirect feedbacks and can be used to explore the vulnerabilities of livelihoods to identified hazards. We studied characteristics and drivers of rural livelihoods in the Great Limpopo Transfrontier Conservation Area in southern Africa to assess the vulnerability of inhabitants to the different hazards they face. The process involved four steps: (1) surveys and interviews to identify the major livelihood types; (2) description of specific livelihood types in a system format using fuzzy cognitive maps (FCMs), a semi-quantitative tool that models systems based on people's knowledge; (3) linking variables and drivers in FCMs by attaching weights; and (4) defining and applying scenarios to visualize the effects of drought and changing park boundaries on cash and household food security. FCMs successfully gave information concerning the nature (increase or decrease) and magnitude by which a livelihood system changed under different scenarios. However, they did not explain the recovery path in relation to time and pattern (e. g., how long it takes for cattle to return to desired numbers after a drought). Using FCMs revealed that issues of policy, such as changing situations at borders, can strongly aggravate effects of climate change such as drought. FCMs revealed hidden knowledge and gave insights that improved the understanding of the complexity of livelihood systems in a way that is better appreciated by stakeholders.",""
"In this paper we investigate the geometry of a discrete Bayesian network whose graph is a tree all of whose variables are binary and the only observed variables are those labeling its leaves. We provide the full geometric description of these models which is given by a set of polynomial equations together with a set of complementary implies inequalities induces by the positivity of probabilities on hidden variables. The phylogenetic invariants given by the equations can be useful in the construction of simple diagnostic tests. However, in this paper we point out the importance of also incorporating the associated inequalities into any statistical analysis. The full characterization of these inequality constraints derived in this paper helps us determine how and why routine statistical methods can break down for this model class.",""
"An increased emphasis on integrated water management at a catchment scale has led to the development of numerous modelling tools. To support efficient decision making and to better target investment in management actions, such modelling tools need to link socioeconomic information with biophysical data. However, there is still limited experience in developing catchment models that consider environmental changes and economic values in a single framework. We describe a model development process where biophysical modelling is integrated with economic information on the non-market environmental costs and benefits of catchment management changes for a study of the George catchment in northeast Tasmania, Australia. An integrated assessment approach and Bayesian network modelling techniques were used to integrate knowledge about hydrological, ecological and economic systems. Rather than coupling existing information and models, synchronous data collection and model development ensured tailored information exchange between the different components. The approach is largely transferable to the development of integrated hydro-economic models in other river catchments. Our experiences highlight the challenges in synchronizing economic and scientific modelling. These include the selection of common attributes and definition of their levels suitable for the catchment modelling and economic valuation. The lessons from the model development process are useful for future studies that aim to integrate scientific and economic knowledge. (C) 2010 Elsevier Ltd. All rights reserved.",""
"Gene regulatory networks (GRNs) are complex networks consisted of nodes representing genes, transcription factors, microRNAs and edges that represent the interactions between nodes. GRNs can expose and depict underlying cells' gene regulatory mechanisms. In this paper, we propose a new model for inference of GRNs. This model includes prior knowledge into network inference. Our model was applied on generated gene expression time series with different number of genes and time points and one subset of experimental data received from colorectal cancer microarray experiment. To validate the inference capabilities of the proposed model, we compare the ROC curves and AUC values for proposed model and the common used models: dynamic Bayesian networks, Boolean networks and graphical Gaussian models. The proposed model has shown competitive inference capabilities in comparison with other models.","In this paper, we propose a new model for inference of GRNs."
"The analysis of gene regulatory networks provides enormous information on various fundamental cellular processes involving growth, development, hormone secretion, and cellular communication. Their extraction from available gene expression profiles is a challenging problem. Such reverse engineering of genetic networks offers insight into cellular activity toward prediction of adverse effects of new drugs or possible identification of new drug targets. Tasks such as classification, clustering, and feature selection enable efficient mining of knowledge about gene interactions in the form of networks. It is known that biological data is prone to different kinds of noise and ambiguity. Soft computing tools, such as fuzzy sets, evolutionary strategies, and neurocomputing, have been found to be helpful in providing low-cost, acceptable solutions in the presence of various types of uncertainties. In this paper, we survey the role of these soft methodologies and their hybridizations, for the purpose of generating genetic networks.","Tasks such as classification, clustering, and feature selection enable efficient mining of knowledge about gene interactions in the form of networks."
"Constraint-based structure learning algorithms generally perform well on sparse graphs. Although sparsity is not uncommon, there are some domains where the underlying graph can have some dense regions; one of these domains is gene regulatory networks, which is the main motivation to undertake the study described in this paper. We propose a new constraint-based algorithm that can both increase the quality of output and decrease the computational requirements for learning the structure of gene regulatory networks. The algorithm is based on and extends the PC algorithm. Two different types of information are derived from the prior knowledge; one is the probability of existence of edges, and the other is the nodes that seem to be dependent on a large number of nodes compared to other nodes in the graph. Also a new method based on Gene Ontology for gene regulatory network validation is proposed. We demonstrate the applicability and effectiveness of the proposed algorithms on both synthetic and real data sets.",""
"Recent experimental advances facilitate the collection of time series data that indicate which genes in a cell are expressed. This information can be used to understand the genetic regulatory network that generates the data. Typically, Bayesian analysis approaches are applied which neglect the time series nature of the experimental data, have difficulty in determining the direction of causality, and do not perform well on networks with tight feedback. To address these problems, this paper presents a method to learn genetic network connectivity which exploits the time series nature of experimental data to achieve better causal predictions. This method first breaks up the data into bins. Next, it determines an initial set of potential influence vectors for each gene based upon the probability of the gene's expression increasing in the next time step. These vectors are then combined to form new vectors with better scores. Finally, these influence vectors are competed against each other to determine the final influence vector for each gene. The result is a directed graph representation of the genetic network's repression and activation connections. Results are reported for several synthetic networks with tight feedback showing significant improvements in recall and runtime over Yu's dynamic Bayesian approach. Promising preliminary results are also reported for an analysis of experimental data for genes involved in the yeast cell cycle.",""
"Today, anomaly detection is a highly valuable application in the analysis of current huge datasets. Insurance companies, banks and many manufacturing industries need systems to help humans to detect anomalies in their daily information. In general, anomalies are a very small fraction of the data, therefore their detection is not an easy task. Usually real sources of an anomaly are given by specific values expressed on selective dimensions of datasets, furthermore, many anomalies are not really interesting for humans, due to the fact that interestingness of anomalies is categorized subjectively by the human user. In this paper we propose a new semi-supervised algorithm that actively learns to detect relevant anomalies by interacting with an expert user in order to obtain semantic information about user preferences. Our approach is based on 3 main steps. First, a Bayes network identifies an initial set of candidate anomalies. Afterwards, a subspace clustering technique identifies relevant subsets of dimensions. Finally, a probabilistic active learning scheme, based on properties of Dirichlet distribution, uses the feedback from an expert user to efficiently search for relevant anomalies. Our results, using synthetic and real datasets, indicate that, under noisy data and anomalies presenting regular patterns, our approach correctly identifies relevant anomalies.",""
"This work proposes and discusses an approach for inducing Bayesian classifiers aimed at balancing the tradeoff between the precise probability estimates produced by time consuming unrestricted Bayesian networks and the computational efficiency of Naive Bayes (NB) classifiers. The proposed approach is based on the fundamental principles of the Heuristic Search Bayesian network learning. The Markov Blanket concept, as well as a proposed \"approximate Markov Blanket\" are used to reduce the number of nodes that form the Bayesian network to be induced from data. Consequently, the usually high computational cost of the heuristic search learning algorithms can be lessened, while Bayesian network structures better than NB can be achieved. The resulting algorithms, called DMBC (Dynamic Markov Blanket Classifier) and A-DMBC (Approximate DMBC), are empirically assessed in twelve domains that illustrate scenarios of particular interest. The obtained results are compared with NB and Tree Augmented Network (TAN) classifiers, and confinn that both proposed algorithms can provide good classification accuracies and better probability estimates than NB and TAN, while being more computationally efficient than the widely used K2 Algorithm.","This work proposes and discusses an approach for inducing Bayesian classifiers aimed at balancing the tradeoff between the precise probability estimates produced by time consuming unrestricted Bayesian networks and the computational efficiency of Naive Bayes (NB) classifiers."
"The present paper introduces a new kind of representation for the potentials in a Bayesian network: binary probability trees. They enable the representation of context-specific independences in more detail than probability trees. This enhanced capability leads to more efficient inference algorithms for some types of Bayesian networks. This paper explains the procedure for building a binary probability tree from a given potential, which is similar to the one employed for building standard probability trees. It also offers a way of pruning a binary tree in order to reduce its size. This allows us to obtain exact or approximate results in inference depending on an input threshold. This paper also provides detailed algorithms for performing the basic operations on potentials (restriction, combination and marginalization) directly to binary trees. Finally, some experiments are described where binary trees are used with the variable elimination algorithm to compare the performance with that obtained for standard probability trees. (C) 2010 Elsevier Inc. All rights reserved.","This enhanced capability leads to more efficient inference algorithms for some types of Bayesian networks."
"Categorical spatial data, such as land use classes and socioeconomic statistics data, are important data sources in geographical information science (GIS). The investigation of spatial patterns implied in these data can benefit many aspects of GIS research, such as classification of spatial data, spatial data mining, and spatial uncertainty modeling. However, the discrete nature of categorical data limits the application of traditional kriging methods widely used in Gaussian random fields. In this article, we present a new probabilistic method for modeling the posterior probability of class occurrence at any target location in space-given known class labels at source data locations within a neighborhood around that prediction location. In the proposed method, transition probabilities rather than indicator covariances or variograms are used as measures of spatial structure and the conditional or posterior (multi-point) probability is approximated by a weighted combination of preposterior (two-point) transition probabilities, while accounting for spatial interdependencies often ignored by existing approaches. In addition, the connections of the proposed method with probabilistic graphical models (Bayesian networks) and weights of evidence method are also discussed. The advantages of this new proposed approach are analyzed and highlighted through a case study involving the generation of spatial patterns via sequential indicator simulation.","The investigation of spatial patterns implied in these data can benefit many aspects of GIS research, such as classification of spatial data, spatial data mining, and spatial uncertainty modeling."
"This paper suggests a probabilistic inference method of quantifying a buyer's likelihood to purchase a highly customised product. The probabilistic inference method utilises the principles of Bayesian networks. This method is integrated into an Internet based supply chain control logic where supply chain partners provide real time or near real time information, regarding the availability of parts needed for the production of highly customisable products. The supply chain plan that is generated is robust, ensuring the supply of the right part at the right time at a rather reasonable cost, thus eliminating the quality defects of the product. The concept is demonstrated in a typical supply chain case, taken from the automotive industry.","This paper suggests a probabilistic inference method of quantifying a buyer's likelihood to purchase a highly customised product."
"Wildfire can result in significant economic costs with inquiries following such events often recommending an increase in management effort to reduce the risk of future losses. Currently, there are no objective frameworks in which to assess the relative merits of management actions or the synergistic way in which the various combinations may act. We examine the value of Bayes Nets as a method for assessing the risk reduction from fire management practices using a case study from a forested landscape. Specifically, we consider the relative reduction in wildfire risk from investing in prescribed burning, initial or rapid attack and suppression. The Bayes Net was developed using existing datasets, a process model and expert opinion. We compared the results of the models with the recorded fire data for an 11-year period from 1997 to 2000 with the model successfully duplicating these data. Initial attack and suppression effort had the greatest effect on the distribution of the fire sizes for a season. Bayes Nets provide a holistic model for considering the effect of multiple fire management methods on the risk of wildfires. The methods could be further advanced by including the costs of management and conducting a formal decision analysis.",""
"The new constraint-based algorithm for learning dependency structures from data is developed. The novelty of the proposed algorithm is conditioned by the rules of acceleration of inductive inference, which drastically reduce the search area of separators while derivation of the model skeleton. On examples of the Bayesian networks of moderate saturation we have demonstrated that proposed algorithm learns Bayesian nets (of moderate density) multiple times faster than well-known PC algorithm.","The novelty of the proposed algorithm is conditioned by the rules of acceleration of inductive inference, which drastically reduce the search area of separators while derivation of the model skeleton."
"A need exists for a breast cancer risk identification paradigm that utilizes relevant demographic, clinical, and other readily obtainable patient-specific data in order to provide individualized cancer risk assessment, direct screening efforts, and detect breast cancer at an early disease stage in historically underserved populations, such as younger women (under age 40) and minority populations, who represent a disproportionate number of military beneficiaries. Recognizing this unique need for military beneficiaries, a consensus panel was convened by the USA TATRC to review available evidence for individualized breast cancer risk assessment and screening in young (< 40), ethnically diverse women with an overall goal of improving care for military beneficiaries. In the process of review and discussion, it was determined to publish our findings as the panel believes that our recommendations have the potential to reduce health disparities in risk assessment, health promotion, disease prevention, and early cancer detection within and in other underserved populations outside of the military. This paper aims to provide clinicians with an overview of the clinical factors, evidence and recommendations that are being used to advance risk assessment and screening for breast cancer in the military.",""
"The implementation of an ecosystem approach to marine spatial management requires practical tools to support risk-based decision-making. We combined a Bayesian Belief Network with a Geographical Information System (GIS) for the spatially explicit quantification of the ecological and economic risks of spatial management options. As an example we assessed the German exclusive economic zone (EEZ) in the North Sea to determine the potential effects of 2 scenarios on the vulnerability of plaice Pleuronectes platessa to fishing, fishing fleets and their revenues. In the first scenario we simulated a shift in plaice distribution due to changes in bottom temperatures to assess spatial management options. Then we imitated an expansion of offshore wind energy development with an associated reallocation of international fishing effort to assess the ecological and economic consequences. We predicted that an increase of 0.5 degrees C in the average bottom temperature would require a significant reduction in fishing effort to maintain the current relative level of the vulnerability of plaice to fishing. The likely consequences of the second scenario were a homogenous increase in plaice catches around the areas closed for fishing, together with a decrease in the vulnerability of plaice to fishing within 17% of the study area. Our results showed the great potential of this framework to integrate the spatially explicit assessment of the economic and ecological risks of spatial management options. We conclude that this modelling framework can support the implementation of an ecosystem approach to marine spatial management, as it enables the derivation of probabilistic estimates which can be used directly in risk-based decision-making.",""
"Background: Mouth breathing is a chronic syndrome that may bring about postural changes. Finding characteristic patterns of changes occurring in the complex musculoskeletal system of mouth-breathing children has been a challenge. Learning vector quantization (LVQ) is an artificial neural network model that can be applied for this purpose. Objectives: The aim of the present study was to apply LVQ to determine the characteristic postural profiles shown by mouth-breathing children, in order to further understand abnormal posture among mouth breathers. Methods: Postural training data on 52 children (30 mouth breathers and 22 nose breathers) and postural validation data on 32 children (22 mouth breathers and 10 nose breathers) were used. The performance of LVQ and other classification models was compared in relation to self-organizing maps, back-propagation applied to multilayer perceptrons, Bayesian networks, naive Bayes, 148 decision trees, k*, and k-nearest-neighbor classifiers. Classifier accuracy was assessed by means of leave-one-out cross-validation, area under ROC curve (AUC), and inter-rater agreement (Kappa statistics). Results: By using the LVQ model, five postural profiles for mouth-breathing children could be determined. LVQ showed satisfactory results for mouth-breathing and nose-breathing classification: sensitivity and specificity rates of 0.90 and 0.95, respectively, when using the training dataset, and 0.95 and 0.90, respectively, when using the validation dataset. Conclusions: The five postural profiles for mouth-breathing children suggested by LVQ were incorporated into application software for classifying the severity of mouth breathers' abnormal posture.","The performance of LVQ and other classification models was compared in relation to self-organizing maps, back-propagation applied to multilayer perceptrons, Bayesian networks, naive Bayes, 148 decision trees, k*, and k-nearest-neighbor classifiers."
"This paper aims to address the problem of anomaly detection and discrimination in complex behaviours, where anomalies are subtle and difficult to detect owing to the complex temporal dynamics and correlations among multiple objects' behaviours. Specifically, we decompose a complex behaviour pattern according to its temporal characteristics or spatial-temporal visual contexts. The decomposed behaviour is then modelled using a cascade of Dynamic Bayesian Networks (CasDBNs). In contrast to existing standalone models, the proposed behaviour decomposition and cascade modelling offers distinct advantage in simplicity for complex behaviour modelling. Importantly, the decomposition and cascade structure map naturally to the structure of complex behaviour, allowing for a more effective detection of subtle anomalies in surveillance videos. Comparative experiments using both indoor and outdoor data are carried out to demonstrate that, in addition to the novel capability of discriminating different types of anomalies, the proposed framework outperforms existing methods in detecting durational anomalies in complex behaviours and subtle anomalies that are difficult to detect when objects are viewed in isolation. (C) 2010 Elsevier Ltd. All rights reserved.",""
"The very early appearance of abstract knowledge is often taken as evidence for innateness. We explore the relative learning speeds of abstract and specific knowledge within a Bayesian framework and the role for innate structure. We focus on knowledge about causality, seen as a domain-general intuitive theory, and ask whether this knowledge can be learned from co-occurrence of events. We begin by phrasing the causal Bayes nets theory of causality and a range of alternatives in a logical language for relational theories. This allows us to explore simultaneous inductive learning of an abstract theory of causality and a causal model for each of several causal systems. We find that the correct theory of causality can be learned relatively quickly, often becoming available before specific causal theories have been learned-an effect we term the blessing of abstraction. We then explore the effect of providing a variety of auxiliary evidence and find that a collection of simple perceptual input analyzers can help to bootstrap abstract knowledge. Together, these results suggest that the most efficient route to causal knowledge may be to build in not an abstract notion of causality but a powerful inductive learning mechanism and a variety of perceptual supports. While these results are purely computational, they have implications for cognitive development, which we explore in the conclusion.",""
"A novel Bayesian design support tool is empirically investigated for its potential to support the early design stages. The design support tool provides dynamic guidance with the use of morphological design matrices during the conceptual or preliminary design stages. This paper tests the appropriateness of adopting a stochastic approach for supporting the early design phase. The rationale for the stochastic approach is based on the uncertain nature of the design during this part of the design process. The support tool is based on Bayesian belief networks (BBNs) and uses a simple but effective information content-based metric to learn or induce the model structure. The dynamically interactive tool is assessed with two empirical trials. First, the laboratory-based trial with novice designers illustrates a novel emergent design search methodology. Second, the industrial-based trial with expert designers illustrates the hurdles that are faced when deploying a design support tool in a highly pressurised industrial environment. The conclusion from these trials is that there is a need for designers to better understand the stochastic methodology for them to both be able to interpret and trust the BBN model of the design domain. Further, there is a need for a lightweight domain-specific front end interface is needed to enable a better fit between the generic support tool and the domain-specific design process and associated tools.",""
"The authors previous work discussed a scalable abstract knowledge representation and reasoning scheme for Pervasive Computing Systems where both low-level and abstract knowledge is maintained in the form of temporal first-order logic (TFOL) predicates Furthermore we introduced a novel concept of a generalised event an abstract event which we define as a change in the truth value of an abstract TFOL predicate Abstract events represent realtime knowledge about the system and they are defined with the help of well-formed TFOL expressions whose leaf nodes are concrete low-level events using our AESL language In this paper we propose to simulate pervasive systems by providing estimated knowledge about its entities and situations that involve them To achieve this goal we enhance AESL with higher-order function predicates that denote approximate knowledge about the likelihood of a predicate instance having the value True with respect to a time reference We define a mapping function between a TFOL predicate and a Bayesian network that calculates likelihood estimates for that predicate as well as a confidence level i e a metric of how reliable the likelihood estimation is for that predicate Higher-order likelihood predicates are implemented by a novel middleware component the Likelihood Estimation Service (LES) LES implements the above mapping first for each abstract predicate it learns a Bayesian network that corresponds to that predicate from the knowledge stored in the sensor-driven system Once trained and validated the Bayesian networks generate a likelihood estimate and a confidence level This new knowledge is maintained in the middleware as approximate knowledge therefore providing a simulation of the pervasive system in the absence of real-time data Last but not least we describe an experimental evaluation of our system using the Active BAT location system (C) 2010 Elsevier B V All rights reserved",""
"The Recursive Bayesian Net (RBN) formalism was originally developed for modelling nested causal relationships. In this paper we argue that the formalism can also be applied to modelling the hierarchical structure of mechanisms. The resulting network contains quantitative information about probabilities, as well as qualitative information about mechanistic structure and causal relations. Since information about probabilities, mechanisms and causal relations is vital for prediction, explanation and control respectively, an RBN can be applied to all these tasks. We show in particular how a simple two-level RBN can be used to model a mechanism in cancer science. The higher level of our model contains variables at the clinical level, while the lower level maps the structure of the cell's mechanism for apoptosis.",""
"An inter-basin water transfer project is one of the effective ways to resolve the problem of an uneven distribution of water resources. Temporal and spatial variations in rainfall in different basins greatly affect water supply and demand in inter-basin water transfer projects, leading to risks to the operation of the water transfer projects. This paper applies a Bayesian network model to analyze this risk and studies the rich-poor rainfall encounter risk between a water source area and water receiving areas in the middle route of the South-to-North Water Transfer Project in China. Real time scenario simulations with the input of new observations were also studied. The results show that the rich-poor rainfall encounter risk is high for the Tangbai River receiving area in the fourth quarter, for the Huai River and South of Hai River receiving area in the second quarter, and for the North of the Hai River receiving area in the fourth and first quarters. The scenario simulations reflect risk change in the operation of water transfer projects, providing scientific decision support for the management of the water resource distribution in the inter-basin water transfer projects.",""
"Probabilistic models based on Bayes' rule are an increasingly popular approach to understanding human cognition. Bayesian models allow immense representational latitude and complexity. Because they use normative Bayesian mathematics to process those representations, they define optimal performance on a given task. This article focuses on key mechanisms of Bayesian information processing, and provides numerous examples illustrating Bayesian approaches to the study of human cognition. We start by providing an overview of Bayesian modeling and Bayesian networks. We then describe three types of information processing operations-inference, parameter learning, and structure learning-in both Bayesian networks and human cognition. This is followed by a discussion of the important roles of prior knowledge and of active learning. We conclude by outlining some challenges for Bayesian models of human cognition that will need to be addressed by future research. (C) 2010 John Wiley & Sons, Ltd. WIREs Cogn Sci 2011 2 8-21 DOI: 10.1002/wcs.80","We then describe three types of information processing operations-inference, parameter learning, and structure learning-in both Bayesian networks and human cognition."
"Influencing the management of private landholders is a key element of improving the condition of Australia's natural resources. Despite substantial investment by governments, effecting behavioural change on a scale likely to stem biodiversity losses has proven difficult. Understanding landholder decision-making is now acknowledged as fundamental to achieving better policy outcomes. There is a large body of research examining landholder adoption of conservation practices. Social researchers are able to employ a suite of conventional techniques to analyse their survey data and assist in identifying significant and causal relationships between variables. However, these techniques can be limited by the type of data available, the breadth of issues that can be represented and the extent that causality can be explored. In this paper we discuss the findings of a unique study exploring the benefits of combining Bayesian Networks (BNs) with conventional statistical analysis to examine landholder adoption. Our research examined the landholder fencing of native bushland in the Wimmera region in south east Australia. Findings from this study suggest that BNs provided enhanced understanding of the presence and strength of causal relationships. There was also the additional benefit that a BN could be quickly developed and that this process helped the research team clarify and understand relationships between variables. However, a key finding was that the interpretation of the results of the BNs was complemented by the conventional data analysis and expert review. An additional benefit of the BNs is their capacity to present key findings in a format that is more easily interpreted by researchers and enables researchers to more easily communicate their findings to natural resource practitioners and policy makers. (C) 2010 Elsevier Ltd. All rights reserved.",""
NA,""
